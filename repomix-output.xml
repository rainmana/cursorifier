This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: **/*.py
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
.github/
  helpers/
    gh_changelog_generator/
      gh_changelog_generator.py
label_studio/
  core/
    feature_flags/
      __init__.py
      base.py
      stale_feature_flags.py
      utils.py
    management/
      commands/
        locked_migrate.py
        show_async_migrations.py
    migrations/
      0001_initial.py
    settings/
      __init__.py
      base.py
      label_studio.py
    templatetags/
      filters.py
    utils/
      __init__.py
      common.py
      contextlog.py
      db.py
      exceptions.py
      formatter.py
      io.py
      mail.py
      manifest_assets.py
      openapi_extensions.py
      params.py
      secret_key.py
      sentry.py
      static_serve.py
    __init__.py
    api_permissions.py
    argparser.py
    bulk_update_utils.py
    context_processors.py
    current_request.py
    decorators.py
    filters.py
    label_config.py
    middleware.py
    mixins.py
    models.py
    old_ls_migration.py
    permissions.py
    redis.py
    storage.py
    urls.py
    validators.py
    version.py
    views.py
    wsgi.py
  data_export/
    migrations/
      __init__.py
      0001_initial.py
      0002_auto_20210921_0954.py
      0003_auto_20211004_1416.py
      0004_auto_20211019_0852.py
      0005_auto_20211025_1137.py
      0006_convertedformat.py
      0007_auto_20230327_1910.py
      0008_convertedformat_traceback.py
      0009_alter_convertedformat_traceback.py
      0010_alter_convertedformat_export_type.py
    __init__.py
    api.py
    apps.py
    mixins.py
    models.py
    serializers.py
    urls.py
  data_import/
    migrations/
      __init__.py
      0001_initial.py
      0002_alter_fileupload_file.py
    __init__.py
    api.py
    functions.py
    models.py
    serializers.py
    uploader.py
    urls.py
  data_manager/
    actions/
      __init__.py
      basic.py
      cache_labels.py
      experimental.py
      next_task.py
      predictions_to_annotations.py
      remove_duplicates.py
    migrations/
      __init__.py
      0001_squashed_0005_view_user.py
      0002_remove_annotations_ids.py
      0003_remove_predictions_model_versions.py
      0004_remove_avg_lead_time.py
      0005_remove_updated_by.py
      0006_remove_inner_id.py
      0007_auto_20220708_0832.py
      0008_manual_counters_update.py
      0009_alter_view_user.py
      0010_auto_20230718_1423.py
      0011_auto_20240718_1355.py
      0012_alter_view_user.py
    __init__.py
    api.py
    apps.py
    functions.py
    managers.py
    models.py
    prepare_params.py
    serializers.py
    urls.py
    views.py
  io_storages/
    azure_blob/
      __init__.py
      api.py
      models.py
      openapi_schema.py
      serializers.py
      utils.py
    gcs/
      __init__.py
      api.py
      models.py
      openapi_schema.py
      serializers.py
      utils.py
    localfiles/
      api.py
      models.py
      openapi_schema.py
      serializers.py
    migrations/
      __init__.py
      0001_squashed_0002_auto_20210302_1827.py
      0002_auto_20210311_0530.py
      0003_localfilesimportstorage.py
      0004_gcsstoragemixin_google_application_credentials.py
      0005_s3importstorage_recursive_scan.py
      0006_auto_20210906_1323.py
      0007_auto_20210928_1252.py
      0008_auto_20211129_1132.py
      0009_auto_20220310_0922.py
      0010_auto_20221014_1708.py
      0011_gcsstoragemixin_google_project_id.py
      0012_auto_20230418_1510.py
      0013_auto_20230420_0259.py
      0014_init_statuses.py
      0015_auto_20230804_1732.py
      0016_add_aws_sse_kms_key.py
      0017_auto_20240731_1638.py
      0018_alter_azureblobexportstorage_project_and_more.py
    redis/
      __init__.py
      api.py
      models.py
      openapi_schema.py
      serializers.py
    s3/
      __init__.py
      api.py
      models.py
      openapi_schema.py
      serializers.py
      utils.py
    __init__.py
    all_api.py
    api.py
    base_models.py
    filesystem.py
    functions.py
    models.py
    permissions.py
    serializers.py
    urls.py
    utils.py
  jwt_auth/
    migrations/
      0001_initial.py
    admin.py
    apps.py
    auth.py
    middleware.py
    models.py
    serializers.py
    urls.py
    views.py
  labels_manager/
    migrations/
      0001_initial.py
      0002_auto_20220131_1325.py
      0003_auto_20221213_1612.py
    api.py
    apps.py
    exceptions.py
    functions.py
    models.py
    serializers.py
    urls.py
  ml/
    migrations/
      __init__.py
      0001_initial.py
      0002_auto_20210308_1559.py
      0003_auto_20210309_1239.py
      0004_auto_20210820_1610.py
      0005_auto_20211010_1344.py
      0006_mlbackend_auto_update.py
      0007_auto_20240314_1957.py
    __init__.py
    api_connector.py
    api.py
    mixins.py
    models.py
    serializers.py
    urls.py
  ml_model_providers/
    migrations/
      0001_initial.py
      0002_auto_20240722_2054.py
      0003_modelproviderconnection_cached_available_models.py
      0004_auto_20240830_1206.py
      0005_modelproviderconnection_budget_alert_threshold_and_more.py
      0006_modelproviderconnection_google_application_credentials_and_more.py
      0007_alter_modelproviderconnection_provider.py
      0008_alter_modelproviderconnection_provider.py
      0009_alter_modelproviderconnection_provider.py
    models.py
  ml_models/
    migrations/
      0001_initial.py
      0002_modelrun.py
      0003_auto_20240228_2228.py
      0004_modelrun_job_id.py
      0005_auto_20240319_1738.py
      0006_alter_modelrun_project_subset.py
      0007_auto_20240617_2200.py
      0008_modelrun_total_tasks.py
      0009_alter_thirdpartymodelversion_provider.py
      0010_modelinterface_skill_name.py
      0011_thirdpartymodelversion_model_provider_connection.py
      0012_alter_thirdpartymodelversion_provider.py
      0013_alter_thirdpartymodelversion_provider.py
      0014_alter_thirdpartymodelversion_provider.py
      0015_alter_thirdpartymodelversion_provider.py
      0016_alter_thirdpartymodelversion_provider.py
    models.py
  organizations/
    management/
      commands/
        destroy_organization.py
    migrations/
      __init__.py
      0001_initial.py
      0001_squashed_0008_auto_20201005_1552.py
      0002_auto_20210310_2044.py
      0003_auto_20211010_1339.py
      0004_organization_contact_info.py
      0005_organizationmember_deleted_at.py
      0006_alter_organizationmember_deleted_at.py
    tests/
      factories.py
    __init__.py
    admin.py
    api.py
    apps.py
    forms.py
    functions.py
    middleware.py
    mixins.py
    models.py
    serializers.py
    urls.py
    views.py
  projects/
    functions/
      __init__.py
      next_task.py
      stream_history.py
      utils.py
    migrations/
      __init__.py
      0001_squashed_0065_auto_20210223_2014.py
      0002_auto_20210304_1457.py
      0003_auto_20210305_1008.py
      0003_project_color.py
      0004_auto_20210306_0506.py
      0005_merge_20210308_1141.py
      0006_auto_20210308_1559.py
      0007_auto_20210309_1304.py
      0008_auto_20210314_1840.py
      0009_project_evaluate_predictions_automatically.py
      0010_auto_20210505_2037.py
      0011_auto_20210517_2101.py
      0012_auto_20210906_1323.py
      0013_project_reveal_preannotations_interactively.py
      0013_project_skip_queue.py
      0014_project_parsed_label_config.py
      0015_merge_20220117_0749.py
      0016_auto_20220211_2218.py
      0017_project_pinned_at.py
      0018_alter_project_control_weights.py
      0019_labelstreamhistory.py
      0019_project_project_pinned__a39ccb_idx.py
      0020_labelstreamhistory_unique_history.py
      0021_merge_20230215_1943.py
      0022_projectimport.py
      0022_projectsummary_created_labels_drafts.py
      0023_merge_20230512_1333.py
      0023_projectreimport.py
      0024_merge_0023_merge_20230512_1333_0023_projectreimport.py
      0025_project_label_config_hash.py
      0026_auto_20231103_0020.py
      0027_project_custom_task_lock_ttl.py
      0028_auto_20241107_1031.py
    templatetags/
      __init__.py
      custom_filters.py
    tests/
      factories.py
      test_api.py
      test_project_sample_task.py
    __init__.py
    api.py
    mixins.py
    models.py
    permissions.py
    serializers.py
    signals.py
    urls.py
    views.py
  tasks/
    management/
      commands/
        annotations_fill_updated_by.py
        calculate_stats_all_orgs.py
        calculate_stats.py
    migrations/
      __init__.py
      0001_squashed_0041_taskcompletionhistory_was_cancelled.py
      0002_auto_20210304_1423.py
      0002_auto_20210305_2035.py
      0003_merge_20210308_1141.py
      0004_auto_20210308_1559.py
      0005_auto_20210309_1239.py
      0006_remove_annotation_state.py
      0007_auto_20210618_1653.py
      0008_auto_20210903_1332.py
      0009_auto_20210913_0739.py
      0009_auto_20210914_0020.py
      0010_auto_20210914_0032.py
      0011_merge_20210914_1036.py
      0012_auto_20211010_1339.py
      0013_task_updated_by.py
      0014_task_inner_id.py
      0015_task_fill_inner_id.py
      0016_auto_20220414_1408.py
      0017_auto_20220330_1310.py
      0017_new_index_anno_result.py
      0018_manual_migrate_counters.py
      0019_merge_20220512_2038.py
      0020_auto_20220515_2332.py
      0020_auto_20220516_0545.py
      0021_auto_20220515_2358.py
      0022_merge_20220517_1128.py
      0023_auto_20220620_1007.py
      0024_manual_migrate_counters_again.py
      0025_auto_20220721_0110.py
      0026_auto_20220725_1705.py
      0027_auto_20220801_1728.py
      0028_auto_20220802_2220.py
      0029_annotation_project.py
      0030_auto_20221102_1118.py
      0031_alter_task_options.py
      0032_annotation_updated_by.py
      0033_annotation_updated_by_fill.py
      0034_annotation_unique_id.py
      0034_auto_20221221_1101.py
      0035_auto_20221221_1116.py
      0035_tasklock_unique_id.py
      0036_auto_20221223_1102.py
      0037_merge_0035_auto_20221221_1116_0036_auto_20221223_1102.py
      0038_auto_20230209_1412.py
      0039_annotation_draft_created_at.py
      0040_auto_20230628_1101.py
      0041_prediction_project.py
      0042_auto_20230810_2304.py
      0043_auto_20230825.py
      0044_auto_20230907_0155.py
      0045_auto_20231124_1238.py
      0046_auto_20240314_1957.py
      0046_prediction_model_run.py
      0047_merge_20240318_2210.py
      0048_failedprediction.py
      0049_auto_20240905_1602.py
      0050_alter_predictionmeta_failed_prediction_and_more.py
      0051_tasklock_created_at.py
      0052_auto_20241030_1757.py
      0053_annotation_bulk_created.py
    __init__.py
    api.py
    choices.py
    exceptions.py
    functions.py
    mixins.py
    models.py
    openapi_schema.py
    serializers.py
    urls.py
    validation.py
  tests/
    data_import/
      test_uploader.py
    data_manager/
      actions/
        test_cache_labels.py
        test_predictions_to_annotations.py
      test_api_actions.py
      test_api_tasks.py
      test_columns_api.py
      test_ordering_filters.py
      test_undefined.py
      test_views_api.py
    io_storages/
      s3/
        test_utils.py
    jwt_auth/
      test_auth.py
      test_middleware.py
      test_models.py
      test_views.py
      utils.py
    loadtests/
      locustfile_collabs.py
      locustfile_db_load.py
      locustfile.py
      one_imports_other_annotate.py
    ml/
      test_api.py
      test_predict.py
    sdk/
      legacy/
        test_annotations.py
        test_projects.py
        test_storages.py
        test_tasks.py
        test_users.py
        test_views.py
      common.py
      fixtures.py
      test_annotations.py
      test_export.py
      test_ml.py
      test_predictions.py
      test_projects.py
      test_storages.py
      test_tasks.py
      test_users.py
      test_views.py
      utils.py
    tasks/
      test_functions.py
    test_data/
      __init__.py
      gen_tasks_and_annotations.py
    test_suites/
      converter.py
    webhooks/
      test_webhooks.py
    __init__.py
    conftest.py
    test_annotations_result_count.py
    test_annotations.py
    test_api.py
    test_block_objects.py
    test_bulk_operations.py
    test_cli.py
    test_config_validation.py
    test_contextlog.py
    test_core.py
    test_endpoints.py
    test_exception.py
    test_export.py
    test_has_lock.py
    test_invites.py
    test_io_storages.py
    test_next_task.py
    test_organizations.py
    test_predictions.py
    test_presign_storage_data.py
    test_project_reset_summary.py
    test_project_validation.py
    test_project.py
    test_tasks_upload.py
    test_upload_svg.py
    utils.py
  users/
    migrations/
      __init__.py
      0001_squashed_0009_auto_20210219_1237.py
      0002_auto_20210308_1559.py
      0003_user_active_organization.py
      0004_auto_20210914_0109.py
      0005_auto_20211010_1339.py
      0006_user_allow_newsletters.py
      0007_user_is_deleted.py
      0008_alter_user_managers.py
      0009_auto_20231201_0001.py
      0010_userproducttour.py
    product_tours/
      api.py
      models.py
      serializers.py
    tests/
      factories.py
    __init__.py
    admin.py
    api.py
    apps.py
    forms.py
    functions.py
    mixins.py
    models.py
    serializers.py
    urls.py
    views.py
  webhooks/
    migrations/
      0001_initial.py
      0002_auto_20220319_0013.py
      0003_alter_webhookaction_action.py
      0004_auto_20221221_1101.py
    api.py
    apps.py
    models.py
    serializers_for_hooks.py
    serializers.py
    urls.py
    utils.py
  __init__.py
  constants.py
  manage.py
  server.py
  sitecustomize.py
scripts/
  split_import_json.py
  update_ml_tutorials.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".github/helpers/gh_changelog_generator/gh_changelog_generator.py">
import json
import os
import re
from urllib.parse import quote

import jira
import requests
from github import Github
from jira import JIRA

COMMIT_PATTERN = re.compile(r'^(\w*):\s*(.*?)?:\s*(.*?)\s*(\(#(\d+)\))?$')

RELEASE_VERSION = os.getenv("RELEASE_VERSION").strip('\"')
CURRENT_REF = os.getenv("CURRENT_REF").strip('\"')
PREVIOUS_REF = os.getenv("PREVIOUS_REF").strip('\"')

JIRA_SERVER = os.getenv("JIRA_SERVER", "https://heartex.atlassian.net").strip('\"')
JIRA_USERNAME = os.getenv("JIRA_USERNAME").strip('\"')
JIRA_TOKEN = os.getenv("JIRA_TOKEN").strip('\"')
JIRA_RN_FIELD = os.getenv("JIRA_RN_FIELD", "customfield_10064").strip('\"')
JIRA_PROJECTS = os.getenv("JIRA_PROJECTS", "PLT,LEAP,OPTIC,DIA").split(",")
JIRA_RELEASE_PREFIX = os.getenv("JIRA_RELEASE_PREFIX", None).strip('\"')

GH_REPO = os.getenv("GH_REPO", "").strip('\"')
GH_TOKEN = os.getenv("GH_TOKEN").strip('\"')  # https://github.com/settings/tokens/new

LAUNCHDARKLY_SDK_KEY = os.getenv("LAUNCHDARKLY_SDK_KEY", '').strip('\"')
LAUNCHDARKLY_ENVIRONMENT = os.getenv("LAUNCHDARKLY_ENVIRONMENT", '').strip('\"')

HELM_CHART_REPO = os.getenv("HELM_CHART_REPO", None)
HELM_CHART_PATH = os.getenv("HELM_CHART_PATH", None)

OUTPUT_FILE_MD = os.getenv("OUTPUT_FILE_MD", 'output.md')
OUTPUT_FILE_JSON = os.getenv("OUTPUT_FILE_JSON", 'output.json')

WORKFLOW_RUN_LINK = os.getenv("WORKFLOW_RUN_LINK", '')

COMMIT_LABEL_MAP = {
    'feat': 'New Features',
    'fix': 'Bug Fixes',
    'perf': 'Performance Improvements',
    'refactor': 'Code Refactoring',
    'docs': 'Documentation'
}

DEFAULT_LABEL = "Other"

LABEL_SORT = [
    "New Feature",
    "Improvement",
    "Customer Bug Fix",
    "Bug Fix",
    "Refactor",
    "Research",
    DEFAULT_LABEL,
]

github_client = Github(GH_TOKEN)
github_repo = github_client.get_repo(GH_REPO)
jira_client = JIRA(JIRA_SERVER, basic_auth=(JIRA_USERNAME, JIRA_TOKEN))

FEATURE_FLAGS = {}


def ff_is_on(ff: dict) -> bool:
    return ff.get('fallthrough').get('variation') == 0 if ff.get('on') else ff.get('offVariation') == 0


def ff_status(ff: dict) -> str:
    is_on = ff_is_on(ff)
    is_on_text = "On" if is_on else "Off"
    options = []
    if ff.get('rules'):
        options.append('rules')
    if ff.get('targets'):
        options.append('targets')
    if options:
        options_text = ' and '.join(options)
        return f'{is_on_text} with {options_text}'
    return is_on_text


def ff_link(ff: dict) -> str:
    key = ff.get('key')
    return f'https://app.launchdarkly.com/default/{LAUNCHDARKLY_ENVIRONMENT}/features/{key}/targeting'


class JiraIssue:
    pr = None

    def __init__(self, issue_number: str, pr: int = None):
        self.type = "Jira Issue"
        self.pr = pr
        issue = jira_client.issue(issue_number)
        self.key = str(issue)
        self.status = str(issue.fields.status)
        self.label = DEFAULT_LABEL
        self.summary = str(issue.raw['fields']['summary'])
        self.release_note = issue.get_field(JIRA_RN_FIELD)
        self.desc = self.release_note if self.release_note else self.summary
        self.link = f"{JIRA_SERVER}/browse/{self.key}"
        self.releases_tags = []
        self.ffs = self.get_ffs()

    def set_releases_tags(self, tags: list[str]):
        pass

    def __str__(self):
        return f"<{self.link}|[{self.key}]>: {self.desc} -- *{self.status}*"

    def __key(self):
        return self.key

    def __dict__(self):
        return {
            "desc": self.desc,
            "link": self.link,
            "key": self.key,
            "status": self.status,
            "pr": self.pr,
            "ffs": self.ffs,
        }

    def get_ffs(self):
        ff_key = self.key.lower().replace('-', '_')
        result = []
        for name, ff in FEATURE_FLAGS.items():
            if ff_key in name:
                key = ff.get('key')
                on = ff_is_on(ff)
                link = ff_link(ff)
                status = ff_status(ff)
                result.append(
                    {
                        'key': key,
                        'on': on,
                        'link': link,
                        'status': status,
                    }
                )
        return result


TASK_CACHE = {}


def get_task(task_number: str, pr: int = None) -> JiraIssue or None:
    if task_number in TASK_CACHE.keys():
        return TASK_CACHE.get(task_number)
    try:
        task = JiraIssue(task_number, pr)
        TASK_CACHE[task_number] = task
        return task
    except Exception as e:
        print(f'Could not find Issue {task_number} in Jira: {e}')
    return None


def get_jira_release(project: str, version: str) -> jira.client.Version or None:
    jira_project_versions = jira_client.project_versions(project=project)
    jira_sorted_project_versions = sorted(jira_project_versions, key=lambda x: x.name, reverse=True)
    return next((e for e in jira_sorted_project_versions if version in e.name), None)


def get_jira_release_issues(project_id: str, release_id: str) -> list[JiraIssue]:
    issues = jira_client.search_issues(
        f"project = {project_id} AND fixVersion = {release_id} ORDER BY priority DESC, key ASC")
    tasks = set()
    for issue in issues:
        if task := get_task(issue.key):
            tasks.add(task)
    return list(tasks)


def get_github_release(previous_ref: str, current_ref: str):
    return github_repo.compare(previous_ref, current_ref)


def get_github_release_tasks(commits) -> list[JiraIssue]:
    tasks = set()
    for commit in commits:
        message_first_line = commit.commit.message.split("\n")[0]
        if (match := re.match(COMMIT_PATTERN, message_first_line)) is not None:
            label = match.group(1)
            if label in COMMIT_LABEL_MAP.keys():
                task_key = match.group(2)
                pr = None
                try:
                    pr = int(match.group(5))
                except Exception as e:
                    print(f'Could not parse pr from "{message_first_line}": {str(e)}')
                if task := get_task(task_key, pr):
                    tasks.add(task)
    return list(tasks)


def get_feature_flags() -> dict:
    if LAUNCHDARKLY_SDK_KEY:
        response = requests.get(
            url="https://sdk.launchdarkly.com/sdk/latest-all",
            headers={
                "Authorization": LAUNCHDARKLY_SDK_KEY,
            },
            timeout=30,
        )
        return response.json().get('flags', {})
    return {}


def missing_tasks(left: list[JiraIssue], right: list[JiraIssue]) -> list[JiraIssue]:
    r_keys = [x.key for x in right]
    missing = [task for task in left if task.key not in r_keys]
    missing_sorted = sorted(missing, key=lambda x: int(x.key.split('-')[-1]))
    return missing_sorted


def sort_task_by_label(tasks: list[JiraIssue]) -> dict[str, list[JiraIssue]]:
    result = {}
    for task in tasks:
        result[task.label] = result.get(task.label, []) + [task]
    return result


def render_link_md(text: str, link: str) -> str:
    return f"[{text}]({link})"


def render_tasks_md(tasks: list[JiraIssue]) -> list[str]:
    result = []
    for task in tasks:
        summary = task.desc.replace('\n', ' ')
        result.append(f'- {summary} {render_link_md(task.key, task.link)}')
    return result


def render_ffs_md(ffs: list) -> list[str]:
    result = []
    for ff in ffs:
        key = ff.get('key')
        link = ff_link(ff)
        result.append(f'- {render_link_md(key, link)}')
    return result


def render_add_quote_md(lines: list[str]) -> list[str]:
    return [f'> {line}' for line in lines]


def render_add_spoiler_md(title: str, lines: list[str]) -> list[str]:
    result = ["<details>", f"<summary>{title}</summary>", ""]
    result.extend(lines)
    result.append("")
    result.append("</details>")
    return result


def render_add_header_md(title: str, lines: list[str]) -> list[str]:
    result = [f"### {title}"]
    result.extend(lines)
    result.append('')
    return result


def render_output_md(
        gh_release,
        jira_releases_issues_jql_url,
        sorted_release_tasks: dict[str, list[JiraIssue]],
        missing_in_gh: list[JiraIssue],
        missing_in_tracker: list[JiraIssue],
        missing_release_note_field: list[JiraIssue],
        turned_off_feature_flags: list,
        helm_chart_version: str = None,
) -> str:
    release_notes_lines = []

    if helm_chart_version:
        release_notes_lines.append(f'Helm Chart version: {helm_chart_version}')

    for label, tasks in sorted(sorted_release_tasks.items(),
                               key=lambda x: LABEL_SORT.index(x[0]) if x[0] in LABEL_SORT else 100):
        release_notes_lines.extend(
            render_add_header_md(
                label,
                render_tasks_md(tasks)
            )
        )

    comment = []

    comment.append(f'Full Changelog: [{PREVIOUS_REF}...{RELEASE_VERSION}]({gh_release.html_url})')
    comment.append(
        f'This changelog was updated in response to a push of {CURRENT_REF} [Workflow run]({WORKFLOW_RUN_LINK})')
    comment.append('')

    if jira_releases_issues_jql_url:
        comment.append(f'[Jira Release {RELEASE_VERSION} Issues Filter]({jira_releases_issues_jql_url})')
    else:
        comment.append('Jira Release not found')

    if len(missing_in_tracker) == 0:
        comment.append('Release Notes are generated based on git log: No tasks found in Task Tracker.')
    else:
        comment.append('Release Notes are generated based on Task Tracker.')

    if missing_in_gh:
        comment.extend(
            render_add_spoiler_md(
                f'Missing in GitHub release ({len(missing_in_gh)})',
                render_tasks_md(missing_in_gh)
            )
        )
    if missing_in_tracker:
        comment.extend(
            render_add_spoiler_md(
                f'Missing in Task Tracker release ({len(missing_in_tracker)})',
                render_tasks_md(missing_in_tracker)
            )
        )
    if missing_release_note_field:
        comment.extend(
            render_add_spoiler_md(
                f'Missing Release note field ({len(missing_release_note_field)})',
                render_tasks_md(missing_release_note_field)
            )
        )
    if turned_off_feature_flags:
        comment.extend(
            render_add_spoiler_md(
                f'Turned off Feature Flags ({len(turned_off_feature_flags)})',
                render_ffs_md(turned_off_feature_flags)
            )
        )

    comment.append('')
    comment.append('**ALL LINES STARTING FROM QUOTE WILL BE IGNORED**')

    release_notes_lines.extend(
        render_add_quote_md(comment)
    )

    return '\n'.join(release_notes_lines)


def render_output_json(
        sorted_release_tasks: dict[str, list[JiraIssue]],
) -> dict:
    sorted_release_tasks_json = {}
    for label, tasks in sorted_release_tasks.items():
        sorted_release_tasks_json[label] = [t.__dict__() for t in tasks]
    result = {
        'sorted_release_tasks': sorted_release_tasks_json
    }
    return result


def get_helm_chart_version(repo: str, path: str) -> str or None:
    chart_repo = github_client.get_repo(repo)
    content = chart_repo.get_contents(path)
    version_regexp = re.compile(r'version:\s*(.*)')
    match = re.search(version_regexp, content.decoded_content.decode('utf-8'))
    return match.group(1)


def main():
    gh_release = get_github_release(PREVIOUS_REF, CURRENT_REF)
    print(f"Compare url: {gh_release.html_url}")
    print(f"Ahead by {gh_release.ahead_by}")
    print(f"Behind by {gh_release.behind_by}")
    print(f"Merge base commit: {gh_release.merge_base_commit}")
    print(f"Commits: {gh_release.commits}")

    global FEATURE_FLAGS
    try:
        FEATURE_FLAGS = get_feature_flags()
    except Exception as e:
        print(f'Failed to fetch Feature Flags: {e}')

    gh_release_tasks = get_github_release_tasks(gh_release.commits)

    jira_release_issues = []
    jira_releases_urls = []
    jira_releases_issues_jql_url = None

    for jira_project in JIRA_PROJECTS:
        jira_fix_version = f"{JIRA_RELEASE_PREFIX}/{RELEASE_VERSION}"
        jira_release = get_jira_release(jira_project, jira_fix_version)
        if jira_release:
            jira_release_url = f"{JIRA_SERVER}/projects/{jira_project}/versions/{jira_release.id}"
            jira_releases_urls.append(jira_release_url)
            print(f"Found Jira Release {jira_release.name} in project {jira_project}: {jira_release_url}")
            jira_release_issues = get_jira_release_issues(jira_release.projectId, jira_release.id)
            jira_release_issues.extend(jira_release_issues)
            issues_jql = quote(f"fixversion=\"{jira_fix_version}\" ORDER BY created DESC")
            jira_releases_issues_jql_url = f"{JIRA_SERVER}/issues/?jql={issues_jql}"

    tracker_release_tasks = jira_release_issues

    if tracker_release_tasks:
        print(f"{len(tracker_release_tasks)} tasks found in Task Tracker")
        print("Using Task Tracker as a source")
        sorted_release_tasks = sort_task_by_label(tracker_release_tasks)
        missing_in_gh = missing_tasks(tracker_release_tasks, gh_release_tasks)
        missing_in_tracker = missing_tasks(gh_release_tasks, tracker_release_tasks)
        missing_release_note_field = [x for x in tracker_release_tasks if not x.release_note]
    else:
        print("No tasks found in Task Tracker")
        print("Using GitHub as a source")
        sorted_release_tasks = sort_task_by_label(gh_release_tasks)
        missing_in_gh = []
        missing_in_tracker = []
        missing_release_note_field = [x for x in tracker_release_tasks if not x.release_note]

    turned_off_feature_flags = [ff for name, ff in FEATURE_FLAGS.items() if not ff_is_on(ff)]

    helm_chart_version = None
    if HELM_CHART_REPO and HELM_CHART_PATH:
        try:
            helm_chart_version = get_helm_chart_version(HELM_CHART_REPO, HELM_CHART_PATH)
        except Exception as e:
            print(f'Failed to fetch Helm Chart Version: {e}')

    output_md = render_output_md(
        gh_release,
        jira_releases_issues_jql_url,
        sorted_release_tasks,
        missing_in_gh,
        missing_in_tracker,
        missing_release_note_field,
        turned_off_feature_flags,
        helm_chart_version=helm_chart_version,
    )
    if OUTPUT_FILE_MD:
        with open(OUTPUT_FILE_MD, 'w') as f:
            print(f"Creating a markdown output file: '{OUTPUT_FILE_MD}'")
            f.write(output_md)
    else:
        print("OUTPUT_FILE_MD is not specified")
    print(output_md)

    output_json = render_output_json(sorted_release_tasks)
    if OUTPUT_FILE_JSON:
        with open(OUTPUT_FILE_JSON, 'w') as f:
            print(f"Creating a json output file: '{OUTPUT_FILE_JSON}'")
            json.dump(output_json, f)
    else:
        print("OUTPUT_FILE_JSON is not specified")
    print(output_json)


if __name__ == "__main__":
    main()
</file>

<file path="label_studio/core/feature_flags/__init__.py">
from .base import all_flags, flag_set, get_feature_file_path
</file>

<file path="label_studio/core/feature_flags/base.py">
import logging

import ldclient
from django.conf import settings
from django.contrib.auth.models import AnonymousUser
from ldclient.config import Config, HTTPConfig
from ldclient.feature_store import CacheConfig
from ldclient.integrations import Files, Redis

from label_studio.core.current_request import get_current_request
from label_studio.core.utils.common import load_func
from label_studio.core.utils.io import find_node
from label_studio.core.utils.params import get_all_env_with_prefix, get_bool_env

from .stale_feature_flags import STALE_FEATURE_FLAGS

logger = logging.getLogger(__name__)

get_user_repr = load_func(settings.FEATURE_FLAGS_GET_USER_REPR)


def get_feature_file_path():
    package_name = 'label_studio' if settings.VERSION_EDITION == 'Community' else 'label_studio_enterprise'
    if settings.FEATURE_FLAGS_FILE.startswith('/'):
        return settings.FEATURE_FLAGS_FILE
    else:
        return find_node(package_name, settings.FEATURE_FLAGS_FILE, 'file')


if settings.FEATURE_FLAGS_FROM_FILE:
    # Feature flags from file
    if not settings.FEATURE_FLAGS_FILE:
        raise ValueError(
            'When "FEATURE_FLAGS_FROM_FILE" is set, you have to specify a valid path for feature flags file, e.g.'
            'FEATURE_FLAGS_FILE=my_flags.yml'
        )

    feature_flags_file = get_feature_file_path()

    logger.info(f'Read flags from file {feature_flags_file}')
    data_source = Files.new_data_source(paths=[feature_flags_file])
    config = Config(
        sdk_key=settings.FEATURE_FLAGS_API_KEY or 'whatever', update_processor_class=data_source, send_events=False
    )
    ldclient.set_config(config)
    client = ldclient.get()
elif settings.FEATURE_FLAGS_OFFLINE:
    # On-prem usage, without feature flags file
    ldclient.set_config(Config(settings.FEATURE_FLAGS_API_KEY or 'whatever', offline=True))
    client = ldclient.get()
else:
    # Production usage
    if hasattr(settings, 'REDIS_LOCATION'):
        logger.debug(f'Set LaunchDarkly config with Redis feature store at {settings.REDIS_LOCATION}')
        store_kwargs = {
            'url': settings.REDIS_LOCATION,
            'prefix': 'feature-flags',
            'caching': CacheConfig(expiration=30),
        }
        if settings.REDIS_LOCATION.startswith('rediss'):
            store_kwargs['redis_opts'] = settings.REDIS_SSL_SETTINGS
        store = Redis.new_feature_store(**store_kwargs)
        ldclient.set_config(
            Config(settings.FEATURE_FLAGS_API_KEY, feature_store=store, http=HTTPConfig(connect_timeout=5))
        )
    else:
        logger.debug('Set LaunchDarkly config without Redis...')
        ldclient.set_config(Config(settings.FEATURE_FLAGS_API_KEY, http=HTTPConfig(connect_timeout=5)))
    client = ldclient.get()


def flag_set(feature_flag, user=None, override_system_default=None):
    """Use this method to check whether this flag is set ON to the current user, to split the logic on backend
    For example,
    ```
    if flag_set('ff-dev-123-some-fixed-issue-231221-short', user):
        run_new_code()
    else:
        run_old_code()
    ```
    `override_default` is used to override any system defaults in place in case no files or LD API flags provided

    stale_feature_flags will be checked to confirm if the feature flags are still active

    stale feature flags are considered "deprecated" and should not be changeable in any circumstance.
    They are an intermediary step before code references to the flag being removed completely.
    """

    if feature_flag in STALE_FEATURE_FLAGS:
        return STALE_FEATURE_FLAGS[feature_flag]

    if user is None:
        user = AnonymousUser
    elif user == 'auto':
        user = AnonymousUser
        request = get_current_request()
        if request and getattr(request, 'user', None) and request.user.is_authenticated:
            user = request.user

    env_value = get_bool_env(feature_flag, default=None)
    if env_value is not None:
        return env_value
    if override_system_default is not None:
        system_default = override_system_default
    else:
        system_default = settings.FEATURE_FLAGS_DEFAULT_VALUE
    user_dict = get_user_repr(user)
    return client.variation(feature_flag, user_dict, system_default)


def all_flags(user):
    """Return the output of this method in API response, to bootstrap client-side flags.
    More on https://docs.launchdarkly.com/sdk/features/bootstrapping#javascript
    stale_feature_flags will override any client configuration
    """
    user_dict = get_user_repr(user)
    logger.debug(f'Resolve all flags state for user {user_dict}')
    state = client.all_flags_state(user_dict)
    flags = state.to_json_dict()

    env_ff = get_all_env_with_prefix('ff_', is_bool=True)
    env_fflag = get_all_env_with_prefix('fflag_', is_bool=True)
    env_fflag2 = get_all_env_with_prefix('fflag-', is_bool=True)
    env_fflag3 = get_all_env_with_prefix('feat_', is_bool=True)
    env_ff.update(env_fflag)
    env_ff.update(env_fflag2)
    env_ff.update(env_fflag3)

    for env_flag_name, env_flag_on in env_ff.items():
        flags[env_flag_name] = env_flag_on

    for feature_flag, value in STALE_FEATURE_FLAGS.items():
        flags[feature_flag] = value

    return flags
</file>

<file path="label_studio/core/feature_flags/stale_feature_flags.py">
STALE_FEATURE_FLAGS = {
    'fflag_feat_all_dia_13_structured_data_support_short': False,
    'fflag_feat_front_optic_66_lazy_chart_evaluation_19092023_short': False,
    'fflag_fix_front_lsdv_4600_lead_time_27072023_short': False,
    'fflag_fix_back_lsdv_4648_annotator_filter_29052023_short': True,
    'ff_back_dev_4664_remove_storage_file_on_export_delete_29032023_short': False,
    'fflag_feat_front_dev_3873_labeling_ui_improvements_short': True,
    'fflag_feat_back_dev_3756_queue_enrollment_min_short': False,
    'ff_front_dev_2432_auto_save_polygon_draft_210622_short': True,
    'ff_front_1170_outliner_030222_short': True,
    'fflag_fix_front_lsdv_4620_memory_leaks_100723_short': False,
    'fflag_feat_all_lsdv_4915_async_task_import_13042023_short': True,
    'fflag_fix_all_lsdv_4971_async_reimport_09052023_short': True,
    # Jan 16
    'fflag_feat_front_optic_767_annotator_project_multiselect_short': True,
    'fflag_fix_back_leap_612_explore_review_09042024_short': True,
    'fflag_fix_optic_214_extra_blank_dashboard_charts_short': True,
    'fflag_fix_optic_391_tasks_outside_low_agreement_project_counts_short': True,
    'fflag_fix_all_leap_877_annotator_membership_api_03042024_short': True,
    'fflag_feat_all_optic_520_annotator_report_short': True,
    'feat_all_optic_71_dashboard_multiple_labeling_group_support_v1_01092023_short': True,
    'fflag_feat_front_prod_281_project_list_search_19072023_short': True,
    'fflag_feat_all_lsdv_e_295_project_level_roles_via_saml_scim_ldap_short': True,
    'ff_back_2884_comments_notifications_02092022_short': True,
    'ff_back_DEV_1711_review_queue_140222_short': True,
    'ff_front_dev_1480_created_on_in_review_180122_short': True,
    'fflag_fix_front_leap_32_zoom_perf_190923_short': True,
    'fflag_feat_front_lsdv_5452_taxonomy_labeling_110823_short': True,
    'fflag_fix_front_dev_3793_relative_coords_short': True,
    'ff_front_dev_2715_audio_3_280722_short': True,
    # Feb 5
    'fflag_feat_front_optic_1351_use_new_projects_counts_api_short': True,
    'fflag_feature_all_optic_1421_cold_start_v2': False,
    'fflag_fix_back_optic_1407_optimize_tasks_api_pagination_counts': True,
    'fflag_fix_optic_1259_lse_projects_read_apis_use_replica_short': True,
    'fflag_feat_all_optic_1181_membership_performance': True,
    'fflag_feat_optic_1025_zendesk_widget_integration': False,
    'fflag_feat_all_optic_991_dashboard_v2_short': True,
    'fflag_feat_optic_378_limit_projects_per_page_to_ten_short': True,
    'fflag_feat_optic_67_drag_and_drop_charts': True,
    # Feb 6
    'fflag_feat_dia_1528_gemini_models_support_vertex_ai_support_short': True,
    'fflag_feat_all_dia_1576_prompts_easy_breezy_onboarding_short_async_presets_ks': False,
    'fflag_front_dia_1150_ddisco_sneak_preview': False,
    # Feb 25
    'fflag_feat_front_leap_1198_unsaved_changes_180724': True,
    'fflag_fix_leap_246_multi_object_hotkeys_160124_short': True,
    'fflag_fix_leap_466_text_sanitization': True,
    'fflag_fix_front_leap_218_improve_performance_of_taxonomy_search_short': True,
}
</file>

<file path="label_studio/core/feature_flags/utils.py">
def get_user_repr(user):
    """Turn user object into dict with required properties"""
    if user.is_anonymous:
        return {'key': str(user), 'custom': {'organization': None}}
    user_data = {'email': user.email}
    user_data['key'] = user_data['email']
    if user.active_organization is not None:
        user_data['custom'] = {'organization': user.active_organization.created_by.email}
    else:
        user_data['custom'] = {'organization': None}
    return user_data
</file>

<file path="label_studio/core/management/commands/locked_migrate.py">
import logging
import time

from django.conf import settings
from django.core.management.commands.migrate import Command as MigrateCommand
from django.db import connections, transaction

logger = logging.getLogger(__name__)

DEFAULT_LOCK_ID = getattr(settings, 'MIGRATE_LOCK_ID', 1000)

LOCKED_MIGRATE_CMD_CONNECTION_ALIAS = 'locked_migrate_cmd_connection'
RETRY_INTERVAL = 5  # Time to wait between retries in seconds
MAX_WAIT_TIME = 300  # Maximum time to wait for the lock in seconds (5 minutes)


class Command(MigrateCommand):
    help = 'Run Django migrations safely, using a lock'

    def add_arguments(self, parser):
        MigrateCommand.add_arguments(self, parser)
        parser.add_argument(
            '--migrate-lock-id',
            default=DEFAULT_LOCK_ID,
            type=int,
            help='The id of the advisory lock to use',
        )

    def handle(self, *args, **options):
        lock_id = options.pop('migrate_lock_id')

        # Create a separate database connection to hold the lock
        separate_lock_connection = connections.create_connection('default')
        connections[LOCKED_MIGRATE_CMD_CONNECTION_ALIAS] = separate_lock_connection
        try:
            # Use a transaction to hold the lock for the duration of the migration
            with transaction.atomic(using=LOCKED_MIGRATE_CMD_CONNECTION_ALIAS):
                # Attempt to acquire the lock with retries
                self.acquire_lock_with_retry(separate_lock_connection, lock_id)
                # Run the standard Django migration once lock is acquired
                super().handle(*args, **options)
            logger.info('Migration complete, the migration lock has now been released.')
        finally:
            # Ensure the lock connection is closed to free resources
            separate_lock_connection.close()

    def acquire_lock_with_retry(self, lock_connection, lock_id):
        start_time = time.time()

        while True:
            with lock_connection.cursor() as cursor:
                logger.info(f'Attempting to acquire the postgres advisory transaction lock with id: {lock_id}.')

                # Attempt to acquire the transaction-level lock without blocking
                cursor.execute(f'SELECT pg_try_advisory_xact_lock({lock_id})')
                lock_acquired = cursor.fetchone()[0]

                if lock_acquired:
                    logger.info('Acquired the transaction lock, proceeding with migration.')
                    return  # Exit the function if the lock is acquired

                # Check if the maximum wait time has been reached
                elapsed_time = time.time() - start_time
                if elapsed_time >= MAX_WAIT_TIME:
                    logger.info('Could not acquire the transaction lock within the timeout period.')
                    raise TimeoutError('Failed to acquire PostgreSQL advisory transaction lock within 5 minutes.')

                # Wait before retrying
                logger.info(f'Lock not acquired. Retrying in {RETRY_INTERVAL} seconds...')
                time.sleep(RETRY_INTERVAL)
</file>

<file path="label_studio/core/management/commands/show_async_migrations.py">
import logging

from core.models import AsyncMigrationStatus
from django.core.management.base import BaseCommand

logger = logging.getLogger(__name__)


class Command(BaseCommand):
    help = 'Show async migrations'

    def add_arguments(self, parser):
        parser.add_argument('-o', '--organization', type=int, help='organization id', default=-1)

    def handle(self, *args, **options):
        org = options['organization']
        logger.debug(f"===> AsyncMigrationStatus for Organization {org if org > -1 else 'ALL'}")
        if org == -1:
            migrations = AsyncMigrationStatus.objects.all().order_by('project_id')
        else:
            migrations = AsyncMigrationStatus.objects.filter(project__organization_id=org)

        for m in migrations:
            logger.debug(f'{m.name} \t {m.created_at} \t Project <{m.project}> \t {m.status} \t {m.meta}')

        logger.debug(f"===> AsyncMigrationStatus for Organization {org if org > -1 else 'ALL'} printed")
</file>

<file path="label_studio/core/migrations/0001_initial.py">
# Generated by Django 3.2.13 on 2022-06-27 01:16

from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    initial = True

    dependencies = [
        ('projects', '0016_auto_20220211_2218'),
    ]

    operations = [
        migrations.CreateModel(
            name='AsyncMigrationStatus',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('meta', models.JSONField(default=dict, help_text='Meta is for any params for migrations, e.g.: project, filter or error message.', null=True, verbose_name='meta')),
                ('name', models.TextField(help_text='Migration name', verbose_name='migration_name')),
                ('status', models.CharField(choices=[('STARTED', 'Migration is started or queued.'), ('IN PROGRESS', 'Migration is in progress. Check meta for job_id or status.'), ('FINISHED', 'Migration completed successfully.'), ('ERROR', 'Migration completed with errors. Check meta for more info.')], default=None, max_length=100, null=True)),
                ('created_at', models.DateTimeField(auto_now_add=True, help_text='Creation time', verbose_name='created at')),
                ('updated_at', models.DateTimeField(auto_now=True, help_text='Last updated time', verbose_name='updated at')),
                ('project', models.ForeignKey(help_text='Project ID for this migration', null=True, on_delete=django.db.models.deletion.CASCADE, related_name='asyncmigrationstatus', to='projects.project')),
            ],
        ),
    ]
</file>

<file path="label_studio/core/settings/__init__.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
</file>

<file path="label_studio/core/settings/base.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license."""

"""
Django Base settings for Label Studio.

For more information on this file, see
https://docs.djangoproject.com/en/3.1/topics/settings/

For the full list of settings and their values, see
https://docs.djangoproject.com/en/3.1/ref/settings/
"""
import json
import logging
import os
import re
from datetime import timedelta

from django.core.exceptions import ImproperlyConfigured

from label_studio.core.utils.params import get_bool_env, get_env_list

formatter = 'standard'
JSON_LOG = get_bool_env('JSON_LOG', False)
if JSON_LOG:
    formatter = 'json'

LOGGING = {
    'version': 1,
    'disable_existing_loggers': False,
    'formatters': {
        'json': {
            '()': 'label_studio.core.utils.formatter.CustomJsonFormatter',
            'format': '[%(asctime)s] [%(name)s::%(funcName)s::%(lineno)d] [%(levelname)s] [%(user_id)s] %(message)s',
            'datefmt': '%d/%b/%Y:%H:%M:%S %z',
        },
        'standard': {
            'format': '[%(asctime)s] [%(name)s::%(funcName)s::%(lineno)d] [%(levelname)s] %(message)s',
        },
    },
    'handlers': {
        'console': {
            'class': 'logging.StreamHandler',
            'formatter': formatter,
        },
    },
    'root': {
        'handlers': ['console'],
        'level': os.environ.get('LOG_LEVEL', 'DEBUG'),
    },
    'loggers': {
        'pykwalify': {'level': 'ERROR', 'propagate': False},
        'tavern': {'level': 'ERROR', 'propagate': False},
        'asyncio': {'level': 'WARNING'},
        'rules': {'level': 'WARNING'},
        'django': {
            'handlers': ['console'],
            # 'propagate': True,
        },
        'django_auth_ldap': {'level': os.environ.get('LOG_LEVEL', 'DEBUG')},
        'rq.worker': {
            'handlers': ['console'],
            'level': os.environ.get('LOG_LEVEL', 'INFO'),
        },
        'ddtrace': {
            'handlers': ['console'],
            'level': 'WARNING',
        },
        'ldclient.util': {
            'handlers': ['console'],
            'level': 'ERROR',
        },
    },
}

# for printing messages before main logging config applied
if not logging.getLogger().hasHandlers():
    logging.basicConfig(level=logging.DEBUG, format='%(message)s')

from label_studio.core.utils.io import get_data_dir
from label_studio.core.utils.params import get_bool_env, get_env

logger = logging.getLogger(__name__)
SILENCED_SYSTEM_CHECKS = []

# Hostname is used for proper path generation to the resources, pages, etc
HOSTNAME = get_env('HOST', '')
if HOSTNAME:
    if not HOSTNAME.startswith('http://') and not HOSTNAME.startswith('https://'):
        logger.info(
            '! HOST variable found in environment, but it must start with http:// or https://, ignore it: %s', HOSTNAME
        )
        HOSTNAME = ''
    else:
        logger.info('=> Hostname correctly is set to: %s', HOSTNAME)
        if HOSTNAME.endswith('/'):
            HOSTNAME = HOSTNAME[0:-1]

        # for django url resolver
        if HOSTNAME:
            # http[s]://domain.com:8080/script_name => /script_name
            pattern = re.compile(r'^http[s]?:\/\/([^:\/\s]+(:\d*)?)(.*)?')
            match = pattern.match(HOSTNAME)
            FORCE_SCRIPT_NAME = match.group(3)
            if FORCE_SCRIPT_NAME:
                logger.info('=> Django URL prefix is set to: %s', FORCE_SCRIPT_NAME)

FRONTEND_HMR = get_bool_env('FRONTEND_HMR', False)
FRONTEND_HOSTNAME = get_env('FRONTEND_HOSTNAME', 'http://localhost:8010' if FRONTEND_HMR else HOSTNAME)

DOMAIN_FROM_REQUEST = get_bool_env('DOMAIN_FROM_REQUEST', False)

if DOMAIN_FROM_REQUEST:
    # in this mode HOSTNAME can be only subpath
    if HOSTNAME and not HOSTNAME.startswith('/'):
        raise ImproperlyConfigured('LABEL_STUDIO_HOST must be a subpath if DOMAIN_FROM_REQUEST is True')

INTERNAL_PORT = '8080'

# SECURITY WARNING: don't run with debug turned on in production!
DEBUG = get_bool_env('DEBUG', True)
DEBUG_MODAL_EXCEPTIONS = get_bool_env('DEBUG_MODAL_EXCEPTIONS', True)

# Whether to verify SSL certs when making external requests, eg in the uploader
#  Turning this off means assuming risk. 
# Overridable at organization level via Organization#verify_ssl_certs
VERIFY_SSL_CERTS = get_bool_env('VERIFY_SSL_CERTS', True)

# 'sqlite-dll-<arch>-<version>.zip' should be hosted at this prefix
WINDOWS_SQLITE_BINARY_HOST_PREFIX = get_env('WINDOWS_SQLITE_BINARY_HOST_PREFIX', 'https://www.sqlite.org/2023/')

# Build paths inside the project like this: os.path.join(BASE_DIR, ...)
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

# Base path for media root and other uploaded files
BASE_DATA_DIR = get_env('BASE_DATA_DIR')
if BASE_DATA_DIR is None:
    BASE_DATA_DIR = get_data_dir()
os.makedirs(BASE_DATA_DIR, exist_ok=True)
logger.info('=> Database and media directory: %s', BASE_DATA_DIR)

# This indicates whether the code is running in a Continuous Integration environment.
CI = get_bool_env('CI', False)

# Databases
# https://docs.djangoproject.com/en/2.1/ref/settings/#databases
DJANGO_DB_MYSQL = 'mysql'
DJANGO_DB_SQLITE = 'sqlite'
DJANGO_DB_POSTGRESQL = 'postgresql'
DJANGO_DB = 'default'
DATABASE_NAME_DEFAULT = os.path.join(BASE_DATA_DIR, 'label_studio.sqlite3')
DATABASE_NAME = get_env('DATABASE_NAME', DATABASE_NAME_DEFAULT)
DATABASES_ALL = {
    DJANGO_DB_POSTGRESQL: {
        'ENGINE': 'django.db.backends.postgresql',
        'USER': get_env('POSTGRE_USER', 'postgres'),
        'PASSWORD': get_env('POSTGRE_PASSWORD', 'postgres'),
        'NAME': get_env('POSTGRE_NAME', 'postgres'),
        'HOST': get_env('POSTGRE_HOST', 'localhost'),
        'PORT': int(get_env('POSTGRE_PORT', '5432')),
    },
    DJANGO_DB_MYSQL: {
        'ENGINE': 'django.db.backends.mysql',
        'USER': get_env('MYSQL_USER', 'root'),
        'PASSWORD': get_env('MYSQL_PASSWORD', ''),
        'NAME': get_env('MYSQL_NAME', 'labelstudio'),
        'HOST': get_env('MYSQL_HOST', 'localhost'),
        'PORT': int(get_env('MYSQL_PORT', '3306')),
    },
    DJANGO_DB_SQLITE: {
        'ENGINE': 'django.db.backends.sqlite3',
        'NAME': DATABASE_NAME,
        'OPTIONS': {
            # 'timeout': 20,
        },
    },
}
DATABASES_ALL['default'] = DATABASES_ALL[DJANGO_DB_POSTGRESQL]
DATABASES = {'default': DATABASES_ALL.get(get_env('DJANGO_DB', 'default'))}

DEFAULT_AUTO_FIELD = 'django.db.models.AutoField'

if get_bool_env('GOOGLE_LOGGING_ENABLED', False):
    logging.info('Google Cloud Logging handler is enabled.')
    try:
        import google.cloud.logging
        from google.auth.exceptions import GoogleAuthError

        client = google.cloud.logging.Client()
        client.setup_logging()

        LOGGING['handlers']['google_cloud_logging'] = {
            'level': get_env('LOG_LEVEL', 'WARNING'),
            'class': 'google.cloud.logging.handlers.CloudLoggingHandler',
            'client': client,
        }
        LOGGING['root']['handlers'].append('google_cloud_logging')
    except GoogleAuthError:
        logger.exception('Google Cloud Logging handler could not be setup.')

INSTALLED_APPS = [
    'django.contrib.admin',
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'django.contrib.messages',
    'django.contrib.staticfiles',
    'django.contrib.humanize',
    'drf_yasg',
    'corsheaders',
    'django_extensions',
    'django_rq',
    'django_filters',
    'rules',
    'annoying',
    'rest_framework',
    'rest_framework.authtoken',
    'rest_framework_simplejwt.token_blacklist',
    'drf_generators',
    'core',
    'users',
    'organizations',
    'data_import',
    'data_export',
    'projects',
    'tasks',
    'data_manager',
    'io_storages',
    'ml',
    'webhooks',
    'labels_manager',
    'ml_models',
    'ml_model_providers',
    'jwt_auth',
]

MIDDLEWARE = [
    'corsheaders.middleware.CorsMiddleware',
    'django.middleware.security.SecurityMiddleware',
    'django.contrib.sessions.middleware.SessionMiddleware',
    'django.middleware.locale.LocaleMiddleware',
    'core.middleware.DisableCSRF',
    'django.middleware.csrf.CsrfViewMiddleware',
    'core.middleware.XApiKeySupportMiddleware',
    'django.contrib.auth.middleware.AuthenticationMiddleware',
    'django.contrib.messages.middleware.MessageMiddleware',
    'core.middleware.CommonMiddlewareAppendSlashWithoutRedirect',  # instead of 'CommonMiddleware'
    'django_user_agents.middleware.UserAgentMiddleware',
    'core.middleware.SetSessionUIDMiddleware',
    'core.middleware.ContextLogMiddleware',
    'core.middleware.DatabaseIsLockedRetryMiddleware',
    'core.current_request.ThreadLocalMiddleware',
    'jwt_auth.middleware.JWTAuthenticationMiddleware',
]

REST_FRAMEWORK = {
    'DEFAULT_FILTER_BACKENDS': ['django_filters.rest_framework.DjangoFilterBackend'],
    'DEFAULT_AUTHENTICATION_CLASSES': (
        'jwt_auth.auth.TokenAuthenticationPhaseout',
        'rest_framework.authentication.SessionAuthentication',
    ),
    'DEFAULT_PERMISSION_CLASSES': [
        'core.api_permissions.HasObjectPermission',
        'rest_framework.permissions.IsAuthenticated',
    ],
    'EXCEPTION_HANDLER': 'core.utils.common.custom_exception_handler',
    'DEFAULT_RENDERER_CLASSES': ('rest_framework.renderers.JSONRenderer',),
    'DEFAULT_VERSIONING_CLASS': 'rest_framework.versioning.NamespaceVersioning',
    'PAGE_SIZE': 100,
    # 'DEFAULT_PAGINATION_CLASS': 'rest_framework.pagination.PageNumberPagination'
}
SILENCED_SYSTEM_CHECKS += ['rest_framework.W001']

# CORS & Host settings
INTERNAL_IPS = [  # django debug toolbar for django==2.2 requirement
    '127.0.0.1',
    'localhost',
]
CORS_ORIGIN_ALLOW_ALL = True
CORS_ALLOW_METHODS = [
    'DELETE',
    'GET',
    'OPTIONS',
    'PATCH',
    'POST',
    'PUT',
]
ALLOWED_HOSTS = get_env_list('ALLOWED_HOSTS', default=['*'])

# Auth modules
AUTH_USER_MODEL = 'users.User'
AUTHENTICATION_BACKENDS = [
    'rules.permissions.ObjectPermissionBackend',
    'django.contrib.auth.backends.ModelBackend',
]
USE_USERNAME_FOR_LOGIN = False

DISABLE_SIGNUP_WITHOUT_LINK = get_bool_env('DISABLE_SIGNUP_WITHOUT_LINK', False)

# Password validation:
# https://docs.djangoproject.com/en/2.1/ref/settings/#auth-password-validators
AUTH_PASSWORD_VALIDATORS = [
    {'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator'},
    {'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator'},
    {'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator'},
    {'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator'},
]

# Django templates
TEMPLATES_DIR = os.path.join(os.path.dirname(BASE_DIR), 'templates')  # ../../from_this = 'web' dir
TEMPLATES = [
    {
        'BACKEND': 'django.template.backends.django.DjangoTemplates',
        'DIRS': [TEMPLATES_DIR],
        'APP_DIRS': True,
        'OPTIONS': {
            'context_processors': [
                'django.template.context_processors.debug',
                'django.template.context_processors.request',
                'django.contrib.auth.context_processors.auth',
                'django.contrib.messages.context_processors.messages',
                'core.context_processors.settings',
            ],
            'builtins': ['django.templatetags.i18n'],
        },
    }
]

# RQ
RQ_QUEUES = {
    'critical': {
        'HOST': 'localhost',
        'PORT': 6379,
        'DB': 0,
        'DEFAULT_TIMEOUT': 180,
    },
    'high': {
        'HOST': 'localhost',
        'PORT': 6379,
        'DB': 0,
        'DEFAULT_TIMEOUT': 180,
    },
    'default': {
        'HOST': 'localhost',
        'PORT': 6379,
        'DB': 0,
        'DEFAULT_TIMEOUT': 180,
    },
    'low': {
        'HOST': 'localhost',
        'PORT': 6379,
        'DB': 0,
        'DEFAULT_TIMEOUT': 180,
    },
}

# specify the list of the extensions that are allowed to be presented in auto generated OpenAPI schema
# for example, by specifying in swagger_auto_schema(..., x_fern_sdk_group_name='projects') we can group endpoints
# /api/projects/:
#   get:
#     x-fern-sdk-group-name: projects
X_VENDOR_OPENAPI_EXTENSIONS = ['x-fern']

# Swagger: automatic API documentation
SWAGGER_SETTINGS = {
    'SECURITY_DEFINITIONS': {
        'Token': {
            'type': 'apiKey',
            'name': 'Authorization',
            'in': 'header',
            'description': 'The token (or API key) must be passed as a request header. '
            'You can find your user token on the User Account page in Label Studio. Example: '
            '<br><pre><code class="language-bash">'
            'curl https://label-studio-host/api/projects -H "Authorization: Token [your-token]"'
            '</code></pre>',
        }
    },
    'APIS_SORTER': 'alpha',
    'SUPPORTED_SUBMIT_METHODS': ['get', 'post', 'put', 'delete', 'patch'],
    'OPERATIONS_SORTER': 'alpha',
    'DEFAULT_AUTO_SCHEMA_CLASS': 'core.utils.openapi_extensions.XVendorExtensionsAutoSchema',
    'DEFAULT_INFO': 'core.urls.open_api_info',
}

SENTRY_DSN = get_env('SENTRY_DSN', None)
SENTRY_RATE = float(get_env('SENTRY_RATE', 0.02))
SENTRY_ENVIRONMENT = get_env('SENTRY_ENVIRONMENT', 'stage.opensource')
SENTRY_REDIS_ENABLED = False
FRONTEND_SENTRY_DSN = get_env('FRONTEND_SENTRY_DSN', None)
FRONTEND_SENTRY_RATE = get_env('FRONTEND_SENTRY_RATE', 0.01)
FRONTEND_SENTRY_ENVIRONMENT = get_env('FRONTEND_SENTRY_ENVIRONMENT', 'stage.opensource')

# Exceptions that should not be logged to Sentry and aren't children of drf's APIException class
SENTRY_IGNORED_EXCEPTIONS = [
    'Http404',
    'XMLSyntaxError',
    'FileUpload.DoesNotExist',
    'Forbidden',
    'KeyboardInterrupt',
]

ROOT_URLCONF = 'core.urls'
WSGI_APPLICATION = 'core.wsgi.application'
GRAPHIQL = True

# Internationalization
# https://docs.djangoproject.com/en/2.1/topics/i18n/
LANGUAGE_CODE = 'en-us'
TIME_ZONE = 'UTC'
USE_I18N = False
USE_L10N = True
USE_TZ = True

# Static files (CSS, JavaScript, Images)
# https://docs.djangoproject.com/en/2.1/howto/static-files/
STATIC_URL = '/static/'
# if FORCE_SCRIPT_NAME:
#    STATIC_URL = FORCE_SCRIPT_NAME + STATIC_URL
logger.info(f'=> Static URL is set to: {STATIC_URL}')

STATIC_ROOT = os.path.join(BASE_DIR, 'static_build')
STATICFILES_DIRS = [os.path.join(BASE_DIR, 'static')]
STATICFILES_FINDERS = (
    'django.contrib.staticfiles.finders.FileSystemFinder',
    'django.contrib.staticfiles.finders.AppDirectoriesFinder',
)
STORAGES = {
    'default': {
        'BACKEND': 'django.core.files.storage.FileSystemStorage',
    },
    'staticfiles': {
        'BACKEND': 'core.storage.SkipMissedManifestStaticFilesStorage',
    },
}

# Sessions and CSRF
SESSION_COOKIE_SECURE = bool(int(get_env('SESSION_COOKIE_SECURE', False)))
SESSION_COOKIE_SAMESITE = get_env('SESSION_COOKIE_SAMESITE', 'Lax')

CSRF_COOKIE_SECURE = bool(int(get_env('CSRF_COOKIE_SECURE', SESSION_COOKIE_SECURE)))
CSRF_COOKIE_HTTPONLY = bool(int(get_env('CSRF_COOKIE_HTTPONLY', SESSION_COOKIE_SECURE)))
CSRF_COOKIE_SAMESITE = get_env('CSRF_COOKIE_SAMESITE', 'Lax')

# default value is from django docs: https://docs.djangoproject.com/en/5.1/ref/settings/#csrf-cookie-age
# approximately 1 year
CSRF_COOKIE_AGE = int(get_env('CSRF_COOKIE_AGE', 31449600))


# Inactivity user sessions
INACTIVITY_SESSION_TIMEOUT_ENABLED = bool(int(get_env('INACTIVITY_SESSION_TIMEOUT_ENABLED', True)))
# The most time a login will last, regardless of activity
MAX_SESSION_AGE = int(get_env('MAX_SESSION_AGE', timedelta(days=14).total_seconds()))
# The most time that can elapse between activity with the server before the user is logged out
MAX_TIME_BETWEEN_ACTIVITY = int(get_env('MAX_TIME_BETWEEN_ACTIVITY', timedelta(days=5).total_seconds()))

SSRF_PROTECTION_ENABLED = get_bool_env('SSRF_PROTECTION_ENABLED', False)
USE_DEFAULT_BANNED_SUBNETS = get_bool_env('USE_DEFAULT_BANNED_SUBNETS', True)
USER_ADDITIONAL_BANNED_SUBNETS = get_env_list('USER_ADDITIONAL_BANNED_SUBNETS', default=[])

# user media files
MEDIA_ROOT = os.path.join(BASE_DATA_DIR, 'media')
os.makedirs(MEDIA_ROOT, exist_ok=True)
MEDIA_URL = '/data/'
UPLOAD_DIR = 'upload'
AVATAR_PATH = 'avatars'

SUPPORTED_EXTENSIONS = set(
    [
        '.bmp',
        '.csv',
        '.flac',
        '.gif',
        '.htm',
        '.html',
        '.jpg',
        '.jpeg',
        '.json',
        '.m4a',
        '.mp3',
        '.ogg',
        '.png',
        '.svg',
        '.tsv',
        '.txt',
        '.wav',
        '.xml',
        '.mp4',
        '.webm',
        '.webp',
    ]
)

# directory for files created during unit tests
TEST_DATA_ROOT = os.path.join(BASE_DATA_DIR, 'test_data')
os.makedirs(TEST_DATA_ROOT, exist_ok=True)

# project exports
EXPORT_DIR = os.path.join(BASE_DATA_DIR, 'export')
EXPORT_URL_ROOT = '/export/'
EXPORT_MIXIN = 'data_export.mixins.ExportMixin'
# old export dir
os.makedirs(EXPORT_DIR, exist_ok=True)
# dir for delayed export
DELAYED_EXPORT_DIR = 'export'
os.makedirs(os.path.join(BASE_DATA_DIR, MEDIA_ROOT, DELAYED_EXPORT_DIR), exist_ok=True)

# file / task size limits
DATA_UPLOAD_MAX_MEMORY_SIZE = int(get_env('DATA_UPLOAD_MAX_MEMORY_SIZE', 250 * 1024 * 1024))
DATA_UPLOAD_MAX_NUMBER_FILES = int(get_env('DATA_UPLOAD_MAX_NUMBER_FILES', 100))
TASKS_MAX_NUMBER = 1000000
TASKS_MAX_FILE_SIZE = DATA_UPLOAD_MAX_MEMORY_SIZE

TASK_LOCK_TTL = int(get_env('TASK_LOCK_TTL', default=86400))

LABEL_STREAM_HISTORY_LIMIT = int(get_env('LABEL_STREAM_HISTORY_LIMIT', default=100))

RANDOM_NEXT_TASK_SAMPLE_SIZE = int(get_env('RANDOM_NEXT_TASK_SAMPLE_SIZE', 50))

TASK_API_PAGE_SIZE_MAX = int(get_env('TASK_API_PAGE_SIZE_MAX', 0)) or None

# Email backend
FROM_EMAIL = get_env('FROM_EMAIL', 'Label Studio <hello@labelstud.io>')
EMAIL_BACKEND = get_env('EMAIL_BACKEND', 'django.core.mail.backends.dummy.EmailBackend')

ENABLE_LOCAL_FILES_STORAGE = get_bool_env('ENABLE_LOCAL_FILES_STORAGE', default=True)
LOCAL_FILES_SERVING_ENABLED = get_bool_env('LOCAL_FILES_SERVING_ENABLED', default=False)
LOCAL_FILES_DOCUMENT_ROOT = get_env('LOCAL_FILES_DOCUMENT_ROOT', default=os.path.abspath(os.sep))

SYNC_ON_TARGET_STORAGE_CREATION = get_bool_env('SYNC_ON_TARGET_STORAGE_CREATION', default=True)

ALLOW_IMPORT_TASKS_WITH_UNKNOWN_EMAILS = get_bool_env('ALLOW_IMPORT_TASKS_WITH_UNKNOWN_EMAILS', default=False)

""" React Libraries: do not forget to change this dir in /etc/nginx/nginx.conf """

# EDITOR = label-studio-frontend repository
EDITOR_ROOT = os.path.join(BASE_DIR, '../../web/dist/libs/editor')
# DM = data manager (included into FRONTEND due npm building, we need only version.json file from there)
DM_ROOT = os.path.join(BASE_DIR, '../../web/dist/libs/datamanager')
# FRONTEND = GUI for django backend
REACT_APP_ROOT = os.path.join(BASE_DIR, '../../web/dist/apps/labelstudio')

# per project settings
BATCH_SIZE = 1000
PROJECT_TITLE_MIN_LEN = 3
PROJECT_TITLE_MAX_LEN = 50
LOGIN_REDIRECT_URL = '/'
LOGIN_URL = '/user/login/'
MIN_GROUND_TRUTH = 10
DATA_UNDEFINED_NAME = '$undefined$'
LICENSE = {}
VERSIONS = {}
VERSION_EDITION = 'Community'
LATEST_VERSION_CHECK = True
VERSIONS_CHECK_TIME = 0
ALLOW_ORGANIZATION_WEBHOOKS = get_bool_env('ALLOW_ORGANIZATION_WEBHOOKS', False)
CONVERTER_DOWNLOAD_RESOURCES = get_bool_env('CONVERTER_DOWNLOAD_RESOURCES', True)
EXPERIMENTAL_FEATURES = get_bool_env('EXPERIMENTAL_FEATURES', False)
USE_ENFORCE_CSRF_CHECKS = get_bool_env('USE_ENFORCE_CSRF_CHECKS', True)  # False is for tests
CLOUD_FILE_STORAGE_ENABLED = False

IO_STORAGES_IMPORT_LINK_NAMES = [
    'io_storages_s3importstoragelink',
    'io_storages_gcsimportstoragelink',
    'io_storages_azureblobimportstoragelink',
    'io_storages_localfilesimportstoragelink',
    'io_storages_redisimportstoragelink',
]

CREATE_ORGANIZATION = 'organizations.functions.create_organization'
SAVE_USER = 'users.functions.save_user'
POST_PROCESS_REIMPORT = 'core.utils.common.empty'
USER_SERIALIZER = 'users.serializers.BaseUserSerializer'
USER_SERIALIZER_UPDATE = 'users.serializers.BaseUserSerializerUpdate'
TASK_SERIALIZER = 'tasks.serializers.BaseTaskSerializer'
EXPORT_DATA_SERIALIZER = 'data_export.serializers.BaseExportDataSerializer'
DATA_MANAGER_GET_ALL_COLUMNS = 'data_manager.functions.get_all_columns'
DATA_MANAGER_ANNOTATIONS_MAP = {}
DATA_MANAGER_ACTIONS = {}
DATA_MANAGER_CUSTOM_FILTER_EXPRESSIONS = 'data_manager.functions.custom_filter_expressions'
DATA_MANAGER_PREPROCESS_FILTER = 'data_manager.functions.preprocess_filter'
USER_LOGIN_FORM = 'users.forms.LoginForm'
PROJECT_MIXIN = 'projects.mixins.ProjectMixin'
TASK_MIXIN = 'tasks.mixins.TaskMixin'
LSE_PROJECT = None
GET_TASKS_AGREEMENT_QUERYSET = None
ANNOTATION_MIXIN = 'tasks.mixins.AnnotationMixin'
ORGANIZATION_MIXIN = 'organizations.mixins.OrganizationMixin'
USER_MIXIN = 'users.mixins.UserMixin'
ORGANIZATION_MEMBER_MIXIN = 'organizations.mixins.OrganizationMemberMixin'
MEMBER_PERM = 'core.api_permissions.MemberHasOwnerPermission'
RECALCULATE_ALL_STATS = None
GET_STORAGE_LIST = 'io_storages.functions.get_storage_list'
STORAGE_ANNOTATION_SERIALIZER = 'io_storages.serializers.StorageAnnotationSerializer'
TASK_SERIALIZER_BULK = 'tasks.serializers.BaseTaskSerializerBulk'
PREPROCESS_FIELD_NAME = 'data_manager.functions.preprocess_field_name'
INTERACTIVE_DATA_SERIALIZER = 'data_export.serializers.BaseExportDataSerializerForInteractive'
STORAGE_PERMISSION = 'io_storages.permissions.StoragePermission'
PROJECT_IMPORT_PERMISSION = 'projects.permissions.ProjectImportPermission'
DELETE_TASKS_ANNOTATIONS_POSTPROCESS = None
FEATURE_FLAGS_GET_USER_REPR = 'core.feature_flags.utils.get_user_repr'


def project_delete(project):
    project.delete()


def user_auth(user_model, email, password):
    return None


def collect_versions_dummy(**kwargs):
    return {}


PROJECT_DELETE = project_delete
USER_AUTH = user_auth
COLLECT_VERSIONS = collect_versions_dummy

WEBHOOK_TIMEOUT = float(get_env('WEBHOOK_TIMEOUT', 1.0))
WEBHOOK_BATCH_SIZE = int(get_env('WEBHOOK_BATCH_SIZE', 100))
WEBHOOK_SERIALIZERS = {
    'project': 'webhooks.serializers_for_hooks.ProjectWebhookSerializer',
    'task': 'webhooks.serializers_for_hooks.TaskWebhookSerializer',
    'annotation': 'webhooks.serializers_for_hooks.AnnotationWebhookSerializer',
    'label': 'labels_manager.serializers.LabelSerializer',
    'label_link': 'labels_manager.serializers.LabelLinkSerializer',
}

EDITOR_KEYMAP = json.dumps(get_env('EDITOR_KEYMAP'))

# fix a problem with Windows mimetypes for JS and PNG
import mimetypes

mimetypes.add_type('application/javascript', '.js', True)
mimetypes.add_type('image/png', '.png', True)

# fields name was used in DM api before
REST_FLEX_FIELDS = {'FIELDS_PARAM': 'include'}

INTERPOLATE_KEY_FRAMES = get_env('INTERPOLATE_KEY_FRAMES', False)

# Feature Flags
FEATURE_FLAGS_API_KEY = get_env('FEATURE_FLAGS_API_KEY', default='any key')

# we may set feature flags from file
FEATURE_FLAGS_FROM_FILE = get_bool_env('FEATURE_FLAGS_FROM_FILE', False)
FEATURE_FLAGS_FILE = get_env('FEATURE_FLAGS_FILE', 'feature_flags.json')
# or if file is not set, default is using offline mode
FEATURE_FLAGS_OFFLINE = get_bool_env('FEATURE_FLAGS_OFFLINE', True)
# default value for feature flags (if not overridden by environment or client)
FEATURE_FLAGS_DEFAULT_VALUE = False

# Whether to send analytics telemetry data. Fall back to old lowercase name for legacy compatibility.
COLLECT_ANALYTICS = get_bool_env('COLLECT_ANALYTICS', get_bool_env('collect_analytics', True))

# Strip harmful content from SVG files by default
SVG_SECURITY_CLEANUP = get_bool_env('SVG_SECURITY_CLEANUP', False)

ML_BLOCK_LOCAL_IP = get_bool_env('ML_BLOCK_LOCAL_IP', False)

RQ_LONG_JOB_TIMEOUT = int(get_env('RQ_LONG_JOB_TIMEOUT', 36000))

APP_WEBSERVER = get_env('APP_WEBSERVER', 'django')

BATCH_JOB_RETRY_TIMEOUT = int(get_env('BATCH_JOB_RETRY_TIMEOUT', 60))

FUTURE_SAVE_TASK_TO_STORAGE = get_bool_env('FUTURE_SAVE_TASK_TO_STORAGE', default=False)
FUTURE_SAVE_TASK_TO_STORAGE_JSON_EXT = get_bool_env('FUTURE_SAVE_TASK_TO_STORAGE_JSON_EXT', default=True)
STORAGE_IN_PROGRESS_TIMER = float(get_env('STORAGE_IN_PROGRESS_TIMER', 5.0))
STORAGE_EXPORT_CHUNK_SIZE = int(get_env('STORAGE_EXPORT_CHUNK_SIZE', 100))

USE_NGINX_FOR_EXPORT_DOWNLOADS = get_bool_env('USE_NGINX_FOR_EXPORT_DOWNLOADS', False)

if get_env('MINIO_STORAGE_ENDPOINT') and not get_bool_env('MINIO_SKIP', False):
    CLOUD_FILE_STORAGE_ENABLED = True
    STORAGES['default']['BACKEND'] = 'storages.backends.s3boto3.S3Boto3Storage'
    AWS_STORAGE_BUCKET_NAME = get_env('MINIO_STORAGE_BUCKET_NAME')
    AWS_ACCESS_KEY_ID = get_env('MINIO_STORAGE_ACCESS_KEY')
    AWS_SECRET_ACCESS_KEY = get_env('MINIO_STORAGE_SECRET_KEY')
    AWS_S3_ENDPOINT_URL = get_env('MINIO_STORAGE_ENDPOINT')
    AWS_QUERYSTRING_AUTH = False
    # make domain for FileUpload.file
    AWS_S3_SECURE_URLS = False
    AWS_S3_URL_PROTOCOL = 'http:' if HOSTNAME.startswith('http://') else 'https:'
    AWS_S3_CUSTOM_DOMAIN = HOSTNAME.replace('http://', '').replace('https://', '') + '/data'

if get_env('STORAGE_TYPE') == 's3':
    CLOUD_FILE_STORAGE_ENABLED = True
    STORAGES['default']['BACKEND'] = 'core.storage.CustomS3Boto3Storage'
    if get_env('STORAGE_AWS_ACCESS_KEY_ID'):
        AWS_ACCESS_KEY_ID = get_env('STORAGE_AWS_ACCESS_KEY_ID')
    if get_env('STORAGE_AWS_SECRET_ACCESS_KEY'):
        AWS_SECRET_ACCESS_KEY = get_env('STORAGE_AWS_SECRET_ACCESS_KEY')
    AWS_STORAGE_BUCKET_NAME = get_env('STORAGE_AWS_BUCKET_NAME')
    AWS_S3_REGION_NAME = get_env('STORAGE_AWS_REGION_NAME', None)
    AWS_S3_ENDPOINT_URL = get_env('STORAGE_AWS_ENDPOINT_URL', None)
    if get_env('STORAGE_AWS_OBJECT_PARAMETERS'):
        AWS_S3_OBJECT_PARAMETERS = json.loads(get_env('STORAGE_AWS_OBJECT_PARAMETERS'))
    AWS_QUERYSTRING_EXPIRE = int(get_env('STORAGE_AWS_X_AMZ_EXPIRES', '86400'))
    AWS_LOCATION = get_env('STORAGE_AWS_FOLDER', default='')
    AWS_S3_USE_SSL = get_bool_env('STORAGE_AWS_S3_USE_SSL', True)
    AWS_S3_VERIFY = get_env('STORAGE_AWS_S3_VERIFY', None)
    if AWS_S3_VERIFY == 'false' or AWS_S3_VERIFY == 'False' or AWS_S3_VERIFY == '0':
        AWS_S3_VERIFY = False
    AWS_S3_SIGNATURE_VERSION = get_env('STORAGE_AWS_S3_SIGNATURE_VERSION', None)

if get_env('STORAGE_TYPE') == 'azure':
    CLOUD_FILE_STORAGE_ENABLED = True
    STORAGES['default']['BACKEND'] = 'core.storage.CustomAzureStorage'
    AZURE_ACCOUNT_NAME = get_env('STORAGE_AZURE_ACCOUNT_NAME')
    AZURE_ACCOUNT_KEY = get_env('STORAGE_AZURE_ACCOUNT_KEY')
    AZURE_CONTAINER = get_env('STORAGE_AZURE_CONTAINER_NAME')
    AZURE_URL_EXPIRATION_SECS = int(get_env('STORAGE_AZURE_URL_EXPIRATION_SECS', '86400'))
    AZURE_LOCATION = get_env('STORAGE_AZURE_FOLDER', default='')

if get_env('STORAGE_TYPE') == 'gcs':
    CLOUD_FILE_STORAGE_ENABLED = True
    STORAGES['default']['BACKEND'] = 'core.storage.AlternativeGoogleCloudStorage'
    GS_PROJECT_ID = get_env('STORAGE_GCS_PROJECT_ID')
    GS_BUCKET_NAME = get_env('STORAGE_GCS_BUCKET_NAME')
    GS_EXPIRATION = timedelta(seconds=int(get_env('STORAGE_GCS_EXPIRATION_SECS', '86400')))
    GS_LOCATION = get_env('STORAGE_GCS_FOLDER', default='')
    GS_CUSTOM_ENDPOINT = get_env('STORAGE_GCS_ENDPOINT')

CSRF_TRUSTED_ORIGINS = get_env('CSRF_TRUSTED_ORIGINS', [])
if CSRF_TRUSTED_ORIGINS:
    CSRF_TRUSTED_ORIGINS = CSRF_TRUSTED_ORIGINS.split(',')

# Custom S3 endpoints on these domains will get detailed error reporting
S3_TRUSTED_STORAGE_DOMAINS = get_env_list(
    'S3_TRUSTED_STORAGE_DOMAINS',
    [
        'amazonaws.com',
        'scw.cloud',
        'yandexcloud.net',
        'digitaloceanspaces.com',
        'orange-business.com',
        'computecanada.ca',
        'cloudflarestorage.com',
        'wasabisys.com',
        'oracle.com',
        'amazon.com',
        'appdomain.cloud',
    ],
)

REAL_HOSTNAME = os.getenv('HOSTNAME')  # we have to use getenv, because we don't use LABEL_STUDIO_ prefix
GCS_CLOUD_STORAGE_FORCE_DEFAULT_CREDENTIALS = get_bool_env('GCS_CLOUD_STORAGE_FORCE_DEFAULT_CREDENTIALS', False)
PUBLIC_API_DOCS = get_bool_env('PUBLIC_API_DOCS', False)

# By default, we disallow filters with foreign keys in data manager for security reasons.
# Add to this list (either here in code, or via the env) to allow specific filters that rely on foreign keys.
DATA_MANAGER_FILTER_ALLOWLIST = list(
    set(get_env_list('DATA_MANAGER_FILTER_ALLOWLIST') + ['updated_by__active_organization'])
)

if ENABLE_CSP := get_bool_env('ENABLE_CSP', True):
    CSP_DEFAULT_SRC = (
        "'self'",
        "'report-sample'",
    )
    CSP_STYLE_SRC = ("'self'", "'report-sample'", "'unsafe-inline'")
    CSP_SCRIPT_SRC = (
        "'self'",
        "'report-sample'",
        "'unsafe-inline'",
        "'unsafe-eval'",
        'blob:',
        'browser.sentry-cdn.com',
        'https://*.googletagmanager.com',
    )
    CSP_IMG_SRC = (
        "'self'",
        "'report-sample'",
        'data:',
        'https://*.google-analytics.com',
        'https://*.googletagmanager.com',
        'https://*.google.com',
    )
    CSP_CONNECT_SRC = (
        "'self'",
        "'report-sample'",
        'https://*.google-analytics.com',
        'https://*.analytics.google.com',
        'https://analytics.google.com',
        'https://*.googletagmanager.com',
        'https://*.g.double' + 'click.net',  # hacky way of suppressing codespell complaint
        'https://*.ingest.sentry.io',
    )
    # Note that this will be overridden to real CSP for views that use the override_report_only_csp decorator
    CSP_REPORT_ONLY = get_bool_env('LS_CSP_REPORT_ONLY', True)
    CSP_REPORT_URI = get_env('LS_CSP_REPORT_URI', None)
    CSP_INCLUDE_NONCE_IN = ['script-src', 'default-src']

    MIDDLEWARE.append('core.middleware.HumanSignalCspMiddleware')

CLOUD_STORAGE_CHECK_FOR_RECORDS_PAGE_SIZE = get_env('CLOUD_STORAGE_CHECK_FOR_RECORDS_PAGE_SIZE', 10000)
CLOUD_STORAGE_CHECK_FOR_RECORDS_TIMEOUT = get_env('CLOUD_STORAGE_CHECK_FOR_RECORDS_TIMEOUT', 60)

CONTEXTLOG_SYNC = False
TEST_ENVIRONMENT = get_bool_env('TEST_ENVIRONMENT', False)
DEBUG_CONTEXTLOG = get_bool_env('DEBUG_CONTEXTLOG', False)

_REDIS_SSL_CERTS_REQS = get_env('REDIS_SSL_CERTS_REQS', 'required')
REDIS_SSL_SETTINGS = {
    'ssl_cert_reqs': None if _REDIS_SSL_CERTS_REQS.lower() == 'none' else _REDIS_SSL_CERTS_REQS,
    'ssl_ca_certs': get_env('REDIS_SSL_CA_CERTS', None),
    'ssl_keyfile': get_env('REDIS_SSL_KEYFILE', None),
    'ssl_certfile': get_env('REDIS_SSL_CERTFILE', None),
}

OPENAI_API_VERSION = get_env('OPENAI_API_VERSION', '2024-06-01')
APPEND_SLASH = False

if CI:
    INSTALLED_APPS += ['django_migration_linter']
    MIGRATION_LINTER_OPTIONS = {
        'no_cache': True,
        'ignore_name': '0002_auto_20210304_1457',
        'sql-analyser': 'postgresql',
    }

LOGOUT_REDIRECT_URL = get_env('LOGOUT_REDIRECT_URL', None)
</file>

<file path="label_studio/core/settings/label_studio.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import json

from core.settings.base import *  # noqa
from core.utils.secret_key import generate_secret_key_if_missing

# SECURITY WARNING: keep the secret key used in production secret!
SECRET_KEY = generate_secret_key_if_missing(BASE_DATA_DIR)

DJANGO_DB = get_env('DJANGO_DB', DJANGO_DB_SQLITE)
DATABASES = {'default': DATABASES_ALL[DJANGO_DB]}

MIDDLEWARE.append('organizations.middleware.DummyGetSessionMiddleware')
MIDDLEWARE.append('core.middleware.UpdateLastActivityMiddleware')
if INACTIVITY_SESSION_TIMEOUT_ENABLED:
    MIDDLEWARE.append('core.middleware.InactivitySessionTimeoutMiddleWare')

ADD_DEFAULT_ML_BACKENDS = False

LOGGING['root']['level'] = get_env('LOG_LEVEL', 'WARNING')

DEBUG = get_bool_env('DEBUG', False)

DEBUG_PROPAGATE_EXCEPTIONS = get_bool_env('DEBUG_PROPAGATE_EXCEPTIONS', False)

SESSION_COOKIE_SECURE = get_bool_env('SESSION_COOKIE_SECURE', False)

SESSION_ENGINE = 'django.contrib.sessions.backends.signed_cookies'

RQ_QUEUES = {}

SENTRY_DSN = get_env('SENTRY_DSN', 'https://68b045ab408a4d32a910d339be8591a4@o227124.ingest.sentry.io/5820521')
SENTRY_ENVIRONMENT = get_env('SENTRY_ENVIRONMENT', 'opensource')

FRONTEND_SENTRY_DSN = get_env(
    'FRONTEND_SENTRY_DSN', 'https://5f51920ff82a4675a495870244869c6b@o227124.ingest.sentry.io/5838868'
)
FRONTEND_SENTRY_ENVIRONMENT = get_env('FRONTEND_SENTRY_ENVIRONMENT', 'opensource')

EDITOR_KEYMAP = json.dumps(get_env('EDITOR_KEYMAP'))

from label_studio import __version__
from label_studio.core.utils import sentry

sentry.init_sentry(release_name='label-studio', release_version=__version__)

# we should do it after sentry init
from label_studio.core.utils.common import collect_versions

versions = collect_versions()

# in Label Studio Community version, feature flags are always ON
FEATURE_FLAGS_DEFAULT_VALUE = True
# or if file is not set, default is using offline mode
FEATURE_FLAGS_OFFLINE = get_bool_env('FEATURE_FLAGS_OFFLINE', True)

FEATURE_FLAGS_FILE = get_env('FEATURE_FLAGS_FILE', 'feature_flags.json')
FEATURE_FLAGS_FROM_FILE = True
try:
    from core.utils.io import find_node

    find_node('label_studio', FEATURE_FLAGS_FILE, 'file')
except IOError:
    FEATURE_FLAGS_FROM_FILE = False

STORAGE_PERSISTENCE = get_bool_env('STORAGE_PERSISTENCE', True)
</file>

<file path="label_studio/core/templatetags/filters.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""

import json
import re
from datetime import datetime

from core.utils.manifest_assets import get_manifest_asset
from django import template
from django.conf import settings
from django.utils.html import format_html

register = template.Library()


@register.simple_tag
def manifest_asset(path):
    """Maps a path to its hashed filename using manifest.json, or falls back to /react-app/ prefix

    Usage in template:
    {% manifest_asset 'main.js' %}
    """
    return get_manifest_asset(path)


@register.filter
def initials(val, jn=''):
    """Given a string return its initials join by $jn"""
    res = []
    parts = val.split(' ')
    if len(parts) <= 1:
        parts = re.findall('[A-Z][^A-Z]*', val)
        print(parts)

    if len(parts) > 1:
        res = [parts[0][0], parts[1][0]]
    elif len(parts) == 1:
        res = [val[0], val[1]]

    return jn.join(res).upper()


@register.filter
def get_at_index(l, index):  # noqa: E741
    return l[index]


@register.filter
def get_item(dictionary, key):
    return dictionary.get(key, None)


@register.filter
def json_dumps_ensure_ascii(dictionary):
    return json.dumps(dictionary, ensure_ascii=False)


@register.filter
def json_escape_quote(data):
    data_str = json.dumps(data, ensure_ascii=False)
    return data_str.replace("'", "\\'")


@register.filter
def escape_lt_gt(s):
    return s.replace('<', '&lt;').replace('>', '&gt;')


@register.filter
def datetime2str(d):
    if isinstance(d, str):
        return d
    return d.strftime('%Y-%m-%d %H:%M:%S')


@register.filter
def start_zero_padding(number):
    return '%5.5i' % number


@register.filter
def collaborator_id_in_url(id_, url):
    return ('collaborator_id=' + str(id_)) in url


@register.filter
def date_for_license(date):
    if isinstance(date, str):
        date = datetime.strptime(date, '%Y-%m-%d')
    return date.strftime('%d %b %Y %H:%M')


@register.filter
def current_date(some):
    return datetime.now()


@register.filter
def is_current_date_greater_than(date):
    if date is None:
        return False
    if isinstance(date, str):
        date = datetime.strptime(date, '%Y-%m-%d')
    return datetime.now() > date


@register.filter
def multiply(value, arg):
    return value * arg


@register.simple_tag
def custom_autocomplete(key=''):
    if settings.LICENSE.get('disable_autocomplete', False):
        if key == 'password':
            return format_html('autocomplete="new-password"')
        return format_html('autocomplete="off"')
    else:
        return ''


@register.simple_tag(takes_context=True)
def var_exists(context, name):
    dicts = context.dicts  # array of dicts
    if dicts:
        for d in dicts:
            if name in d:
                return True
    return False
</file>

<file path="label_studio/core/utils/__init__.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
</file>

<file path="label_studio/core/utils/common.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
from __future__ import unicode_literals

import calendar
import contextlib
import copy
import importlib
import logging
import os
import random
import re
import time
import traceback as tb
import uuid
from collections import defaultdict
from copy import deepcopy
from functools import wraps
from typing import Any, Callable, Generator, Iterable, Mapping, Optional

import drf_yasg.openapi as openapi
import pytz
import requests
import ujson as json
from colorama import Fore
from core.utils.params import get_env
from django.conf import settings
from django.contrib.postgres.operations import BtreeGinExtension, TrigramExtension
from django.core.exceptions import ValidationError
from django.core.paginator import EmptyPage, Paginator
from django.core.validators import URLValidator
from django.db import models, transaction
from django.db.models.signals import (
    post_delete,
    post_init,
    post_migrate,
    post_save,
    pre_delete,
    pre_init,
    pre_migrate,
    pre_save,
)
from django.db.utils import OperationalError
from django.utils import timezone
from django.utils.crypto import get_random_string
from django.utils.module_loading import import_string
from django_filters.rest_framework import DjangoFilterBackend
from drf_yasg.inspectors import CoreAPICompatInspector, NotHandled
from label_studio_sdk._extensions.label_studio_tools.core.utils.exceptions import (
    LabelStudioXMLSyntaxErrorSentryIgnored,
)
from packaging.version import parse as parse_version
from pyboxen import boxen
from rest_framework import status
from rest_framework.exceptions import APIException, ErrorDetail
from rest_framework.views import Response, exception_handler

import label_studio

try:
    from sentry_sdk import capture_exception, set_tag

    sentry_sdk_loaded = True
except (ModuleNotFoundError, ImportError):
    sentry_sdk_loaded = False

from core import version
from core.utils.exceptions import LabelStudioDatabaseLockedException

# these functions will be included to another modules, don't remove them
from core.utils.params import int_from_request

logger = logging.getLogger(__name__)
url_validator = URLValidator()


def _override_exceptions(exc):
    if isinstance(exc, OperationalError) and 'database is locked' in str(exc):
        return LabelStudioDatabaseLockedException()

    return exc


def custom_exception_handler(exc, context):
    """Make custom exception treatment in RestFramework

    :param exc: Exception - you can check specific exception
    :param context: context
    :return: response with error desc
    """
    exception_id = uuid.uuid4()

    sentry_skip = False
    if isinstance(exc, APIException) and exc.status_code < 500:
        # Skipping Sentry for non-500 unhandled exceptions
        sentry_skip = True

    logger.error(
        '{} {}'.format(exception_id, exc),
        exc_info=True,
        extra={'sentry_skip': sentry_skip, 'exception_id': exception_id},
    )

    exc = _override_exceptions(exc)

    # error body structure
    response_data = {
        'id': exception_id,
        'status_code': status.HTTP_500_INTERNAL_SERVER_ERROR,  # default value
        'version': label_studio.__version__,
        'detail': 'Unknown error',  # default value
        'exc_info': None,
    }

    if hasattr(exc, 'display_context'):
        response_data['display_context'] = deepcopy(exc.display_context)

    # try rest framework handler
    response = exception_handler(exc, context)
    if response is not None:
        response_data['status_code'] = response.status_code

        if 'detail' in response.data and isinstance(response.data['detail'], ErrorDetail):
            response_data['detail'] = response.data['detail']
            response.data = response_data
        # move validation errors to separate namespace
        else:
            response_data['detail'] = 'Validation error'
            response_data['validation_errors'] = (
                response.data if isinstance(response.data, dict) else {'non_field_errors': response.data}
            )
            response.data = response_data

    # non-standard exception
    else:
        if sentry_sdk_loaded:
            # pass exception to sentry
            set_tag('exception_id', exception_id)
            capture_exception(exc)

        exc_tb = tb.format_exc()
        logger.debug(exc_tb)
        response_data['detail'] = str(exc)
        if not settings.DEBUG_MODAL_EXCEPTIONS:
            exc_tb = None
        response_data['exc_info'] = exc_tb
        # Thrown by sdk when label config is invalid
        if isinstance(exc, LabelStudioXMLSyntaxErrorSentryIgnored):
            response_data['status_code'] = status.HTTP_400_BAD_REQUEST
            response = Response(status=status.HTTP_400_BAD_REQUEST, data=response_data)
        else:
            response = Response(status=status.HTTP_500_INTERNAL_SERVER_ERROR, data=response_data)

    return response


def create_hash() -> str:
    """This function creates a secure token for the organization"""
    return get_random_string(length=40)


def paginator(objects, request, default_page=1, default_size=50):
    """DEPRECATED
    TODO: change to standard drf pagination class

    Get from request page and page_size and return paginated objects

    :param objects: all queryset
    :param request: view request object
    :param default_page: start page if there is no page in GET
    :param default_size: page size if there is no page in GET
    :return: paginated objects
    """
    page_size = request.GET.get('page_size', request.GET.get('length', default_size))
    if settings.TASK_API_PAGE_SIZE_MAX and (int(page_size) > settings.TASK_API_PAGE_SIZE_MAX or page_size == '-1'):
        page_size = settings.TASK_API_PAGE_SIZE_MAX

    if 'start' in request.GET:
        page = int_from_request(request.GET, 'start', default_page)
        if page and int(page) > int(page_size) > 0:
            page = int(page / int(page_size)) + 1
        else:
            page += 1
    else:
        page = int_from_request(request.GET, 'page', default_page)

    if page_size == '-1':
        return objects

    try:
        return Paginator(objects, page_size).page(page).object_list
    except ZeroDivisionError:
        return []
    except EmptyPage:
        return []


def paginator_help(objects_name, tag):
    """API help for paginator, use it with swagger_auto_schema

    :return: dict
    """
    if settings.TASK_API_PAGE_SIZE_MAX:
        page_size_description = f'[or "length"] {objects_name} per page. Max value {settings.TASK_API_PAGE_SIZE_MAX}'
    else:
        page_size_description = (
            f'[or "length"] {objects_name} per page, use -1 to obtain all {objects_name} '
            '(in this case "page" has no effect and this operation might be slow)'
        )
    return dict(
        tags=[tag],
        manual_parameters=[
            openapi.Parameter(
                name='page', type=openapi.TYPE_INTEGER, in_=openapi.IN_QUERY, description='[or "start"] current page'
            ),
            openapi.Parameter(
                name='page_size', type=openapi.TYPE_INTEGER, in_=openapi.IN_QUERY, description=page_size_description
            ),
        ],
        responses={
            200: openapi.Response(title='OK', description='')
            # 404: openapi.Response(title='', description=f'No more {objects_name} found')
        },
    )


def string_is_url(url):
    try:
        url_validator(url)
    except ValidationError:
        return False
    else:
        return True


def safe_float(v, default=0):
    if v != v:
        return default
    return v


def sample_query(q, sample_size):
    n = q.count()
    if n == 0:
        raise ValueError("Can't sample from empty query")
    ids = q.values_list('id', flat=True)
    random_ids = random.sample(list(ids), sample_size)
    return q.filter(id__in=random_ids)


def get_client_ip(request):
    """Get IP address from django request

    :param request: django request
    :return: str with ip
    """
    x_forwarded_for = request.META.get('HTTP_X_FORWARDED_FOR')
    if x_forwarded_for:
        ip = x_forwarded_for.split(',')[0]
    else:
        ip = request.META.get('REMOTE_ADDR')
    return ip


def get_attr_or_item(obj, key):
    if hasattr(obj, key):
        return getattr(obj, key)
    elif isinstance(obj, dict) and key in obj:
        return obj[key]
    else:
        raise KeyError(f"Can't get attribute or dict key '{key}' from {obj}")


def datetime_to_timestamp(dt):
    if dt.tzinfo:
        dt = dt.astimezone(pytz.UTC)
    return calendar.timegm(dt.timetuple())


def timestamp_now():
    return datetime_to_timestamp(timezone.now())


def find_first_one_to_one_related_field_by_prefix(instance, prefix):
    if hasattr(instance, '_find_first_one_to_one_related_field_by_prefix_cache'):
        return getattr(instance, '_find_first_one_to_one_related_field_by_prefix_cache')

    result = None
    for field in instance._meta.get_fields():
        if issubclass(type(field), models.fields.related.OneToOneRel):
            attr_name = field.get_accessor_name()
            if re.match(prefix, attr_name) and hasattr(instance, attr_name):
                result = getattr(instance, attr_name)
                break

    instance._find_first_one_to_one_related_field_by_prefix_cache = result
    return result


def start_browser(ls_url, no_browser):
    import threading
    import webbrowser

    if no_browser:
        return

    browser_url = ls_url
    threading.Timer(2.5, lambda: webbrowser.open(browser_url)).start()
    logger.info('Start browser at URL: ' + browser_url)


def db_is_not_sqlite() -> bool:
    """
    A common predicate for use with conditional_atomic.

    Checks if the DB is NOT sqlite, because sqlite dbs are locked during any write.
    """

    return settings.DJANGO_DB != settings.DJANGO_DB_SQLITE


@contextlib.contextmanager
def conditional_atomic(
    predicate: Callable[..., bool],
    predicate_args: Optional[Iterable[Any]] = None,
    predicate_kwargs: Optional[Mapping[str, Any]] = None,
) -> Generator[None, None, None]:
    """Use transaction if and only if the passed predicate function returns true

    Params:
        predicate: function taking any combination of args and kwargs
        predicate_args: optional array of positional args for the predicate
        predicate_kwargs: optional map of keyword args for the predicate
    """

    should_use_transaction = predicate(*(predicate_args or []), **(predicate_kwargs or {}))

    if should_use_transaction:
        with transaction.atomic():
            yield
    else:
        yield


def retry_database_locked():
    back_off = 2

    def deco_retry(f):
        @wraps(f)
        def f_retry(*args, **kwargs):
            mtries, mdelay = 10, 3
            while mtries > 0:
                try:
                    return f(*args, **kwargs)
                except OperationalError as e:
                    if 'database is locked' in str(e):
                        time.sleep(mdelay)
                        mtries -= 1
                        mdelay *= back_off
                    else:
                        raise
            return f(*args, **kwargs)

        return f_retry

    return deco_retry


def get_app_version():
    return importlib.metadata.version('label-studio')


def get_latest_version():
    """Get version from pypi"""
    pypi_url = 'https://pypi.org/pypi/%s/json' % label_studio.package_name
    try:
        response = requests.get(pypi_url, timeout=10).text
        data = json.loads(response)
        latest_version = data['info']['version']
        upload_time = data.get('releases', {}).get(latest_version, [{}])[-1].get('upload_time', None)
    except Exception:
        logger.warning("Can't get latest version", exc_info=True)
    else:
        return {'latest_version': latest_version, 'upload_time': upload_time}


def current_version_is_outdated(latest_version):
    latest_version = parse_version(latest_version)
    current_version = parse_version(label_studio.__version__)
    return current_version < latest_version


def check_for_the_latest_version(print_message):
    """Check latest pypi version"""
    if not settings.LATEST_VERSION_CHECK:
        return

    import label_studio

    # prevent excess checks by time intervals
    current_time = time.time()
    if label_studio.__latest_version_check_time__ and current_time - label_studio.__latest_version_check_time__ < 60:
        return
    label_studio.__latest_version_check_time__ = current_time

    data = get_latest_version()
    if not data:
        return
    latest_version = data['latest_version']
    outdated = latest_version and current_version_is_outdated(latest_version)

    def update_package_message():
        update_command = 'pip install -U ' + label_studio.package_name
        return boxen(
            'Update available {curr_version}  {latest_version}\nRun {command}'.format(
                curr_version=label_studio.__version__, latest_version=latest_version, command=update_command
            ),
            style='double',
        ).replace(update_command, Fore.CYAN + update_command + Fore.RESET)

    if outdated and print_message:
        print(update_package_message())

    label_studio.__latest_version__ = latest_version
    label_studio.__latest_version_upload_time__ = data['upload_time']
    label_studio.__current_version_is_outdated__ = outdated


# check version ASAP while package loading
# skip notification for uwsgi, as we're running in production ready mode
if settings.APP_WEBSERVER != 'uwsgi':
    check_for_the_latest_version(print_message=True)


def collect_versions(force=False):
    """Collect versions for all modules

    :return: dict with sub-dicts of version descriptions
    """
    import label_studio

    # prevent excess checks by time intervals
    current_time = time.time()
    need_check = current_time - settings.VERSIONS_CHECK_TIME > 300
    settings.VERSIONS_CHECK_TIME = current_time

    if settings.VERSIONS and not force and not need_check:
        return settings.VERSIONS

    # main pypi package
    result = {
        'release': label_studio.__version__,
        'label-studio-os-package': {
            'version': label_studio.__version__,
            'short_version': '.'.join(label_studio.__version__.split('.')[:2]),
            'latest_version_from_pypi': label_studio.__latest_version__,
            'latest_version_upload_time': label_studio.__latest_version_upload_time__,
            'current_version_is_outdated': label_studio.__current_version_is_outdated__,
        },
        # backend full git info
        'label-studio-os-backend': version.get_git_commit_info(ls=True),
    }

    # label studio frontend
    try:
        with open(os.path.join(settings.EDITOR_ROOT, 'version.json')) as f:
            lsf = json.load(f)
        result['label-studio-frontend'] = lsf
    except:  # noqa: E722
        pass

    # data manager
    try:
        with open(os.path.join(settings.DM_ROOT, 'version.json')) as f:
            dm = json.load(f)
        result['dm2'] = dm
    except:  # noqa: E722
        pass

    # converter from label-studio-sdk
    try:
        import label_studio_sdk.converter

        result['label-studio-converter'] = {'version': label_studio_sdk.__version__}
    except Exception:
        pass

    # ml
    try:
        import label_studio_ml

        result['label-studio-ml'] = {'version': label_studio_ml.__version__}
    except Exception:
        pass

    result.update(settings.COLLECT_VERSIONS(result=result))

    for key in result:
        if 'message' in result[key] and len(result[key]['message']) > 70:
            result[key]['message'] = result[key]['message'][0:70] + ' ...'

    if settings.SENTRY_DSN:
        import sentry_sdk

        sentry_sdk.set_context('versions', copy.deepcopy(result))

        for package in result:
            if 'version' in result[package]:
                sentry_sdk.set_tag('version-' + package, result[package]['version'])
            if 'commit' in result[package]:
                sentry_sdk.set_tag('commit-' + package, result[package]['commit'])

    # edition type
    result['edition'] = settings.VERSION_EDITION

    settings.VERSIONS = result
    return result


def get_organization_from_request(request):
    """Helper for backward compatibility with org_pk in session"""
    # TODO remove session logic in next release
    user = request.user
    if user and user.is_authenticated:
        if user.active_organization is None:
            organization_pk = request.session.get('organization_pk')
            if organization_pk:
                user.active_organization_id = organization_pk
                user.save()
                request.session.pop('organization_pk', None)
                request.session.modified = True
        return user.active_organization_id


def load_func(func_string):
    """
    If the given setting is a string import notation,
    then perform the necessary import or imports.
    """
    if func_string is None:
        return None
    elif isinstance(func_string, str):
        return import_from_string(func_string)
    return func_string


def import_from_string(func_string):
    """
    Attempt to import a class from a string representation.
    """
    try:
        return import_string(func_string)
    except ImportError as e:
        msg = f'Could not import {func_string} from settings: {e}'
        raise ImportError(msg)


class temporary_disconnect_signal:
    """Temporarily disconnect a model from a signal

    Example:
        with temporary_disconnect_all_signals(
            signals.post_delete, update_is_labeled_after_removing_annotation, Annotation):
            do_something()
    """

    def __init__(self, signal, receiver, sender, dispatch_uid=None):
        self.signal = signal
        self.receiver = receiver
        self.sender = sender
        self.dispatch_uid = dispatch_uid

    def __enter__(self):
        self.signal.disconnect(receiver=self.receiver, sender=self.sender, dispatch_uid=self.dispatch_uid)

    def __exit__(self, type_, value, traceback):
        self.signal.connect(receiver=self.receiver, sender=self.sender, dispatch_uid=self.dispatch_uid)


class temporary_disconnect_all_signals(object):
    def __init__(self, disabled_signals=None):
        self.stashed_signals = defaultdict(list)
        self.disabled_signals = disabled_signals or [
            pre_init,
            post_init,
            pre_save,
            post_save,
            pre_delete,
            post_delete,
            pre_migrate,
            post_migrate,
        ]

    def __enter__(self):
        for signal in self.disabled_signals:
            self.disconnect(signal)

    def __exit__(self, exc_type, exc_val, exc_tb):
        for signal in list(self.stashed_signals):
            self.reconnect(signal)

    def disconnect(self, signal):
        self.stashed_signals[signal] = signal.receivers
        signal.receivers = []

    def reconnect(self, signal):
        signal.receivers = self.stashed_signals.get(signal, [])
        del self.stashed_signals[signal]


class DjangoFilterDescriptionInspector(CoreAPICompatInspector):
    def get_filter_parameters(self, filter_backend):
        if isinstance(filter_backend, DjangoFilterBackend):
            result = super(DjangoFilterDescriptionInspector, self).get_filter_parameters(filter_backend)
            if not isinstance(result, Iterable):
                return result

            for param in result:
                if not param.get('description', ''):
                    param.description = 'Filter the returned list by {field_name}'.format(field_name=param.name)

            return result

        return NotHandled


def batch(iterable, n=1):
    l = len(iterable)  # noqa: E741
    for ndx in range(0, l, n):
        yield iterable[ndx : min(ndx + n, l)]


def round_floats(o):
    if isinstance(o, float):
        return round(o, 2)
    if isinstance(o, dict):
        return {k: round_floats(v) for k, v in o.items()}
    if isinstance(o, (list, tuple)):
        return [round_floats(x) for x in o]
    return o


class temporary_disconnect_list_signal:
    """Temporarily disconnect a list of signals
    Each signal tuple: (signal_type, signal_method, object)
    Example:
        with temporary_disconnect_list_signal(
            [(signals.post_delete, update_is_labeled_after_removing_annotation, Annotation)]
            ):
            do_something()
    """

    def __init__(self, signals):
        self.signals = signals

    def __enter__(self):
        for signal in self.signals:
            sig = signal[0]
            receiver = signal[1]
            sender = signal[2]
            dispatch_uid = signal[3] if len(signal) > 3 else None
            sig.disconnect(receiver=receiver, sender=sender, dispatch_uid=dispatch_uid)

    def __exit__(self, type_, value, traceback):
        for signal in self.signals:
            sig = signal[0]
            receiver = signal[1]
            sender = signal[2]
            dispatch_uid = signal[3] if len(signal) > 3 else None
            sig.connect(receiver=receiver, sender=sender, dispatch_uid=dispatch_uid)


def trigram_migration_operations(next_step):
    ops = [
        TrigramExtension(),
        next_step,
    ]
    SKIP_TRIGRAM_EXTENSION = get_env('SKIP_TRIGRAM_EXTENSION', None)
    if SKIP_TRIGRAM_EXTENSION == '1' or SKIP_TRIGRAM_EXTENSION == 'yes' or SKIP_TRIGRAM_EXTENSION == 'true':
        ops = [next_step]
    if SKIP_TRIGRAM_EXTENSION == 'full':
        ops = []

    return ops


def btree_gin_migration_operations(next_step):
    ops = [
        BtreeGinExtension(),
        next_step,
    ]
    SKIP_BTREE_GIN_EXTENSION = get_env('SKIP_BTREE_GIN_EXTENSION', None)
    if SKIP_BTREE_GIN_EXTENSION == '1' or SKIP_BTREE_GIN_EXTENSION == 'yes' or SKIP_BTREE_GIN_EXTENSION == 'true':
        ops = [next_step]
    if SKIP_BTREE_GIN_EXTENSION == 'full':
        ops = []

    return ops


def merge_labels_counters(dict1, dict2):
    """
    Merge two dictionaries with nested dictionary values into a single dictionary.

    Args:
        dict1 (dict): The first dictionary to merge.
        dict2 (dict): The second dictionary to merge.

    Returns:
        dict: A new dictionary with the merged nested dictionaries.

    Example:
        dict1 = {'sentiment': {'Negative': 1, 'Positive': 1}}
        dict2 = {'sentiment': {'Positive': 2, 'Neutral': 1}}
        result_dict = merge_nested_dicts(dict1, dict2)
        # {'sentiment': {'Negative': 1, 'Positive': 3, 'Neutral': 1}}
    """
    result_dict = {}

    # iterate over keys in both dictionaries
    for key in set(dict1.keys()) | set(dict2.keys()):
        # add the corresponding values if they exist in both dictionaries
        value = {}
        if key in dict1:
            value.update(dict1[key])
        if key in dict2:
            for subkey in dict2[key]:
                value[subkey] = value.get(subkey, 0) + dict2[key][subkey]
        # add the key-value pair to the result dictionary
        result_dict[key] = value

    return result_dict


def timeit(func):
    def wrapper(*args, **kwargs):
        start = time.time()
        result = func(*args, **kwargs)
        end = time.time()
        logging.debug(f'{func.__name__} execution time: {end-start} seconds')
        return result

    return wrapper


def empty(*args, **kwargs):
    pass


def get_ttl_hash(seconds: int = 60) -> int:
    """Return the same value within `seconds` time period"""
    return round(time.time() / seconds)
</file>

<file path="label_studio/core/utils/contextlog.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""

import calendar
import io
import json
import logging
import os
import platform
import sys
import threading
from datetime import datetime
from uuid import uuid4

import requests
from django.conf import settings

from .common import get_app_version, get_client_ip
from .io import find_file, get_config_dir

logger = logging.getLogger(__name__)


def _load_log_payloads():
    try:
        all_urls_file = find_file('all_urls.json')
        with open(all_urls_file) as f:
            log_payloads = json.load(f)
    except Exception as exc:
        logger.error(exc)
        return None
    out = {}
    for item in log_payloads:
        out[item['name']] = {
            'exclude_from_logs': item.get('exclude_from_logs', False),
            'log_payloads': item.get('log_payloads'),
        }
    return out


class ContextLog(object):

    _log_payloads = _load_log_payloads()

    def __init__(self):
        self.version = get_app_version()
        self.server_id = self._get_server_id()

    def _get_server_id(self):
        user_id_file = os.path.join(get_config_dir(), 'user_id')
        if not os.path.exists(user_id_file):
            user_id = str(uuid4())
            try:
                with io.open(user_id_file, mode='w', encoding='utf-8') as fout:
                    fout.write(user_id)
            except OSError:
                return 'np-' + user_id  # not persistent user id
        else:
            with io.open(user_id_file, encoding='utf-8') as f:
                user_id = f.read()
        return user_id

    def _is_docker(self):
        path = '/proc/self/cgroup'
        return (
            os.path.exists('/.dockerenv')
            or os.path.isfile(path)
            and any('docker' in line for line in open(path, encoding='utf-8'))
        )

    def _get_timestamp_now(self):
        return calendar.timegm(datetime.now().utctimetuple())

    def _get_response_content(self, response):
        try:
            return json.loads(response.content)
        except:  # noqa: E722
            return

    def _assert_field_in_test(self, field, payload, view_name):
        if settings.TEST_ENVIRONMENT:
            assert field in payload, f'The field "{field}" should be presented for "{view_name}"'

    def _assert_type_in_test(self, type, payload, view_name):
        if settings.TEST_ENVIRONMENT:
            assert isinstance(payload, type), f'The type of payload is not "{type}" for "{view_name}"'

    def _get_fields(self, view_name, payload, fields):
        out = {}
        for field in fields:
            self._assert_field_in_test(field, payload, view_name)
            out[field] = payload.get(field)
        if not out:
            return None
        return out

    def _secure_data(self, payload, request):
        view_name = payload['view_name']

        if view_name in ('user-signup', 'user-login') and payload['method'] == 'POST':
            payload['json'] = None

        if payload['status_code'] < 200 or payload['status_code'] > 299:
            if payload['status_code'] >= 400:
                payload['json'] = None
            return

        # ======== CUSTOM ======
        if view_name == 'data_manager:dm-actions' and payload['values'].get('id') == 'next_task':
            self._assert_type_in_test(dict, payload['response'], view_name)
            new_response = {}
            self._assert_field_in_test('drafts', payload['response'], view_name)
            new_response['drafts'] = (
                len(payload['response']['drafts'])
                if isinstance(payload['response']['drafts'], list)
                else payload['response']['drafts']
            )
            for key in [
                'id',
                'inner_id',
                'cancelled_annotations',
                'total_annotations',
                'total_predictions',
                'updated_by',
                'created_at',
                'updated_at',
                'overlap',
                'comment_count',
                'unresolved_comment_count',
                'last_comment_updated_at',
                'project',
                'comment_authors',
                'queue',
            ]:
                self._assert_field_in_test(key, payload['response'], view_name)
                new_response[key] = payload['response'][key]
            payload['response'] = new_response
            return

        if view_name == 'user-list' and payload['method'] == 'GET':
            self._assert_type_in_test(list, payload['response'], view_name)
            payload['response'] = {'count': len(payload['response'])}
            return

        if view_name == 'projects:api-templates:template-list' and payload['method'] == 'GET':
            self._assert_type_in_test(list, payload['response'].get('templates'), view_name)
            payload['response']['templates'] = [t['title'] for t in payload['response']['templates']]
            return

        if view_name == 'data_manager:dm-actions' and payload['method'] == 'GET':
            self._assert_type_in_test(list, payload['response'], view_name)
            payload['response'] = [item.get('id') for item in payload['response']]
            return

        if view_name == 'data_manager:dm-columns' and payload['method'] == 'GET':
            self._assert_field_in_test('columns', payload['response'], view_name)
            payload['response']['columns'] = [item.get('id') for item in payload['response']['columns']]
            return

        if view_name == 'data_export:api-projects:project-export-formats' and payload['method'] == 'GET':
            self._assert_type_in_test(list, payload['response'], view_name)
            payload['response'] = [item.get('title') for item in payload['response']]
            return

        if (
            (view_name == 'tasks:api:task-annotations' and payload['method'] in 'POST')
            or (view_name == 'tasks:api-annotations:annotation-detail' and payload['method'] == 'PATCH')
            or (view_name == 'tasks:api:task-annotations-drafts' and payload['method'] == 'POST')
            or (view_name == 'tasks:api-drafts:draft-detail' and payload['method'] == 'PATCH')
        ):
            self._assert_field_in_test('lead_time', payload['json'], view_name)
            self._assert_field_in_test('result', payload['json'], view_name)
            self._assert_type_in_test(list, payload['json']['result'], view_name)
            payload['json']['result'] = [
                self._get_fields(view_name, item, ('from_name', 'to_name', 'type', 'origin'))
                for item in payload['json']['result']
            ]

        # ======== DEFAULT ======
        log_payloads = self._log_payloads.get(view_name)

        if not log_payloads or not log_payloads.get('log_payloads'):
            return

        log_payloads = log_payloads['log_payloads']
        for payload_key in log_payloads:
            if not payload.get(payload_key):
                payload[payload_key] = None
                continue
            log_fields = log_payloads[payload_key].get(payload['method'])
            if log_fields is not None:
                payload[payload_key] = self._get_fields(view_name, payload[payload_key], log_fields)

    def _exclude_endpoint(self, request):
        if request.resolver_match and request.resolver_match.view_name:
            view_name = request.resolver_match.view_name
            if view_name not in self._log_payloads:
                return True
            if self._log_payloads[view_name].get('exclude_from_logs'):
                return True
        if request.GET.get('interaction', None) == 'timer':
            return True

    def dont_send(self, request):
        return not settings.COLLECT_ANALYTICS or self._exclude_endpoint(request)

    def send(self, request=None, response=None, body=None):
        if self.dont_send(request):
            return
        try:
            payload = self.create_payload(request, response, body)
        except Exception as exc:
            logger.debug(exc, exc_info=True)
            if settings.TEST_ENVIRONMENT:
                raise
        else:
            if settings.TEST_ENVIRONMENT:
                pass
            elif settings.DEBUG_CONTEXTLOG:
                logger.debug('In DEBUG mode, contextlog is not sent.')
                logger.debug(json.dumps(payload, indent=2))
            elif settings.CONTEXTLOG_SYNC:
                self.send_job(request, response, body)
            else:
                thread = threading.Thread(target=self.send_job, args=(request, response, body))
                thread.start()

    @staticmethod
    def browser_exists(request):
        return (
            hasattr(request, 'user_agent')
            and request.user_agent
            and hasattr(request.user_agent, 'browser')
            and request.user_agent.browser
        )

    def create_payload(self, request, response, body):
        advanced_json = None
        user_id, user_email = None, None
        if hasattr(request, 'user') and hasattr(request.user, 'id'):
            user_id = request.user.id
            if hasattr(request.user, 'email'):
                user_email = request.user.email
        if hasattr(request, 'advanced_json'):
            advanced_json = request.advanced_json
        elif hasattr(request, 'user') and hasattr(request.user, 'advanced_json'):
            advanced_json = request.user.advanced_json

        url = request.build_absolute_uri()
        view_name = request.resolver_match.view_name if request.resolver_match else None
        metrics_payload = request.GET.get('__')
        is_metrics_payload = view_name == 'collect_metrics' and metrics_payload is not None

        if is_metrics_payload:
            values = json.loads(metrics_payload)
        else:
            values = request.GET.dict()

        # If the values contains url use it as the url, otherwise use the absolute uri
        if is_metrics_payload and 'url' in values:
            url = values.pop('url')

        # If this is a metrics payload, we will add the namespace and view name
        # to describe the payload as an event payload
        if is_metrics_payload:
            namespace = 'collect_metrics'
            view_name = f'event:{values.pop("event")}'
            status_code = 200
            content_type = None
            response_content = None
        else:
            namespace = request.resolver_match.namespace if request.resolver_match else None
            status_code = response.status_code
            content_type = getattr(response, 'content_type', None)
            response_content = self._get_response_content(response)

        payload = {
            'url': url,
            'server_id': self.server_id,
            'user_id': user_id,
            'user_email': user_email,
            'server_time': self._get_timestamp_now(),
            'session_id': request.session.get('uid', None),
            'client_ip': get_client_ip(request),
            'is_docker': self._is_docker(),
            'python': str(sys.version_info[0]) + '.' + str(sys.version_info[1]),
            'version': self.version,
            'view_name': view_name,
            'namespace': namespace,
            'scheme': request.scheme,
            'method': request.method,
            'values': values,
            'json': body,
            'advanced_json': advanced_json,
            'language': request.LANGUAGE_CODE,
            'content_type': content_type,
            'content_length': (
                int(request.environ.get('CONTENT_LENGTH')) if request.environ.get('CONTENT_LENGTH') else None
            ),
            'status_code': status_code,
            'response': response_content,
        }
        if self.browser_exists(request):
            payload.update(
                {
                    'is_mobile': request.user_agent.is_mobile,
                    'is_tablet': request.user_agent.is_tablet,
                    'is_touch_capable': request.user_agent.is_touch_capable,
                    'is_pc': request.user_agent.is_pc,
                    'is_bot': request.user_agent.is_bot,
                    'browser': request.user_agent.browser.family,
                    'browser_version': request.user_agent.browser.version_string,
                    'os': request.user_agent.os.family,
                    'platform_system': platform.system(),
                    'platform_release': platform.release(),
                    'os_version': request.user_agent.os.version_string,
                    'device': request.user_agent.device.family,
                }
            )
        self._secure_data(payload, request)
        for key in ('json', 'response', 'values'):
            payload[key] = payload[key] or None
        return payload

    def send_job(self, request, response, body):
        try:
            payload = self.create_payload(request, response, body)
        except:  # noqa: E722
            pass
        else:
            try:
                url = 'https://tele.labelstud.io'
                requests.post(url=url, json=payload, timeout=3.0)
            except:  # noqa: E722
                pass
</file>

<file path="label_studio/core/utils/db.py">
import logging
from typing import Optional, TypeVar

from django.db import models
from django.db.models import Model, QuerySet, Subquery

logger = logging.getLogger(__name__)


class SQCount(Subquery):
    template = '(SELECT count(*) FROM (%(subquery)s) _count)'
    output_field = models.IntegerField()


ModelType = TypeVar('ModelType', bound=Model)


def fast_first(queryset: QuerySet[ModelType]) -> Optional[ModelType]:
    """Replacement for queryset.first() when you don't need ordering,
    queryset.first() works slowly in some cases
    """

    if result := queryset[:1]:
        return result[0]
    return None


def fast_first_or_create(model, **model_params) -> Optional[ModelType]:
    """Like get_or_create, but using fast_first instead of first(). Additionally, unlike get_or_create, this method will not raise an exception if more than one model instance matching the given params is returned, making it a safer choice than get_or_create for models that don't have a uniqueness constraint on the fields used."""
    if instance := fast_first(model.objects.filter(**model_params)):
        return instance
    return model.objects.create(**model_params)
</file>

<file path="label_studio/core/utils/exceptions.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
from rest_framework import status
from rest_framework.exceptions import APIException


class LabelStudioError(Exception):
    pass


class LabelStudioAPIException(APIException):
    status_code = status.HTTP_500_INTERNAL_SERVER_ERROR
    default_detail = 'Unknown error'


class LabelStudioDatabaseException(LabelStudioAPIException):
    default_detail = 'Error executing database query'


class LabelStudioDatabaseLockedException(LabelStudioAPIException):
    default_detail = "Sqlite <a href='https://docs.djangoproject.com/en/3.1/ref/databases/#database-is-locked-errors'>doesn't operate well</a> on multiple transactions. \
    Please be patient and try update your pages, or ping us on Slack to  get more about production-ready db"


class ProjectExistException(LabelStudioAPIException):
    status_code = status.HTTP_422_UNPROCESSABLE_ENTITY
    default_detail = 'Project with the same title already exists'


class InvalidUploadUrlError(LabelStudioAPIException):
    default_detail = (
        'The provided URL was not valid. URLs must begin with http:// or https://, and cannot be local IPs.'
    )
    status_code = status.HTTP_403_FORBIDDEN
</file>

<file path="label_studio/core/utils/formatter.py">
from pythonjsonlogger import jsonlogger

from label_studio.core.current_request import get_current_request


class CustomJsonFormatter(jsonlogger.JsonFormatter):
    def add_fields(self, log_record, record, message_dict):
        super(CustomJsonFormatter, self).add_fields(log_record, record, message_dict)
        request_id = None
        request = get_current_request()
        if request and 'X-Request-ID' in request.headers:
            request_id = request.headers['X-Request-ID']
        log_record['request_id'] = request_id
</file>

<file path="label_studio/core/utils/io.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import glob
import importlib
import io
import ipaddress
import itertools
import os
import shutil
import socket
from contextlib import contextmanager
from tempfile import mkdtemp, mkstemp

import requests
import ujson as json
import yaml
from appdirs import user_cache_dir, user_config_dir, user_data_dir
from django.conf import settings
from django.core.files.temp import NamedTemporaryFile
from urllib3.util import parse_url

# full path import results in unit test failures
from .exceptions import InvalidUploadUrlError

_DIR_APP_NAME = 'label-studio'


def good_path(path):
    return os.path.abspath(os.path.expanduser(path))


def find_node(package_name, node_path, node_type):
    assert node_type in ('dir', 'file', 'any')
    basedir = importlib.resources.files(package_name).joinpath('')
    node_path = os.path.join(*node_path.split('/'))  # linux to windows compatibility
    search_by_path = '/' in node_path or '\\' in node_path

    for path, dirs, filenames in os.walk(basedir):
        if node_type == 'file':
            nodes = filenames
        elif node_type == 'dir':
            nodes = dirs
        else:
            nodes = filenames + dirs
        if search_by_path:
            for found_node in nodes:
                found_node = os.path.join(path, found_node)
                if found_node.endswith(node_path):
                    return found_node
        elif node_path in nodes:
            return os.path.join(path, node_path)
    else:
        raise IOError('Could not find "%s" at package "%s"' % (node_path, basedir))


def find_file(file):
    return find_node('label_studio', file, 'file')


def find_dir(directory):
    return find_node('label_studio', directory, 'dir')


@contextmanager
def get_temp_file():
    fd, path = mkstemp()
    yield path
    os.close(fd)


@contextmanager
def get_temp_dir():
    dirpath = mkdtemp()
    yield dirpath
    shutil.rmtree(dirpath)


def get_config_dir():
    config_dir = user_config_dir(appname=_DIR_APP_NAME)
    try:
        os.makedirs(config_dir, exist_ok=True)
    except OSError:
        pass
    return config_dir


def get_data_dir():
    data_dir = user_data_dir(appname=_DIR_APP_NAME)
    os.makedirs(data_dir, exist_ok=True)
    return data_dir


def get_cache_dir():
    cache_dir = user_cache_dir(appname=_DIR_APP_NAME)
    os.makedirs(cache_dir, exist_ok=True)
    return cache_dir


def delete_dir_content(dirpath):
    for f in glob.glob(dirpath + '/*'):
        remove_file_or_dir(f)


def remove_file_or_dir(path):
    if os.path.isfile(path):
        os.remove(path)
    elif os.path.isdir(path):
        shutil.rmtree(path)


def get_all_files_from_dir(d):
    out = []
    for name in os.listdir(d):
        filepath = os.path.join(d, name)
        if os.path.isfile(filepath):
            out.append(filepath)
    return out


def iter_files(root_dir, ext):
    for root, _, files in os.walk(root_dir):
        for f in files:
            if f.lower().endswith(ext):
                yield os.path.join(root, f)


def json_load(file, int_keys=False):
    with io.open(file, encoding='utf8') as f:
        data = json.load(f)
        if int_keys:
            return {int(k): v for k, v in data.items()}
        else:
            return data


def read_yaml(filepath):
    if not os.path.exists(filepath):
        filepath = find_file(filepath)
    with io.open(filepath, encoding='utf-8') as f:
        data = yaml.load(f, Loader=yaml.FullLoader)  # nosec
    return data


def path_to_open_binary_file(filepath) -> io.BufferedReader:
    """
    Copy the file at filepath to a named temporary file and return that file object.
    Unusually, this function deliberately doesn't close the file; the caller is responsible for this.
    """
    tmp = NamedTemporaryFile()
    shutil.copy2(filepath, tmp.name)
    return tmp


def get_all_dirs_from_dir(d):
    out = []
    for name in os.listdir(d):
        filepath = os.path.join(d, name)
        if os.path.isdir(filepath):
            out.append(filepath)
    return out


class SerializableGenerator(list):
    """Generator that is serializable by JSON"""

    def __init__(self, iterable):
        tmp_body = iter(iterable)
        try:
            self._head = iter([next(tmp_body)])
            self.append(tmp_body)
        except StopIteration:
            self._head = []

    def __iter__(self):
        return itertools.chain(self._head, *self[:1])


def validate_upload_url(url, block_local_urls=True):
    """Utility function for defending against SSRF attacks. Raises
        - InvalidUploadUrlError if the url is not HTTP[S], or if block_local_urls is enabled
          and the URL resolves to a local address.
        - LabelStudioApiException if the hostname cannot be resolved

    :param url: Url to be checked for validity/safety,
    :param block_local_urls: Whether urls that resolve to local/private networks should be allowed.
    """

    parsed_url = parse_url(url)

    if parsed_url.scheme not in ('http', 'https'):
        raise InvalidUploadUrlError

    domain = parsed_url.host
    try:
        ip = socket.gethostbyname(domain)
    except socket.error:
        from core.utils.exceptions import LabelStudioAPIException

        raise LabelStudioAPIException(f"Can't resolve hostname {domain}")

    if block_local_urls:
        validate_ip(ip)


def validate_ip(ip: str) -> None:
    """If settings.USE_DEFAULT_BANNED_SUBNETS is True, this function checks
    if an IP is reserved for any of the reasons in
    https://en.wikipedia.org/wiki/Reserved_IP_addresses
    and raises an exception if so. Additionally, if settings.USER_ADDITIONAL_BANNED_SUBNETS
    is set, it will also check against those subnets.

    If settings.USE_DEFAULT_BANNED_SUBNETS is False, this function will only check
    the IP against settings.USER_ADDITIONAL_BANNED_SUBNETS. Turning off the default
    subnets is **risky** and should only be done if you know what you're doing.

    :param ip: IP address to be checked.
    """

    default_banned_subnets = [
        '0.0.0.0/8',  # current network
        '10.0.0.0/8',  # private network
        '100.64.0.0/10',  # shared address space
        '127.0.0.0/8',  # loopback
        '169.254.0.0/16',  # link-local
        '172.16.0.0/12',  # private network
        '192.0.0.0/24',  # IETF protocol assignments
        '192.0.2.0/24',  # TEST-NET-1
        '192.88.99.0/24',  # Reserved, formerly ipv6 to ipv4 relay
        '192.168.0.0/16',  # private network
        '198.18.0.0/15',  # network interconnect device benchmark testing
        '198.51.100.0/24',  # TEST-NET-2
        '203.0.113.0/24',  # TEST-NET-3
        '224.0.0.0/4',  # multicast
        '233.252.0.0/24',  # MCAST-TEST-NET
        '240.0.0.0/4',  # reserved for future use
        '255.255.255.255/32',  # limited broadcast
        '::/128',  # unspecified address
        '::1/128',  # loopback
        '::ffff:0:0/96',  # IPv4-mapped address
        '::ffff:0:0:0/96',  # IPv4-translated address
        '64:ff9b::/96',  # IPv4/IPv6 translation
        '64:ff9b:1::/48',  # IPv4/IPv6 translation
        '100::/64',  # discard prefix
        '2001:0000::/32',  # Teredo tunneling
        '2001:20::/28',  # ORCHIDv2
        '2001:db8::/32',  # documentation
        '2002::/16',  # 6to4
        'fc00::/7',  # unique local
        'fe80::/10',  # link-local
        'ff00::/8',  # multicast
    ]

    banned_subnets = [
        *(default_banned_subnets if settings.USE_DEFAULT_BANNED_SUBNETS else []),
        *(settings.USER_ADDITIONAL_BANNED_SUBNETS or []),
    ]

    for subnet in banned_subnets:
        if ipaddress.ip_address(ip) in ipaddress.ip_network(subnet):
            raise InvalidUploadUrlError(f'URL resolves to a reserved network address (block: {subnet})')


def ssrf_safe_get(url, *args, **kwargs):
    validate_upload_url(url, block_local_urls=settings.SSRF_PROTECTION_ENABLED)
    # Reason for #nosec: url has been validated as SSRF safe by the
    # validation check above.
    response = requests.get(url, *args, **kwargs)   # nosec

    # second check for SSRF for prevent redirect and dns rebinding attacks
    if settings.SSRF_PROTECTION_ENABLED:
        response_ip = response.raw._connection.sock.getpeername()[0]
        validate_ip(response_ip)
    return response
</file>

<file path="label_studio/core/utils/mail.py">
import ssl

from django.core.mail.backends.smtp import EmailBackend
from django.utils.functional import cached_property


class NoVerificationEmailBackend(EmailBackend):
    """SMTP email backend that does not verify SSL certificates or hostname
    if no certfile or keyfile is provided. This is equivalent to the behavior
    of Django's smtp.EmailBackend prior to Django 4. If EmailBackend
    works for you, prefer that as it's more secure than this.
    """

    @cached_property
    def ssl_context(self):
        if self.ssl_certfile or self.ssl_keyfile:
            ssl_context = ssl.SSLContext(protocol=ssl.PROTOCOL_TLS_CLIENT)
            ssl_context.load_cert_chain(self.ssl_certfile, self.ssl_keyfile)
            return ssl_context
        else:
            ssl_context = ssl.create_default_context()
            ssl_context.check_hostname = False
            ssl_context.verify_mode = ssl.CERT_NONE
            return ssl_context
</file>

<file path="label_studio/core/utils/manifest_assets.py">
import json
from pathlib import Path

from django.conf import settings

# Load manifest.json once at module scope
_MANIFEST = {}
try:
    # If HMR is enabled, we don't need to read the manifest as it's not used
    # All assets are served from the webpack dev server in that case
    if not settings.FRONTEND_HMR:
        manifest_path = Path(settings.STATIC_ROOT) / 'js/manifest.json'
        if manifest_path.exists():
            with open(manifest_path, 'r') as f:
                _MANIFEST = json.load(f)
except Exception:
    # If there's any error reading the manifest, we'll use the default mapping
    pass


def get_manifest_asset(path: str) -> str:
    """Maps a path to its hashed filename using manifest.json, or falls back to /react-app/ prefix

    Usage in template:
    {% manifest_asset 'main.js' %}
    """
    if path in _MANIFEST:
        return f'{settings.FRONTEND_HOSTNAME}{_MANIFEST[path]}'
    return f'{settings.FRONTEND_HOSTNAME}/react-app/{path}'
</file>

<file path="label_studio/core/utils/openapi_extensions.py">
from django.conf import settings
from drf_yasg.inspectors import SwaggerAutoSchema


class XVendorExtensionsAutoSchema(SwaggerAutoSchema):
    allowed_extensions = tuple([e.replace('-', '_') for e in settings.X_VENDOR_OPENAPI_EXTENSIONS])

    def get_operation(self, operation_keys=None):
        operation = super(XVendorExtensionsAutoSchema, self).get_operation(operation_keys)
        for key, value in self.overrides.items():
            if key.startswith(self.allowed_extensions):
                operation[key.replace('_', '-')] = value
        return operation
</file>

<file path="label_studio/core/utils/params.py">
import os
from typing import Callable, Optional, Sequence, TypeVar

from rest_framework.exceptions import ValidationError


def cast_bool_from_str(value):
    if isinstance(value, str):
        if value.lower() in ['true', 'yes', 'on', '1']:
            value = True
        elif value.lower() in ['false', 'no', 'not', 'off', '0']:
            value = False
        else:
            raise ValueError(f'Incorrect bool value "{value}". ' f'It should be one of [1, 0, true, false, yes, no]')
    return value


def bool_from_request(params, key, default):
    """Get boolean value from request GET, POST, etc

    :param params: dict POST, GET, etc
    :param key: key to find
    :param default: default value
    :return: boolean
    """
    value = params.get(key, default)

    try:
        if isinstance(value, str):
            value = cast_bool_from_str(value)
        return bool(int(value))
    except Exception as e:
        raise ValidationError({key: str(e)})


def int_from_request(params, key, default):
    """Get integer from request GET, POST, etc

    :param params: dict POST, GET, etc
    :param key: key to find
    :param default: default value
    :return: int
    """
    value = params.get(key, default)

    # str
    if isinstance(value, str):
        try:
            return int(value)
        except ValueError:
            raise ValidationError({key: f'Incorrect value in key "{key}" = "{value}". It should be digit string.'})
        except Exception as e:
            raise ValidationError({key: str(e)})
    # int
    elif isinstance(value, int):
        return value
    # other
    else:
        raise ValidationError(
            {key: f'Incorrect value type in key "{key}" = "{value}". ' f'It should be digit string or integer.'}
        )


def float_from_request(params, key, default):
    """Get float from request GET, POST, etc

    :param params: dict POST, GET, etc
    :param key: key to find
    :param default: default value
    :return: float
    """
    value = params.get(key, default)

    # str
    if isinstance(value, str):
        try:
            return float(value)
        except ValueError:
            raise ValidationError({key: f'Incorrect value in key "{key}" = "{value}". It should be digit string.'})
    # float
    elif isinstance(value, float) or isinstance(value, int):
        return float(value)
    # other
    else:
        raise ValidationError(
            {key: f'Incorrect value type in key "{key}" = "{value}". ' f'It should be digit string or float.'}
        )


def list_of_strings_from_request(params, key, default):
    """Get list of strings from request GET, POST, etc

    :param params: dict POST, GET, etc
    :param key: key to find
    :param default: default value
    :return: float
    """
    value = params.get(key, default)
    if value is None:
        return
    splitters = (',', ';', '|')
    # str
    if isinstance(value, str):
        for splitter in splitters:
            if splitter in value:
                return value.split(splitter)
        return [value]
    else:
        raise ValidationError(
            {key: f'Incorrect value type in key "{key}" = "{value}". ' f'It should be digit string or float.'}
        )


def get_env(name, default=None, is_bool=False):
    for env_key in ['LABEL_STUDIO_' + name, 'HEARTEX_' + name, name]:
        value = os.environ.get(env_key)
        if value is not None:
            if is_bool:
                return bool_from_request(os.environ, env_key, default)
            else:
                return value
    return default


def get_bool_env(key, default):
    return get_env(key, default, is_bool=True)


T = TypeVar('T')


def get_env_list(
    key: str, default: Optional[Sequence[T]] = None, value_transform: Callable[[str], T] = str
) -> Sequence[T]:
    """
    "foo,bar,baz" in env variable => ["foo", "bar", "baz"] in python.
    Use value_transform to convert the strings to any other type.
    """
    value = get_env(key)
    if not value:
        if default is None:
            return []
        return default

    return [value_transform(el) for el in value.split(',')]


def get_env_list_int(key, default=None) -> Sequence[int]:
    return get_env_list(key, default=default, value_transform=int)


def get_all_env_with_prefix(prefix=None, is_bool=True, default_value=None):
    out = {}
    for key in os.environ.keys():
        if not key.startswith(prefix):
            continue
        if is_bool:
            out[key] = bool_from_request(os.environ, key, default_value)
        else:
            out[key] = os.environ[key]
    return out
</file>

<file path="label_studio/core/utils/secret_key.py">
import logging
import os
import sys

import environ
from django.core.management.utils import get_random_secret_key

logger = logging.getLogger(__name__)


def is_collectstatic() -> bool:
    for arg in sys.argv:
        if 'collectstatic' in arg:
            return True

    return False


def generate_secret_key_if_missing(data_dir: str) -> str:
    env_key = 'SECRET_KEY'
    env = environ.Env()
    env_filepath = os.path.join(data_dir, '.env')
    environ.Env.read_env(env_filepath)

    if existing_secret := env.str(env_key, ''):
        return existing_secret

    logger.warning(f'Warning: {env_key} not found in environment variables. Will generate a random key.')
    new_secret = get_random_secret_key()

    if is_collectstatic():
        logger.info(
            'Random SECRET_KEY was generated, but it is not being persisted because this is a collectstatic run'
        )
        return new_secret

    try:
        with open(env_filepath, 'a') as f:
            f.write(f'\n{env_key}={new_secret}\n')   # nosec
    except Exception as e:
        logger.warning(
            f'Warning: failed to write {env_key} to .env file: {e}, new key will be regenerated on every '
            f'server restart. If this key is used for signing, it will invalidate all existing sessions '
            f'or tokens. Please set {env_key} in your environment variables to avoid this warning.'
        )

    os.environ[env_key] = new_secret
    return new_secret
</file>

<file path="label_studio/core/utils/sentry.py">
from django.conf import settings


def event_processor(event, hint):
    # skip all transactions without exceptions, unless it's a log record
    if 'exc_info' not in hint:
        # special flag inside of logger.error(..., extra={'sentry_force': True}) to force sentry to log the error
        if 'log_record' in hint and event.get('extra', {}).get('sentry_force', False):
            return event

        return None

    # skip specified exceptions
    exceptions = event.get('exception', {}).get('values', [{}])
    last = exceptions[-1]
    if last.get('type') in settings.SENTRY_IGNORED_EXCEPTIONS:
        return None

    # sentry ignored factory class
    if 'SentryIgnored' in last.get('type'):
        return None

    if last.get('type') == 'OperationalError':
        value = last.get('value')
        messages = [
            'sorry, too many clients already',
            'Name or service not known',
            'could not connect to server',
            'the database system is shutting down',
            'remaining connection slots are reserved for non-replication superuser connections',
            'unable to open database file',
        ]
        for message in messages:
            if message in value:
                return None

    if last.get('type') == 'OSError':
        value = last.get('value')
        messages = [
            'Too many open files: ',
        ]
        for message in messages:
            if message in value:
                return None

    # special flag inside of logger.error(..., extra={'sentry_skip': True}) to skip error message
    if event.get('extra', {}).get('sentry_skip', False):
        return None

    # skip transactions by urls
    if event.get('transaction') in [
        '/static/{path}',
        '/dm/{path}',
        '/react-app/{path}',
        '/label-studio-frontend/{path}',
        '/favicon.ico',
        '/health',
    ]:
        return None

    return event  # to return all other events


def init_sentry(release_name, release_version):
    if settings.SENTRY_DSN:
        import sentry_sdk
        from sentry_sdk.integrations.django import DjangoIntegration

        if settings.SENTRY_REDIS_ENABLED:
            from sentry_sdk.integrations.redis import RedisIntegration
            from sentry_sdk.integrations.rq import RqIntegration

            advanced = [RedisIntegration(), RqIntegration()]
        else:
            advanced = []

        # define the event processor, this runs before before_send if enabled
        sentry_sdk.scope.add_global_event_processor(event_processor)

        sentry_sdk.init(
            dsn=settings.SENTRY_DSN,
            integrations=[DjangoIntegration()] + advanced,
            traces_sample_rate=settings.SENTRY_RATE,
            send_default_pii=True,
            environment=settings.SENTRY_ENVIRONMENT,
            release=release_name + '@' + str(release_version),
        )
</file>

<file path="label_studio/core/utils/static_serve.py">
"""
Views and functions for serving static files. These are only to be used
during development, and SHOULD NOT be used in a production setting.
"""

import mimetypes
import posixpath
from pathlib import Path

from core.utils.manifest_assets import get_manifest_asset
from django.http import (
    Http404,
    HttpResponseNotModified,
)
from django.utils._os import safe_join
from django.utils.http import http_date
from django.utils.translation import gettext as _
from django.views.static import was_modified_since
from ranged_fileresponse import RangedFileResponse


def serve(request, path, document_root=None, show_indexes=False, manifest_asset_prefix=None):
    """
    Serve static files below a given point in the directory structure.

    To use, put a URL pattern such as::

        from django.views.static import serve

        path('<path:path>', serve, {'document_root': '/path/to/my/files/'})

    in your URLconf. You must provide the ``document_root`` param. You may
    also set ``show_indexes`` to ``True`` if you'd like to serve a basic index
    of the directory.  This index view will use the template hardcoded below,
    but if you'd like to override it, you can create a template called
    ``static/directory_index.html``.

    If manifest_asset_prefix is provided, we will try to serve the file from the manifest.json
    if the file is not found in the document_root.

    Example:
        path = "main.js"
        document_root = "/dist/apps/labelstudio/"
        manifest_asset_prefix = "react-app"
        manifest_json = {"main.js": "/react-app/main.123456.js"}
        fullpath = Path(safe_join(document_root, "main.123456.js"))
    """
    path = posixpath.normpath(path).lstrip('/')
    fullpath = Path(safe_join(document_root, path))
    if fullpath.is_dir():
        raise Http404(_('Directory indexes are not allowed here.'))
    if manifest_asset_prefix and not fullpath.exists():
        possible_asset = get_manifest_asset(path)
        manifest_asset_prefix = (
            f'/{manifest_asset_prefix}' if not manifest_asset_prefix.startswith('/') else manifest_asset_prefix
        )
        if possible_asset.startswith(manifest_asset_prefix):
            possible_asset = possible_asset[len(manifest_asset_prefix) :]
        fullpath = Path(safe_join(document_root, possible_asset))
    if not fullpath.exists():
        raise Http404(_('%(path)s does not exist') % {'path': fullpath})
    # Respect the If-Modified-Since header.
    statobj = fullpath.stat()
    if not was_modified_since(request.META.get('HTTP_IF_MODIFIED_SINCE'), statobj.st_mtime):
        return HttpResponseNotModified()
    content_type, encoding = mimetypes.guess_type(str(fullpath))
    content_type = content_type or 'application/octet-stream'

    response = RangedFileResponse(request, fullpath.open('rb'), content_type=content_type)
    response['Last-Modified'] = http_date(statobj.st_mtime)
    if encoding:
        response['Content-Encoding'] = encoding
    return response
</file>

<file path="label_studio/core/__init__.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
</file>

<file path="label_studio/core/api_permissions.py">
from rest_framework.permissions import SAFE_METHODS, BasePermission


class HasObjectPermission(BasePermission):
    def has_object_permission(self, request, view, obj):
        return obj.has_permission(request.user)


class MemberHasOwnerPermission(BasePermission):
    def has_object_permission(self, request, view, obj):
        if request.method not in SAFE_METHODS and not request.user.own_organization:
            return False

        return obj.has_permission(request.user)
</file>

<file path="label_studio/core/argparser.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import os

from .settings.base import EXPORT_DIR
from .utils.io import find_file


def parse_input_args(input_args):
    """Combine args with json config

    :return: config dict
    """
    import argparse

    def valid_filepath(filepath):
        path = os.path.abspath(os.path.expanduser(filepath))
        if os.path.exists(path):
            return path
        raise FileNotFoundError(filepath)

    def project_name(raw_name):
        """Remove trailing / and leading ./ from project name"""
        return os.path.normpath(raw_name)

    root_parser = argparse.ArgumentParser(add_help=False)
    root_parser.add_argument('--version', dest='version', action='store_true', help='Show Label Studio version')
    root_parser.add_argument(
        '-b',
        '--no-browser',
        dest='no_browser',
        action='store_true',
        help='Do not open browser when starting Label Studio',
    )
    root_parser.add_argument(
        '-db', '--database', dest='database', help='Database file path for storing tasks and annotations'
    )
    root_parser.add_argument('--data-dir', dest='data_dir', help='Directory for storing all application related data')
    root_parser.add_argument('-d', '--debug', dest='debug', action='store_true', help='Debug mode', default=False)
    default_config_path = find_file('default_config.json')
    root_parser.add_argument(
        '-c', '--config', dest='config_path', type=valid_filepath, default=default_config_path, help='Server config'
    )
    root_parser.add_argument(
        '-l', '--label-config', dest='label_config', type=valid_filepath, help='Label config file path'
    )
    root_parser.add_argument(
        '--skip-long-migrations',
        dest='skip_long_migrations',
        action='store_true',
        help='Skip long migrations on start',
    )
    root_parser.add_argument('--ml-backends', dest='ml_backends', nargs='+', help='Machine learning backends URLs')
    root_parser.add_argument(
        '--sampling',
        dest='sampling',
        choices=['sequential', 'uniform', 'prediction-score-min'],
        default='sequential',
        help='Sampling type that defines order for labeling tasks',
    )
    root_parser.add_argument(
        '--log-level',
        dest='log_level',
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
        default='WARNING',
        help='Logging level',
    )
    root_parser.add_argument(
        '--internal-host',
        dest='internal_host',
        type=str,
        default='0.0.0.0',  # nosec
        help='Web server internal host, e.g.: "localhost" or "0.0.0.0"',
    )
    root_parser.add_argument('-p', '--port', dest='port', type=int, help='Web server port')
    root_parser.add_argument(
        '--host',
        dest='host',
        type=str,
        default='',
        help='Label Studio full hostname for generating imported task urls, sample task urls, static loading, etc.\n'
        "Leave it empty to make all paths relative to the domain root, it's preferable for work for most cases."
        'Examples: "https://77.42.77.42:1234", "http://ls.domain.com/subdomain/"',
    )
    root_parser.add_argument(
        '--cert', dest='cert_file', type=valid_filepath, help='Certificate file for HTTPS (in PEM format)'
    )
    root_parser.add_argument(
        '--key', dest='key_file', type=valid_filepath, help='Private key file for HTTPS (in PEM format)'
    )
    root_parser.add_argument(
        '--initial-project-description', dest='project_desc', help='Project description to identify project'
    )
    root_parser.add_argument('--password', dest='password', default='', help='Password for default user')
    root_parser.add_argument('--username', dest='username', default='', help='Username for default user')
    root_parser.add_argument('--user-token', dest='user_token', default='', help='User token for API access')
    root_parser.add_argument(
        '--agree-fix-sqlite',
        dest='agree_fix_sqlite',
        action='store_true',
        help='Agree to fix SQLite issues on python 3.6-3.8 on Windows automatically',
    )

    parser = argparse.ArgumentParser(description='Label studio', parents=[root_parser])

    subparsers = parser.add_subparsers(dest='command', help='Available commands')
    subparsers.required = False

    # init sub-command parser

    subparsers.add_parser('version', help='Print version info', parents=[root_parser])

    subparsers.add_parser('user', help='Print user info', parents=[root_parser])

    parser_init = subparsers.add_parser('init', help='Initialize Label Studio', parents=[root_parser])
    parser_init.add_argument(
        'project_name', help='Path to directory where project state will be initialized', type=project_name, nargs='?'
    )
    parser_init.add_argument(
        '-q',
        '--quiet',
        dest='quiet_mode',
        action='store_true',
        help='Quiet (silence) mode for init when it does not ask about username and password',
        default=False,
    )

    # start sub-command parser

    parser_start = subparsers.add_parser('start', help='Start Label Studio server', parents=[root_parser])
    parser_start.add_argument('project_name', help='Project name', type=project_name, default='', nargs='?')
    parser_start.add_argument(
        '--init', dest='init', action='store_true', help='Initialize if project is not initialized yet'
    )

    # reset_password sub-command parser

    subparsers.add_parser('reset_password', help='Reset password for a specific username', parents=[root_parser])

    subparsers.add_parser('shell', help='Run django shell', parents=[root_parser])

    calculate_stats_all_orgs = subparsers.add_parser(
        'calculate_stats_all_orgs', help='Calculate task counters and statistics', parents=[root_parser]
    )
    calculate_stats_all_orgs.add_argument(
        '--from-scratch', dest='from_scratch', default=False, action='store_true', help='Recalculate from scratch'
    )

    # export_project sub-command parser
    export_project = subparsers.add_parser('export', help='Export project in a specific format', parents=[root_parser])
    export_project.add_argument('project_id', help='Project ID')
    export_project.add_argument('export_format', help='Export format (JSON, JSON_MIN, CSV, etc)')
    export_project.add_argument('--export-path', help='Export file path or directory', default=EXPORT_DIR)
    default_params = '{"annotations__completed_by": {"only_id": null}, "interpolate_key_frames": true}'
    export_project.add_argument(
        '--export-serializer-context',
        help=f"Export serializer context, default value: '{default_params}'",
        default=default_params,
    )

    subparsers.add_parser(
        'annotations_fill_updated_by', help='Fill the updated_by field for Annotations', parents=[root_parser]
    )

    args = parser.parse_args(input_args)

    if not hasattr(args, 'label_config'):
        args.label_config = None
    return args
</file>

<file path="label_studio/core/bulk_update_utils.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
"""
Main module with the bulk_update function.
"""
import itertools
from collections import defaultdict

from django.db import connections, models
from django.db.models.sql import UpdateQuery


def _get_db_type(field, connection):
    if isinstance(field, (models.PositiveSmallIntegerField, models.PositiveIntegerField)):
        return field.db_type(connection).split(' ', 1)[0]

    return field.db_type(connection)


def _as_sql(obj, field, query, compiler, connection):
    value = getattr(obj, field.attname)

    if hasattr(value, 'resolve_expression'):
        value = value.resolve_expression(query, allow_joins=False, for_save=True)
    else:
        value = field.get_db_prep_save(value, connection=connection)

    if hasattr(value, 'as_sql'):
        placeholder, value = compiler.compile(value)
        if isinstance(value, list):
            value = tuple(value)
    else:
        placeholder = '%s'

    return value, placeholder


def flatten(l, types=(list, float)):  # noqa: E741
    """
    Flat nested list of lists into a single list.
    """
    l = [item if isinstance(item, types) else [item] for item in l]  # noqa: E741
    return [item for sublist in l for item in sublist]


def grouper(iterable, size):
    # http://stackoverflow.com/a/8991553
    it = iter(iterable)
    while True:
        chunk = tuple(itertools.islice(it, size))
        if not chunk:
            return
        yield chunk


def validate_fields(meta, fields):

    fields = frozenset(fields)
    field_names = set()

    for field in meta.fields:
        if not field.primary_key:
            field_names.add(field.name)

            if field.name != field.attname:
                field_names.add(field.attname)

    non_model_fields = fields.difference(field_names)

    if non_model_fields:
        raise TypeError('These fields are not present in ' 'current meta: {}'.format(', '.join(non_model_fields)))


def get_fields(update_fields, exclude_fields, meta, obj=None):

    deferred_fields = set()

    if update_fields is not None:
        validate_fields(meta, update_fields)
    elif obj:
        deferred_fields = obj.get_deferred_fields()

    if exclude_fields is None:
        exclude_fields = set()
    else:
        exclude_fields = set(exclude_fields)
        validate_fields(meta, exclude_fields)

    exclude_fields |= deferred_fields

    fields = [
        field
        for field in meta.concrete_fields
        if (
            not field.primary_key
            and field.attname not in deferred_fields
            and field.attname not in exclude_fields
            and field.name not in exclude_fields
            and (update_fields is None or field.attname in update_fields or field.name in update_fields)
        )
    ]

    return fields


def bulk_update(
    objs, meta=None, update_fields=None, exclude_fields=None, using='default', batch_size=None, pk_field='pk'
):
    assert batch_size is None or batch_size > 0

    # force to retrieve objs from the DB at the beginning,
    # to avoid multiple subsequent queries
    objs = list(objs)
    if not objs:
        return
    batch_size = batch_size or len(objs)

    if meta:
        fields = get_fields(update_fields, exclude_fields, meta)
    else:
        meta = objs[0]._meta
        if update_fields is not None:
            fields = get_fields(update_fields, exclude_fields, meta, objs[0])
        else:
            fields = None

    if fields is not None and len(fields) == 0:
        return

    if pk_field == 'pk':
        pk_field = meta.get_field(meta.pk.name)
    else:
        pk_field = meta.get_field(pk_field)

    connection = connections[using]
    query = UpdateQuery(meta.model)
    compiler = query.get_compiler(connection=connection)

    template = '"{column}" = CAST(CASE "{pk_column}" {cases}ELSE "{column}" END AS {type})'

    case_template = 'WHEN %s THEN {} '

    lenpks = 0
    for objs_batch in grouper(objs, batch_size):

        pks = []
        parameters = defaultdict(list)
        placeholders = defaultdict(list)

        for obj in objs_batch:

            pk_value, _ = _as_sql(obj, pk_field, query, compiler, connection)
            pks.append(pk_value)

            loaded_fields = fields or get_fields(update_fields, exclude_fields, meta, obj)

            for field in loaded_fields:
                value, placeholder = _as_sql(obj, field, query, compiler, connection)
                parameters[field].extend(flatten([pk_value, value], types=tuple))
                placeholders[field].append(placeholder)

        values = ', '.join(
            template.format(
                column=field.column,
                pk_column=pk_field.column,
                cases=(case_template * len(placeholders[field])).format(*placeholders[field]),
                type=_get_db_type(field, connection=connection),
            )
            for field in parameters.keys()
        )

        parameters = flatten(parameters.values(), types=list)
        parameters.extend(pks)

        n_pks = len(pks)
        del pks

        dbtable = '"{}"'.format(meta.db_table)

        in_clause = '"{pk_column}" in ({pks})'.format(
            pk_column=pk_field.column,
            pks=', '.join(itertools.repeat('%s', n_pks)),
        )

        sql = 'UPDATE {dbtable} SET {values} WHERE {in_clause}'.format(  # nosec
            dbtable=dbtable,
            values=values,
            in_clause=in_clause,
        )
        del values

        lenpks += n_pks

        connection.cursor().execute(sql, parameters)

    return lenpks
</file>

<file path="label_studio/core/context_processors.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
from core.feature_flags import all_flags
from core.utils.common import collect_versions
from django.conf import settings as django_settings


def sentry_fe(request):
    # return the value you want as a dictionary, you may add multiple values in there
    return {'SENTRY_FE': django_settings.SENTRY_FE}


def settings(request):
    """Make available django settings on each template page"""
    versions = collect_versions()

    os_release = versions.get('label-studio-os-backend', {}).get('commit', 'none')[0:6]
    # django templates can't access names with hyphens
    versions['lsf'] = versions.get('label-studio-frontend', {})
    versions['lsf']['commit'] = versions['lsf'].get('commit', os_release)[0:6]

    versions['dm2'] = versions.get('dm2', {})
    versions['dm2']['commit'] = versions['dm2'].get('commit', os_release)[0:6]

    versions['backend'] = {}
    if 'label-studio-os-backend' in versions:
        versions['backend']['commit'] = versions['label-studio-os-backend'].get('commit', 'none')[0:6]
    if 'label-studio-enterprise-backend' in versions:
        versions['backend']['commit'] = versions['label-studio-enterprise-backend'].get('commit', 'none')[0:6]

    feature_flags = {}
    if hasattr(request, 'user'):
        feature_flags = all_flags(request.user)

    return {'settings': django_settings, 'versions': versions, 'feature_flags': feature_flags}
</file>

<file path="label_studio/core/current_request.py">
from threading import local

from django.core.signals import request_finished
from django.dispatch import receiver
from django.middleware.common import CommonMiddleware

_thread_locals = local()


def get_current_request():
    """returns the request object for this thread"""
    result = getattr(_thread_locals, 'request', None)
    return result


class ThreadLocalMiddleware(CommonMiddleware):
    def process_request(self, request):
        _thread_locals.request = request


@receiver(request_finished)
def clean_request(sender, **kwargs):
    if hasattr(_thread_locals, 'request'):
        del _thread_locals.request
</file>

<file path="label_studio/core/decorators.py">
from functools import wraps


def permission_required(*permissions, fn=None):
    def decorator(view):
        def wrapped_view(self, request, *args, **kwargs):

            if callable(fn):
                obj = fn(request, *args, **kwargs)
            else:
                obj = fn

            missing_permissions = [perm for perm in permissions if not request.user.has_perm(perm, obj)]
            if any(missing_permissions):
                # raises a permission denied exception causing a 403 response
                self.permission_denied(
                    request, message=('Permission denied: {}'.format(', '.join(missing_permissions)))
                )

            return view(self, request, *args, **kwargs)

        return wrapped_view

    return decorator


def override_report_only_csp(view_func):
    """
    Decorator to switch report-only CSP to regular CSP. For use with core.middleware.HumanSignalCspMiddleware.
    """

    @wraps(view_func)
    def wrapper(*args, **kwargs):
        response = view_func(*args, **kwargs)
        setattr(response, '_override_report_only_csp', True)
        return response

    return wrapper
</file>

<file path="label_studio/core/filters.py">
from django_filters import Filter
from django_filters.constants import EMPTY_VALUES


class ListFilter(Filter):
    def filter(self, qs, value):
        if value in EMPTY_VALUES:
            return qs
        value_list = value.split(',')
        qs = super().filter(qs, value_list)
        return qs
</file>

<file path="label_studio/core/label_config.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import json
import logging
import re
from collections import OrderedDict, defaultdict
from typing import Tuple, Union
from urllib.parse import urlencode

import defusedxml.ElementTree as etree
import jsonschema
import numpy as np
import pandas as pd
import xmljson
from django.conf import settings
from label_studio_sdk._extensions.label_studio_tools.core import label_config
from rest_framework.exceptions import ValidationError

from label_studio.core.utils.io import find_file

logger = logging.getLogger(__name__)


_DATA_EXAMPLES = None
_LABEL_TAGS = {'Label', 'Choice', 'Relation'}
SINGLE_VALUED_TAGS = {'choices': str, 'rating': int, 'number': float, 'textarea': str}
_NOT_CONTROL_TAGS = {
    'Filter',
}
# TODO: move configs in right place
_LABEL_CONFIG_SCHEMA = find_file('label_config_schema.json')
with open(_LABEL_CONFIG_SCHEMA) as f:
    _LABEL_CONFIG_SCHEMA_DATA = json.load(f)


def parse_config(config_string):
    """
    :param config_string: Label config string
    :return: structured config of the form:
    {
        "<ControlTag>.name": {
            "type": "ControlTag",
            "to_name": ["<ObjectTag1>.name", "<ObjectTag2>.name"],
            "inputs: [
                {"type": "ObjectTag1", "value": "<ObjectTag1>.value"},
                {"type": "ObjectTag2", "value": "<ObjectTag2>.value"}
            ],
            "labels": ["Label1", "Label2", "Label3"] // taken from "alias" if exists or "value"
    }
    """
    logger.warning('Using deprecated method - switch to label_studio.tools.label_config.parse_config!')
    return label_config.parse_config(config_string)


def _fix_choices(config):
    """
    workaround for single choice
    https://github.com/HumanSignal/label-studio/issues/1259
    """
    if 'Choices' in config:
        # for single Choices tag in View
        if 'Choice' in config['Choices'] and not isinstance(config['Choices']['Choice'], list):
            config['Choices']['Choice'] = [config['Choices']['Choice']]
        # for several Choices tags in View
        elif isinstance(config['Choices'], list) and all('Choice' in tag_choices for tag_choices in config['Choices']):
            for n in range(len(config['Choices'])):
                # check that Choices tag has only 1 choice
                if not isinstance(config['Choices'][n]['Choice'], list):
                    config['Choices'][n]['Choice'] = [config['Choices'][n]['Choice']]
    if 'View' in config:
        if isinstance(config['View'], OrderedDict):
            config['View'] = _fix_choices(config['View'])
        else:
            config['View'] = [_fix_choices(view) for view in config['View']]
    return config


def parse_config_to_xml(config_string: Union[str, None], raise_on_empty: bool = False) -> Union[OrderedDict, None]:
    if config_string is None:
        if raise_on_empty:
            raise TypeError('config_string is None')
        return None

    xml = etree.fromstring(config_string, forbid_dtd=True)

    # Remove comments
    for comment in xml.findall('.//comment'):
        comment.getparent().remove(comment)

    return xml


def parse_config_to_json(config_string: Union[str, None]) -> Tuple[Union[OrderedDict, None], Union[str, None]]:
    try:
        xml = parse_config_to_xml(config_string, raise_on_empty=True)
    except TypeError:
        raise etree.ParseError('can only parse strings')
    if xml is None:
        raise etree.ParseError('xml is empty or incorrect')
    config = xmljson.badgerfish.data(xml)
    config = _fix_choices(config)
    return config, etree.tostring(xml, encoding='unicode')


def validate_label_config(config_string: Union[str, None]) -> None:
    # xml and schema
    try:
        config, cleaned_config_string = parse_config_to_json(config_string)
        jsonschema.validate(config, _LABEL_CONFIG_SCHEMA_DATA)
    except (etree.ParseError, ValueError) as exc:
        raise ValidationError(str(exc))
    except jsonschema.exceptions.ValidationError as exc:
        # jsonschema4 validation error now includes all errors from "anyOf" subschemas
        # check https://python-jsonschema.readthedocs.io/en/latest/errors/#jsonschema.exceptions.ValidationError.context
        # we pick the first failed schema and show only its error message
        error_message = exc.context[0].message if len(exc.context) else exc.message
        error_message = 'Validation failed on {}: {}'.format(
            '/'.join(map(str, exc.path)), error_message.replace('@', '')
        )
        raise ValidationError(error_message)

    # unique names in config # FIXME: 'name =' (with spaces) won't work
    all_names = re.findall(r'name="([^"]*)"', cleaned_config_string)
    if len(set(all_names)) != len(all_names):
        raise ValidationError('Label config contains non-unique names')

    # toName points to existent name
    names = set(all_names)
    toNames = re.findall(r'toName="([^"]*)"', cleaned_config_string)
    for toName_ in toNames:
        for toName in toName_.split(','):
            if toName not in names:
                raise ValidationError(f'toName="{toName}" not found in names: {sorted(names)}')


def extract_data_types(label_config):
    # load config
    xml = parse_config_to_xml(label_config)
    if xml is None:
        raise etree.ParseError('Project config is empty or incorrect')

    # take all tags with values attribute and fit them to tag types
    data_type = {}
    parent = xml.findall('.//*[@value]')
    for match in parent:
        if not match.get('name'):
            continue
        name = match.get('value')

        # simple one
        if len(name) > 1 and (name[0] == '$'):
            name = name[1:]
            # video has highest priority, e.g.
            # for <Video value="url"/> <Audio value="url"> it must be data_type[url] = Video
            if data_type.get(name) != 'Video':
                data_type[name] = match.tag

        # regex
        else:
            pattern = r'\$\w+'  # simple one: r'\$\w+'
            regex = re.findall(pattern, name)
            first = regex[0][1:] if len(regex) > 0 else ''

            if first:
                if data_type.get(first) != 'Video':
                    data_type[first] = match.tag

    return data_type


def get_all_labels(label_config):
    outputs = parse_config(label_config)
    labels = defaultdict(list)
    dynamic_labels = defaultdict(bool)
    for control_name in outputs:
        for label in outputs[control_name].get('labels', []):
            labels[control_name].append(label)
        if outputs[control_name].get('dynamic_labels', False):
            dynamic_labels[control_name] = True
    return labels, dynamic_labels


def get_annotation_tuple(from_name, to_name, type):
    if isinstance(to_name, list):
        to_name = ','.join(to_name)
    return '|'.join([from_name, to_name, type.lower()])


def get_all_control_tag_tuples(label_config):
    outputs = parse_config(label_config)
    out = []
    for control_name, info in outputs.items():
        out.append(get_annotation_tuple(control_name, info['to_name'], info['type']))
    return out


def get_all_object_tag_names(label_config):
    return set(extract_data_types(label_config))


def config_line_stipped(c):
    xml = parse_config_to_xml(c)
    if xml is None:
        return None

    return etree.tostring(xml, encoding='unicode').replace('\n', '').replace('\r', '')


def get_task_from_labeling_config(config):
    """Get task, annotations and predictions from labeling config comment,
    it must start from "<!-- {" and end as "} -->"
    """
    # try to get task data, annotations & predictions from config comment
    task_data, annotations, predictions = {}, None, None
    start = config.find('<!-- {')
    start = start if start >= 0 else config.find('<!--{')
    start += 4
    end = config[start:].find('-->') if start >= 0 else -1
    if 3 < start < start + end:
        try:
            logger.debug('Parse ' + config[start : start + end])
            body = json.loads(config[start : start + end])
        except Exception:
            logger.error("Can't parse task from labeling config", exc_info=True)
            pass
        else:
            logger.debug(json.dumps(body, indent=2))
            dont_use_root = 'predictions' in body or 'annotations' in body
            task_data = body['data'] if 'data' in body else (None if dont_use_root else body)
            predictions = body['predictions'] if 'predictions' in body else None
            annotations = body['annotations'] if 'annotations' in body else None
    return task_data, annotations, predictions


def data_examples(mode):
    """Data examples for editor preview and task upload examples"""
    global _DATA_EXAMPLES

    if _DATA_EXAMPLES is None:
        with open(find_file('data_examples.json'), encoding='utf-8') as f:
            _DATA_EXAMPLES = json.load(f)

        roots = ['editor_preview', 'upload']
        for root in roots:
            for key, value in _DATA_EXAMPLES[root].items():
                if isinstance(value, str):
                    _DATA_EXAMPLES[root][key] = value.replace('<HOSTNAME>', settings.HOSTNAME)

    return _DATA_EXAMPLES[mode]


def generate_sample_task_without_check(label_config, mode='upload', secure_mode=False):
    """Generate sample task only"""
    # load config
    xml = parse_config_to_xml(label_config)
    if xml is None:
        raise etree.ParseError('Project config is empty or incorrect')

    # make examples pretty
    examples = data_examples(mode=mode)

    # iterate over xml tree and find elements with 'value' or 'valueList' attributes
    task = {}
    # Include both 'value' and 'valueList' attributes in the search
    parent = xml.findall('.//*[@value]') + xml.findall('.//*[@valueList]')
    for p in parent:
        # Extract data placeholder key
        value = p.get('value') or p.get('valueList')
        if not value or not value.startswith('$'):
            continue
        value = value[1:]
        is_value_list = 'valueList' in p.attrib  # Check if the attribute is 'valueList'

        # detect secured mode - objects served as URLs
        value_type = p.get('valueType') or p.get('valuetype')
        only_urls = secure_mode or value_type == 'url'

        example_from_field_name = examples.get('$' + value)
        if example_from_field_name:
            # Get example by variable name
            task[value] = [example_from_field_name] if is_value_list else example_from_field_name

        elif value == 'video' and p.tag == 'HyperText':
            task[value] = examples.get('$videoHack')
        # List with a matching Ranker tag pair
        elif p.tag == 'List':
            task[value] = examples.get('List')
        elif p.tag == 'Paragraphs':
            # Paragraphs special case - replace nameKey/textKey if presented
            name_key = p.get('nameKey') or p.get('namekey') or 'author'
            text_key = p.get('textKey') or p.get('textkey') or 'text'
            if only_urls:
                params = {'nameKey': name_key, 'textKey': text_key}
                task[value] = examples['ParagraphsUrl'] + urlencode(params)
            else:
                task[value] = []
                for item in examples[p.tag]:
                    task[value].append({name_key: item['author'], text_key: item['text']})
        elif p.tag == 'TimeSeries':
            # TimeSeries special case - generate signals on-the-fly
            time_column = p.get('timeColumn')
            value_columns = []
            for ts_child in p:
                if ts_child.tag != 'Channel':
                    continue
                value_columns.append(ts_child.get('column'))
            sep = p.get('sep')
            time_format = p.get('timeFormat')

            if only_urls:
                # data is URL
                params = {'time': time_column, 'values': ','.join(value_columns)}
                if sep:
                    params['sep'] = sep
                if time_format:
                    params['tf'] = time_format
                task[value] = '/samples/time-series.csv?' + urlencode(params)
            else:
                # data is JSON
                task[value] = generate_time_series_json(time_column, value_columns, time_format)
        elif p.tag == 'HyperText':
            if only_urls:
                task[value] = examples['HyperTextUrl']
            else:
                task[value] = examples['HyperText']
        elif p.tag.lower().endswith('labels'):
            task[value] = examples['Labels']
        elif p.tag.lower() == 'choices':
            allow_nested = p.get('allowNested') or p.get('allownested') or 'false'
            if allow_nested == 'true':
                task[value] = examples['NestedChoices']
            else:
                task[value] = examples['Choices']
        else:
            # Patch for valueType="url"
            examples['Text'] = examples['TextUrl'] if only_urls else examples['TextRaw']
            # Not found by name, try to get example by type
            example_value = examples.get(p.tag, 'Something')
            task[value] = [example_value] if is_value_list else example_value

        # support for Repeater tag
        if '[' in value:
            base = value.split('[')[0]
            child = value.split(']')[1]

            # images[{{idx}}].url => { "images": [ {"url": "test.jpg"} ] }
            if child.startswith('.'):
                child_name = child[1:]
                task[base] = [{child_name: task[value]}, {child_name: task[value]}]
            # images[{{idx}}].url => { "images": [ "test.jpg", "test.jpg" ] }
            else:
                task[base] = [task[value], task[value]]

            # remove unused "images[{{idx}}].url"
            task.pop(value, None)

    return task


def _is_strftime_string(s):
    # simple way to detect strftime format
    return '%' in s


def generate_time_series_json(time_column, value_columns, time_format=None):
    """Generate sample for time series"""
    n = 100
    if time_format is not None and not _is_strftime_string(time_format):
        time_fmt_map = {'yyyy-MM-dd': '%Y-%m-%d'}
        time_format = time_fmt_map.get(time_format)

    if time_format is None:
        times = np.arange(n).tolist()
    else:
        times = pd.date_range('2020-01-01', periods=n, freq='D').strftime(time_format).tolist()
    ts = {time_column: times}
    for value_col in value_columns:
        ts[value_col] = np.random.randn(n).tolist()
    return ts


def get_sample_task(label_config, secure_mode=False):
    """Get sample task from labeling config and combine it with generated sample task"""
    predefined_task, annotations, predictions = get_task_from_labeling_config(label_config)
    generated_task = generate_sample_task_without_check(label_config, mode='editor_preview', secure_mode=secure_mode)
    if predefined_task is not None:
        generated_task.update(predefined_task)
    return generated_task, annotations, predictions


def config_essential_data_has_changed(new_config_str, old_config_str):
    """Detect essential changes of the labeling config"""
    new_config = parse_config(new_config_str)
    old_config = parse_config(old_config_str)

    for tag, new_info in new_config.items():
        if tag not in old_config:
            return True
        old_info = old_config[tag]
        if new_info['type'] != old_info['type']:
            return True
        if new_info['inputs'] != old_info['inputs']:
            return True
        if not set(old_info['labels']).issubset(new_info['labels']):
            return True


def replace_task_data_undefined_with_config_field(data, project, first_key=None):
    """Use first key is passed (for speed up) or project.data.types.keys()[0]"""
    # assign undefined key name from data to the first key from config, e.g. for txt loading
    if settings.DATA_UNDEFINED_NAME in data and (first_key or project.data_types.keys()):
        key = first_key or list(project.data_types.keys())[0]
        data[key] = data[settings.DATA_UNDEFINED_NAME]
        del data[settings.DATA_UNDEFINED_NAME]


def check_control_in_config_by_regex(config_string, control_type, filter=None):
    """
    Check if control type is in config including regex filter
    """
    c = parse_config(config_string)
    if filter is not None and len(filter) == 0:
        return False
    if filter:
        c = {key: c[key] for key in filter}
    for control in c:
        item = c[control].get('regex', {})
        expression = control
        for key in item:
            expression = expression.replace(key, item[key])
        pattern = re.compile(expression)
        full_match = pattern.fullmatch(control_type)
        if full_match:
            return True
    return False


def check_toname_in_config_by_regex(config_string, to_name, control_type=None):
    """
    Check if to_name is in config including regex filter
    :return: True if to_name is fullmatch to some pattern ion config
    """
    c = parse_config(config_string)
    if control_type:
        check_list = [control_type]
    else:
        check_list = list(c.keys())
    for control in check_list:
        item = c[control].get('regex', {})
        for to_name_item in c[control]['to_name']:
            expression = to_name_item
            for key in item:
                expression = expression.replace(key, item[key])
            pattern = re.compile(expression)
            full_match = pattern.fullmatch(to_name)
            if full_match:
                return True
    return False


def get_original_fromname_by_regex(config_string, fromname):
    """
    Get from_name from config on from_name key from data after applying regex search or original fromname
    """
    c = parse_config(config_string)
    for control in c:
        item = c[control].get('regex', {})
        expression = control
        for key in item:
            expression = expression.replace(key, item[key])
        pattern = re.compile(expression)
        full_match = pattern.fullmatch(fromname)
        if full_match:
            return control
    return fromname


def get_all_types(label_config):
    """
    Get all types from label_config
    """
    outputs = parse_config(label_config)
    out = []
    for control_name, info in outputs.items():
        out.append(info['type'].lower())
    return out
</file>

<file path="label_studio/core/middleware.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import logging
import time
from uuid import uuid4

import ujson as json
from core.utils.contextlog import ContextLog
from csp.middleware import CSPMiddleware
from django.conf import settings
from django.contrib.auth import logout
from django.core.exceptions import MiddlewareNotUsed
from django.core.handlers.base import BaseHandler
from django.http import HttpResponsePermanentRedirect
from django.middleware.common import CommonMiddleware
from django.utils.deprecation import MiddlewareMixin
from django.utils.http import escape_leading_slashes
from rest_framework.permissions import SAFE_METHODS

logger = logging.getLogger(__name__)


def enforce_csrf_checks(func):
    """Enable csrf for specified view func"""
    # USE_ENFORCE_CSRF_CHECKS=False is for tests
    if settings.USE_ENFORCE_CSRF_CHECKS:

        def wrapper(request, *args, **kwargs):
            return func(request, *args, **kwargs)

        wrapper._dont_enforce_csrf_checks = False
        return wrapper
    else:
        return func


class DisableCSRF(MiddlewareMixin):
    # disable csrf for api requests
    def process_view(self, request, callback, *args, **kwargs):
        if hasattr(callback, '_dont_enforce_csrf_checks'):
            setattr(request, '_dont_enforce_csrf_checks', callback._dont_enforce_csrf_checks)
        elif request.GET.get('enforce_csrf_checks'):  # _dont_enforce_csrf_checks is for test
            setattr(request, '_dont_enforce_csrf_checks', False)
        else:
            setattr(request, '_dont_enforce_csrf_checks', True)


class HttpSmartRedirectResponse(HttpResponsePermanentRedirect):
    pass


class CommonMiddlewareAppendSlashWithoutRedirect(CommonMiddleware):
    """This class converts HttpSmartRedirectResponse to the common response
    of Django view, without redirect. This is necessary to match status_codes
    for urls like /url?q=1 and /url/?q=1. If you don't use it, you will have 302
    code always on pages without slash.
    """

    response_redirect_class = HttpSmartRedirectResponse

    def __init__(self, *args, **kwargs):
        # create django request resolver
        self.handler = BaseHandler()

        # prevent recursive includes
        old = settings.MIDDLEWARE
        name = self.__module__ + '.' + self.__class__.__name__
        settings.MIDDLEWARE = [i for i in settings.MIDDLEWARE if i != name]

        self.handler.load_middleware()

        settings.MIDDLEWARE = old
        super(CommonMiddlewareAppendSlashWithoutRedirect, self).__init__(*args, **kwargs)

    def get_full_path_with_slash(self, request):
        """Return the full path of the request with a trailing slash appended
        without Exception in Debug mode
        """
        new_path = request.get_full_path(force_append_slash=True)
        # Prevent construction of scheme relative urls.
        new_path = escape_leading_slashes(new_path)
        return new_path

    def process_response(self, request, response):
        response = super(CommonMiddlewareAppendSlashWithoutRedirect, self).process_response(request, response)

        request.editor_keymap = settings.EDITOR_KEYMAP

        if isinstance(response, HttpSmartRedirectResponse):
            if not request.path.endswith('/'):
                # remove prefix SCRIPT_NAME
                path = request.path[len(settings.FORCE_SCRIPT_NAME) :] if settings.FORCE_SCRIPT_NAME else request.path
                request.path = path + '/'
            # we don't need query string in path_info because it's in request.GET already
            request.path_info = request.path
            response = self.handler.get_response(request)

        return response

    def should_redirect_with_slash(self, request):
        """
        Override the original method to keep global APPEND_SLASH setting false
        """
        if not request.path_info.endswith('/'):
            return True
        return False


class SetSessionUIDMiddleware(CommonMiddleware):
    def process_request(self, request):
        if 'uid' not in request.session:
            request.session['uid'] = str(uuid4())


class ContextLogMiddleware(CommonMiddleware):
    def __init__(self, get_response):
        self.get_response = get_response
        self.log = ContextLog()

    def __call__(self, request):
        body = None
        try:
            body = json.loads(request.body)
        except:  # noqa: E722
            try:
                body = request.body.decode('utf-8')
            except:  # noqa: E722
                pass

        if 'server_id' not in request:
            setattr(request, 'server_id', self.log._get_server_id())

        response = self.get_response(request)
        self.log.send(request=request, response=response, body=body)

        return response

    def process_request(self, request):
        if 'server_id' not in request:
            setattr(request, 'server_id', self.log._get_server_id())


class DatabaseIsLockedRetryMiddleware(CommonMiddleware):
    """Workaround for sqlite performance issues
    we wait and retry request if database is locked"""

    def __init__(self, get_response):
        if settings.DJANGO_DB != settings.DJANGO_DB_SQLITE:
            raise MiddlewareNotUsed()
        self.get_response = get_response

    def __call__(self, request):
        response = self.get_response(request)
        retries_number = 0
        sleep_time = 1
        backoff = 1.5
        while (
            response.status_code == 500
            and hasattr(response, 'content')
            and b'database-is-locked-error' in response.content
            and retries_number < 15
        ):
            time.sleep(sleep_time)
            response = self.get_response(request)
            retries_number += 1
            sleep_time *= backoff
        return response


class XApiKeySupportMiddleware:
    """Middleware that adds support for the X-Api-Key header, by having its value supersede
    anything that's set in the Authorization header."""

    def __init__(self, get_response):
        self.get_response = get_response

    def __call__(self, request):
        if 'HTTP_X_API_KEY' in request.META:
            request.META['HTTP_AUTHORIZATION'] = f'Token {request.META["HTTP_X_API_KEY"]}'
            del request.META['HTTP_X_API_KEY']

        return self.get_response(request)


class UpdateLastActivityMiddleware(CommonMiddleware):
    def process_view(self, request, view_func, view_args, view_kwargs):
        if hasattr(request, 'user') and request.method not in SAFE_METHODS:
            if request.user.is_authenticated:
                request.user.update_last_activity()


class InactivitySessionTimeoutMiddleWare(CommonMiddleware):
    """Log the user out if they have been logged in for too long
    or inactive for too long"""

    # paths that don't count as user activity
    NOT_USER_ACTIVITY_PATHS = []

    def process_request(self, request) -> None:
        if (
            not hasattr(request, 'session')
            or request.session.is_empty()
            or not hasattr(request, 'user')
            or not request.user.is_authenticated
            or
            # scim assign request.user implicitly, check CustomSCIMAuthCheckMiddleware
            (hasattr(request, 'is_scim') and request.is_scim)
            or (hasattr(request, 'is_jwt') and request.is_jwt)
        ):
            return

        current_time = time.time()
        last_login = request.session['last_login'] if 'last_login' in request.session else 0

        # Check if this request is too far from when the login happened
        if (current_time - last_login) > settings.MAX_SESSION_AGE:
            logger.info(
                f'Request is too far from last login {current_time - last_login:.0f} > {settings.MAX_SESSION_AGE}; logout'
            )
            logout(request)

        # Push the expiry to the max every time a new request is made to a url that indicates user activity
        # but only if it's not a URL we want to ignore
        for path in self.NOT_USER_ACTIVITY_PATHS:
            if isinstance(path, str) and path == str(request.path_info):
                return
            elif 'query' in path:
                parts = str(request.path_info).split('?')
                if len(parts) == 2 and path['query'] in parts[1]:
                    return

        request.session.set_expiry(
            settings.MAX_TIME_BETWEEN_ACTIVITY if request.session.get('keep_me_logged_in', True) else 0
        )


class HumanSignalCspMiddleware(CSPMiddleware):
    """
    Extend CSPMiddleware to support switching report-only CSP to regular CSP.

    For use with core.decorators.override_report_only_csp.
    """

    def process_response(self, request, response):
        response = super().process_response(request, response)
        if getattr(response, '_override_report_only_csp', False):
            if csp_policy := response.get('Content-Security-Policy-Report-Only'):
                response['Content-Security-Policy'] = csp_policy
                del response['Content-Security-Policy-Report-Only']
            delattr(response, '_override_report_only_csp')
        return response
</file>

<file path="label_studio/core/mixins.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import logging

from django.db.models.query import QuerySet
from django.utils.functional import cached_property
from rest_framework.generics import get_object_or_404

logger = logging.getLogger(__name__)


class DummyModelMixin:
    def has_permission(self, user):
        return True


class GetParentObjectMixin:
    parent_queryset = None

    @cached_property
    def parent_object(self):
        return self._get_parent_object()

    def _get_parent_object(self):
        """
        The same as get_object method from DRF, but for the parent object
        For example if you want to get project inside /api/projects/ID/tasks handler
        """
        assert self.parent_queryset is not None, (
            "'%s' should include a `parent_queryset` attribute, " % self.__class__.__name__
        )
        queryset = self.parent_queryset
        if isinstance(queryset, QuerySet):
            # Ensure queryset is re-evaluated on each request.
            queryset = queryset.all()

        # Perform the lookup filtering.
        lookup_url_kwarg = self.lookup_url_kwarg or self.lookup_field

        assert lookup_url_kwarg in self.kwargs, (
            'Expected view %s to be called with a URL keyword argument '
            'named "%s". Fix your URL conf, or set the `.lookup_field` '
            'attribute on the view correctly.' % (self.__class__.__name__, lookup_url_kwarg)
        )

        filter_kwargs = {self.lookup_field: self.kwargs[lookup_url_kwarg]}
        obj = get_object_or_404(queryset, **filter_kwargs)

        # May raise a permission denied
        self.check_object_permissions(self.request, obj)

        return obj
</file>

<file path="label_studio/core/models.py">
import logging

from django.db import models
from django.db.models import JSONField
from django.utils.translation import gettext_lazy as _

logger = logging.getLogger(__name__)


class AsyncMigrationStatus(models.Model):
    meta = JSONField(
        'meta',
        null=True,
        default=dict,
        help_text='Meta is for any params for migrations, e.g.: project, filter or error message.',
    )

    project = models.ForeignKey(
        'projects.Project',
        related_name='asyncmigrationstatus',
        on_delete=models.CASCADE,
        null=True,
        help_text='Project ID for this migration',
    )

    name = models.TextField('migration_name', help_text='Migration name')

    STATUS_STARTED = 'STARTED'
    STATUS_IN_PROGRESS = 'IN PROGRESS'
    STATUS_FINISHED = 'FINISHED'
    STATUS_ERROR = 'ERROR'
    STATUS_CHOICES = (
        (STATUS_STARTED, 'Migration is started or queued.'),
        (STATUS_IN_PROGRESS, 'Migration is in progress. Check meta for job_id or status.'),
        (STATUS_FINISHED, 'Migration completed successfully.'),
        (STATUS_ERROR, 'Migration completed with errors. Check meta for more info.'),
    )
    status = models.CharField(max_length=100, choices=STATUS_CHOICES, null=True, default=None)

    created_at = models.DateTimeField(_('created at'), auto_now_add=True, help_text='Creation time')
    updated_at = models.DateTimeField(_('updated at'), auto_now=True, help_text='Last updated time')

    def __str__(self):
        return f'(id={self.id}) ' + self.name + (' at project ' + str(self.project) if self.project else '')
</file>

<file path="label_studio/core/old_ls_migration.py">
import contextlib
import datetime
import io
import json
import os
import pathlib

from core.utils.io import get_data_dir
from core.utils.params import get_env
from data_import.models import FileUpload
from data_manager.models import Filter, FilterGroup, View
from django.core.files.base import File
from io_storages.azure_blob.models import AzureBlobExportStorage, AzureBlobImportStorage
from io_storages.gcs.models import GCSExportStorage, GCSImportStorage
from io_storages.redis.models import RedisExportStorage, RedisImportStorage
from io_storages.s3.models import S3ExportStorage, S3ImportStorage
from ml.models import MLBackend
from tasks.models import Annotation, Prediction, Task


@contextlib.contextmanager
def suppress_autotime(model, fields):
    """allow to keep original created_at value for auto_now_add=True field"""
    _original_values = {}
    for field in model._meta.local_fields:
        if field.name in fields:
            _original_values[field.name] = {'auto_now': field.auto_now, 'auto_now_add': field.auto_now_add}
            field.auto_now = False
            field.auto_now_add = False
    try:
        yield
    finally:
        for field in model._meta.local_fields:
            if field.name in fields:
                field.auto_now = _original_values[field.name]['auto_now']
                field.auto_now_add = _original_values[field.name]['auto_now_add']


def _migrate_tasks(project_path, project):
    """Migrate tasks from json file to database objects"""
    tasks_path = project_path / 'tasks.json'
    with io.open(os.path.abspath(tasks_path), encoding='utf-8') as t:
        tasks_data = json.load(t)
        for task_id, task_data in tasks_data.items():
            task = Task.objects.create(data=task_data.get('data', {}), project=project)

            # migrate annotations
            annotations_path = project_path / 'completions' / '{}.json'.format(task_id)
            if annotations_path.exists():
                with io.open(os.path.abspath(annotations_path), encoding='utf-8') as c:
                    annotations_data = json.load(c)
                    for annotation in annotations_data['completions']:
                        task_annotation = Annotation(
                            result=annotation['result'],
                            task=task,
                            lead_time=annotation['lead_time'],
                            was_cancelled=annotation.get('was_cancelled', False),
                            completed_by=project.created_by,
                        )
                        with suppress_autotime(task_annotation, ['created_at']):
                            task_annotation.created_at = datetime.datetime.fromtimestamp(
                                annotation['created_at'], tz=datetime.datetime.now().astimezone().tzinfo
                            )
                            task_annotation.save()

            # migrate predictions
            predictions_data = task_data.get('predictions', [])
            for prediction in predictions_data:
                task_prediction = Prediction(
                    result=prediction['result'],
                    task=task,
                    score=prediction.get('score'),
                    project=task.project,
                )
                with suppress_autotime(task_prediction, ['created_at']):
                    task_prediction.created_at = datetime.datetime.fromtimestamp(
                        prediction['created_at'], tz=datetime.datetime.now().astimezone().tzinfo
                    )
                    task_prediction.save()


def _migrate_tabs(project_path, project):
    """Migrate tabs from tabs.json to Views table"""
    tabs_path = project_path / 'tabs.json'
    if tabs_path.exists():
        with io.open(os.path.abspath(tabs_path), encoding='utf-8') as t:
            tabs_data = json.load(t)
            for tab in tabs_data['tabs']:
                view = View.objects.create(project=project)
                tab['id'] = view.id
                ordering = tab.pop('ordering', None)
                selected_items = tab.pop('selectedItems', None)

                # migrate filters
                filter_group = None
                filters = tab.pop('filters', None)
                if filters is not None:
                    filter_group = FilterGroup.objects.create(conjunction=filters.get('conjunction', 'and'))
                    if 'items' in filters:
                        for f in filters['items']:
                            view_filter = Filter.objects.create(
                                **{
                                    'column': f.get('filter', ''),
                                    'operator': f.get('operator', ''),
                                    'type': f.get('type', ''),
                                    'value': f.get('value', {}),
                                }
                            )
                            filter_group.filters.add(view_filter)
                hidden_columns = {'explore': [], 'labeling': []}
                hidden_columns_data = tab.pop('hiddenColumns', None)

                # apply naming change to tabs internal data
                if hidden_columns_data is not None:
                    for c in hidden_columns_data.get('explore', []):
                        hidden_columns['explore'].append(c.replace('completion', 'annotation'))
                    for c in hidden_columns_data.get('labeling', []):
                        hidden_columns['labeling'].append(c.replace('completion', 'annotation'))
                    tab['hiddenColumns'] = hidden_columns
                view.data = tab
                view.ordering = ordering
                view.selected_items = selected_items
                view.filter_group = filter_group
                view.save()


def _migrate_storages(project, config):
    """Migrate source and target storages from config.json to database"""

    # source storages migration
    source = config.get('source', None)
    if source:
        if source.get('type') == 'gcs':
            params = source.get('params', {})
            GCSImportStorage.objects.create(
                project=project,
                bucket=source.get('path'),
                prefix=params.get('prefix'),
                regex_filter=params.get('regex'),
                use_blob_urls=params.get('use_blob_urls'),
            )
        elif source.get('type') == 'azure-blob':
            params = source.get('params', {})
            AzureBlobImportStorage.objects.create(
                project=project,
                container=source.get('path'),
                prefix=params.get('prefix'),
                regex_filter=params.get('regex'),
                use_blob_urls=params.get('use_blob_urls'),
            )
        elif source.get('type') == 's3':
            params = source.get('params', {})
            S3ImportStorage.objects.create(
                project=project,
                bucket=source.get('path'),
                prefix=params.get('prefix'),
                regex_filter=params.get('regex'),
                use_blob_urls=params.get('use_blob_urls'),
                region_name=params.get('region'),
            )
        elif source.get('type') == 'redis':
            params = source.get('params', {})
            RedisImportStorage.objects.create(
                project=project,
                path=source.get('path'),
                host=params.get('host'),
                port=params.get('port'),
                password=params.get('password'),
                db=params.get('db', 1),
            )
    # target storages migration
    target = config.get('target', None)
    if target:
        if target.get('type') == 'gcs':
            params = target.get('params', {})
            GCSExportStorage.objects.create(
                project=project,
                bucket=target.get('path'),
                prefix=params.get('prefix'),
                regex_filter=params.get('regex'),
                use_blob_urls=params.get('use_blob_urls'),
            )
        elif target.get('type') == 'azure-blob':
            params = target.get('params', {})
            AzureBlobExportStorage.objects.create(
                project=project,
                container=target.get('path'),
                prefix=params.get('prefix'),
                regex_filter=params.get('regex'),
                use_blob_urls=params.get('use_blob_urls'),
            )
        elif target.get('type') == 's3':
            params = target.get('params', {})
            S3ExportStorage.objects.create(
                project=project,
                bucket=target.get('path'),
                prefix=params.get('prefix'),
                regex_filter=params.get('regex'),
                use_blob_urls=params.get('use_blob_urls'),
                region_name=params.get('region'),
            )
        elif target.get('type') == 'redis':
            params = target.get('params', {})
            RedisExportStorage.objects.create(
                project=project,
                path=target.get('path'),
                host=params.get('host'),
                port=params.get('port'),
                password=params.get('password'),
                db=params.get('db', 1),
            )


def _migrate_ml_backends(project, config):
    """Migrate ml backend settings from config.json to database"""
    ml_backends = config.get('ml_backends', [])
    for ml_backend in ml_backends:
        MLBackend.objects.create(project=project, url=ml_backend.get('url'), title=ml_backend.get('name'))


def _migrate_uploaded_files(project, project_path):
    """Migrate files uploaded by user"""
    source_upload_path = project_path / 'upload'
    if not source_upload_path.exists():
        return
    target_upload_path = pathlib.Path(get_env('LABEL_STUDIO_BASE_DATA_DIR', get_data_dir())) / 'upload'
    if not target_upload_path.exists():
        os.makedirs(str(target_upload_path), exist_ok=True)

    src_files = os.listdir(str(source_upload_path))
    for file_name in src_files:
        full_file_name = os.path.join(str(source_upload_path), file_name)
        with open(full_file_name, 'rb') as f:
            FileUpload.objects.create(user=project.created_by, project=project, file=File(f, name=file_name))


def migrate_existing_project(project_path, project, config):
    """Migration projects from previous version of Label Studio"""

    _migrate_tasks(project_path, project)
    _migrate_tabs(project_path, project)
    _migrate_storages(project, config)
    _migrate_ml_backends(project, config)
    _migrate_uploaded_files(project, project_path)
</file>

<file path="label_studio/core/permissions.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import logging   # noqa: I001
from typing import Optional

from pydantic import BaseModel, ConfigDict

import rules

logger = logging.getLogger(__name__)


class AllPermissions(BaseModel):
    model_config = ConfigDict(protected_namespaces=('__.*__', '_.*'))

    organizations_create: str = 'organizations.create'
    organizations_view: str = 'organizations.view'
    organizations_change: str = 'organizations.change'
    organizations_delete: str = 'organizations.delete'
    organizations_invite: str = 'organizations.invite'
    projects_create: str = 'projects.create'
    projects_view: str = 'projects.view'
    projects_change: str = 'projects.change'
    projects_delete: str = 'projects.delete'
    tasks_create: str = 'tasks.create'
    tasks_view: str = 'tasks.view'
    tasks_change: str = 'tasks.change'
    tasks_delete: str = 'tasks.delete'
    annotations_create: str = 'annotations.create'
    annotations_view: str = 'annotations.view'
    annotations_change: str = 'annotations.change'
    annotations_delete: str = 'annotations.delete'
    actions_perform: str = 'actions.perform'
    predictions_any: str = 'predictions.any'
    avatar_any: str = 'avatar.any'
    labels_create: str = 'labels.create'
    labels_view: str = 'labels.view'
    labels_change: str = 'labels.change'
    labels_delete: str = 'labels.delete'
    models_create: str = 'models.create'
    models_view: str = 'models.view'
    models_change: str = 'models.change'
    models_delete: str = 'models.delete'
    model_provider_connection_create: str = 'model_provider_connection.create'
    model_provider_connection_view: str = 'model_provider_connection.view'
    model_provider_connection_change: str = 'model_provider_connection.change'
    model_provider_connection_delete: str = 'model_provider_connection.delete'


all_permissions = AllPermissions()


class ViewClassPermission(BaseModel):
    GET: Optional[str] = None
    PATCH: Optional[str] = None
    PUT: Optional[str] = None
    DELETE: Optional[str] = None
    POST: Optional[str] = None


def make_perm(name, pred, overwrite=False):
    if rules.perm_exists(name):
        if overwrite:
            rules.remove_perm(name)
        else:
            return
    rules.add_perm(name, pred)


for _, permission_name in all_permissions:
    make_perm(permission_name, rules.is_authenticated)
</file>

<file path="label_studio/core/redis.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import logging
import sys
from datetime import timedelta
from functools import partial

import django_rq
import redis
from django_rq import get_connection
from rq.command import send_stop_job_command
from rq.exceptions import InvalidJobOperation
from rq.registry import StartedJobRegistry

logger = logging.getLogger(__name__)

try:
    _redis = get_connection()
    _redis.ping()
    logger.debug('=> Redis is connected successfully.')
except:  # noqa: E722
    logger.debug('=> Redis is not connected.')
    _redis = None


def redis_healthcheck():
    if not _redis:
        return False
    try:
        _redis.ping()
    except redis.exceptions.ConnectionError as exc:
        logger.error(f'Redis healthcheck failed with ConnectionError: {exc}', exc_info=True)
        return False
    except redis.exceptions.TimeoutError as exc:
        logger.error(f'Redis healthcheck failed with TimeoutError: {exc}', exc_info=True)
        return False
    except redis.exceptions.RedisError as exc:
        logger.error(f'Redis healthcheck failed: {exc}', exc_info=True)
        return False
    else:
        logger.debug('Redis client is alive!')
        return True


def redis_connected():
    return redis_healthcheck()


def redis_get(key):
    if not redis_healthcheck():
        return
    return _redis.get(key)


def redis_hget(key1, key2):
    if not redis_healthcheck():
        return
    return _redis.hget(key1, key2)


def redis_set(key, value, ttl=None):
    if not redis_healthcheck():
        return
    return _redis.set(key, value, ex=ttl)


def redis_hset(key1, key2, value):
    if not redis_healthcheck():
        return
    return _redis.hset(key1, key2, value)


def redis_delete(key):
    if not redis_healthcheck():
        return
    return _redis.delete(key)


def start_job_async_or_sync(job, *args, in_seconds=0, **kwargs):
    """
    Start job async with redis or sync if redis is not connected
    :param job: Job function
    :param args: Function arguments
    :param in_seconds: Job will be delayed for in_seconds
    :param kwargs: Function keywords arguments
    :return: Job or function result
    """

    redis = redis_connected() and kwargs.get('redis', True)
    queue_name = kwargs.get('queue_name', 'default')
    if 'queue_name' in kwargs:
        del kwargs['queue_name']
    if 'redis' in kwargs:
        del kwargs['redis']
    job_timeout = None
    if 'job_timeout' in kwargs:
        job_timeout = kwargs['job_timeout']
        del kwargs['job_timeout']
    if redis:
        logger.info(f'Start async job {job.__name__} on queue {queue_name}.')
        queue = django_rq.get_queue(queue_name)
        enqueue_method = queue.enqueue
        if in_seconds > 0:
            enqueue_method = partial(queue.enqueue_in, timedelta(seconds=in_seconds))
        job = enqueue_method(job, *args, **kwargs, job_timeout=job_timeout)
        return job
    else:
        on_failure = kwargs.pop('on_failure', None)
        try:
            return job(*args, **kwargs)
        except Exception:
            exc_info = sys.exc_info()
            if on_failure:
                on_failure(job, *exc_info)
            raise


def is_job_in_queue(queue, func_name, meta):
    """
    Checks if func_name with kwargs[meta] is in queue (doesn't check workers)
    :param queue: queue object
    :param func_name: function name
    :param meta: job meta information
    :return: True if job in queue
    """
    # get all jobs from Queue
    jobs = get_jobs_by_meta(queue, func_name, meta)
    # check if there is job with meta in list
    return any(jobs)


def is_job_on_worker(job_id, queue_name):
    """
    Checks if job id is on workers
    :param job_id: Job ID
    :param queue_name: Queue name
    :return: True if job on worker
    """
    registry = StartedJobRegistry(queue_name, connection=_redis)
    ids = registry.get_job_ids()
    return job_id in ids


def delete_job_by_id(queue, id):
    """
    Delete job by id from queue
    @param queue: Queue on redis to delete from
    @param id: Job id
    """
    job = queue.fetch_job(id)
    if job is not None:
        # stop job if it is in master redis node (in the queue)
        logger.info(f'Stopping job {id} from queue {queue.name}.')
        try:
            job.cancel()
            job.delete()
            logger.debug(f'Fetched job {id} and stopped.')
        except InvalidJobOperation:
            logger.debug(f'Job {id} was already cancelled.')
    else:
        # try to stop job on worker (job started)
        logger.info(f'Stopping job {id} on worker from queue {queue.name}.')
        try:
            send_stop_job_command(_redis, id)
            logger.debug(f'Send stop job {id} to redis worker.')
        except Exception as e:
            logger.debug(f'Redis job {id} was not found: {str(e)}')


def get_jobs_by_meta(queue, func_name, meta):
    """
    Get jobs from queue by func_name and meta data
    :param queue: Queue on redis to check in
    :param func_name: Started function name
    :param meta: meta dict
    :return: Job list
    """
    # get all jobs from Queue
    jobs = (job for job in queue.get_jobs() if job.func.__name__ == func_name)
    # return only with same meta data
    return [job for job in jobs if hasattr(job, 'meta') and job.meta == meta]
</file>

<file path="label_studio/core/storage.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import logging
import os
import threading
from urllib.parse import unquote, urlsplit, urlunsplit

import google.auth
from django.conf import settings
from django.contrib.staticfiles.storage import ManifestStaticFilesStorage
from storages.backends.azure_storage import AzureStorage
from storages.backends.gcloud import GoogleCloudStorage, _quote, clean_name
from storages.backends.s3boto3 import S3Boto3Storage

logger = logging.getLogger(__name__)


class SkipMissedManifestStaticFilesStorage(ManifestStaticFilesStorage):
    """We need this class to escape missing files from
    django.contrib.staticfiles.finders.FileSystemFinder:
    this class tries to find js/css/png/jpg/... inside of you js/css/...
    """

    # Disable strict cache manifest checking
    manifest_strict = False

    def hashed_name(self, name, content=None, filename=None):
        # `filename` is the name of file to hash if `content` isn't given.
        # `name` is the base name to construct the new hashed filename from.
        parsed_name = urlsplit(unquote(name))
        clean_name = parsed_name.path.strip()
        filename = (filename and urlsplit(unquote(filename)).path.strip()) or clean_name
        opened = content is None
        if opened:
            if not self.exists(filename):
                return ''
            try:
                content = self.open(filename)
            except IOError:
                # Handle directory paths and fragments
                return name
        try:
            file_hash = self.file_hash(clean_name, content)
        finally:
            if opened:
                content.close()
        path, filename = os.path.split(clean_name)
        root, ext = os.path.splitext(filename)
        if file_hash is not None:
            file_hash = '.%s' % file_hash
        hashed_name = os.path.join(path, '%s%s%s' % (root, file_hash, ext))
        unparsed_name = list(parsed_name)
        unparsed_name[2] = hashed_name
        # Special casing for a @font-face hack, like url(myfont.eot?#iefix")
        # http://www.fontspring.com/blog/the-new-bulletproof-font-face-syntax
        if '?#' in name and not unparsed_name[3]:
            unparsed_name[2] += '?'
        return urlunsplit(unparsed_name)


class StorageProxyMixin:
    def url(self, name, storage_url=False, *args, **kwargs):
        if storage_url is True:
            return super().url(name, *args, **kwargs)
        return f'{settings.HOSTNAME}/storage-data/uploaded/?filepath={name}'


class CustomS3Boto3Storage(StorageProxyMixin, S3Boto3Storage):
    pass


class CustomAzureStorage(StorageProxyMixin, AzureStorage):
    pass


class AlternativeGoogleCloudStorageBase(GoogleCloudStorage):
    """A subclass to force the use of the IAM signBlob API
    This allows the signing of blob URLs without having to use a credential file.
    The service account must have the iam.serviceAccounts.signBlob permission."""

    def __init__(self, **settings):
        super().__init__(**settings)
        self._signing_credentials = None
        self._signing_credentials_lock = threading.Lock()

    def url(self, name):
        """
        Return public url or a signed url for the Blob.
        This DOES NOT check for existence of Blob - that makes codes too slow
        for many use cases.
        Overridden to force the use of the IAM signBlob API.
        See https://github.com/googleapis/python-storage/blob/519074112775c19742522158f612b467cf590219/google/cloud/storage/_signing.py#L628  # NOQA
        """
        name = self._normalize_name(clean_name(name))
        blob = self.bucket.blob(name)
        blob_params = self.get_object_parameters(name)
        no_signed_url = blob_params.get('acl', self.default_acl) == 'publicRead' or not self.querystring_auth

        if not self.custom_endpoint and no_signed_url:
            return blob.public_url
        elif no_signed_url:
            out = '{storage_base_url}/{quoted_name}'.format(
                storage_base_url=self.custom_endpoint,
                quoted_name=_quote(name, safe=b'/~'),
            )
            return out
        elif not self.custom_endpoint:
            out2 = blob.generate_signed_url(expiration=self.expiration, version='v4', **self._get_signing_kwargs())
            return out2
        else:
            out3 = blob.generate_signed_url(
                bucket_bound_hostname=self.custom_endpoint,
                expiration=self.expiration,
                version='v4',
                **self._get_signing_kwargs(),
            )
            return out3

    def _get_signing_credentials(self):
        with self._signing_credentials_lock:
            if self._signing_credentials is None or self._signing_credentials.expired:
                credentials, _ = google.auth.default(['https://www.googleapis.com/auth/cloud-platform'])
                auth_req = google.auth.transport.requests.Request()
                credentials.refresh(auth_req)
                self._signing_credentials = credentials
        return self._signing_credentials

    def _get_signing_kwargs(self):
        credentials = self._get_signing_credentials()
        out = {
            'service_account_email': credentials.service_account_email,
            'access_token': credentials.token,
            'credentials': credentials,
        }
        return out


class AlternativeGoogleCloudStorage(StorageProxyMixin, AlternativeGoogleCloudStorageBase):
    pass
</file>

<file path="label_studio/core/urls.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.

URL Configurations

The `urlpatterns` list routes URLs to views. For more information please see:
    https://docs.djangoproject.com/en/2.0/topics/http/urls/
Examples:
Function views
    1. Add an import:  from my_app import views
    2. Add a URL to urlpatterns:  path('', views.home, name='home')
Class-based views
    1. Add an import:  from other_app.views import Home
    2. Add a URL to urlpatterns:  path('', Home.as_view(), name='home')
Including another URLconf
    1. Import the include() function: from django.urls import include, path
    2. Add a URL to urlpatterns:  path('blog/', include('blog.urls'))
"""
from core import views
from core.utils.common import collect_versions
from core.utils.static_serve import serve
from django.conf import settings
from django.conf.urls import include
from django.contrib import admin
from django.urls import path, re_path
from django.views.generic.base import RedirectView
from drf_yasg import openapi
from drf_yasg.views import get_schema_view
from rest_framework.permissions import AllowAny, IsAuthenticated

versions = collect_versions()
open_api_info = openapi.Info(
    title='Label Studio API',
    default_version='v' + versions['release'],
    contact=openapi.Contact(url='https://labelstud.io'),
    x_logo={'url': '../../static/icons/logo-black.svg'},
)

private_schema_view = get_schema_view(
    open_api_info,
    public=True,
    permission_classes=[IsAuthenticated],
)

public_schema_view = get_schema_view(
    open_api_info,
    public=True,
    permission_classes=[AllowAny],
)

urlpatterns = [
    re_path(r'^$', views.main, name='main'),
    re_path(r'^sw\.js$', views.static_file_with_host_resolver('static/js/sw.js', content_type='text/javascript')),
    re_path(
        r'^sw-fallback\.js$',
        views.static_file_with_host_resolver('static/js/sw-fallback.js', content_type='text/javascript'),
    ),
    re_path(r'^favicon\.ico$', RedirectView.as_view(url='/static/images/favicon.ico', permanent=True)),
    re_path(
        r'^label-studio-frontend/(?P<path>.*)$',
        serve,
        kwargs={'document_root': settings.EDITOR_ROOT, 'show_indexes': True},
    ),
    re_path(r'^dm/(?P<path>.*)$', serve, kwargs={'document_root': settings.DM_ROOT, 'show_indexes': True}),
    re_path(
        r'^react-app/(?P<path>.*)$',
        serve,
        kwargs={
            'document_root': settings.REACT_APP_ROOT,
            'show_indexes': True,
            'manifest_asset_prefix': 'react-app',
        },
    ),
    re_path(
        r'^static/fonts/roboto/roboto.css$',
        views.static_file_with_host_resolver('static/fonts/roboto/roboto.css', content_type='text/css'),
    ),
    re_path(r'^static/(?P<path>.*)$', serve, kwargs={'document_root': settings.STATIC_ROOT, 'show_indexes': True}),
    re_path(r'^', include('organizations.urls')),
    re_path(r'^', include('projects.urls')),
    re_path(r'^', include('data_import.urls')),
    re_path(r'^', include('data_manager.urls')),
    re_path(r'^', include('data_export.urls')),
    re_path(r'^', include('users.urls')),
    re_path(r'^', include('tasks.urls')),
    re_path(r'^', include('io_storages.urls')),
    re_path(r'^', include('ml.urls')),
    re_path(r'^', include('webhooks.urls')),
    re_path(r'^', include('labels_manager.urls')),
    re_path(r'data/local-files/', views.localfiles_data, name='localfiles_data'),
    re_path(r'version/', views.version_page, name='version'),  # html page
    re_path(r'api/version/', views.version_page, name='api-version'),  # json response
    re_path(r'health/', views.health, name='health'),
    re_path(r'metrics/', views.metrics, name='metrics'),
    re_path(r'trigger500/', views.TriggerAPIError.as_view(), name='metrics'),
    re_path(r'samples/time-series.csv', views.samples_time_series, name='static_time_series'),
    re_path(r'samples/paragraphs.json', views.samples_paragraphs, name='samples_paragraphs'),
    re_path(
        r'^swagger(?P<format>\.json|\.yaml)$', private_schema_view.without_ui(cache_timeout=0), name='schema-json'
    ),
    re_path(r'^swagger/$', private_schema_view.with_ui('swagger', cache_timeout=0), name='schema-swagger-ui'),
    path('docs/api/', public_schema_view.with_ui('redoc', cache_timeout=0), name='schema-redoc'),
    path(
        'docs/',
        RedirectView.as_view(url='/static/docs/public/guide/introduction.html', permanent=False),
        name='docs-redirect',
    ),
    path('admin/', admin.site.urls),
    path('django-rq/', include('django_rq.urls')),
    path('feature-flags/', views.feature_flags, name='feature_flags'),
    path('heidi-tips/', views.heidi_tips, name='heidi_tips'),
    path('__lsa/', views.collect_metrics, name='collect_metrics'),
    re_path(r'^api-auth/', include('rest_framework.urls', namespace='rest_framework')),
    re_path(r'^', include('jwt_auth.urls')),
]

if settings.DEBUG:
    try:
        import debug_toolbar

        urlpatterns = [path('__debug__/', include(debug_toolbar.urls))] + urlpatterns
    except ImportError:
        pass
</file>

<file path="label_studio/core/validators.py">
import django
import jsonschema
from django.core.validators import BaseValidator


class JSONSchemaValidator(BaseValidator):
    def compare(self, input, schema):
        try:
            jsonschema.validate(input, schema)
        except jsonschema.exceptions.ValidationError:
            raise django.core.exceptions.ValidationError('%(value)s failed JSON schema check', params={'value': input})
</file>

<file path="label_studio/core/version.py">
from __future__ import print_function

"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
""" Version Lib
Copyright (C) 2019 Maxim Tkachenko 

This library automatically generates version of package based on git.
If 'git desc' is successful it will write version to __version__.py:git_version.  
If 'git desc' is fail it will read __version__.py:git_version.

ATTENTION: do not include version_.py to git! It will affect git commit always!
"""
import json
import os
import sys
from subprocess import STDOUT, CalledProcessError
from subprocess import check_output as run

VERSION_FILE = 'version_.py'
LS_VERSION_FILE = 'ls-version_.py'
VERSION_OVERRIDE = os.getenv('VERSION_OVERRIDE', '')
BRANCH_OVERRIDE = os.getenv('BRANCH_OVERRIDE', '')


def _write_py(info):
    # go to current dir to package __init__.py
    cwd = os.getcwd()
    d = os.path.dirname(__file__)
    d = d if d else '.'
    os.chdir(d)

    info_str = json.dumps(info)

    # write txt
    with open(VERSION_FILE, 'w') as f:
        os.chdir(cwd)  # back current dir
        f.write(
            'info = %s\n' % info_str + '\n# This file is automatically generated by version.py'
            '\n# Do not include it to git!\n'
        )


def _read_py(ls=False):
    # go to current dir to package __init__.py
    cwd = os.getcwd()
    d = os.path.dirname(__file__)
    d = d if d else '.'
    sys.path.append(d)
    os.chdir(d)

    # read version
    def import_version_module(file_path):
        try:
            return __import__(os.path.splitext(file_path)[0])
        except ImportError:
            return None

    try:
        version_module = import_version_module(LS_VERSION_FILE if ls else VERSION_FILE)

        if not version_module and ls:
            version_module = import_version_module(VERSION_FILE)

        if version_module:
            return version_module.info
        else:
            return {}
    finally:
        os.chdir(cwd)  # back to current dir


# get commit info: message, date, hash, branch
def get_git_commit_info(skip_os=True, ls=False):

    cwd = os.getcwd()
    d = os.path.dirname(__file__)
    d = d if d else '.'
    os.chdir(d)

    try:
        # take version from git
        try:
            desc = run('git describe --long --tags --always --dirty', stderr=STDOUT, shell=True).decode('utf-8')
            info = {
                'message': run('git show -s --format=%s', stderr=STDOUT, shell=True).strip().decode('utf8'),
                'commit': run('git show -s --format=%H', stderr=STDOUT, shell=True).strip().decode('utf8'),
                'date': run('git log -1 --format="%cd" --date="format:%Y/%m/%d %H:%M:%S"', stderr=STDOUT, shell=True)
                .strip()
                .decode('utf8'),
                'branch': BRANCH_OVERRIDE
                if BRANCH_OVERRIDE
                else run(
                    "git branch --sort=committerdate -r --contains | grep -m 1 -v HEAD | cut -d'/' -f2-",
                    stderr=STDOUT,
                    shell=True,
                )
                .strip()
                .decode('utf8'),
            }
        except CalledProcessError:
            os.chdir(cwd)
            return _read_py(ls=True)

        # create package version
        version = desc.lstrip('v').rstrip().replace('-', '+', 1).replace('-', '.')
        # take OS name
        if not skip_os:
            keys = ('ID=', 'VERSION_ID=', 'RELEASE=')
            with open('/etc/os-release') as f:
                os_version = ''.join(
                    str(s).split('=', 1)[1].rstrip().strip('"').replace('.', '') for s in f if str(s).startswith(keys)
                )
                version += '.' + os_version
        info['version'] = VERSION_OVERRIDE if VERSION_OVERRIDE else version

        _write_py(info)
        return info

    except Exception as e:
        raise e

    finally:
        os.chdir(cwd)  # back current dir


def get_git_version(skip_os=True):
    info = get_git_commit_info(skip_os)
    return info.get('version', '')


# get only tag from git
def get_short_version():
    version = get_git_version()
    return version.split('+')[0]


if __name__ == '__main__':
    # init version_.py file
    get_git_version()
</file>

<file path="label_studio/core/views.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import io
import json
import logging
import mimetypes
import os
import posixpath
from pathlib import Path
from wsgiref.util import FileWrapper

import pandas as pd
import requests
from core import utils
from core.feature_flags import all_flags, flag_set, get_feature_file_path
from core.label_config import generate_time_series_json
from core.utils.common import collect_versions
from core.utils.io import find_file
from django.conf import settings
from django.contrib.auth import logout
from django.db.models import CharField, F, Value
from django.http import (
    HttpResponse,
    HttpResponseForbidden,
    HttpResponseNotFound,
    JsonResponse,
)
from django.shortcuts import redirect, render, reverse
from django.utils._os import safe_join
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods
from drf_yasg.utils import swagger_auto_schema
from io_storages.localfiles.models import LocalFilesImportStorage
from ranged_fileresponse import RangedFileResponse
from rest_framework.decorators import api_view, permission_classes
from rest_framework.permissions import IsAuthenticated
from rest_framework.views import APIView

logger = logging.getLogger(__name__)


_PARAGRAPH_SAMPLE = None


def main(request):
    user = request.user

    if user.is_authenticated:

        if user.active_organization is None and 'organization_pk' not in request.session:
            logout(request)
            return redirect(reverse('user-login'))

        # business mode access
        if flag_set('fflag_all_feat_dia_1777_ls_homepage_short', user):
            print('redirect to home page')
            return render(request, 'home/home.html')
        else:
            return redirect(reverse('projects:project-index'))

    # not authenticated
    return redirect(reverse('user-login'))


def version_page(request):
    """Get platform version"""
    # update the latest version from pypi response
    # from label_studio.core.utils.common import check_for_the_latest_version
    # check_for_the_latest_version(print_message=False)
    http_page = request.path == '/version/'
    result = collect_versions(force=http_page)

    # html / json response
    if request.path == '/version/':
        # other settings from backend
        if not getattr(settings, 'CLOUD_INSTANCE', False) and request.user.is_superuser:
            result['settings'] = {
                key: str(getattr(settings, key))
                for key in dir(settings)
                if not key.startswith('_') and not hasattr(getattr(settings, key), '__call__')
            }

        result = json.dumps(result, indent=2)
        result = result.replace('},', '},\n').replace('\\n', ' ').replace('\\r', '')
        return HttpResponse('<pre>' + result + '</pre>')
    else:
        return JsonResponse(result)


def health(request):
    """System health info"""
    logger.debug('Got /health request.')
    return HttpResponse(json.dumps({'status': 'UP'}))


def metrics(request):
    """Empty page for metrics evaluation"""
    return HttpResponse('')


class TriggerAPIError(APIView):
    """500 response for testing"""

    authentication_classes = ()
    permission_classes = ()

    @swagger_auto_schema(auto_schema=None)
    def get(self, request):
        raise Exception('test')


def editor_files(request):
    """Get last editor files"""
    response = utils.common.find_editor_files()
    return HttpResponse(json.dumps(response), status=200)


def samples_time_series(request):
    """Generate time series example for preview"""
    time_column = request.GET.get('time', '')
    value_columns = request.GET.get('values', '').split(',')
    time_format = request.GET.get('tf')

    # separator processing
    separator = request.GET.get('sep', ',')
    separator = separator.replace('\\t', '\t')
    aliases = {'dot': '.', 'comma': ',', 'tab': '\t', 'space': ' '}
    if separator in aliases:
        separator = aliases[separator]

    # check headless or not
    header = True
    if all(n.isdigit() for n in [time_column] + value_columns):
        header = False

    # generate all columns for headless csv
    if not header:
        max_column_n = max([int(v) for v in value_columns] + [0])
        value_columns = range(1, max_column_n + 1)

    ts = generate_time_series_json(time_column, value_columns, time_format)
    csv_data = pd.DataFrame.from_dict(ts).to_csv(index=False, header=header, sep=separator).encode('utf-8')

    # generate response data as file
    filename = 'time-series.csv'
    response = HttpResponse(csv_data, content_type='application/csv')
    response['Content-Disposition'] = f'attachment; filename="{filename}"'
    response['filename'] = filename
    return response


def samples_paragraphs(request):
    """Generate paragraphs example for preview"""
    global _PARAGRAPH_SAMPLE

    if _PARAGRAPH_SAMPLE is None:
        with open(find_file('paragraphs.json'), encoding='utf-8') as f:
            _PARAGRAPH_SAMPLE = json.load(f)
    name_key = request.GET.get('nameKey', 'author')
    text_key = request.GET.get('textKey', 'text')

    result = []
    for line in _PARAGRAPH_SAMPLE:
        result.append({name_key: line['author'], text_key: line['text']})

    return HttpResponse(json.dumps(result), content_type='application/json')


def heidi_tips(request):
    """Fetch live tips from github raw liveContent.json to avoid caching and client side CORS issues"""
    url = 'https://raw.githubusercontent.com/HumanSignal/label-studio/refs/heads/develop/web/apps/labelstudio/src/components/HeidiTips/liveContent.json'

    response = None
    try:
        response = requests.get(
            url,
            headers={'Cache-Control': 'no-cache', 'Content-Type': 'application/json', 'Accept': 'application/json'},
            timeout=5,
        )
        # Raise an exception for bad status codes to avoid caching
        response.raise_for_status()
    # Catch all exceptions and return either the status code if there was a response, or default to 404 if there are network issues
    # This is done this way to catch thrown exceptions from the request itself which will occur for air-gapped environments
    except Exception:
        # Any other HTTP error will return the error code, and other errors like connection/timeout errors will be a 404
        content = {}
        status_code = 404
        if response is not None:
            content['detail'] = response.reason
            status_code = response.status_code
        return HttpResponse(json.dumps(content), content_type='application/json', status=status_code)

    return HttpResponse(response.content, content_type='application/json')


@swagger_auto_schema(methods=['GET'], auto_schema=None)
@api_view(['GET'])
@permission_classes([IsAuthenticated])
def localfiles_data(request):
    """Serving files for LocalFilesImportStorage"""
    user = request.user
    path = request.GET.get('d')
    if settings.LOCAL_FILES_SERVING_ENABLED is False:
        return HttpResponseForbidden(
            "Serving local files can be dangerous, so it's disabled by default. "
            'You can enable it with LOCAL_FILES_SERVING_ENABLED environment variable, '
            'please check docs: https://labelstud.io/guide/storage.html#Local-storage'
        )

    local_serving_document_root = settings.LOCAL_FILES_DOCUMENT_ROOT
    if path and request.user.is_authenticated:
        path = posixpath.normpath(path).lstrip('/')
        full_path = Path(safe_join(local_serving_document_root, path))
        user_has_permissions = False

        # Try to find Local File Storage connection based prefix:
        # storage.path=/home/user, full_path=/home/user/a/b/c/1.jpg =>
        # full_path.startswith(path) => True
        localfiles_storage = LocalFilesImportStorage.objects.annotate(
            _full_path=Value(os.path.dirname(full_path), output_field=CharField())
        ).filter(_full_path__startswith=F('path'))
        if localfiles_storage.exists():
            user_has_permissions = any(storage.project.has_permission(user) for storage in localfiles_storage)

        if user_has_permissions and os.path.exists(full_path):
            content_type, encoding = mimetypes.guess_type(str(full_path))
            content_type = content_type or 'application/octet-stream'
            return RangedFileResponse(request, open(full_path, mode='rb'), content_type)
        else:
            return HttpResponseNotFound()

    return HttpResponseForbidden()


def static_file_with_host_resolver(path_on_disk, content_type):
    """Load any file, replace {{HOSTNAME}} => settings.HOSTNAME, send it as http response"""
    path_on_disk = os.path.join(os.path.dirname(__file__), path_on_disk)

    def serve_file(request):
        with open(path_on_disk, 'r') as f:
            body = f.read()
            body = body.replace('{{HOSTNAME}}', settings.HOSTNAME)

            out = io.StringIO()
            out.write(body)
            out.seek(0)

            wrapper = FileWrapper(out)
            response = HttpResponse(wrapper, content_type=content_type)
            response['Content-Length'] = len(body)
            return response

    return serve_file


def feature_flags(request):
    user = request.user
    if not user.is_authenticated:
        return HttpResponseForbidden()

    flags = all_flags(request.user)
    flags['$system'] = {
        'FEATURE_FLAGS_DEFAULT_VALUE': settings.FEATURE_FLAGS_DEFAULT_VALUE,
        'FEATURE_FLAGS_FROM_FILE': settings.FEATURE_FLAGS_FROM_FILE,
        'FEATURE_FLAGS_FILE': get_feature_file_path(),
        'VERSION_EDITION': settings.VERSION_EDITION,
        'CLOUD_INSTANCE': settings.CLOUD_INSTANCE if hasattr(settings, 'CLOUD_INSTANCE') else None,
    }

    return HttpResponse('<pre>' + json.dumps(flags, indent=4) + '</pre>', status=200)


@csrf_exempt
@require_http_methods(['POST', 'GET'])
def collect_metrics(request):
    """Lightweight endpoint to collect usage metrics from the frontend only when COLLECT_ANALYTICS is enabled"""
    return HttpResponse(status=204)
</file>

<file path="label_studio/core/wsgi.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""

import os

from django.core.wsgi import get_wsgi_application

os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'core.settings')

application = get_wsgi_application()
</file>

<file path="label_studio/data_export/migrations/__init__.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
</file>

<file path="label_studio/data_export/migrations/0001_initial.py">
# Generated by Django 3.1.12 on 2021-09-08 18:13

from django.conf import settings
from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    initial = True

    dependencies = [
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
        ('projects', '0012_auto_20210906_1323'),
    ]

    operations = [
        migrations.CreateModel(
            name='Export',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('created_at', models.DateTimeField(auto_now_add=True, help_text='Creation time', verbose_name='created at')),
                ('file', models.FileField(null=True, upload_to='export')),
                ('md5', models.CharField(default='', max_length=128, verbose_name='md5 of file')),
                ('finished_at', models.DateTimeField(default=None, help_text='Complete or fail time', null=True, verbose_name='finished at')),
                ('status', models.CharField(choices=[('created', 'Created'), ('in_progress', 'In progress'), ('failed', 'Failed'), ('completed', 'Completed')], default='created', max_length=64, verbose_name='Exporting status')),
                ('counters', models.JSONField(default=dict, verbose_name='Exporting meta data')),
                ('only_finished', models.BooleanField(default=False, help_text='If true - it exports only finished tasks', verbose_name='Only finished')),
                ('task_ids', models.JSONField(default=list, help_text='If list is empty - download all tasks', verbose_name='Task ids list')),
                ('created_by', models.ForeignKey(null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='created_exports', to=settings.AUTH_USER_MODEL, verbose_name='created by')),
                ('project', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='exports', to='projects.project')),
            ],
        ),
    ]
</file>

<file path="label_studio/data_export/migrations/0002_auto_20210921_0954.py">
# Generated by Django 3.1.13 on 2021-09-21 09:54

from django.db import migrations


class Migration(migrations.Migration):

    dependencies = [
        ('data_export', '0001_initial'),
    ]

    operations = [
        migrations.RemoveField(
            model_name='export',
            name='only_finished',
        ),
        migrations.RemoveField(
            model_name='export',
            name='task_ids',
        ),
    ]
</file>

<file path="label_studio/data_export/migrations/0003_auto_20211004_1416.py">
# Generated by Django 3.1.13 on 2021-10-04 14:16

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('data_export', '0002_auto_20210921_0954'),
    ]

    operations = [
        migrations.AddField(
            model_name='export',
            name='title',
            field=models.CharField(blank=True, default='', max_length=50, verbose_name='title'),
        ),
        migrations.AlterField(
            model_name='export',
            name='status',
            field=models.CharField(
                choices=[
                    ('created', 'Created'),
                    ('in_progress', 'In progress'),
                    ('failed', 'Failed'),
                    ('completed', 'Completed'),
                ],
                default='created',
                max_length=64,
                verbose_name='Export status',
            ),
        ),
    ]
</file>

<file path="label_studio/data_export/migrations/0004_auto_20211019_0852.py">
# Generated by Django 3.1.13 on 2021-10-19 08:52

from django.conf import settings
from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    dependencies = [
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
        ('data_export', '0003_auto_20211004_1416'),
    ]

    operations = [
        migrations.AlterField(
            model_name='export',
            name='created_by',
            field=models.ForeignKey(null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='+', to=settings.AUTH_USER_MODEL, verbose_name='created by'),
        ),
    ]
</file>

<file path="label_studio/data_export/migrations/0005_auto_20211025_1137.py">
# Generated by Django 3.1.13 on 2021-10-25 11:37

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('data_export', '0004_auto_20211019_0852'),
    ]

    operations = [
        migrations.AlterField(
            model_name='export',
            name='title',
            field=models.CharField(blank=True, default='', max_length=2048, verbose_name='title'),
        ),
    ]
</file>

<file path="label_studio/data_export/migrations/0006_convertedformat.py">
# Generated by Django 3.2.16 on 2023-03-23 14:12

from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    dependencies = [
        ('data_export', '0005_auto_20211025_1137'),
    ]

    operations = [
        migrations.CreateModel(
            name='ConvertedFormat',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('file', models.FileField(null=True, upload_to='export')),
                ('status', models.CharField(choices=[('created', 'Created'), ('in_progress', 'In progress'), ('failed', 'Failed'), ('completed', 'Completed')], default='created', max_length=64)),
                ('export_type', models.CharField(choices=[('created', 'Created'), ('in_progress', 'In progress'), ('failed', 'Failed'), ('completed', 'Completed')], default='created', max_length=64)),
                ('export', models.ForeignKey(help_text='Export snapshot for this converted file', on_delete=django.db.models.deletion.CASCADE, related_name='converted_formats', to='data_export.export')),
            ],
        ),
    ]
</file>

<file path="label_studio/data_export/migrations/0007_auto_20230327_1910.py">
# Generated by Django 3.2.16 on 2023-03-27 19:10

from django.conf import settings
from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    dependencies = [
        ('organizations', '0003_auto_20211010_1339'),
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
        ('projects', '0021_merge_20230215_1943'),
        ('data_export', '0006_convertedformat'),
    ]

    operations = [
        migrations.AddField(
            model_name='convertedformat',
            name='created_at',
            field=models.DateTimeField(auto_now_add=True, help_text='Creation time', null=True, verbose_name='created at'),
        ),
        migrations.AddField(
            model_name='convertedformat',
            name='created_by',
            field=models.ForeignKey(null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='+', to=settings.AUTH_USER_MODEL, verbose_name='created by'),
        ),
        migrations.AddField(
            model_name='convertedformat',
            name='finished_at',
            field=models.DateTimeField(default=None, help_text='Complete or fail time', null=True, verbose_name='finished at'),
        ),
        migrations.AddField(
            model_name='convertedformat',
            name='organization',
            field=models.ForeignKey(null=True, on_delete=django.db.models.deletion.CASCADE, related_name='export_conversions', to='organizations.organization'),
        ),
        migrations.AddField(
            model_name='convertedformat',
            name='project',
            field=models.ForeignKey(null=True, on_delete=django.db.models.deletion.CASCADE, related_name='export_conversions', to='projects.project'),
        ),
        migrations.AddField(
            model_name='convertedformat',
            name='updated_at',
            field=models.DateTimeField(auto_now_add=True, help_text='Updated time', null=True, verbose_name='updated at'),
        ),
    ]
</file>

<file path="label_studio/data_export/migrations/0008_convertedformat_traceback.py">
# Generated by Django 3.2.16 on 2023-03-31 17:50

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('data_export', '0007_auto_20230327_1910'),
    ]

    operations = [
        migrations.AddField(
            model_name='convertedformat',
            name='traceback',
            field=models.TextField(blank=True, null=True),
        ),
    ]
</file>

<file path="label_studio/data_export/migrations/0009_alter_convertedformat_traceback.py">
# Generated by Django 3.2.16 on 2023-04-13 23:01

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('data_export', '0008_convertedformat_traceback'),
    ]

    operations = [
        migrations.AlterField(
            model_name='convertedformat',
            name='traceback',
            field=models.TextField(blank=True, help_text='Traceback report in case of errors', null=True),
        ),
    ]
</file>

<file path="label_studio/data_export/migrations/0010_alter_convertedformat_export_type.py">
# Generated by Django 3.2.16 on 2023-04-19 11:36

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('data_export', '0009_alter_convertedformat_traceback'),
    ]

    operations = [
        migrations.AlterField(
            model_name='convertedformat',
            name='export_type',
            field=models.CharField(max_length=64),
        ),
    ]
</file>

<file path="label_studio/data_export/__init__.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
</file>

<file path="label_studio/data_export/api.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import logging
import os
import traceback as tb
from datetime import datetime
from urllib.parse import urlparse

from core.feature_flags import flag_set
from core.permissions import all_permissions
from core.redis import start_job_async_or_sync
from core.utils.common import batch
from django.conf import settings
from django.core.files import File
from django.core.files.storage import FileSystemStorage
from django.db import transaction
from django.http import FileResponse, HttpResponse
from django.utils.decorators import method_decorator
from drf_yasg import openapi as openapi
from drf_yasg.utils import swagger_auto_schema
from projects.models import Project
from ranged_fileresponse import RangedFileResponse
from rest_framework import generics, status
from rest_framework.exceptions import NotFound, ValidationError
from rest_framework.response import Response
from rest_framework.views import APIView
from tasks.models import Task

from .models import ConvertedFormat, DataExport, Export
from .serializers import (
    ExportConvertSerializer,
    ExportCreateSerializer,
    ExportDataSerializer,
    ExportParamSerializer,
    ExportSerializer,
)

logger = logging.getLogger(__name__)


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Export'],
        operation_summary='Get export formats',
        x_fern_sdk_group_name=['projects', 'exports'],
        x_fern_sdk_method_name='list_formats',
        x_fern_audiences=['public'],
        operation_description='Retrieve the available export formats for the current project by ID.',
        manual_parameters=[
            openapi.Parameter(
                name='id',
                type=openapi.TYPE_INTEGER,
                in_=openapi.IN_PATH,
                description='A unique integer value identifying this project.',
            ),
        ],
        responses={
            200: openapi.Response(
                description='Export formats',
                schema=openapi.Schema(
                    title='Format list',
                    description='List of available formats',
                    type=openapi.TYPE_ARRAY,
                    items=openapi.Schema(title='Export format', type=openapi.TYPE_STRING),
                ),
            )
        },
    ),
)
class ExportFormatsListAPI(generics.RetrieveAPIView):
    permission_required = all_permissions.projects_view

    def get_queryset(self):
        return Project.objects.filter(organization=self.request.user.active_organization)

    def get(self, request, *args, **kwargs):
        project = self.get_object()
        formats = DataExport.get_export_formats(project)
        return Response(formats)


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        x_fern_sdk_group_name='projects',
        x_fern_sdk_method_name='create_export',
        x_fern_audiences=['public'],
        manual_parameters=[
            openapi.Parameter(
                name='export_type',
                type=openapi.TYPE_STRING,
                in_=openapi.IN_QUERY,
                description='Selected export format (JSON by default)',
            ),
            openapi.Parameter(
                name='download_all_tasks',
                type=openapi.TYPE_STRING,
                in_=openapi.IN_QUERY,
                description="""
                          If true, download all tasks regardless of status. If false, download only annotated tasks.
                          """,
            ),
            openapi.Parameter(
                name='download_resources',
                type=openapi.TYPE_BOOLEAN,
                in_=openapi.IN_QUERY,
                description="""
                          If true, download all resource files such as images, audio, and others relevant to the tasks.
                          """,
            ),
            openapi.Parameter(
                name='ids',
                type=openapi.TYPE_ARRAY,
                items=openapi.Schema(title='Task ID', description='Individual task ID', type=openapi.TYPE_INTEGER),
                in_=openapi.IN_QUERY,
                description="""
                          Specify a list of task IDs to retrieve only the details for those tasks.
                          """,
            ),
            openapi.Parameter(
                name='id',
                type=openapi.TYPE_INTEGER,
                in_=openapi.IN_PATH,
                description='A unique integer value identifying this project.',
            ),
        ],
        tags=['Export'],
        operation_summary='Easy export of tasks and annotations',
        operation_description="""
        <i>Note: if you have a large project it's recommended to use
        export snapshots, this easy export endpoint might have timeouts.</i><br/><br>
        Export annotated tasks as a file in a specific format.
        For example, to export JSON annotations for a project to a file called `annotations.json`,
        run the following from the command line:
        ```bash
        curl -X GET {}/api/projects/{{id}}/export?exportType=JSON -H \'Authorization: Token abc123\' --output 'annotations.json'
        ```
        To export all tasks, including skipped tasks and others without annotations, run the following from the command line:
        ```bash
        curl -X GET {}/api/projects/{{id}}/export?exportType=JSON&download_all_tasks=true -H \'Authorization: Token abc123\' --output 'annotations.json'
        ```
        To export specific tasks with IDs of 123 and 345, run the following from the command line:
        ```bash
        curl -X GET {}/api/projects/{{id}}/export?ids[]=123\&ids[]=345 -H \'Authorization: Token abc123\' --output 'annotations.json'
        ```
        """.format(
            settings.HOSTNAME or 'https://localhost:8080',
            settings.HOSTNAME or 'https://localhost:8080',
            settings.HOSTNAME or 'https://localhost:8080',
        ),
        responses={
            200: openapi.Response(
                description='Exported data',
                schema=openapi.Schema(
                    title='Export file', description='Export file with results', type=openapi.TYPE_FILE
                ),
            )
        },
    ),
)
class ExportAPI(generics.RetrieveAPIView):
    permission_required = all_permissions.projects_change

    def get_queryset(self):
        return Project.objects.filter(organization=self.request.user.active_organization)

    def get_task_queryset(self, queryset):
        return queryset.select_related('project').prefetch_related('annotations', 'predictions')

    def get(self, request, *args, **kwargs):
        project = self.get_object()
        query_serializer = ExportParamSerializer(data=request.GET)
        query_serializer.is_valid(raise_exception=True)

        export_type = (
            query_serializer.validated_data.get('exportType') or query_serializer.validated_data['export_type']
        )
        only_finished = not query_serializer.validated_data['download_all_tasks']
        download_resources = query_serializer.validated_data['download_resources']
        interpolate_key_frames = query_serializer.validated_data['interpolate_key_frames']

        tasks_ids = request.GET.getlist('ids[]')

        logger.debug('Get tasks')
        query = Task.objects.filter(project=project)
        if tasks_ids and len(tasks_ids) > 0:
            logger.debug(f'Select only subset of {len(tasks_ids)} tasks')
            query = query.filter(id__in=tasks_ids)
        if only_finished:
            query = query.filter(annotations__isnull=False).distinct()

        task_ids = query.values_list('id', flat=True)

        logger.debug('Serialize tasks for export')
        tasks = []
        for _task_ids in batch(task_ids, 1000):
            tasks += ExportDataSerializer(
                self.get_task_queryset(query.filter(id__in=_task_ids)),
                many=True,
                expand=['drafts'],
                context={'interpolate_key_frames': interpolate_key_frames},
            ).data
        logger.debug('Prepare export files')

        export_file, content_type, filename = DataExport.generate_export_file(
            project, tasks, export_type, download_resources, request.GET, hostname=request.build_absolute_uri('/')
        )

        r = FileResponse(export_file, as_attachment=True, content_type=content_type, filename=filename)
        r['filename'] = filename
        return r


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Export'],
        operation_summary='List exported files',
        operation_description="""
        Retrieve a list of files exported from the Label Studio UI using the Export button on the Data Manager page.
        To retrieve the files themselves, see [Download export file](/api#operation/api_projects_exports_download_read).
        """,
    ),
)
class ProjectExportFiles(generics.RetrieveAPIView):
    permission_required = all_permissions.projects_change
    swagger_schema = None  # hide export files endpoint from swagger

    def get_queryset(self):
        return Project.objects.filter(organization=self.request.user.active_organization)

    def get(self, request, *args, **kwargs):
        # project permission check
        self.get_object()

        paths = []
        for name in os.listdir(settings.EXPORT_DIR):
            if name.endswith('.json') and not name.endswith('-info.json'):
                project_id = name.split('-')[0]
                if str(kwargs['pk']) == project_id:
                    paths.append(settings.EXPORT_URL_ROOT + name)

        items = [{'name': p.split('/')[2].split('.')[0], 'url': p} for p in sorted(paths)[::-1]]
        return Response({'export_files': items}, status=status.HTTP_200_OK)


class ProjectExportFilesAuthCheck(APIView):
    """Check auth for nginx auth_request (/api/auth/export/)"""

    swagger_schema = None
    http_method_names = ['get']
    permission_required = all_permissions.projects_change

    def get(self, request, *args, **kwargs):
        """Get export files list"""
        original_url = request.META['HTTP_X_ORIGINAL_URI']
        filename = original_url.replace('/export/', '')
        project_id = filename.split('-')[0]
        try:
            pk = int(project_id)
        except ValueError:
            return Response({'detail': 'Incorrect filename in export'}, status=status.HTTP_422_UNPROCESSABLE_ENTITY)

        generics.get_object_or_404(Project.objects.filter(organization=self.request.user.active_organization), pk=pk)
        return Response({'detail': 'auth ok'}, status=status.HTTP_200_OK)


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Export'],
        x_fern_sdk_group_name=['projects', 'exports'],
        x_fern_sdk_method_name='list',
        x_fern_audiences=['public'],
        operation_summary='List all export snapshots',
        operation_description="""
        Returns a list of exported files for a specific project by ID.
        """,
        manual_parameters=[
            openapi.Parameter(
                name='id',
                type=openapi.TYPE_INTEGER,
                in_=openapi.IN_PATH,
                description='A unique integer value identifying this project.',
            )
        ],
    ),
)
@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Export'],
        operation_summary='Create new export snapshot',
        x_fern_sdk_group_name=['projects', 'exports'],
        x_fern_sdk_method_name='create',
        x_fern_audiences=['public'],
        operation_description="""
        Create a new export request to start a background task and generate an export file for a specific project by ID.
        """,
        manual_parameters=[
            openapi.Parameter(
                name='id',
                type=openapi.TYPE_INTEGER,
                in_=openapi.IN_PATH,
                description='A unique integer value identifying this project.',
            )
        ],
    ),
)
class ExportListAPI(generics.ListCreateAPIView):
    queryset = Export.objects.all().order_by('-created_at')
    project_model = Project
    serializer_class = ExportSerializer
    permission_required = all_permissions.projects_change

    def get_serializer_class(self):
        if self.request.method == 'GET':
            return ExportSerializer
        if self.request.method == 'POST':
            return ExportCreateSerializer
        return super().get_serializer_class()

    def get_serializer_context(self):
        context = super(ExportListAPI, self).get_serializer_context()
        context['user'] = self.request.user
        return context

    def _get_project(self):
        project_pk = self.kwargs.get('pk')
        project = generics.get_object_or_404(
            self.project_model.objects.for_user(self.request.user),
            pk=project_pk,
        )
        return project

    def perform_create(self, serializer):
        task_filter_options = serializer.validated_data.pop('task_filter_options')
        annotation_filter_options = serializer.validated_data.pop('annotation_filter_options')
        serialization_options = serializer.validated_data.pop('serialization_options')

        project = self._get_project()
        serializer.save(project=project, created_by=self.request.user)
        instance = serializer.instance

        instance.run_file_exporting(
            task_filter_options=task_filter_options,
            annotation_filter_options=annotation_filter_options,
            serialization_options=serialization_options,
        )

    def get_queryset(self):
        project = self._get_project()
        return super().get_queryset().filter(project=project)

    def filter_queryset(self, queryset):
        queryset = super().filter_queryset(queryset)

        return queryset.order_by('-created_at')[:100]


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Export'],
        x_fern_sdk_group_name=['projects', 'exports'],
        x_fern_sdk_method_name='get',
        x_fern_audiences=['public'],
        operation_summary='Get export snapshot by ID',
        operation_description="""
        Retrieve information about an export file by export ID for a specific project.
        """,
        manual_parameters=[
            openapi.Parameter(
                name='id',
                type=openapi.TYPE_INTEGER,
                in_=openapi.IN_PATH,
                description='A unique integer value identifying this project.',
            ),
            openapi.Parameter(
                name='export_pk',
                type=openapi.TYPE_STRING,
                in_=openapi.IN_PATH,
                description='Primary key identifying the export file.',
            ),
        ],
    ),
)
@method_decorator(
    name='delete',
    decorator=swagger_auto_schema(
        tags=['Export'],
        x_fern_sdk_group_name=['projects', 'exports'],
        x_fern_sdk_method_name='delete',
        x_fern_audiences=['public'],
        operation_summary='Delete export snapshot',
        operation_description="""
        Delete an export file by specified export ID.
        """,
        manual_parameters=[
            openapi.Parameter(
                name='id',
                type=openapi.TYPE_INTEGER,
                in_=openapi.IN_PATH,
                description='A unique integer value identifying this project.',
            ),
            openapi.Parameter(
                name='export_pk',
                type=openapi.TYPE_STRING,
                in_=openapi.IN_PATH,
                description='Primary key identifying the export file.',
            ),
        ],
    ),
)
class ExportDetailAPI(generics.RetrieveDestroyAPIView):
    queryset = Export.objects.all()
    project_model = Project
    serializer_class = ExportSerializer
    lookup_url_kwarg = 'export_pk'
    permission_required = all_permissions.projects_change

    def delete(self, *args, **kwargs):
        if flag_set('ff_back_dev_4664_remove_storage_file_on_export_delete_29032023_short'):
            try:
                export = self.get_object()
                export.file.delete()

                for converted_format in export.converted_formats.all():
                    if converted_format.file:
                        converted_format.file.delete()
            except Exception as e:
                return Response(
                    status=status.HTTP_500_INTERNAL_SERVER_ERROR,
                    data={
                        'detail': 'Could not delete file from storage. Check that your user has permissions to delete files: %s'
                        % str(e)
                    },
                )

        return super().delete(*args, **kwargs)

    def _get_project(self):
        project_pk = self.kwargs.get('pk')
        project = generics.get_object_or_404(
            self.project_model.objects.for_user(self.request.user),
            pk=project_pk,
        )
        return project

    def get_queryset(self):
        project = self._get_project()
        return super().get_queryset().filter(project=project)


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Export'],
        x_fern_sdk_group_name=['projects', 'exports'],
        x_fern_sdk_method_name='download',
        x_fern_audiences=['public'],
        operation_summary='Download export snapshot as file in specified format',
        operation_description="""
        Download an export file in the specified format for a specific project. Specify the project ID with the `id`
        parameter in the path and the ID of the export file you want to download using the `export_pk` parameter
        in the path.

        Get the `export_pk` from the response of the request to [Create new export](/api#operation/api_projects_exports_create)
        or after [listing export files](/api#operation/api_projects_exports_list).
        """,
        manual_parameters=[
            openapi.Parameter(
                name='exportType',
                type=openapi.TYPE_STRING,
                in_=openapi.IN_QUERY,
                description='Selected export format',
            ),
            openapi.Parameter(
                name='id',
                type=openapi.TYPE_INTEGER,
                in_=openapi.IN_PATH,
                description='A unique integer value identifying this project.',
            ),
            openapi.Parameter(
                name='export_pk',
                type=openapi.TYPE_STRING,
                in_=openapi.IN_PATH,
                description='Primary key identifying the export file.',
            ),
        ],
    ),
)
class ExportDownloadAPI(generics.RetrieveAPIView):
    queryset = Export.objects.all()
    project_model = Project
    serializer_class = None
    lookup_url_kwarg = 'export_pk'
    permission_required = all_permissions.projects_change

    def _get_project(self):
        project_pk = self.kwargs.get('pk')
        project = generics.get_object_or_404(
            self.project_model.objects.for_user(self.request.user),
            pk=project_pk,
        )
        return project

    def get_queryset(self):
        project = self._get_project()
        return super().get_queryset().filter(project=project)

    def get(self, request, *args, **kwargs):
        snapshot = self.get_object()
        export_type = request.GET.get('exportType')

        if snapshot.status != Export.Status.COMPLETED:
            return HttpResponse('Export is not completed', status=404)

        if flag_set('fflag_fix_all_lsdv_4813_async_export_conversion_22032023_short', request.user):
            file = snapshot.file
            if export_type is not None and export_type != 'JSON':
                converted_file = snapshot.converted_formats.filter(export_type=export_type).first()
                if converted_file is None:
                    raise NotFound(f'{export_type} format is not converted yet')
                file = converted_file.file

            if isinstance(file.storage, FileSystemStorage):
                url = file.storage.url(file.name)
            else:
                url = file.storage.url(file.name, storage_url=True)
            protocol = urlparse(url).scheme

            # NGINX downloads are a solid way to make uwsgi workers free
            if settings.USE_NGINX_FOR_EXPORT_DOWNLOADS:
                # let NGINX handle it
                response = HttpResponse()
                # below header tells NGINX to catch it and serve, see docker-config/nginx-app.conf
                redirect = '/file_download/' + protocol + '/' + url.replace(protocol + '://', '')
                response['X-Accel-Redirect'] = redirect
                response['Content-Disposition'] = 'attachment; filename="{}"'.format(file.name)
                response['filename'] = os.path.basename(file.name)
                return response

            # No NGINX: standard way for export downloads in the community edition
            else:
                ext = file.name.split('.')[-1]
                response = RangedFileResponse(request, file, content_type=f'application/{ext}')
                response['Content-Disposition'] = f'attachment; filename="{file.name}"'
                response['filename'] = os.path.basename(file.name)
                return response
        else:
            if export_type is None:
                file_ = snapshot.file
            else:
                file_ = snapshot.convert_file(export_type)

            if file_ is None:
                return HttpResponse("Can't get file", status=404)

            ext = file_.name.split('.')[-1]

            response = RangedFileResponse(request, file_, content_type=f'application/{ext}')
            response['Content-Disposition'] = f'attachment; filename="{file_.name}"'
            response['filename'] = file_.name
            return response


def async_convert(converted_format_id, export_type, project, hostname, download_resources=False, **kwargs):
    with transaction.atomic():
        try:
            converted_format = ConvertedFormat.objects.get(id=converted_format_id)
        except ConvertedFormat.DoesNotExist:
            logger.error(f'ConvertedFormat with id {converted_format_id} not found, conversion failed')
            return
        if converted_format.status != ConvertedFormat.Status.CREATED:
            logger.error(f'Conversion for export id {converted_format.export.id} to {export_type} already started')
            return
        converted_format.status = ConvertedFormat.Status.IN_PROGRESS
        converted_format.save(update_fields=['status'])

    snapshot = converted_format.export
    converted_file = snapshot.convert_file(export_type, download_resources=download_resources, hostname=hostname)
    if converted_file is None:
        raise ValidationError('No converted file found, probably there are no annotations in the export snapshot')
    md5 = Export.eval_md5(converted_file)
    ext = converted_file.name.split('.')[-1]

    now = datetime.now()
    file_name = f'project-{project.id}-at-{now.strftime("%Y-%m-%d-%H-%M")}-{md5[0:8]}.{ext}'
    file_path = f'{project.id}/{file_name}'  # finally file will be in settings.DELAYED_EXPORT_DIR/project.id/file_name
    file_ = File(converted_file, name=file_path)
    converted_format.file.save(file_path, file_)
    converted_format.status = ConvertedFormat.Status.COMPLETED
    converted_format.save(update_fields=['file', 'status'])


def set_convert_background_failure(job, connection, type, value, traceback_obj):
    from data_export.models import ConvertedFormat

    convert_id = job.args[0]
    try:
        trace = ''.join(tb.format_exception(type, value, traceback_obj))
    except Exception as e:
        if flag_set('fflag_fix_back_leap_1818_set_convert_background_failure_logging_02062025_short'):
            logger.error(f'Failed to format traceback: {job=} {type=} {value=} {traceback_obj=} {e=}', exc_info=True)
        trace = 'Exception while processing traceback. See stderr for details'
    ConvertedFormat.objects.filter(id=convert_id).update(status=Export.Status.FAILED, traceback=trace)


@method_decorator(name='get', decorator=swagger_auto_schema(auto_schema=None))
@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Export'],
        x_fern_sdk_group_name=['projects', 'exports'],
        x_fern_sdk_method_name='convert',
        x_fern_audiences=['public'],
        operation_summary='Export conversion',
        operation_description="""
        Convert export snapshot to selected format
        """,
        request_body=ExportConvertSerializer,
        manual_parameters=[
            openapi.Parameter(
                name='id',
                type=openapi.TYPE_INTEGER,
                in_=openapi.IN_PATH,
                description='A unique integer value identifying this project.',
            ),
            openapi.Parameter(
                name='export_pk',
                type=openapi.TYPE_STRING,
                in_=openapi.IN_PATH,
                description='Primary key identifying the export file.',
            ),
        ],
    ),
)
class ExportConvertAPI(generics.RetrieveAPIView):
    queryset = Export.objects.all()
    lookup_url_kwarg = 'export_pk'
    permission_required = all_permissions.projects_change

    def post(self, request, *args, **kwargs):
        snapshot = self.get_object()
        serializer = ExportConvertSerializer(data=request.data, context={'project': snapshot.project})
        serializer.is_valid(raise_exception=True)
        export_type = serializer.validated_data['export_type']
        download_resources = serializer.validated_data.get('download_resources')

        with transaction.atomic():
            converted_format, created = ConvertedFormat.objects.get_or_create(export=snapshot, export_type=export_type)

            if not created:
                raise ValidationError(f'Conversion to {export_type} already started')

        start_job_async_or_sync(
            async_convert,
            converted_format.id,
            export_type,
            snapshot.project,
            request.build_absolute_uri('/'),
            download_resources=download_resources,
            on_failure=set_convert_background_failure,
        )
        return Response({'export_type': export_type, 'converted_format': converted_format.id})
</file>

<file path="label_studio/data_export/apps.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
from django.apps import AppConfig


class DataExportConfig(AppConfig):
    name = 'data_export'
</file>

<file path="label_studio/data_export/mixins.py">
import hashlib
import io
import json
import logging
import pathlib
import shutil
from datetime import datetime
from functools import reduce

import django_rq
from core.redis import redis_connected
from core.utils.common import batch
from core.utils.io import (
    SerializableGenerator,
    get_all_dirs_from_dir,
    get_all_files_from_dir,
    get_temp_dir,
)
from data_manager.models import View
from django.conf import settings
from django.core.files import File
from django.core.files import temp as tempfile
from django.db import transaction
from django.db.models import Prefetch
from django.db.models.query_utils import Q
from django.utils import dateformat, timezone
from label_studio_sdk.converter import Converter
from tasks.models import Annotation, Task

ONLY = 'only'
EXCLUDE = 'exclude'


logger = logging.getLogger(__name__)


class ExportMixin:
    def has_permission(self, user):
        user.project = self.project  # link for activity log
        return self.project.has_permission(user)

    def get_default_title(self):
        return f"{self.project.title.replace(' ', '-')}-at-{dateformat.format(timezone.now(), 'Y-m-d-H-i')}"

    def _get_filtered_tasks(self, tasks, task_filter_options=None):
        """
        task_filter_options: None or Dict({
            view: optional int id or View
            skipped: optional None or str:("include|exclude")
            finished: optional None or str:("include|exclude")
            annotated: optional None or str:("include|exclude")
        })
        """
        if not isinstance(task_filter_options, dict):
            return tasks
        if 'view' in task_filter_options:
            try:
                value = int(task_filter_options['view'])
                prepare_params = View.objects.get(project=self.project, id=value).get_prepare_tasks_params(
                    add_selected_items=True
                )
                tab_tasks = Task.prepared.only_filtered(prepare_params=prepare_params).values_list('id', flat=True)
                tasks = tasks.filter(id__in=tab_tasks)
            except (ValueError, View.DoesNotExist) as exc:
                logger.warning(f'Incorrect view params {exc}')
        if 'skipped' in task_filter_options:
            value = task_filter_options['skipped']
            if value == ONLY:
                tasks = tasks.filter(annotations__was_cancelled=True)
            elif value == EXCLUDE:
                tasks = tasks.exclude(annotations__was_cancelled=True)
        if 'finished' in task_filter_options:
            value = task_filter_options['finished']
            if value == ONLY:
                tasks = tasks.filter(is_labeled=True)
            elif value == EXCLUDE:
                tasks = tasks.exclude(is_labeled=True)
        if 'annotated' in task_filter_options:
            value = task_filter_options['annotated']
            # if any annotation exists and is not cancelled
            if value == ONLY:
                tasks = tasks.filter(annotations__was_cancelled=False)
            elif value == EXCLUDE:
                tasks = tasks.exclude(annotations__was_cancelled=False)

        return tasks

    def _get_filtered_annotations_queryset(self, annotation_filter_options=None):
        """
        Filtering using disjunction of conditions

        annotation_filter_options: None or Dict({
            usual: optional None or bool:("true|false")
            ground_truth: optional None or bool:("true|false")
            skipped: optional None or bool:("true|false")
        })
        """
        queryset = Annotation.objects.all()
        if not isinstance(annotation_filter_options, dict):
            return queryset

        q_list = []
        if annotation_filter_options.get('usual'):
            q_list.append(Q(was_cancelled=False, ground_truth=False))
        if annotation_filter_options.get('ground_truth'):
            q_list.append(Q(ground_truth=True))
        if annotation_filter_options.get('skipped'):
            q_list.append(Q(was_cancelled=True))
        if not q_list:
            return queryset

        q = reduce(lambda x, y: x | y, q_list)
        return queryset.filter(q)

    @staticmethod
    def _get_export_serializer_option(serialization_options):
        options = {'expand': []}
        if isinstance(serialization_options, dict):
            if (
                'drafts' in serialization_options
                and isinstance(serialization_options['drafts'], dict)
                and not serialization_options['drafts'].get('only_id')
            ):
                options['expand'].append('drafts')
            if (
                'predictions' in serialization_options
                and isinstance(serialization_options['predictions'], dict)
                and not serialization_options['predictions'].get('only_id')
            ):
                options['expand'].append('predictions')
            if 'annotations__completed_by' in serialization_options and not serialization_options[
                'annotations__completed_by'
            ].get('only_id'):
                options['expand'].append('annotations.completed_by')
            options['context'] = {'interpolate_key_frames': settings.INTERPOLATE_KEY_FRAMES}
            if 'interpolate_key_frames' in serialization_options:
                options['context']['interpolate_key_frames'] = serialization_options['interpolate_key_frames']
            if serialization_options.get('include_annotation_history') is False:
                options['omit'] = ['annotations.history']
            # download resources
            if serialization_options.get('download_resources') is True:
                options['download_resources'] = True
        return options

    def get_task_queryset(self, ids, annotation_filter_options):
        annotations_qs = self._get_filtered_annotations_queryset(annotation_filter_options=annotation_filter_options)
        return (
            Task.objects.filter(id__in=ids)
            .prefetch_related(
                Prefetch(
                    'annotations',
                    queryset=annotations_qs,
                )
            )
            .prefetch_related('predictions', 'drafts')
        )

    def get_export_data(self, task_filter_options=None, annotation_filter_options=None, serialization_options=None):
        """
        serialization_options: None or Dict({
            drafts: optional
                None
                    or
                Dict({
                    only_id: true/false
                })
            predictions: optional
                None
                    or
                Dict({
                    only_id: true/false
                })
            annotations__completed_by: optional
                None
                    or
                Dict({
                    only_id: true/false
                })
        })
        """
        from .serializers import ExportDataSerializer

        logger.debug('Run get_task_queryset')

        start = datetime.now()
        with transaction.atomic():
            # TODO: make counters from queryset
            # counters = Project.objects.with_counts().filter(id=self.project.id)[0].get_counters()
            self.counters = {'task_number': 0}
            all_tasks = self.project.tasks
            logger.debug('Tasks filtration')
            task_ids = (
                self._get_filtered_tasks(all_tasks, task_filter_options=task_filter_options)
                .distinct()
                .values_list('id', flat=True)
            )
            base_export_serializer_option = self._get_export_serializer_option(serialization_options)
            i = 0
            BATCH_SIZE = 1000
            for ids in batch(task_ids, BATCH_SIZE):
                i += 1
                tasks = list(self.get_task_queryset(ids, annotation_filter_options))
                logger.debug(f'Batch: {i*BATCH_SIZE}')
                if isinstance(task_filter_options, dict) and task_filter_options.get('only_with_annotations'):
                    tasks = [task for task in tasks if task.annotations.exists()]

                if serialization_options and serialization_options.get('include_annotation_history') is True:
                    task_ids = [task.id for task in tasks]
                    annotation_ids = Annotation.objects.filter(task_id__in=task_ids).values_list('id', flat=True)
                    base_export_serializer_option = self.update_export_serializer_option(
                        base_export_serializer_option, annotation_ids
                    )

                serializer = ExportDataSerializer(tasks, many=True, **base_export_serializer_option)
                self.counters['task_number'] += len(tasks)
                for task in serializer.data:
                    yield task
        duration = datetime.now() - start
        logger.info(
            f'{self.counters["task_number"]} tasks from project {self.project_id} exported in {duration.total_seconds():.2f} seconds'
        )

    def update_export_serializer_option(self, base_export_serializer_option, annotation_ids):
        return base_export_serializer_option

    @staticmethod
    def eval_md5(file):
        md5_object = hashlib.md5()   # nosec
        block_size = 128 * md5_object.block_size
        chunk = file.read(block_size)
        while chunk:
            md5_object.update(chunk)
            chunk = file.read(block_size)
        md5 = md5_object.hexdigest()
        return md5

    def save_file(self, file, md5):
        now = datetime.now()
        file_name = f'project-{self.project.id}-at-{now.strftime("%Y-%m-%d-%H-%M")}-{md5[0:8]}.json'
        file_path = f'{self.project.id}/{file_name}'  # finally file will be in settings.DELAYED_EXPORT_DIR/self.project.id/file_name
        file_ = File(file, name=file_path)
        self.file.save(file_path, file_)
        self.md5 = md5
        self.save(update_fields=['file', 'md5', 'counters'])

    def export_to_file(self, task_filter_options=None, annotation_filter_options=None, serialization_options=None):
        logger.debug(
            f'Run export for {self.id} with params:\n'
            f'task_filter_options: {task_filter_options}\n'
            f'annotation_filter_options: {annotation_filter_options}\n'
            f'serialization_options: {serialization_options}\n'
        )
        try:
            iter_json = json.JSONEncoder(ensure_ascii=False).iterencode(
                SerializableGenerator(
                    self.get_export_data(
                        task_filter_options=task_filter_options,
                        annotation_filter_options=annotation_filter_options,
                        serialization_options=serialization_options,
                    )
                )
            )
            with tempfile.NamedTemporaryFile(suffix='.export.json', dir=settings.FILE_UPLOAD_TEMP_DIR) as file:
                for chunk in iter_json:
                    encoded_chunk = chunk.encode('utf-8')
                    file.write(encoded_chunk)
                file.seek(0)

                md5 = self.eval_md5(file)
                self.save_file(file, md5)

            self.status = self.Status.COMPLETED
            self.save(update_fields=['status'])

        except Exception:
            self.status = self.Status.FAILED
            self.save(update_fields=['status'])
            logger.exception('Export was failed')
        finally:
            self.finished_at = datetime.now()
            self.save(update_fields=['finished_at'])

    def run_file_exporting(self, task_filter_options=None, annotation_filter_options=None, serialization_options=None):
        if self.status == self.Status.IN_PROGRESS:
            logger.warning('Try to export with in progress stage')
            return

        self.status = self.Status.IN_PROGRESS
        self.save(update_fields=['status'])

        if redis_connected():
            queue = django_rq.get_queue('default')
            queue.enqueue(
                export_background,
                self.id,
                task_filter_options,
                annotation_filter_options,
                serialization_options,
                on_failure=set_export_background_failure,
                job_timeout='3h',  # 3 hours
            )
        else:
            self.export_to_file(
                task_filter_options=task_filter_options,
                annotation_filter_options=annotation_filter_options,
                serialization_options=serialization_options,
            )

    def convert_file(self, to_format, download_resources=False, hostname=None):
        with get_temp_dir() as tmp_dir:
            OUT = 'out'
            out_dir = pathlib.Path(tmp_dir) / OUT
            out_dir.mkdir(mode=0o700, parents=True, exist_ok=True)

            converter = Converter(
                config=self.project.get_parsed_config(),
                project_dir=None,
                upload_dir=out_dir,
                download_resources=download_resources,
                # for downloading resource we need access to the API
                access_token=self.project.organization.created_by.auth_token.key,
                hostname=hostname,
            )
            input_name = pathlib.Path(self.file.name).name
            input_file_path = pathlib.Path(tmp_dir) / input_name

            with open(input_file_path, 'wb') as file_:
                file_.write(self.file.open().read())

            converter.convert(input_file_path, out_dir, to_format, is_dir=False)

            files = get_all_files_from_dir(out_dir)
            dirs = get_all_dirs_from_dir(out_dir)

            if len(files) == 0 and len(dirs) == 0:
                return None
            elif len(files) == 1 and len(dirs) == 0:
                output_file = files[0]
                filename = pathlib.Path(input_name).stem + pathlib.Path(output_file).suffix
            else:
                shutil.make_archive(out_dir, 'zip', out_dir)
                output_file = pathlib.Path(tmp_dir) / (str(out_dir.stem) + '.zip')
                filename = pathlib.Path(input_name).stem + '.zip'

            # TODO(jo): can we avoid the `f.read()` here?
            with open(output_file, mode='rb') as f:
                return File(
                    io.BytesIO(f.read()),
                    name=filename,
                )


def export_background(
    export_id, task_filter_options, annotation_filter_options, serialization_options, *args, **kwargs
):
    from data_export.models import Export

    Export.objects.get(id=export_id).export_to_file(
        task_filter_options,
        annotation_filter_options,
        serialization_options,
    )


def set_export_background_failure(job, connection, type, value, traceback):
    from data_export.models import Export

    export_id = job.args[0]
    Export.objects.filter(id=export_id).update(status=Export.Status.FAILED)
</file>

<file path="label_studio/data_export/models.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import hashlib
import logging
import os
import shutil
from copy import deepcopy
from datetime import datetime

import ujson as json
from core import version
from core.feature_flags import flag_set
from core.utils.common import load_func
from core.utils.io import get_all_files_from_dir, get_temp_dir, path_to_open_binary_file
from django.conf import settings
from django.db import models
from django.db.models.signals import post_save
from django.dispatch import receiver
from django.utils.translation import gettext_lazy as _
from label_studio_sdk.converter import Converter
from tasks.models import Annotation

logger = logging.getLogger(__name__)


ExportMixin = load_func(settings.EXPORT_MIXIN)


class Export(ExportMixin, models.Model):
    class Status(models.TextChoices):
        CREATED = 'created', _('Created')
        IN_PROGRESS = 'in_progress', _('In progress')
        FAILED = 'failed', _('Failed')
        COMPLETED = 'completed', _('Completed')

    title = models.CharField(
        _('title'),
        blank=True,
        default='',
        max_length=2048,
    )
    created_at = models.DateTimeField(
        _('created at'),
        auto_now_add=True,
        help_text='Creation time',
    )
    file = models.FileField(
        upload_to=settings.DELAYED_EXPORT_DIR,
        null=True,
    )
    md5 = models.CharField(
        _('md5 of file'),
        max_length=128,
        default='',
    )
    finished_at = models.DateTimeField(
        _('finished at'),
        help_text='Complete or fail time',
        null=True,
        default=None,
    )

    status = models.CharField(
        _('Export status'),
        max_length=64,
        choices=Status.choices,
        default=Status.CREATED,
    )
    counters = models.JSONField(
        _('Exporting meta data'),
        default=dict,
    )
    project = models.ForeignKey(
        'projects.Project',
        related_name='exports',
        on_delete=models.CASCADE,
    )
    created_by = models.ForeignKey(
        settings.AUTH_USER_MODEL,
        related_name='+',
        on_delete=models.SET_NULL,
        null=True,
        verbose_name=_('created by'),
    )


@receiver(post_save, sender=Export)
def set_export_default_name(sender, instance, created, **kwargs):
    if created and not instance.title:
        instance.title = instance.get_default_title()
        instance.save()


class DataExport(object):
    # TODO: deprecated
    @staticmethod
    def save_export_files(project, now, get_args, data, md5, name):
        """Generate two files: meta info and result file and store them locally for logging"""
        filename_results = os.path.join(settings.EXPORT_DIR, name + '.json')
        filename_info = os.path.join(settings.EXPORT_DIR, name + '-info.json')
        annotation_number = Annotation.objects.filter(project=project).count()
        try:
            platform_version = version.get_git_version()
        except:  # noqa: E722
            platform_version = 'none'
            logger.error('Version is not detected in save_export_files()')
        info = {
            'project': {
                'title': project.title,
                'id': project.id,
                'created_at': project.created_at.strftime('%Y-%m-%dT%H:%M:%SZ'),
                'created_by': project.created_by.email,
                'task_number': project.tasks.count(),
                'annotation_number': annotation_number,
            },
            'platform': {'version': platform_version},
            'download': {
                'GET': dict(get_args),
                'time': now.strftime('%Y-%m-%dT%H:%M:%SZ'),
                'result_filename': filename_results,
                'md5': md5,
            },
        }

        with open(filename_results, 'w', encoding='utf-8') as f:
            f.write(data)
        with open(filename_info, 'w', encoding='utf-8') as f:
            json.dump(info, f, ensure_ascii=False)
        return filename_results

    @staticmethod
    def get_export_formats(project):
        converter = Converter(config=project.get_parsed_config(), project_dir=None)
        formats = []
        supported_formats = set(converter.supported_formats)
        for format, format_info in converter.all_formats().items():
            format_info = deepcopy(format_info)
            format_info['name'] = format.name
            if format.name not in supported_formats:
                format_info['disabled'] = True
            formats.append(format_info)
        return sorted(formats, key=lambda f: f.get('disabled', False))

    @staticmethod
    def generate_export_file(project, tasks, output_format, download_resources, get_args, hostname=None):
        """Generate export file and return it as an open file object.

        Be sure to close the file after using it, to avoid wasting disk space.
        """

        # prepare for saving
        now = datetime.now()
        data = json.dumps(tasks, ensure_ascii=False)
        md5 = hashlib.md5(json.dumps(data).encode('utf-8')).hexdigest()   # nosec
        name = 'project-' + str(project.id) + '-at-' + now.strftime('%Y-%m-%d-%H-%M') + f'-{md5[0:8]}'

        input_json = DataExport.save_export_files(project, now, get_args, data, md5, name)

        converter = Converter(
            config=project.get_parsed_config(),
            project_dir=None,
            upload_dir=os.path.join(settings.MEDIA_ROOT, settings.UPLOAD_DIR),
            download_resources=download_resources,
            access_token=project.organization.created_by.auth_token.key,
            hostname=hostname,
        )
        with get_temp_dir() as tmp_dir:
            converter.convert(input_json, tmp_dir, output_format, is_dir=False)
            files = get_all_files_from_dir(tmp_dir)
            # if only one file is exported - no need to create archive
            if len(os.listdir(tmp_dir)) == 1:
                output_file = files[0]
                ext = os.path.splitext(output_file)[-1]
                content_type = f'application/{ext}'
                out = path_to_open_binary_file(output_file)
                filename = name + os.path.splitext(output_file)[-1]
                return out, content_type, filename

            # otherwise pack output directory into archive
            shutil.make_archive(tmp_dir, 'zip', tmp_dir)
            out = path_to_open_binary_file(os.path.abspath(tmp_dir + '.zip'))
            content_type = 'application/zip'
            filename = name + '.zip'
            return out, content_type, filename


class ConvertedFormat(models.Model):
    class Status(models.TextChoices):
        CREATED = 'created', _('Created')
        IN_PROGRESS = 'in_progress', _('In progress')
        FAILED = 'failed', _('Failed')
        COMPLETED = 'completed', _('Completed')

    project = models.ForeignKey(
        'projects.Project',
        null=True,
        related_name='export_conversions',
        on_delete=models.CASCADE,
    )
    organization = models.ForeignKey(
        'organizations.Organization',
        null=True,
        on_delete=models.CASCADE,
        related_name='export_conversions',
    )
    export = models.ForeignKey(
        Export,
        related_name='converted_formats',
        on_delete=models.CASCADE,
        help_text='Export snapshot for this converted file',
    )
    file = models.FileField(
        upload_to=settings.DELAYED_EXPORT_DIR,
        null=True,
    )
    status = models.CharField(
        max_length=64,
        choices=Status.choices,
        default=Status.CREATED,
    )
    traceback = models.TextField(null=True, blank=True, help_text='Traceback report in case of errors')
    export_type = models.CharField(max_length=64)
    created_at = models.DateTimeField(
        _('created at'),
        null=True,
        auto_now_add=True,
        help_text='Creation time',
    )
    updated_at = models.DateTimeField(
        _('updated at'),
        null=True,
        auto_now_add=True,
        help_text='Updated time',
    )
    finished_at = models.DateTimeField(
        _('finished at'),
        help_text='Complete or fail time',
        null=True,
        default=None,
    )
    created_by = models.ForeignKey(
        settings.AUTH_USER_MODEL,
        related_name='+',
        on_delete=models.SET_NULL,
        null=True,
        verbose_name=_('created by'),
    )

    def delete(self, *args, **kwargs):
        if flag_set('ff_back_dev_4664_remove_storage_file_on_export_delete_29032023_short'):
            if self.file:
                self.file.delete()
        super().delete(*args, **kwargs)
</file>

<file path="label_studio/data_export/serializers.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
from core.label_config import replace_task_data_undefined_with_config_field
from core.utils.common import load_func
from data_export.models import DataExport
from django.conf import settings
from label_studio_sdk._extensions.label_studio_tools.core.label_config import is_video_object_tracking
from label_studio_sdk._extensions.label_studio_tools.postprocessing.video import extract_key_frames
from ml.mixins import InteractiveMixin
from rest_flex_fields import FlexFieldsModelSerializer
from rest_framework import serializers
from tasks.models import Annotation, Task
from tasks.serializers import AnnotationDraftSerializer, PredictionSerializer
from users.models import User
from users.serializers import UserSimpleSerializer

from .models import ConvertedFormat, Export


class CompletedBySerializer(serializers.ModelSerializer):
    class Meta:
        model = User
        fields = ['id', 'email', 'first_name', 'last_name']


class AnnotationSerializer(FlexFieldsModelSerializer):
    completed_by = serializers.PrimaryKeyRelatedField(read_only=True)
    result = serializers.SerializerMethodField()

    class Meta:
        model = Annotation
        fields = '__all__'
        expandable_fields = {'completed_by': (CompletedBySerializer,)}

    def get_result(self, obj):
        # run frames extraction on param, result and result type
        if (
            obj.result
            and self.context.get('interpolate_key_frames', False)
            and is_video_object_tracking(parsed_config=obj.project.get_parsed_config())
        ):
            return extract_key_frames(obj.result)
        return obj.result


class BaseExportDataSerializer(FlexFieldsModelSerializer):
    annotations = AnnotationSerializer(many=True, read_only=True)
    file_upload = serializers.ReadOnlyField(source='file_upload_name')
    drafts = serializers.PrimaryKeyRelatedField(many=True, read_only=True)
    predictions = serializers.PrimaryKeyRelatedField(many=True, read_only=True)

    # resolve $undefined$ key in task data, if any
    def to_representation(self, task):
        # avoid long project initializations
        project = getattr(self, '_project', None)
        if project is None:
            project = task.project
            setattr(self, '_project', project)

        data = task.data
        # add interpolate_key_frames param to annotations serializer
        if 'annotations' in self.fields:
            self.fields['annotations'].context['interpolate_key_frames'] = self.context.get(
                'interpolate_key_frames', False
            )
        replace_task_data_undefined_with_config_field(data, project)

        return super().to_representation(task)

    class Meta:
        model = Task
        exclude = ('overlap', 'is_labeled')
        expandable_fields = {
            'drafts': (AnnotationDraftSerializer, {'many': True}),
            'predictions': (PredictionSerializer, {'many': True}),
            'annotations': (AnnotationSerializer, {'many': True}),
        }


class ConvertedFormatSerializer(serializers.ModelSerializer):
    class Meta:
        model = ConvertedFormat
        fields = ['id', 'status', 'export_type', 'traceback']


class ExportSerializer(serializers.ModelSerializer):
    class Meta:
        model = Export
        read_only = [
            'id',
            'created_by',
            'created_at',
            'finished_at',
            'status',
            'md5',
            'counters',
            'converted_formats',
        ]
        fields = ['title'] + read_only

    created_by = UserSimpleSerializer(required=False)
    converted_formats = ConvertedFormatSerializer(many=True, required=False)


ONLY_OR_EXCLUDE_CHOICE = [
    2 * ['only'],
    2 * ['exclude'],
    2 * [None],
]


class TaskFilterOptionsSerializer(serializers.Serializer):
    view = serializers.IntegerField(
        required=False, help_text='Apply filters from the view ID (a tab from the Data Manager)'
    )
    skipped = serializers.ChoiceField(
        choices=ONLY_OR_EXCLUDE_CHOICE,
        allow_null=True,
        required=False,
        help_text='`only` - include all tasks with skipped annotations<br>'
        '`exclude` - exclude all tasks with skipped annotations',
    )
    finished = serializers.ChoiceField(
        choices=ONLY_OR_EXCLUDE_CHOICE,
        allow_null=True,
        required=False,
        help_text='`only` - include all finished tasks (is_labeled = true)<br>'
        '`exclude` - exclude all finished tasks',
    )
    annotated = serializers.ChoiceField(
        choices=ONLY_OR_EXCLUDE_CHOICE,
        allow_null=True,
        required=False,
        help_text='`only` - include all tasks with at least one not skipped annotation<br>'
        '`exclude` - exclude all tasks with at least one not skipped annotation',
    )
    only_with_annotations = serializers.BooleanField(default=False, required=False, help_text='')


class AnnotationFilterOptionsSerializer(serializers.Serializer):
    usual = serializers.BooleanField(
        allow_null=True, required=False, default=True, help_text='Include not skipped and not ground truth annotations'
    )
    ground_truth = serializers.BooleanField(
        allow_null=True, required=False, help_text='Include ground truth annotations'
    )
    skipped = serializers.BooleanField(allow_null=True, required=False, help_text='Include skipped annotations')


class SerializationOptionsSerializer(serializers.Serializer):
    class SerializationOption(serializers.Serializer):
        only_id = serializers.BooleanField(
            default=False, required=False, help_text='Include a full json body or IDs only'
        )

    drafts = SerializationOption(required=False, help_text='JSON dict with parameters')
    predictions = SerializationOption(required=False, help_text='JSON dict with parameters')
    include_annotation_history = serializers.BooleanField(
        default=False, help_text='Include annotation history', required=False
    )
    annotations__completed_by = SerializationOption(required=False, help_text='JSON dict with parameters')
    interpolate_key_frames = serializers.BooleanField(
        default=settings.INTERPOLATE_KEY_FRAMES, help_text='Interpolate video key frames', required=False
    )


class ExportConvertSerializer(serializers.Serializer):
    export_type = serializers.CharField(help_text='Export file format.')
    download_resources = serializers.BooleanField(help_text='Download resources in converter.', required=False)

    def validate_export_type(self, value):
        project = self.context.get('project')
        export_formats = [f['name'] for f in DataExport.get_export_formats(project)]
        if value not in export_formats:
            raise serializers.ValidationError(f'{value} is not supported export format')
        return value


class ExportCreateSerializer(ExportSerializer):
    class Meta(ExportSerializer.Meta):
        fields = ExportSerializer.Meta.fields + [
            'task_filter_options',
            'annotation_filter_options',
            'serialization_options',
        ]

    task_filter_options = TaskFilterOptionsSerializer(required=False, default=None)
    annotation_filter_options = AnnotationFilterOptionsSerializer(required=False, default=None)
    serialization_options = SerializationOptionsSerializer(required=False, default=None)


class ExportParamSerializer(serializers.Serializer):
    interpolate_key_frames = serializers.BooleanField(
        default=settings.INTERPOLATE_KEY_FRAMES, help_text='Interpolate video key frames.', required=False
    )
    download_resources = serializers.BooleanField(
        default=settings.CONVERTER_DOWNLOAD_RESOURCES, help_text='Download resources in converter.', required=False
    )
    # deprecated param to delete
    export_type = serializers.CharField(default='JSON', help_text='Export file format.', required=False)
    exportType = serializers.CharField(help_text='Export file format.', required=False)
    download_all_tasks = serializers.BooleanField(
        default=False, help_text='Download all tasks or only finished.', required=False
    )


class BaseExportDataSerializerForInteractive(InteractiveMixin, BaseExportDataSerializer):
    pass


ExportDataSerializer = load_func(settings.EXPORT_DATA_SERIALIZER)
</file>

<file path="label_studio/data_export/urls.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
from django.urls import include, path

from . import api

app_name = 'data_export'


_api_urlpatterns = [
    # export api
    path('<int:pk>/export', api.ExportAPI.as_view(), name='project-export'),
    path('<int:pk>/export/formats', api.ExportFormatsListAPI.as_view(), name='project-export-formats'),
    # Previously exported results
    path('<int:pk>/export/files', api.ProjectExportFiles.as_view(), name='project-export-files'),
    path('<int:pk>/exports/', api.ExportListAPI.as_view(), name='project-exports-list'),
    path('<int:pk>/exports/<int:export_pk>', api.ExportDetailAPI.as_view(), name='project-exports-detail'),
    path(
        '<int:pk>/exports/<int:export_pk>/download', api.ExportDownloadAPI.as_view(), name='project-exports-download'
    ),
    path('<int:pk>/exports/<int:export_pk>/convert', api.ExportConvertAPI.as_view(), name='project-exports-convert'),
]

urlpatterns = [
    path('api/projects/', include((_api_urlpatterns, app_name), namespace='api-projects')),
    path('api/auth/export/', api.ProjectExportFilesAuthCheck.as_view(), name='project-export-files-auth-check'),
    # path('api/auth/exports/', api.ExportListAPI.as_view(), name='api-exports'),
]
</file>

<file path="label_studio/data_import/migrations/__init__.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
</file>

<file path="label_studio/data_import/migrations/0001_initial.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
# Generated by Django 3.1.4 on 2021-01-21 17:39

from django.conf import settings
from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    initial = True

    dependencies = [
        ('projects', '__first__'),
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
    ]

    operations = [
        migrations.CreateModel(
            name='FileUpload',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('file', models.FileField(upload_to='upload')),
                ('project', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='file_uploads', to='projects.project')),
                ('user', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='file_uploads', to=settings.AUTH_USER_MODEL)),
            ],
        ),
    ]
</file>

<file path="label_studio/data_import/migrations/0002_alter_fileupload_file.py">
# Generated by Django 3.2.16 on 2022-12-13 16:12

import data_import.models
from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('data_import', '0001_initial'),
    ]

    operations = [
        migrations.AlterField(
            model_name='fileupload',
            name='file',
            field=models.FileField(upload_to=data_import.models.upload_name_generator),
        ),
    ]
</file>

<file path="label_studio/data_import/__init__.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
</file>

<file path="label_studio/data_import/api.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import base64
import json
import logging
import mimetypes
import time
from typing import Union
from urllib.parse import unquote, urlparse

import drf_yasg.openapi as openapi
from core.decorators import override_report_only_csp
from core.feature_flags import flag_set
from core.permissions import ViewClassPermission, all_permissions
from core.redis import start_job_async_or_sync
from core.utils.common import retry_database_locked, timeit
from core.utils.params import bool_from_request, list_of_strings_from_request
from csp.decorators import csp
from django.conf import settings
from django.db import transaction
from django.http import HttpRequest, HttpResponse, HttpResponseRedirect
from django.utils.decorators import method_decorator
from drf_yasg.utils import swagger_auto_schema
from projects.models import Project, ProjectImport, ProjectReimport
from ranged_fileresponse import RangedFileResponse
from rest_framework import generics, status
from rest_framework.exceptions import ValidationError
from rest_framework.parsers import FormParser, JSONParser, MultiPartParser
from rest_framework.permissions import IsAuthenticated
from rest_framework.response import Response
from rest_framework.settings import api_settings
from rest_framework.views import APIView
from tasks.functions import update_tasks_counters
from tasks.models import Prediction, Task
from users.models import User
from webhooks.models import WebhookAction
from webhooks.utils import emit_webhooks_for_instance

from label_studio.core.utils.common import load_func

from .functions import (
    async_import_background,
    async_reimport_background,
    reformat_predictions,
    set_import_background_failure,
    set_reimport_background_failure,
)
from .models import FileUpload
from .serializers import FileUploadSerializer, ImportApiSerializer, PredictionSerializer
from .uploader import create_file_uploads, load_tasks

logger = logging.getLogger(__name__)

ProjectImportPermission = load_func(settings.PROJECT_IMPORT_PERMISSION)

task_create_response_scheme = {
    201: openapi.Response(
        description='Tasks successfully imported',
        schema=openapi.Schema(
            title='Task creation response',
            description='Task creation response',
            type=openapi.TYPE_OBJECT,
            properties={
                'task_count': openapi.Schema(
                    title='task_count', description='Number of tasks added', type=openapi.TYPE_INTEGER
                ),
                'annotation_count': openapi.Schema(
                    title='annotation_count', description='Number of annotations added', type=openapi.TYPE_INTEGER
                ),
                'predictions_count': openapi.Schema(
                    title='predictions_count', description='Number of predictions added', type=openapi.TYPE_INTEGER
                ),
                'duration': openapi.Schema(
                    title='duration', description='Time in seconds to create', type=openapi.TYPE_NUMBER
                ),
                'file_upload_ids': openapi.Schema(
                    title='file_upload_ids',
                    description='Database IDs of uploaded files',
                    type=openapi.TYPE_ARRAY,
                    items=openapi.Schema(title='File Upload IDs', type=openapi.TYPE_INTEGER),
                ),
                'could_be_tasks_list': openapi.Schema(
                    title='could_be_tasks_list',
                    description='Whether uploaded files can contain lists of tasks, like CSV/TSV files',
                    type=openapi.TYPE_BOOLEAN,
                ),
                'found_formats': openapi.Schema(
                    title='found_formats',
                    description='The list of found file formats',
                    type=openapi.TYPE_ARRAY,
                    items=openapi.Schema(title='File format', type=openapi.TYPE_STRING),
                ),
                'data_columns': openapi.Schema(
                    title='data_columns',
                    description='The list of found data columns',
                    type=openapi.TYPE_ARRAY,
                    items=openapi.Schema(title='Data column name', type=openapi.TYPE_STRING),
                ),
            },
        ),
    ),
    400: openapi.Schema(
        title='Incorrect task data', description='String with error description', type=openapi.TYPE_STRING
    ),
}


@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Import'],
        x_fern_sdk_group_name='projects',
        x_fern_sdk_method_name='import_tasks',
        x_fern_audiences=['public'],
        responses=task_create_response_scheme,
        manual_parameters=[
            openapi.Parameter(
                name='id',
                type=openapi.TYPE_INTEGER,
                in_=openapi.IN_PATH,
                description='A unique integer value identifying this project.',
            ),
            openapi.Parameter(
                name='commit_to_project',
                type=openapi.TYPE_BOOLEAN,
                in_=openapi.IN_QUERY,
                description='Set to "true" to immediately commit tasks to the project.',
                default=True,
                required=False,
            ),
            openapi.Parameter(
                name='return_task_ids',
                type=openapi.TYPE_BOOLEAN,
                in_=openapi.IN_QUERY,
                description='Set to "true" to return task IDs in the response.',
                default=False,
                required=False,
            ),
            openapi.Parameter(
                name='preannotated_from_fields',
                type=openapi.TYPE_ARRAY,
                items=openapi.Schema(type=openapi.TYPE_STRING),
                in_=openapi.IN_QUERY,
                description='List of fields to preannotate from the task data. For example, if you provide a list of'
                ' `{"text": "text", "prediction": "label"}` items in the request, the system will create '
                'a task with the `text` field and a prediction with the `label` field when '
                '`preannoted_from_fields=["prediction"]`.',
                default=None,
                required=False,
            ),
        ],
        operation_summary='Import tasks',
        operation_description="""
            Import data as labeling tasks in bulk using this API endpoint. You can use this API endpoint to import multiple tasks.
            One POST request is limited at 250K tasks and 200 MB.

            **Note:** Imported data is verified against a project *label_config* and must
            include all variables that were used in the *label_config*. For example,
            if the label configuration has a *$text* variable, then each item in a data object
            must include a "text" field.
            <br>

            ## POST requests
            <hr style="opacity:0.3">

            There are three possible ways to import tasks with this endpoint:

            ### 1\. **POST with data**
            Send JSON tasks as POST data. Only JSON is supported for POSTing files directly.
            Update this example to specify your authorization token and Label Studio instance host, then run the following from
            the command line.

            ```bash
            curl -H 'Content-Type: application/json' -H 'Authorization: Token abc123' \\
            -X POST '{host}/api/projects/1/import' --data '[{{"text": "Some text 1"}}, {{"text": "Some text 2"}}]'
            ```

            ### 2\. **POST with files**
            Send tasks as files. You can attach multiple files with different names.

            - **JSON**: text files in JavaScript object notation format
            - **CSV**: text files with tables in Comma Separated Values format
            - **TSV**: text files with tables in Tab Separated Value format
            - **TXT**: simple text files are similar to CSV with one column and no header, supported for projects with one source only

            Update this example to specify your authorization token, Label Studio instance host, and file name and path,
            then run the following from the command line:

            ```bash
            curl -H 'Authorization: Token abc123' \\
            -X POST '{host}/api/projects/1/import' -F file=@path/to/my_file.csv
            ```

            ### 3\. **POST with URL**
            You can also provide a URL to a file with labeling tasks. Supported file formats are the same as in option 2.

            ```bash
            curl -H 'Content-Type: application/json' -H 'Authorization: Token abc123' \\
            -X POST '{host}/api/projects/1/import' \\
            --data '[{{"url": "http://example.com/test1.csv"}}, {{"url": "http://example.com/test2.csv"}}]'
            ```

            <br>
        """.format(
            host=(settings.HOSTNAME or 'https://localhost:8080')
        ),
        request_body=openapi.Schema(
            title='tasks',
            description='List of tasks to import',
            type=openapi.TYPE_ARRAY,
            items=openapi.Schema(
                type=openapi.TYPE_OBJECT,
                # TODO: this example doesn't work - perhaps we need to migrate to drf-spectacular for "anyOf" support
                # also fern will change to at least provide a list of examples FER-1969
                # right now we can only rely on documenation examples
                # properties={
                #     'data': openapi.Schema(type=openapi.TYPE_OBJECT, description='Data of the task'),
                #     'annotations': openapi.Schema(
                #         type=openapi.TYPE_ARRAY,
                #         items=annotation_request_schema,
                #         description='Annotations for this task',
                #     ),
                #     'predictions': openapi.Schema(
                #         type=openapi.TYPE_ARRAY,
                #         items=prediction_request_schema,
                #         description='Predictions for this task',
                #     )
                # },
                # example={
                #     'data': {'image': 'http://example.com/image.jpg'},
                #     'annotations': [annotation_response_example],
                #     'predictions': [prediction_response_example]
                # }
            ),
        ),
    ),
)
# Import
class ImportAPI(generics.CreateAPIView):
    permission_required = all_permissions.projects_change
    permission_classes = api_settings.DEFAULT_PERMISSION_CLASSES + [ProjectImportPermission]
    parser_classes = (JSONParser, MultiPartParser, FormParser)
    serializer_class = ImportApiSerializer
    queryset = Task.objects.all()

    def get_serializer_context(self):
        project_id = self.kwargs.get('pk')
        if project_id:
            project = generics.get_object_or_404(Project.objects.for_user(self.request.user), pk=project_id)
        else:
            project = None
        return {'project': project, 'user': self.request.user}

    def post(self, *args, **kwargs):
        return super(ImportAPI, self).post(*args, **kwargs)

    def _save(self, tasks):
        serializer = self.get_serializer(data=tasks, many=True)
        serializer.is_valid(raise_exception=True)
        task_instances = serializer.save(project_id=self.kwargs['pk'])
        project = generics.get_object_or_404(Project.objects.for_user(self.request.user), pk=self.kwargs['pk'])
        emit_webhooks_for_instance(
            self.request.user.active_organization, project, WebhookAction.TASKS_CREATED, task_instances
        )
        return task_instances, serializer

    def sync_import(self, request, project, preannotated_from_fields, commit_to_project, return_task_ids):
        start = time.time()
        tasks = None
        # upload files from request, and parse all tasks
        # TODO: Stop passing request to load_tasks function, make all validation before
        parsed_data, file_upload_ids, could_be_tasks_list, found_formats, data_columns = load_tasks(request, project)

        if preannotated_from_fields:
            # turn flat task JSONs {"column1": value, "column2": value} into {"data": {"column1"..}, "predictions": [{..."column2"}]
            parsed_data = reformat_predictions(parsed_data, preannotated_from_fields)

        if commit_to_project:
            # Immediately create project tasks and update project states and counters
            tasks, serializer = self._save(parsed_data)
            task_count = len(tasks)
            annotation_count = len(serializer.db_annotations)
            prediction_count = len(serializer.db_predictions)

            recalculate_stats_counts = {
                'task_count': task_count,
                'annotation_count': annotation_count,
                'prediction_count': prediction_count,
            }

            # Update counters (like total_annotations) for new tasks and after bulk update tasks stats. It should be a
            # single operation as counters affect bulk is_labeled update
            project.update_tasks_counters_and_task_states(
                tasks_queryset=tasks,
                maximum_annotations_changed=False,
                overlap_cohort_percentage_changed=False,
                tasks_number_changed=True,
                recalculate_stats_counts=recalculate_stats_counts,
            )
            logger.info('Tasks bulk_update finished (sync import)')

            project.summary.update_data_columns(parsed_data)
            # TODO: project.summary.update_created_annotations_and_labels
        else:
            # Do nothing - just output file upload ids for further use
            task_count = len(parsed_data)
            annotation_count = None
            prediction_count = None

        duration = time.time() - start

        response = {
            'task_count': task_count,
            'annotation_count': annotation_count,
            'prediction_count': prediction_count,
            'duration': duration,
            'file_upload_ids': file_upload_ids,
            'could_be_tasks_list': could_be_tasks_list,
            'found_formats': found_formats,
            'data_columns': data_columns,
        }
        if tasks and return_task_ids:
            response['task_ids'] = [task.id for task in tasks]

        return Response(response, status=status.HTTP_201_CREATED)

    @timeit
    def async_import(self, request, project, preannotated_from_fields, commit_to_project, return_task_ids):

        project_import = ProjectImport.objects.create(
            project=project,
            preannotated_from_fields=preannotated_from_fields,
            commit_to_project=commit_to_project,
            return_task_ids=return_task_ids,
        )

        if len(request.FILES):
            logger.debug(f'Import from files: {request.FILES}')
            file_upload_ids, could_be_tasks_list = create_file_uploads(request.user, project, request.FILES)
            project_import.file_upload_ids = file_upload_ids
            project_import.could_be_tasks_list = could_be_tasks_list
            project_import.save(update_fields=['file_upload_ids', 'could_be_tasks_list'])
        elif 'application/x-www-form-urlencoded' in request.content_type:
            logger.debug(f'Import from url: {request.data.get("url")}')
            # empty url
            url = request.data.get('url')
            if not url:
                raise ValidationError('"url" is not found in request data')
            project_import.url = url
            project_import.save(update_fields=['url'])
        # take one task from request DATA
        elif 'application/json' in request.content_type and isinstance(request.data, dict):
            project_import.tasks = [request.data]
            project_import.save(update_fields=['tasks'])

        # take many tasks from request DATA
        elif 'application/json' in request.content_type and isinstance(request.data, list):
            project_import.tasks = request.data
            project_import.save(update_fields=['tasks'])

        # incorrect data source
        else:
            raise ValidationError('load_tasks: No data found in DATA or in FILES')

        start_job_async_or_sync(
            async_import_background,
            project_import.id,
            request.user.id,
            queue_name='high',
            on_failure=set_import_background_failure,
            project_id=project.id,
            organization_id=request.user.active_organization.id,
        )

        response = {'import': project_import.id}
        return Response(response, status=status.HTTP_201_CREATED)

    def create(self, request, *args, **kwargs):
        commit_to_project = bool_from_request(request.query_params, 'commit_to_project', True)
        return_task_ids = bool_from_request(request.query_params, 'return_task_ids', False)
        preannotated_from_fields = list_of_strings_from_request(request.query_params, 'preannotated_from_fields', None)

        # check project permissions
        project = generics.get_object_or_404(Project.objects.for_user(self.request.user), pk=self.kwargs['pk'])

        if (
            flag_set('fflag_feat_all_lsdv_4915_async_task_import_13042023_short', request.user)
            and settings.VERSION_EDITION != 'Community'
        ):
            return self.async_import(request, project, preannotated_from_fields, commit_to_project, return_task_ids)
        else:
            return self.sync_import(request, project, preannotated_from_fields, commit_to_project, return_task_ids)


# Import
class ImportPredictionsAPI(generics.CreateAPIView):
    permission_required = all_permissions.projects_change
    parser_classes = (JSONParser, MultiPartParser, FormParser)
    serializer_class = PredictionSerializer
    queryset = Project.objects.all()
    swagger_schema = None  # TODO: create API schema

    def create(self, request, *args, **kwargs):
        # check project permissions
        project = self.get_object()

        tasks_ids = set(Task.objects.filter(project=project).values_list('id', flat=True))

        logger.debug(
            f'Importing {len(self.request.data)} predictions to project {project} with {len(tasks_ids)} tasks'
        )
        predictions = []
        for item in self.request.data:
            if item.get('task') not in tasks_ids:
                raise ValidationError(
                    f'{item} contains invalid "task" field: corresponding task ID couldn\'t be retrieved '
                    f'from project {project} tasks'
                )
            predictions.append(
                Prediction(
                    task_id=item['task'],
                    project_id=project.id,
                    result=Prediction.prepare_prediction_result(item.get('result'), project),
                    score=item.get('score'),
                    model_version=item.get('model_version', 'undefined'),
                )
            )
        predictions_obj = Prediction.objects.bulk_create(predictions, batch_size=settings.BATCH_SIZE)
        start_job_async_or_sync(update_tasks_counters, Task.objects.filter(id__in=tasks_ids))
        return Response({'created': len(predictions_obj)}, status=status.HTTP_201_CREATED)


class TasksBulkCreateAPI(ImportAPI):
    # just for compatibility - can be safely removed
    swagger_schema = None


class ReImportAPI(ImportAPI):
    permission_required = all_permissions.projects_change

    def sync_reimport(self, project, file_upload_ids, files_as_tasks_list):
        start = time.time()
        tasks, found_formats, data_columns = FileUpload.load_tasks_from_uploaded_files(
            project, file_upload_ids, files_as_tasks_list=files_as_tasks_list
        )

        with transaction.atomic():
            project.remove_tasks_by_file_uploads(file_upload_ids)
            tasks, serializer = self._save(tasks)
        duration = time.time() - start

        task_count = len(tasks)
        annotation_count = len(serializer.db_annotations)
        prediction_count = len(serializer.db_predictions)

        # Update counters (like total_annotations) for new tasks and after bulk update tasks stats. It should be a
        # single operation as counters affect bulk is_labeled update
        project.update_tasks_counters_and_task_states(
            tasks_queryset=tasks,
            maximum_annotations_changed=False,
            overlap_cohort_percentage_changed=False,
            tasks_number_changed=True,
            recalculate_stats_counts={
                'task_count': task_count,
                'annotation_count': annotation_count,
                'prediction_count': prediction_count,
            },
        )
        logger.info('Tasks bulk_update finished (sync reimport)')

        project.summary.update_data_columns(tasks)
        # TODO: project.summary.update_created_annotations_and_labels

        return Response(
            {
                'task_count': task_count,
                'annotation_count': annotation_count,
                'prediction_count': prediction_count,
                'duration': duration,
                'file_upload_ids': file_upload_ids,
                'found_formats': found_formats,
                'data_columns': data_columns,
            },
            status=status.HTTP_201_CREATED,
        )

    def async_reimport(self, project, file_upload_ids, files_as_tasks_list, organization_id):

        project_reimport = ProjectReimport.objects.create(
            project=project, file_upload_ids=file_upload_ids, files_as_tasks_list=files_as_tasks_list
        )

        start_job_async_or_sync(
            async_reimport_background,
            project_reimport.id,
            organization_id,
            self.request.user,
            queue_name='high',
            on_failure=set_reimport_background_failure,
            project_id=project.id,
        )

        response = {'reimport': project_reimport.id}
        return Response(response, status=status.HTTP_201_CREATED)

    @retry_database_locked()
    def create(self, request, *args, **kwargs):
        files_as_tasks_list = bool_from_request(request.data, 'files_as_tasks_list', True)
        file_upload_ids = self.request.data.get('file_upload_ids')

        # check project permissions
        project = generics.get_object_or_404(Project.objects.for_user(self.request.user), pk=self.kwargs['pk'])

        if not file_upload_ids:
            return Response(
                {
                    'task_count': 0,
                    'annotation_count': 0,
                    'prediction_count': 0,
                    'duration': 0,
                    'file_upload_ids': [],
                    'found_formats': {},
                    'data_columns': [],
                },
                status=status.HTTP_200_OK,
            )

        if (
            flag_set('fflag_fix_all_lsdv_4971_async_reimport_09052023_short', request.user)
            and settings.VERSION_EDITION != 'Community'
        ):
            return self.async_reimport(
                project, file_upload_ids, files_as_tasks_list, request.user.active_organization_id
            )
        else:
            return self.sync_reimport(project, file_upload_ids, files_as_tasks_list)

    @swagger_auto_schema(
        auto_schema=None,
        operation_summary='Re-import tasks',
        operation_description="""
        Re-import tasks using the specified file upload IDs for a specific project.
        """,
    )
    def post(self, *args, **kwargs):
        return super(ReImportAPI, self).post(*args, **kwargs)


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Import'],
        x_fern_sdk_group_name=['files'],
        x_fern_sdk_method_name='list',
        x_fern_audiences=['public'],
        operation_summary='Get files list',
        manual_parameters=[
            openapi.Parameter(
                name='all',
                type=openapi.TYPE_BOOLEAN,
                in_=openapi.IN_QUERY,
                description='Set to "true" if you want to retrieve all file uploads',
            ),
            openapi.Parameter(
                name='ids',
                type=openapi.TYPE_ARRAY,
                in_=openapi.IN_QUERY,
                items=openapi.Schema(title='File upload ID', type=openapi.TYPE_INTEGER),
                description='Specify the list of file upload IDs to retrieve, e.g. ids=[1,2,3]',
            ),
        ],
        operation_description="""
        Retrieve the list of uploaded files used to create labeling tasks for a specific project.
        """,
    ),
)
@method_decorator(
    name='delete',
    decorator=swagger_auto_schema(
        tags=['Import'],
        x_fern_sdk_group_name=['files'],
        x_fern_sdk_method_name='delete_many',
        x_fern_audiences=['public'],
        operation_summary='Delete files',
        operation_description="""
        Delete uploaded files for a specific project.
        """,
    ),
)
class FileUploadListAPI(generics.mixins.ListModelMixin, generics.mixins.DestroyModelMixin, generics.GenericAPIView):
    parser_classes = (JSONParser, MultiPartParser, FormParser)
    serializer_class = FileUploadSerializer
    permission_required = ViewClassPermission(
        GET=all_permissions.projects_view,
        DELETE=all_permissions.projects_change,
    )
    queryset = FileUpload.objects.all()

    def get_queryset(self):
        project = generics.get_object_or_404(Project.objects.for_user(self.request.user), pk=self.kwargs.get('pk', 0))
        if project.is_draft or bool_from_request(self.request.query_params, 'all', False):
            # If project is in draft state, we return all uploaded files, ignoring queried ids
            logger.debug(f'Return all uploaded files for draft project {project}')
            return FileUpload.objects.filter(project_id=project.id, user=self.request.user)

        # If requested in regular import, only queried IDs are returned to avoid showing previously imported
        ids = json.loads(self.request.query_params.get('ids', '[]'))
        logger.debug(f'File Upload IDs found: {ids}')
        return FileUpload.objects.filter(project_id=project.id, id__in=ids, user=self.request.user)

    def get(self, request, *args, **kwargs):
        return self.list(request, *args, **kwargs)

    def delete(self, request, *args, **kwargs):
        project = generics.get_object_or_404(Project.objects.for_user(self.request.user), pk=self.kwargs['pk'])
        ids = self.request.data.get('file_upload_ids')
        if ids is None:
            deleted, _ = FileUpload.objects.filter(project=project).delete()
        elif isinstance(ids, list):
            deleted, _ = FileUpload.objects.filter(project=project, id__in=ids).delete()
        else:
            raise ValueError('"file_upload_ids" parameter must be a list of integers')
        return Response({'deleted': deleted}, status=status.HTTP_200_OK)


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Import'],
        x_fern_sdk_group_name=['files'],
        x_fern_sdk_method_name='get',
        x_fern_audiences=['public'],
        operation_summary='Get file upload',
        operation_description='Retrieve details about a specific uploaded file.',
    ),
)
@method_decorator(
    name='patch',
    decorator=swagger_auto_schema(
        tags=['Import'],
        x_fern_sdk_group_name=['files'],
        x_fern_sdk_method_name='update',
        x_fern_audiences=['public'],
        operation_summary='Update file upload',
        operation_description='Update a specific uploaded file.',
        request_body=FileUploadSerializer,
    ),
)
@method_decorator(
    name='delete',
    decorator=swagger_auto_schema(
        tags=['Import'],
        x_fern_sdk_group_name=['files'],
        x_fern_sdk_method_name='delete',
        x_fern_audiences=['public'],
        operation_summary='Delete file upload',
        operation_description='Delete a specific uploaded file.',
    ),
)
class FileUploadAPI(generics.RetrieveUpdateDestroyAPIView):
    parser_classes = (JSONParser, MultiPartParser, FormParser)
    permission_classes = (IsAuthenticated,)
    serializer_class = FileUploadSerializer
    queryset = FileUpload.objects.all()

    def get(self, *args, **kwargs):
        return super(FileUploadAPI, self).get(*args, **kwargs)

    def patch(self, *args, **kwargs):
        return super(FileUploadAPI, self).patch(*args, **kwargs)

    def delete(self, *args, **kwargs):
        return super(FileUploadAPI, self).delete(*args, **kwargs)

    @swagger_auto_schema(auto_schema=None)
    def put(self, *args, **kwargs):
        return super(FileUploadAPI, self).put(*args, **kwargs)


class UploadedFileResponse(generics.RetrieveAPIView):
    permission_classes = (IsAuthenticated,)

    @override_report_only_csp
    @csp(SANDBOX=[])
    @swagger_auto_schema(
        tags=['Import'],
        x_fern_sdk_group_name=['files'],
        x_fern_sdk_method_name='download',
        x_fern_audiences=['public'],
        operation_summary='Download file',
        operation_description='Download a specific uploaded file.',
    )
    def get(self, *args, **kwargs):
        request = self.request
        filename = kwargs['filename']
        # XXX needed, on windows os.path.join generates '\' which breaks FileUpload
        file = settings.UPLOAD_DIR + ('/' if not settings.UPLOAD_DIR.endswith('/') else '') + filename
        logger.debug(f'Fetch uploaded file by user {request.user} => {file}')
        file_upload = FileUpload.objects.filter(file=file).last()

        if not file_upload.has_permission(request.user):
            return Response(status=status.HTTP_403_FORBIDDEN)

        file = file_upload.file
        if file.storage.exists(file.name):
            content_type, encoding = mimetypes.guess_type(str(file.name))
            content_type = content_type or 'application/octet-stream'
            return RangedFileResponse(request, file.open(mode='rb'), content_type=content_type)

        return Response(status=status.HTTP_404_NOT_FOUND)


class DownloadStorageData(APIView):
    """Check auth for nginx auth_request"""

    swagger_schema = None
    http_method_names = ['get']
    permission_classes = (IsAuthenticated,)

    def get(self, request, *args, **kwargs):
        """Get export files list"""
        request = self.request
        filepath = request.GET.get('filepath')
        if filepath is None:
            return Response(status=status.HTTP_404_NOT_FOUND)

        filepath = unquote(request.GET['filepath'])

        url = None
        if filepath.startswith(settings.UPLOAD_DIR):
            logger.debug(f'Fetch uploaded file by user {request.user} => {filepath}')
            file_upload = FileUpload.objects.filter(file=filepath).last()

            if file_upload is not None and file_upload.has_permission(request.user):
                url = file_upload.file.storage.url(file_upload.file.name, storage_url=True)
        elif filepath.startswith(settings.AVATAR_PATH):
            user = User.objects.filter(avatar=filepath).first()
            if user is not None and request.user.active_organization.has_user(user):
                url = user.avatar.storage.url(user.avatar.name, storage_url=True)

        if url is None:
            return Response(status=status.HTTP_403_FORBIDDEN)

        protocol = urlparse(url).scheme

        # Let NGINX handle it
        response = HttpResponse()
        # The below header tells NGINX to catch it and serve, see docker-config/nginx-app.conf
        redirect = '/file_download/' + protocol + '/' + url.replace(protocol + '://', '')

        response['X-Accel-Redirect'] = redirect
        response['Content-Disposition'] = 'attachment; filename="{}"'.format(filepath)
        return response


class PresignAPIMixin:
    def handle_presign(self, request: HttpRequest, fileuri: str, instance: Union[Task, Project]) -> Response:
        model_name = type(instance).__name__

        if not instance.has_permission(request.user):
            return Response(status=status.HTTP_403_FORBIDDEN)

        # Attempt to base64 decode the fileuri
        try:
            fileuri = base64.urlsafe_b64decode(fileuri.encode()).decode()
        # For backwards compatibility, try unquote if this fails
        except Exception as exc:
            logger.debug(
                f'Failed to decode base64 {fileuri} for {model_name} {instance.id}: {exc} falling back to unquote'
            )
            fileuri = unquote(fileuri)

        try:
            resolved = instance.resolve_storage_uri(fileuri)
        except Exception as exc:
            logger.error(f'Failed to resolve storage uri {fileuri} for {model_name} {instance.id}: {exc}')
            return Response(status=status.HTTP_404_NOT_FOUND)

        if resolved is None or resolved.get('url') is None:
            return Response(status=status.HTTP_404_NOT_FOUND)

        url = resolved['url']
        max_age = 0
        if resolved.get('presign_ttl'):
            max_age = resolved.get('presign_ttl') * 60

        # Proxy to presigned url
        response = HttpResponseRedirect(redirect_to=url, status=status.HTTP_303_SEE_OTHER)
        response.headers['Cache-Control'] = f'no-store, max-age={max_age}'

        return response


class TaskPresignStorageData(PresignAPIMixin, APIView):
    """A file proxy to presign storage urls at the task level."""

    swagger_schema = None
    http_method_names = ['get']
    permission_classes = (IsAuthenticated,)

    def get(self, request, *args, **kwargs):
        """Get the presigned url for a given fileuri"""
        request = self.request
        task_id = kwargs.get('task_id')
        fileuri = request.GET.get('fileuri')

        if fileuri is None or task_id is None:
            return Response(status=status.HTTP_400_BAD_REQUEST)

        try:
            task = Task.objects.get(pk=task_id)
        except Task.DoesNotExist:
            return Response(status=status.HTTP_404_NOT_FOUND)

        return self.handle_presign(request, fileuri, task)


class ProjectPresignStorageData(PresignAPIMixin, APIView):
    """A file proxy to presign storage urls at the project level."""

    swagger_schema = None
    http_method_names = ['get']
    permission_classes = (IsAuthenticated,)

    def get(self, request, *args, **kwargs):
        """Get the presigned url for a given fileuri"""
        request = self.request
        project_id = kwargs.get('project_id')
        fileuri = request.GET.get('fileuri')

        if fileuri is None or project_id is None:
            return Response(status=status.HTTP_400_BAD_REQUEST)

        try:
            project = Project.objects.get(pk=project_id)
        except Project.DoesNotExist:
            return Response(status=status.HTTP_404_NOT_FOUND)

        return self.handle_presign(request, fileuri, project)
</file>

<file path="label_studio/data_import/functions.py">
import logging
import time
import traceback
from typing import Callable, Optional

from core.utils.common import load_func
from django.conf import settings
from django.db import transaction
from projects.models import ProjectImport, ProjectReimport, ProjectSummary
from users.models import User
from webhooks.models import WebhookAction
from webhooks.utils import emit_webhooks_for_instance

from .models import FileUpload
from .serializers import ImportApiSerializer
from .uploader import load_tasks_for_async_import

logger = logging.getLogger(__name__)


def async_import_background(
    import_id, user_id, recalculate_stats_func: Optional[Callable[..., None]] = None, **kwargs
):
    with transaction.atomic():
        try:
            project_import = ProjectImport.objects.get(id=import_id)
        except ProjectImport.DoesNotExist:
            logger.error(f'ProjectImport with id {import_id} not found, import processing failed')
            return
        if project_import.status != ProjectImport.Status.CREATED:
            logger.error(f'Processing import with id {import_id} already started')
            return
        project_import.status = ProjectImport.Status.IN_PROGRESS
        project_import.save(update_fields=['status'])

    user = User.objects.get(id=user_id)

    start = time.time()
    project = project_import.project
    tasks = None
    # upload files from request, and parse all tasks
    # TODO: Stop passing request to load_tasks function, make all validation before
    tasks, file_upload_ids, found_formats, data_columns = load_tasks_for_async_import(project_import, user)

    if project_import.preannotated_from_fields:
        # turn flat task JSONs {"column1": value, "column2": value} into {"data": {"column1"..}, "predictions": [{..."column2"}]
        tasks = reformat_predictions(tasks, project_import.preannotated_from_fields)

    if project_import.commit_to_project:
        with transaction.atomic():
            # Lock summary for update to avoid race conditions
            summary = ProjectSummary.objects.select_for_update().get(project=project)

            # Immediately create project tasks and update project states and counters
            serializer = ImportApiSerializer(data=tasks, many=True, context={'project': project})
            serializer.is_valid(raise_exception=True)
            tasks = serializer.save(project_id=project.id)
            emit_webhooks_for_instance(user.active_organization, project, WebhookAction.TASKS_CREATED, tasks)

            task_count = len(tasks)
            annotation_count = len(serializer.db_annotations)
            prediction_count = len(serializer.db_predictions)
            # Update counters (like total_annotations) for new tasks and after bulk update tasks stats. It should be a
            # single operation as counters affect bulk is_labeled update

            recalculate_stats_counts = {
                'task_count': task_count,
                'annotation_count': annotation_count,
                'prediction_count': prediction_count,
            }

            project.update_tasks_counters_and_task_states(
                tasks_queryset=tasks,
                maximum_annotations_changed=False,
                overlap_cohort_percentage_changed=False,
                tasks_number_changed=True,
                recalculate_stats_counts=recalculate_stats_counts,
            )
            logger.info('Tasks bulk_update finished (async import)')

            summary.update_data_columns(tasks)
            # TODO: summary.update_created_annotations_and_labels
    else:
        # Do nothing - just output file upload ids for further use
        task_count = len(tasks)
        annotation_count = None
        prediction_count = None

    duration = time.time() - start

    project_import.task_count = task_count or 0
    project_import.annotation_count = annotation_count or 0
    project_import.prediction_count = prediction_count or 0
    project_import.duration = duration
    project_import.file_upload_ids = file_upload_ids
    project_import.found_formats = found_formats
    project_import.data_columns = data_columns
    if project_import.return_task_ids:
        project_import.task_ids = [task.id for task in tasks]

    project_import.status = ProjectImport.Status.COMPLETED
    project_import.save()


def set_import_background_failure(job, connection, type, value, _):
    import_id = job.args[0]
    ProjectImport.objects.filter(id=import_id).update(
        status=ProjectImport.Status.FAILED, traceback=traceback.format_exc(), error=str(value)
    )


def set_reimport_background_failure(job, connection, type, value, _):
    reimport_id = job.args[0]
    ProjectReimport.objects.filter(id=reimport_id).update(
        status=ProjectReimport.Status.FAILED,
        traceback=traceback.format_exc(),
        error=str(value),
    )


def reformat_predictions(tasks, preannotated_from_fields):
    new_tasks = []
    for task in tasks:
        if 'data' in task:
            task = task['data']
        predictions = [{'result': task.pop(field)} for field in preannotated_from_fields]
        new_tasks.append({'data': task, 'predictions': predictions})
    return new_tasks


post_process_reimport = load_func(settings.POST_PROCESS_REIMPORT)


def async_reimport_background(reimport_id, organization_id, user, **kwargs):

    with transaction.atomic():
        try:
            reimport = ProjectReimport.objects.get(id=reimport_id)
        except ProjectReimport.DoesNotExist:
            logger.error(f'ProjectReimport with id {reimport_id} not found, import processing failed')
            return
        if reimport.status != ProjectReimport.Status.CREATED:
            logger.error(f'Processing reimport with id {reimport_id} already started')
            return
        reimport.status = ProjectReimport.Status.IN_PROGRESS
        reimport.save(update_fields=['status'])

    project = reimport.project

    tasks, found_formats, data_columns = FileUpload.load_tasks_from_uploaded_files(
        reimport.project, reimport.file_upload_ids, files_as_tasks_list=reimport.files_as_tasks_list
    )

    with transaction.atomic():
        # Lock summary for update to avoid race conditions
        summary = ProjectSummary.objects.select_for_update().get(project=project)

        project.remove_tasks_by_file_uploads(reimport.file_upload_ids)
        serializer = ImportApiSerializer(data=tasks, many=True, context={'project': project, 'user': user})
        serializer.is_valid(raise_exception=True)
        tasks = serializer.save(project_id=project.id)
        emit_webhooks_for_instance(organization_id, project, WebhookAction.TASKS_CREATED, tasks)

        task_count = len(tasks)
        annotation_count = len(serializer.db_annotations)
        prediction_count = len(serializer.db_predictions)

        recalculate_stats_counts = {
            'task_count': task_count,
            'annotation_count': annotation_count,
            'prediction_count': prediction_count,
        }

        # Update counters (like total_annotations) for new tasks and after bulk update tasks stats. It should be a
        # single operation as counters affect bulk is_labeled update
        project.update_tasks_counters_and_task_states(
            tasks_queryset=tasks,
            maximum_annotations_changed=False,
            overlap_cohort_percentage_changed=False,
            tasks_number_changed=True,
            recalculate_stats_counts=recalculate_stats_counts,
        )
        logger.info('Tasks bulk_update finished (async reimport)')

        summary.update_data_columns(tasks)
        # TODO: summary.update_created_annotations_and_labels

    reimport.task_count = task_count
    reimport.annotation_count = annotation_count
    reimport.prediction_count = prediction_count
    reimport.found_formats = found_formats
    reimport.data_columns = list(data_columns)
    reimport.status = ProjectReimport.Status.COMPLETED
    reimport.save()

    post_process_reimport(reimport)
</file>

<file path="label_studio/data_import/models.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import logging
import os
import uuid
from collections import Counter

import pandas as pd

try:
    import ujson as json
except:  # noqa: E722
    import json

from django.conf import settings
from django.db import models
from django.utils.functional import cached_property
from rest_framework.exceptions import ValidationError

logger = logging.getLogger(__name__)


def upload_name_generator(instance, filename):
    project = str(instance.project_id)
    project_dir = os.path.join(settings.MEDIA_ROOT, settings.UPLOAD_DIR, project)
    os.makedirs(project_dir, exist_ok=True)
    path = settings.UPLOAD_DIR + '/' + project + '/' + str(uuid.uuid4())[0:8] + '-' + filename
    return path


class FileUpload(models.Model):
    user = models.ForeignKey('users.User', related_name='file_uploads', on_delete=models.CASCADE)
    project = models.ForeignKey('projects.Project', related_name='file_uploads', on_delete=models.CASCADE)
    file = models.FileField(upload_to=upload_name_generator)

    def has_permission(self, user):
        user.project = self.project  # link for activity log
        return self.project.has_permission(user)

    @cached_property
    def filepath(self):
        return self.file.name

    @cached_property
    def file_name(self):
        return os.path.basename(self.file.name)

    @property
    def url(self):
        if settings.FORCE_SCRIPT_NAME and not (settings.HOSTNAME and settings.CLOUD_FILE_STORAGE_ENABLED):
            return settings.FORCE_SCRIPT_NAME + '/' + self.file.url.lstrip('/')
        else:
            return self.file.url

    @property
    def format(self):
        file_format = None
        try:
            file_format = os.path.splitext(self.filepath)[-1]
        except:  # noqa: E722
            pass
        finally:
            logger.debug('Get file format ' + str(file_format))
        return file_format

    @property
    def content(self):
        # cache file body
        if hasattr(self, '_file_body'):
            body = getattr(self, '_file_body')
        else:
            body = self.file.read().decode('utf-8')
            setattr(self, '_file_body', body)
        return body

    def read_tasks_list_from_csv(self, sep=','):
        logger.debug('Read tasks list from CSV file {}'.format(self.filepath))
        tasks = pd.read_csv(self.file.open(), sep=sep).fillna('').to_dict('records')
        tasks = [{'data': task} for task in tasks]
        return tasks

    def read_tasks_list_from_tsv(self):
        return self.read_tasks_list_from_csv('\t')

    def read_tasks_list_from_txt(self):
        logger.debug('Read tasks list from text file {}'.format(self.filepath))
        lines = self.content.splitlines()
        tasks = [{'data': {settings.DATA_UNDEFINED_NAME: line}} for line in lines]
        return tasks

    def read_tasks_list_from_json(self):
        logger.debug('Read tasks list from JSON file {}'.format(self.filepath))

        raw_data = self.content
        # Python 3.5 compatibility fix https://docs.python.org/3/whatsnew/3.6.html#json
        try:
            tasks = json.loads(raw_data)
        except TypeError:
            tasks = json.loads(raw_data.decode('utf8'))
        if isinstance(tasks, dict):
            tasks = [tasks]
        tasks_formatted = []
        for i, task in enumerate(tasks):
            if not task.get('data'):
                task = {'data': task}
            if not isinstance(task['data'], dict):
                raise ValidationError('Task item should be dict')
            tasks_formatted.append(task)
        return tasks_formatted

    def read_task_from_hypertext_body(self):
        logger.debug('Read 1 task from hypertext file {}'.format(self.filepath))
        body = self.content
        tasks = [{'data': {settings.DATA_UNDEFINED_NAME: body}}]
        return tasks

    def read_task_from_uploaded_file(self):
        logger.debug('Read 1 task from uploaded file {}'.format(self.filepath))
        if settings.CLOUD_FILE_STORAGE_ENABLED:
            tasks = [{'data': {settings.DATA_UNDEFINED_NAME: self.filepath}}]
        else:
            tasks = [{'data': {settings.DATA_UNDEFINED_NAME: self.url}}]
        return tasks

    @property
    def format_could_be_tasks_list(self):
        return self.format in ('.csv', '.tsv', '.txt')

    def read_tasks(self, file_as_tasks_list=True):
        file_format = self.format
        try:
            # file as tasks list
            if file_format == '.csv' and file_as_tasks_list:
                tasks = self.read_tasks_list_from_csv()
            elif file_format == '.tsv' and file_as_tasks_list:
                tasks = self.read_tasks_list_from_tsv()
            elif file_format == '.txt' and file_as_tasks_list:
                tasks = self.read_tasks_list_from_txt()
            elif file_format == '.json':
                tasks = self.read_tasks_list_from_json()

            # otherwise - only one object tag should be presented in label config
            elif not self.project.one_object_in_label_config:
                raise ValidationError(
                    'Your label config has more than one data key and direct file upload supports only '
                    'one data key. To import data with multiple data keys, use a JSON or CSV file.'
                )

            # file as a single asset
            elif file_format in ('.html', '.htm', '.xml'):
                tasks = self.read_task_from_hypertext_body()
            else:
                tasks = self.read_task_from_uploaded_file()

        except Exception as exc:
            raise ValidationError('Failed to parse input file ' + self.file_name + ': ' + str(exc))
        return tasks

    @classmethod
    def load_tasks_from_uploaded_files(
        cls, project, file_upload_ids=None, formats=None, files_as_tasks_list=True, trim_size=None
    ):
        tasks = []
        fileformats = []
        common_data_fields = set()

        # scan all files
        file_uploads = FileUpload.objects.filter(project=project)
        if file_upload_ids:
            file_uploads = file_uploads.filter(id__in=file_upload_ids)
        for file_upload in file_uploads:
            file_format = file_upload.format
            if formats and file_format not in formats:
                continue
            new_tasks = file_upload.read_tasks(files_as_tasks_list)
            for task in new_tasks:
                task['file_upload_id'] = file_upload.id

            new_data_fields = set(iter(new_tasks[0]['data'].keys())) if len(new_tasks) > 0 else set()
            if not common_data_fields:
                common_data_fields = new_data_fields
            elif not common_data_fields.intersection(new_data_fields):
                raise ValidationError(
                    _old_vs_new_data_keys_inconsistency_message(
                        new_data_fields, common_data_fields, file_upload.file.name
                    )
                )
            else:
                common_data_fields &= new_data_fields

            tasks += new_tasks
            fileformats.append(file_format)

            if trim_size is not None:
                if len(tasks) > trim_size:
                    break

        return tasks, dict(Counter(fileformats)), common_data_fields


def _old_vs_new_data_keys_inconsistency_message(new_data_keys, old_data_keys, current_file):
    new_data_keys_list = ','.join(new_data_keys)
    old_data_keys_list = ','.join(old_data_keys)
    common_prefix = "You're trying to import inconsistent data:\n"
    if new_data_keys_list == old_data_keys_list:
        return ''
    elif new_data_keys_list == settings.DATA_UNDEFINED_NAME:
        return (
            common_prefix + 'uploading a single file {0} '
            'clashes with data key(s) found from other files:\n"{1}"'.format(current_file, old_data_keys_list)
        )
    elif old_data_keys_list == settings.DATA_UNDEFINED_NAME:
        return (
            common_prefix + 'uploading tabular data from {0} with data key(s) {1}, '
            'clashes with other raw binary files (images, audios, etc.)'.format(current_file, new_data_keys_list)
        )
    else:
        return (
            common_prefix + 'uploading tabular data from "{0}" with data key(s) "{1}", '
            'clashes with data key(s) found from other files:\n"{2}"'.format(
                current_file, new_data_keys_list, old_data_keys_list
            )
        )
</file>

<file path="label_studio/data_import/serializers.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
from rest_framework import serializers
from tasks.models import Task
from tasks.serializers import AnnotationSerializer, PredictionSerializer, TaskSerializer, TaskSerializerBulk

from .models import FileUpload


class ImportApiSerializer(TaskSerializer):
    """Tasks serializer for Import API (TaskBulkCreateAPI)"""

    annotations = AnnotationSerializer(many=True, default=[])
    predictions = PredictionSerializer(many=True, default=[])

    class Meta:
        model = Task
        list_serializer_class = TaskSerializerBulk
        exclude = ('is_labeled', 'project')


class FileUploadSerializer(serializers.ModelSerializer):
    file = serializers.FileField(use_url=False)

    class Meta:
        model = FileUpload
        fields = ['id', 'file']
</file>

<file path="label_studio/data_import/uploader.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import csv
import io
import logging
import mimetypes
import os

try:
    import ujson as json
except:  # noqa: E722
    import json

from core.utils.common import timeit
from core.utils.io import ssrf_safe_get
from django.conf import settings
from django.core.files.uploadedfile import SimpleUploadedFile
from rest_framework.exceptions import ValidationError

from .models import FileUpload

logger = logging.getLogger(__name__)
csv.field_size_limit(131072 * 10)


def is_binary(f):
    return isinstance(f, (io.RawIOBase, io.BufferedIOBase))


def csv_generate_header(file):
    """Generate column names for headless csv file"""
    file.seek(0)
    names = []
    line = file.readline()

    num_columns = len(line.split(b',' if isinstance(line, bytes) else ','))
    for i in range(num_columns):
        names.append('column' + str(i + 1))
    file.seek(0)
    return names


def check_max_task_number(tasks):
    # max tasks
    if len(tasks) > settings.TASKS_MAX_NUMBER:
        raise ValidationError(
            f'Maximum task number is {settings.TASKS_MAX_NUMBER}, ' f'current task number is {len(tasks)}'
        )


def check_tasks_max_file_size(value):
    if value >= settings.TASKS_MAX_FILE_SIZE:
        raise ValidationError(
            f'Maximum total size of all files is {settings.TASKS_MAX_FILE_SIZE} bytes, '
            f'current size is {value} bytes'
        )


def check_extensions(files):
    for filename, file_obj in files.items():
        _, ext = os.path.splitext(file_obj.name)
        if ext.lower() not in settings.SUPPORTED_EXTENSIONS:
            raise ValidationError(f'{ext} extension is not supported')


def check_request_files_size(files):
    total = sum([file.size for _, file in files.items()])

    check_tasks_max_file_size(total)


def create_file_upload(user, project, file):
    instance = FileUpload(user=user, project=project, file=file)
    if settings.SVG_SECURITY_CLEANUP:
        content_type, encoding = mimetypes.guess_type(str(instance.file.name))
        if content_type in ['image/svg+xml']:
            clean_xml = allowlist_svg(instance.file.read().decode())
            instance.file.seek(0)
            instance.file.write(clean_xml.encode())
            instance.file.truncate()
    instance.save()
    return instance


def allowlist_svg(dirty_xml):
    """Filter out malicious/harmful content from SVG files
    by defining allowed tags
    """
    from lxml.html import clean

    allow_tags = [
        'xml',
        'svg',
        'circle',
        'ellipse',
        'line',
        'path',
        'polygon',
        'polyline',
        'rect',
    ]

    cleaner = clean.Cleaner(
        allow_tags=allow_tags,
        style=True,
        links=True,
        add_nofollow=False,
        page_structure=True,
        safe_attrs_only=False,
        remove_unknown_tags=False,
    )

    clean_xml = cleaner.clean_html(dirty_xml)
    return clean_xml


def str_to_json(data):
    try:
        json_acceptable_string = data.replace("'", '"')
        return json.loads(json_acceptable_string)
    except ValueError:
        return None


def tasks_from_url(file_upload_ids, project, user, url, could_be_tasks_list):
    """Download file using URL and read tasks from it"""
    # process URL with tasks
    try:
        filename = url.rsplit('/', 1)[-1]

        response = ssrf_safe_get(
            url, verify=project.organization.should_verify_ssl_certs(), stream=True, headers={'Accept-Encoding': None}
        )
        file_content = response.content
        check_tasks_max_file_size(int(response.headers['content-length']))
        file_upload = create_file_upload(user, project, SimpleUploadedFile(filename, file_content))
        if file_upload.format_could_be_tasks_list:
            could_be_tasks_list = True
        file_upload_ids.append(file_upload.id)
        tasks, found_formats, data_keys = FileUpload.load_tasks_from_uploaded_files(project, file_upload_ids)

    except ValidationError as e:
        raise e
    except Exception as e:
        raise ValidationError(str(e))
    return data_keys, found_formats, tasks, file_upload_ids, could_be_tasks_list


@timeit
def create_file_uploads(user, project, FILES):
    could_be_tasks_list = False
    file_upload_ids = []
    check_request_files_size(FILES)
    check_extensions(FILES)
    for _, file in FILES.items():
        file_upload = create_file_upload(user, project, file)
        if file_upload.format_could_be_tasks_list:
            could_be_tasks_list = True
        file_upload_ids.append(file_upload.id)

    logger.debug(f'created file uploads: {file_upload_ids} could_be_tasks_list: {could_be_tasks_list}')
    return file_upload_ids, could_be_tasks_list


def load_tasks_for_async_import(project_import, user):
    """Load tasks from different types of request.data / request.files saved in project_import model"""
    file_upload_ids, found_formats, data_keys = [], [], set()

    if project_import.file_upload_ids:
        file_upload_ids = project_import.file_upload_ids
        tasks, found_formats, data_keys = FileUpload.load_tasks_from_uploaded_files(
            project_import.project, file_upload_ids
        )

    # take tasks from url address
    elif project_import.url:
        url = project_import.url
        # try to load json with task or tasks from url as string
        json_data = str_to_json(url)
        if json_data:
            file_upload = create_file_upload(
                user,
                project_import.project,
                SimpleUploadedFile('inplace.json', url.encode()),
            )
            file_upload_ids.append(file_upload.id)
            tasks, found_formats, data_keys = FileUpload.load_tasks_from_uploaded_files(
                project_import.project, file_upload_ids
            )

        # download file using url and read tasks from it
        else:
            could_be_tasks_list = False
            (
                data_keys,
                found_formats,
                tasks,
                file_upload_ids,
                could_be_tasks_list,
            ) = tasks_from_url(file_upload_ids, project_import.project, user, url, could_be_tasks_list)
            if could_be_tasks_list:
                project_import.could_be_tasks_list = True
                project_import.save(update_fields=['could_be_tasks_list'])

    elif project_import.tasks:
        tasks = project_import.tasks

    # check is data root is list
    if not isinstance(tasks, list):
        raise ValidationError('load_tasks: Data root must be list')

    # empty tasks error
    if not tasks:
        raise ValidationError('load_tasks: No tasks added')

    check_max_task_number(tasks)
    return tasks, file_upload_ids, found_formats, list(data_keys)


def load_tasks(request, project):
    """Load tasks from different types of request.data / request.files"""
    file_upload_ids, found_formats, data_keys = [], [], set()
    could_be_tasks_list = False

    # take tasks from request FILES
    if len(request.FILES):
        check_request_files_size(request.FILES)
        check_extensions(request.FILES)
        for filename, file in request.FILES.items():
            file_upload = create_file_upload(request.user, project, file)
            if file_upload.format_could_be_tasks_list:
                could_be_tasks_list = True
            file_upload_ids.append(file_upload.id)
        tasks, found_formats, data_keys = FileUpload.load_tasks_from_uploaded_files(project, file_upload_ids)

    # take tasks from url address
    elif 'application/x-www-form-urlencoded' in request.content_type:
        # empty url
        url = request.data.get('url')
        if not url:
            raise ValidationError('"url" is not found in request data')

        # try to load json with task or tasks from url as string
        json_data = str_to_json(url)
        if json_data:
            file_upload = create_file_upload(request.user, project, SimpleUploadedFile('inplace.json', url.encode()))
            file_upload_ids.append(file_upload.id)
            tasks, found_formats, data_keys = FileUpload.load_tasks_from_uploaded_files(project, file_upload_ids)

        # download file using url and read tasks from it
        else:
            (
                data_keys,
                found_formats,
                tasks,
                file_upload_ids,
                could_be_tasks_list,
            ) = tasks_from_url(file_upload_ids, project, request.user, url, could_be_tasks_list)

    # take one task from request DATA
    elif 'application/json' in request.content_type and isinstance(request.data, dict):
        tasks = [request.data]

    # take many tasks from request DATA
    elif 'application/json' in request.content_type and isinstance(request.data, list):
        tasks = request.data

    # incorrect data source
    else:
        raise ValidationError('load_tasks: No data found in DATA or in FILES')

    # check is data root is list
    if not isinstance(tasks, list):
        raise ValidationError('load_tasks: Data root must be list')

    # empty tasks error
    if not tasks:
        raise ValidationError('load_tasks: No tasks added')

    check_max_task_number(tasks)
    return tasks, file_upload_ids, could_be_tasks_list, found_formats, list(data_keys)
</file>

<file path="label_studio/data_import/urls.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
from django.urls import include, path

from . import api

app_name = 'data_import'

_api_urlpatterns = [path('file-upload/<int:pk>', api.FileUploadAPI.as_view(), name='file-upload-detail')]

_api_projects_urlpatterns = [
    # import api
    path('<int:pk>/tasks/bulk/', api.TasksBulkCreateAPI.as_view(), name='project-tasks-bulk-upload'),
    path('<int:pk>/import', api.ImportAPI.as_view(), name='project-import'),
    path('<int:pk>/import/predictions', api.ImportPredictionsAPI.as_view(), name='project-import-predictions'),
    path('<int:pk>/reimport', api.ReImportAPI.as_view(), name='project-reimport'),
    path('<int:pk>/file-uploads', api.FileUploadListAPI.as_view(), name='project-file-upload-list'),
]

urlpatterns = [
    path('api/import/', include((_api_urlpatterns, app_name), namespace='api')),
    path('api/projects/', include((_api_projects_urlpatterns, app_name), namespace='api-projects')),
    # special endpoints for serving imported files
    path('data/upload/<path:filename>', api.UploadedFileResponse.as_view(), name='data-upload'),
    path('storage-data/uploaded/', api.DownloadStorageData.as_view(), name='storage-data-upload'),
    path('tasks/<int:task_id>/presign/', api.TaskPresignStorageData.as_view(), name='task-storage-data-presign'),
    path(
        'projects/<int:project_id>/presign/',
        api.ProjectPresignStorageData.as_view(),
        name='project-storage-data-presign',
    ),
]
</file>

<file path="label_studio/data_manager/actions/__init__.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
""" Actions for tasks and annotations provided by data manager.
    All actions are stored in settings.DATA_MANAGER_ACTIONS dict.
    Data manager uses settings.DATA_MANAGER_ACTIONS to know the list of available actions,
    they are called by entry_points from settings.DATA_MANAGER_ACTIONS dict items.
"""
import copy
import logging
import os
import traceback as tb
from importlib import import_module

from core.feature_flags import flag_set
from data_manager.functions import DataManagerException
from django.conf import settings
from rest_framework.exceptions import PermissionDenied as DRFPermissionDenied

logger = logging.getLogger('django')


def check_permissions(user, action):
    """Actions must have permissions, if only one is in the user role then the action is allowed"""
    if 'permission' not in action:
        logger.error('Action must have "permission" field: %s', str(action))
        return False

    return user.has_perm(action['permission'])


def get_all_actions(user, project):
    """Return dict with registered actions

    :param user: list with user permissions
    :param project: current project
    """
    # copy and sort by order key
    actions = list(settings.DATA_MANAGER_ACTIONS.values())
    actions = copy.deepcopy(actions)
    actions = sorted(actions, key=lambda x: x['order'])
    actions = [
        {key: action[key] for key in action if key != 'entry_point'}
        for action in actions
        if not action.get('hidden', False) and check_permissions(user, action)
    ]
    # remove experimental features if they are disabled
    if not (
        flag_set('ff_back_experimental_features', user=project.organization.created_by)
        or settings.EXPERIMENTAL_FEATURES
    ):
        actions = [action for action in actions if not action.get('experimental', False)]

    # generate form if function is passed
    for action in actions:
        form_generator = action.get('dialog', {}).get('form')
        if callable(form_generator):
            action['dialog']['form'] = form_generator(user, project)

    return actions


def register_action(entry_point, title, order, **kwargs):
    """Register action in global _action instance,
    action_id will be automatically extracted from entry_point function name
    """
    action_id = entry_point.__name__
    if action_id in settings.DATA_MANAGER_ACTIONS:
        logger.debug('Action with id "' + action_id + '" already exists, rewriting registration')

    settings.DATA_MANAGER_ACTIONS[action_id] = {
        'id': action_id,
        'title': title,
        'order': order,
        'entry_point': entry_point,
        **kwargs,
    }


def register_actions_from_dir(base_module, action_dir):
    """Find all python files nearby this file and try to load 'actions' from them"""
    for path in os.listdir(action_dir):
        # skip non module files
        if '__init__' in path or '__pycache' in path or path.startswith('.'):
            continue

        name = path[0 : path.find('.py')]  # get only module name to read *.py and *.pyc
        try:
            module = import_module(f'{base_module}.{name}')
            if not hasattr(module, 'actions'):
                continue
            module_actions = module.actions
        except ModuleNotFoundError as e:
            logger.info(e)
            continue

        for action in module_actions:
            register_action(**action)
            logger.debug('Action registered: ' + str(action['entry_point'].__name__))


def perform_action(action_id, project, queryset, user, **kwargs):
    """Perform action using entry point from actions"""
    if action_id not in settings.DATA_MANAGER_ACTIONS:
        raise DataManagerException("Can't find '" + action_id + "' in registered actions")

    action = settings.DATA_MANAGER_ACTIONS[action_id]

    # check user permissions for this action
    if not check_permissions(user, action):
        raise DRFPermissionDenied(f'Action is not allowed for the current user: {action["id"]}')

    try:
        result = action['entry_point'](project, queryset, **kwargs)
    except Exception as e:
        text = 'Error while perform action: ' + action_id + '\n' + tb.format_exc()
        logger.error(text, extra={'sentry_skip': True})
        raise e

    return result


register_actions_from_dir('data_manager.actions', os.path.dirname(__file__))
</file>

<file path="label_studio/data_manager/actions/basic.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import logging
from datetime import datetime

from core.permissions import AllPermissions
from core.redis import start_job_async_or_sync
from core.utils.common import load_func
from data_manager.functions import evaluate_predictions
from django.conf import settings
from projects.models import Project
from tasks.functions import update_tasks_counters
from tasks.models import Annotation, AnnotationDraft, Prediction, Task
from webhooks.models import WebhookAction
from webhooks.utils import emit_webhooks_for_instance

all_permissions = AllPermissions()
logger = logging.getLogger(__name__)


def retrieve_tasks_predictions(project, queryset, **kwargs):
    """Retrieve predictions by tasks ids

    :param project: project instance
    :param queryset: filtered tasks db queryset
    """
    evaluate_predictions(queryset)
    return {'processed_items': queryset.count(), 'detail': 'Retrieved ' + str(queryset.count()) + ' predictions'}


def delete_tasks(project, queryset, **kwargs):
    """Delete tasks by ids

    :param project: project instance
    :param queryset: filtered tasks db queryset
    """
    tasks_ids = list(queryset.values('id'))
    count = len(tasks_ids)
    tasks_ids_list = [task['id'] for task in tasks_ids]
    project_count = project.tasks.count()
    # unlink tasks from project
    queryset = Task.objects.filter(id__in=tasks_ids_list)
    queryset.update(project=None)
    # delete all project tasks
    if count == project_count:
        start_job_async_or_sync(Task.delete_tasks_without_signals_from_task_ids, tasks_ids_list)
        logger.info(f'calling reset project_id={project.id} delete_tasks()')
        project.summary.reset()

    # delete only specific tasks
    else:
        # update project summary and delete tasks
        start_job_async_or_sync(async_project_summary_recalculation, tasks_ids_list, project.id)

    project.update_tasks_states(
        maximum_annotations_changed=False, overlap_cohort_percentage_changed=False, tasks_number_changed=True
    )
    # emit webhooks for project
    emit_webhooks_for_instance(project.organization, project, WebhookAction.TASKS_DELETED, tasks_ids)

    # remove all tabs if there are no tasks in project
    reload = False
    if not project.tasks.exists():
        project.views.all().delete()
        reload = True

    # Execute actions after delete tasks
    Task.after_bulk_delete_actions(tasks_ids_list)

    return {'processed_items': count, 'reload': reload, 'detail': 'Deleted ' + str(count) + ' tasks'}


def delete_tasks_annotations(project, queryset, **kwargs):
    """Delete all annotations and drafts by tasks ids

    :param project: project instance
    :param queryset: filtered tasks db queryset
    """
    task_ids = queryset.values_list('id', flat=True)
    annotations = Annotation.objects.filter(task__id__in=task_ids)
    count = annotations.count()

    # take only tasks where annotations were deleted
    real_task_ids = set(list(annotations.values_list('task__id', flat=True)))
    annotations_ids = list(annotations.values('id'))
    # remove deleted annotations from project.summary
    project.summary.remove_created_annotations_and_labels(annotations)
    # also remove drafts for the task. This includes task and annotation level
    # drafts by design.
    drafts = AnnotationDraft.objects.filter(task__id__in=task_ids)
    project.summary.remove_created_drafts_and_labels(drafts)

    annotations.delete()
    drafts.delete()  # since task-level annotation drafts will not have been deleted by CASCADE
    emit_webhooks_for_instance(project.organization, project, WebhookAction.ANNOTATIONS_DELETED, annotations_ids)
    request = kwargs['request']

    tasks = Task.objects.filter(id__in=real_task_ids)
    tasks.update(updated_at=datetime.now(), updated_by=request.user)
    # Update tasks counter and is_labeled. It should be a single operation as counters affect bulk is_labeled update
    project.update_tasks_counters_and_is_labeled(tasks_queryset=real_task_ids)

    # LSE postprocess
    postprocess = load_func(settings.DELETE_TASKS_ANNOTATIONS_POSTPROCESS)
    if postprocess is not None:
        tasks = Task.objects.filter(id__in=task_ids)
        postprocess(project, tasks, **kwargs)

    return {'processed_items': count, 'detail': 'Deleted ' + str(count) + ' annotations'}


def delete_tasks_predictions(project, queryset, **kwargs):
    """Delete all predictions by tasks ids

    :param project: project instance
    :param queryset: filtered tasks db queryset
    """
    task_ids = queryset.values_list('id', flat=True)
    predictions = Prediction.objects.filter(task__id__in=task_ids)
    real_task_ids = set(list(predictions.values_list('task__id', flat=True)))
    count = predictions.count()
    predictions.delete()
    start_job_async_or_sync(update_tasks_counters, Task.objects.filter(id__in=real_task_ids))
    return {'processed_items': count, 'detail': 'Deleted ' + str(count) + ' predictions'}


def async_project_summary_recalculation(tasks_ids_list, project_id):
    queryset = Task.objects.filter(id__in=tasks_ids_list)
    project = Project.objects.get(id=project_id)
    project.summary.remove_created_annotations_and_labels(Annotation.objects.filter(task__in=queryset))
    project.summary.remove_data_columns(queryset)
    Task.delete_tasks_without_signals(queryset)


actions = [
    {
        'entry_point': retrieve_tasks_predictions,
        'permission': all_permissions.predictions_any,
        'title': 'Retrieve Predictions',
        'order': 90,
        'dialog': {
            'title': 'Retrieve Predictions',
            'text': 'Send the selected tasks to all ML backends connected to the project.'
            'This operation might be abruptly interrupted due to a timeout. '
            'The recommended way to get predictions is to update tasks using the Label Studio API.'
            'Please confirm your action.',
            'type': 'confirm',
        },
    },
    {
        'entry_point': delete_tasks,
        'permission': all_permissions.tasks_delete,
        'title': 'Delete Tasks',
        'order': 100,
        'reload': True,
        'dialog': {
            'text': 'You are going to delete the selected tasks. Please confirm your action.',
            'type': 'confirm',
        },
    },
    {
        'entry_point': delete_tasks_annotations,
        'permission': all_permissions.tasks_delete,
        'title': 'Delete Annotations',
        'order': 101,
        'dialog': {
            'text': 'You are going to delete all annotations from the selected tasks. Please confirm your action.',
            'type': 'confirm',
        },
    },
    {
        'entry_point': delete_tasks_predictions,
        'permission': all_permissions.predictions_any,
        'title': 'Delete Predictions',
        'order': 102,
        'dialog': {
            'text': 'You are going to delete all predictions from the selected tasks. Please confirm your action.',
            'type': 'confirm',
        },
    },
]
</file>

<file path="label_studio/data_manager/actions/cache_labels.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""

import logging

from core.permissions import AllPermissions
from core.redis import start_job_async_or_sync
from tasks.models import Annotation, Prediction, Task

logger = logging.getLogger(__name__)
all_permissions = AllPermissions()


def cache_labels_job(project, queryset, **kwargs):
    request_data = kwargs['request_data']
    source = request_data.get('source', 'annotations').lower()
    assert source in ['annotations', 'predictions'], 'Source must be annotations or predictions'
    source_class = Annotation if source == 'annotations' else Prediction
    control_tag = request_data.get('custom_control_tag') or request_data.get('control_tag')
    with_counters = request_data.get('with_counters', 'Yes').lower() == 'yes'

    if source == 'annotations':
        column_name = 'cache'
    else:
        column_name = 'cache_predictions'

    # ALL is a special case, we will cache all labels from all control tags into one column
    if control_tag == 'ALL' or control_tag is None:
        control_tag = None
        column_name = f'{column_name}_all'
    else:
        column_name = f'{column_name}_{control_tag}'

    tasks = list(queryset.only('data'))
    logger.info(f'Cache labels for {len(tasks)} tasks and control tag {control_tag}')

    for task in tasks:
        task_labels = []
        annotations = source_class.objects.filter(task=task).only('result')
        for annotation in annotations:
            labels = extract_labels(annotation, control_tag)
            task_labels.extend(labels)

        # cache labels in separate data column
        # with counters
        if with_counters:
            task.data[column_name] = ', '.join(
                sorted([f'{label}: {task_labels.count(label)}' for label in set(task_labels)])
            )
        # no counters
        else:
            task.data[column_name] = ', '.join(sorted(list(set(task_labels))))

    Task.objects.bulk_update(tasks, fields=['data'], batch_size=1000)
    first_task = Task.objects.get(id=queryset.first().id)
    project.summary.update_data_columns([first_task])
    return {'response_code': 200, 'detail': f'Updated {len(tasks)} tasks'}


def extract_labels(annotation, control_tag):
    labels = []
    for region in annotation.result:
        # find regions with specific control tag name or just all regions if control tag is None
        if (control_tag is None or region['from_name'] == control_tag) and 'value' in region:
            # scan value for a field with list of strings,
            # as bonus it will work with textareas too
            for key in region['value']:
                if (
                    isinstance(region['value'][key], list)
                    and region['value'][key]
                    and isinstance(region['value'][key][0], str)
                ):
                    labels.extend(region['value'][key])
                    break
    return labels


def cache_labels(project, queryset, request, **kwargs):
    """Cache labels from annotations to a new column in tasks"""
    start_job_async_or_sync(
        cache_labels_job,
        project,
        queryset,
        organization_id=project.organization_id,
        request_data=request.data,
        job_timeout=60 * 60 * 5,  # max allowed duration is 5 hours
    )
    return {'response_code': 200}


def cache_labels_form(user, project):
    labels = project.get_parsed_config()
    control_tags = ['ALL']
    for key, _ in labels.items():
        control_tags.append(key)

    return [
        {
            'columnCount': 1,
            'fields': [
                {
                    'type': 'select',
                    'name': 'control_tag',
                    'label': 'Choose a control tag',
                    'options': control_tags,
                },
                {
                    'type': 'input',
                    'name': 'custom_control_tag',
                    'label': "Custom control tag if it's not in label config",
                },
                {
                    'type': 'select',
                    'name': 'with_counters',
                    'label': 'With counters',
                    'options': ['Yes', 'No'],
                },
                {
                    'type': 'select',
                    'name': 'source',
                    'label': 'Source',
                    'options': ['Annotations', 'Predictions'],
                },
            ],
        }
    ]


actions = [
    {
        'entry_point': cache_labels,
        'permission': all_permissions.projects_change,
        'title': 'Cache Labels',
        'order': 1,
        'experimental': True,
        'dialog': {
            'text': 'Confirm that you want to add a new task.data field with cached labels from annotations. '
            'This field will help you to quickly filter or order tasks by labels. '
            'After this operation you must refresh the Data Manager page fully to see the new column!',
            'type': 'confirm',
            'form': cache_labels_form,
        },
    },
]
</file>

<file path="label_studio/data_manager/actions/experimental.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""

import logging
import random

import ujson as json
from core.permissions import AllPermissions
from core.utils.db import fast_first
from data_manager.functions import DataManagerException
from django.conf import settings
from tasks.models import Annotation, Task
from tasks.serializers import TaskSerializerBulk

logger = logging.getLogger(__name__)
all_permissions = AllPermissions()


def propagate_annotations(project, queryset, **kwargs):
    request = kwargs['request']
    user = request.user
    source_annotation_id = request.data.get('source_annotation_id')
    annotations = Annotation.objects.filter(project=project, id=source_annotation_id)
    if not annotations:
        raise DataManagerException(f'Source annotation {source_annotation_id} not found in the current project')
    source_annotation = annotations.first()

    tasks = set(queryset.values_list('id', flat=True))
    try:
        tasks.remove(source_annotation.task.id)
    except KeyError:
        pass

    # copy source annotation to new annotations for each task
    db_annotations = []
    for i in tasks:
        body = {
            'task_id': i,
            'completed_by_id': user.id,
            'result': source_annotation.result,
            'result_count': source_annotation.result_count,
            'parent_annotation_id': source_annotation.id,
            'project': project,
        }
        body = TaskSerializerBulk.add_annotation_fields(body, user, 'propagated_annotation')
        db_annotations.append(Annotation(**body))

    db_annotations = Annotation.objects.bulk_create(db_annotations, batch_size=settings.BATCH_SIZE)
    TaskSerializerBulk.post_process_annotations(user, db_annotations, 'propagated_annotation')
    # Update counters for tasks and is_labeled. It should be a single operation as counters affect bulk is_labeled update
    project.update_tasks_counters_and_is_labeled(tasks_queryset=Task.objects.filter(id__in=tasks))
    return {
        'response_code': 200,
        'detail': f'Created {len(db_annotations)} annotations',
    }


def propagate_annotations_form(user, project):
    first_annotation = fast_first(Annotation.objects.filter(project=project))
    field = {
        'type': 'number',
        'name': 'source_annotation_id',
        'label': 'Enter source annotation ID'
        + (f' [first ID: {str(first_annotation.id)}]' if first_annotation else ''),
    }
    return [{'columnCount': 1, 'fields': [field]}]


def rename_labels(project, queryset, **kwargs):
    request = kwargs['request']

    old_label_name = request.data.get('old_label_name')
    new_label_name = request.data.get('new_label_name')
    control_tag = request.data.get('control_tag')

    labels = project.get_parsed_config()
    if control_tag not in labels:
        raise Exception('Wrong old label name, it is not from labeling config: ' + old_label_name)
    label_type = labels[control_tag]['type'].lower()

    annotations = Annotation.objects.filter(project=project)
    if settings.DJANGO_DB == settings.DJANGO_DB_SQLITE:
        annotations = annotations.filter(result__icontains=control_tag).filter(result__icontains=old_label_name)
    else:
        annotations = annotations.filter(result__contains=[{'from_name': control_tag}]).filter(
            result__contains=[{'value': {label_type: [old_label_name]}}]
        )

    label_count = 0
    annotation_count = 0
    for annotation in annotations:
        changed = False
        for sub in annotation.result:
            if sub.get('from_name', None) == control_tag and old_label_name in sub.get('value', {}).get(
                label_type, []
            ):

                new_labels = []
                for label in sub['value'][label_type]:
                    if label == old_label_name:
                        new_labels.append(new_label_name)
                        label_count += 1
                        changed = True
                    else:
                        new_labels.append(label)

                sub['value'][label_type] = new_labels

        if changed:
            annotation.save(update_fields=['result'])
            annotation_count += 1

    # update summaries
    logger.info(f'calling reset project_id={project.id} rename_labels()')
    project.summary.reset()
    project.summary.update_data_columns(project.tasks.all())
    annotations = Annotation.objects.filter(project=project)
    project.summary.update_created_annotations_and_labels(annotations)

    return {
        'response_code': 200,
        'detail': f'Updated {label_count} labels in {annotation_count}',
    }


def rename_labels_form(user, project):
    labels = project.get_parsed_config()

    old_names = []
    control_tags = []
    for key, label in labels.items():
        old_names += label.get('labels', [])
        control_tags.append(key)

    return [
        {
            'columnCount': 1,
            'fields': [
                {
                    'type': 'select',
                    'name': 'control_tag',
                    'label': 'Choose a label control tag',
                    'options': control_tags,
                },
                {
                    'type': 'select',
                    'name': 'old_label_name',
                    'label': 'Old label name',
                    'options': list(set(old_names)),
                },
                {'type': 'input', 'name': 'new_label_name', 'label': 'New label name'},
            ],
        }
    ]


def add_data_field(project, queryset, **kwargs):
    from django.db.models import F, Func, JSONField, Value

    request = kwargs['request']
    value_name = request.data.get('value_name')
    value_type = request.data.get('value_type')
    value = request.data.get('value')
    size = queryset.count()

    cast = {'String': str, 'Number': float, 'Expression': str}
    assert value_type in cast.keys()
    value = cast[value_type](value)

    if value_type == 'Expression':
        add_expression(queryset, size, value, value_name)

    else:

        # sqlite
        if settings.DJANGO_DB == settings.DJANGO_DB_SQLITE:
            tasks = list(queryset.only('data'))
            for task in tasks:
                task.data[value_name] = value
            Task.objects.bulk_update(tasks, fields=['data'], batch_size=1000)

        # postgres and other DB
        else:
            queryset.update(
                data=Func(
                    F('data'),
                    Value([value_name]),
                    Value(value, JSONField()),
                    function='jsonb_set',
                )
            )

    project.summary.update_data_columns([queryset.first()])
    return {'response_code': 200, 'detail': f'Updated {size} tasks'}


def process_arrays(params):
    start, end = params.find('['), -1
    while start != end:
        end = start + params[start:].find(']') + 1
        params = params[0:start] + params[start:end].replace(',', ';') + params[end:]
        start = end + params[end:].find('[') + 1
    return params


add_data_field_examples = (
    'range(2) or '
    'sample() or '
    'random(<min_int>, <max_int>) or '
    'choices(["<value1>", "<value2>", ...], [<weight1>, <weight2>, ...]) or '
    'replace("old-string", "new-string")'
)


def add_expression(queryset, size, value, value_name):
    # simple parsing
    command, args = value.split('(')
    args = process_arrays(args)
    args = args.replace(')', '').split(',')
    args = [] if len(args) == 1 and args[0] == '' else args
    # return comma back, convert quotation mark to doubled quotation mark for json parsing
    for i, arg in enumerate(args):
        args[i] = arg.replace(';', ',').replace("'", '"')

    tasks = list(queryset.only('data'))

    # range
    if command == 'range':
        assert len(args) == 1, 'range(start:int) should have start argument '
        start = int(args[0])
        values = range(start, start + size)
        for i, v in enumerate(values):
            tasks[i].data[value_name] = v

    # permutation sampling
    elif command == 'sample':
        assert len(args) == 0, "sample() doesn't have arguments"
        values = random.sample(range(0, size), size)
        for i, v in enumerate(values):
            tasks[i].data[value_name] = v

    # uniform random
    elif command == 'random':
        assert len(args) == 2, 'random(min, max) should have 2 args: min & max'
        minimum, maximum = int(args[0]), int(args[1])
        for i in range(size):
            tasks[i].data[value_name] = random.randint(minimum, maximum)

    # sampling with choices and weights
    elif command == 'choices':
        assert 0 < len(args) < 3, (
            'choices(values:list, weights:list) ' 'should have 1 or 2 args: values & weights (default=None)'
        )
        weights = json.loads(args[1]) if len(args) == 2 else None
        values = random.choices(population=json.loads(args[0]), weights=weights, k=size)
        for i, v in enumerate(values):
            tasks[i].data[value_name] = v

    # replace
    elif command == 'replace':
        assert len(args) == 2, 'replace(old_value:str, new_value:str) should have 2 args: old value & new value'
        old_value, new_value = json.loads(args[0]), json.loads(args[1])
        for task in tasks:
            if value_name in task.data:
                task.data[value_name] = task.data[value_name].replace(old_value, new_value)

    else:
        raise Exception('Undefined expression, you can use: ' + add_data_field_examples)

    Task.objects.bulk_update(tasks, fields=['data'], batch_size=1000)


def add_data_field_form(user, project):
    return [
        {
            'columnCount': 1,
            'fields': [
                {'type': 'input', 'name': 'value_name', 'label': 'Name'},
                {
                    'type': 'select',
                    'name': 'value_type',
                    'label': 'Type',
                    'options': ['String', 'Number', 'Expression'],
                },
                {'type': 'input', 'name': 'value', 'label': 'Value'},
            ],
        }
    ]


actions = [
    {
        'entry_point': add_data_field,
        'permission': all_permissions.projects_change,
        'title': 'Add Or Modify Data Field',
        'order': 1,
        'experimental': True,
        'dialog': {
            'text': 'Confirm that you want to add a new field in tasks. '
            'After this operation you must refresh the Data Manager page fully to see the new column! '
            'You can use the following expressions: ' + add_data_field_examples,
            'type': 'confirm',
            'form': add_data_field_form,
        },
    },
    {
        'entry_point': propagate_annotations,
        'permission': all_permissions.tasks_change,
        'title': 'Propagate Annotations',
        'order': 1,
        'experimental': True,
        'dialog': {
            'text': 'Confirm that you want to copy the source annotation to all selected tasks. '
            'Note: this action can be applied only for similar source objects: '
            'images with the same width and height, '
            'texts with the same length, '
            'audios with the same durations.',
            'type': 'confirm',
            'form': propagate_annotations_form,
        },
    },
    {
        'entry_point': rename_labels,
        'permission': all_permissions.tasks_change,
        'title': 'Rename Labels',
        'order': 1,
        'experimental': True,
        'dialog': {
            'text': 'Confirm that you want to rename a label in all annotations. '
            'Also you have to change label names in the labeling config manually.',
            'type': 'confirm',
            'form': rename_labels_form,
        },
    },
]
</file>

<file path="label_studio/data_manager/actions/next_task.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import logging

from core.permissions import all_permissions
from data_manager.functions import filters_ordering_selected_items_exist
from projects.functions.next_task import get_next_task
from rest_framework.exceptions import NotFound
from tasks.serializers import NextTaskSerializer

logger = logging.getLogger(__name__)


def next_task(project, queryset, **kwargs):
    """Generate next task for labeling stream

    :param project: project
    :param queryset: task ids to sample from
    :param kwargs: arguments from api request
    """

    request = kwargs['request']
    dm_queue = filters_ordering_selected_items_exist(request.data)
    next_task, queue_info = get_next_task(request.user, queryset, project, dm_queue)

    if next_task is None:
        raise NotFound(
            f'There are still some tasks to complete for the user={request.user}, '
            f'but they seem to be locked by another user.'
        )

    # serialize task
    context = {'request': request, 'project': project, 'resolve_uri': True, 'annotations': False}
    serializer = NextTaskSerializer(next_task, context=context)
    response = serializer.data
    response['queue'] = queue_info
    return response


actions = [
    {
        'entry_point': next_task,
        'permission': all_permissions.projects_view,
        'title': 'Generate Next Task',
        'order': 0,
        'hidden': True,
    }
]
</file>

<file path="label_studio/data_manager/actions/predictions_to_annotations.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import logging

from core.permissions import AllPermissions
from django.utils.timezone import now
from tasks.models import Annotation, Prediction, Task
from tasks.serializers import TaskSerializerBulk
from webhooks.models import WebhookAction
from webhooks.utils import emit_webhooks_for_instance

all_permissions = AllPermissions()
logger = logging.getLogger(__name__)


def predictions_to_annotations(project, queryset, **kwargs):
    request = kwargs['request']
    user = request.user
    model_version = request.data.get('model_version')
    queryset = queryset.filter(predictions__isnull=False)
    predictions = Prediction.objects.filter(task__in=queryset, child_annotations__isnull=True)

    # model version filter
    if model_version is not None:
        if isinstance(model_version, list):
            predictions = predictions.filter(model_version__in=model_version).distinct()
        else:
            predictions = predictions.filter(model_version=model_version)

    predictions_values = list(predictions.values_list('result', 'model_version', 'task_id', 'id'))

    # prepare annotations
    annotations = []
    tasks_ids = []
    for result, model_version, task_id, prediction_id in predictions_values:
        tasks_ids.append(task_id)
        body = {
            'result': result,
            'completed_by_id': user.pk,
            'task_id': task_id,
            'parent_prediction_id': prediction_id,
            'project': project,
        }
        body = TaskSerializerBulk.add_annotation_fields(body, user, 'prediction')
        annotations.append(body)

    count = len(annotations)
    logger.debug(f'{count} predictions will be converter to annotations')
    db_annotations = [Annotation(**annotation) for annotation in annotations]
    db_annotations = Annotation.objects.bulk_create(db_annotations)
    Task.objects.filter(id__in=tasks_ids).update(updated_at=now(), updated_by=request.user)

    if db_annotations:
        TaskSerializerBulk.post_process_annotations(user, db_annotations, 'prediction')
        # Execute webhook for created annotations
        emit_webhooks_for_instance(
            user.active_organization, project, WebhookAction.ANNOTATIONS_CREATED, db_annotations
        )
        # Update counters for tasks and is_labeled. It should be a single operation as counters affect bulk is_labeled update
        project.update_tasks_counters_and_is_labeled(Task.objects.filter(id__in=tasks_ids))

        try:
            from stats.functions.stats import recalculate_stats_async_or_sync

            recalculate_stats_async_or_sync(project, all=False)
        except (ModuleNotFoundError, ImportError):
            logger.info('Predictions converted to annotations in LSO, stats recomputation skipped')

    return {'response_code': 200, 'detail': f'Created {count} annotations'}


def predictions_to_annotations_form(user, project):
    versions = project.get_model_versions()

    # put the current model version on the top of the list
    # if it exists
    first = project.model_version
    if first:
        try:
            versions.remove(first)
        except ValueError:
            pass
        versions = [first] + versions

    return [
        {
            'columnCount': 1,
            'fields': [
                {
                    'type': 'select',
                    'name': 'model_version',
                    'label': 'Choose predictions',
                    'options': versions,
                }
            ],
        }
    ]


actions = [
    {
        'entry_point': predictions_to_annotations,
        'permission': all_permissions.tasks_change,
        'title': 'Create Annotations From Predictions',
        'order': 91,
        'dialog': {
            'title': 'Create Annotations From Predictions',
            'text': 'Create annotations from predictions using selected predictions set '
            'for each selected task.'
            'Your account will be assigned as an owner to those annotations. ',
            'type': 'confirm',
            'form': predictions_to_annotations_form,
        },
    }
]
</file>

<file path="label_studio/data_manager/actions/remove_duplicates.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""

import logging
from collections import defaultdict

import ujson as json
from core.label_config import replace_task_data_undefined_with_config_field
from core.permissions import AllPermissions
from core.redis import start_job_async_or_sync
from data_manager.actions.basic import delete_tasks
from io_storages.azure_blob.models import AzureBlobImportStorageLink
from io_storages.gcs.models import GCSImportStorageLink
from io_storages.localfiles.models import LocalFilesImportStorageLink
from io_storages.redis.models import RedisImportStorageLink
from io_storages.s3.models import S3ImportStorageLink
from tasks.models import Task

logger = logging.getLogger(__name__)
all_permissions = AllPermissions()


def remove_duplicates(project, queryset, **kwargs):
    """Remove duplicated tasks with the same data fields:
    Duplicated tasks will be deleted and all annotations will be moved to the first of the duplicated tasks.
    Storage links will be restored for the first task.
    """
    start_job_async_or_sync(
        remove_duplicates_job,
        project,
        queryset,
        organization_id=project.organization_id,
    )
    return {'response_code': 200}


def remove_duplicates_job(project, queryset, **kwargs):
    """Job for start_job_async_or_sync"""
    duplicates = find_duplicated_tasks_by_data(project, queryset)
    restore_storage_links_for_duplicated_tasks(duplicates)
    move_annotations(duplicates)
    remove_duplicated_tasks(duplicates, project, queryset)

    # totally update tasks counters
    project._update_tasks_counters_and_task_states(
        project.tasks.all(),
        maximum_annotations_changed=True,
        overlap_cohort_percentage_changed=True,
        tasks_number_changed=False,
        from_scratch=True,
    )


def remove_duplicated_tasks(duplicates, project, queryset):
    """Remove duplicated tasks from queryset with condition that they don't have annotations

    :param duplicates: dict with duplicated tasks
    :param project: Project instance
    :param queryset: queryset with input tasks
    :return: queryset with tasks which should be kept
    """
    removing = []
    # prepare main tasks which won't be deleted
    for data in duplicates:
        root = duplicates[data]
        if len(root) == 1:
            continue

        one_task_saved = False
        new_root = []
        for task in root:
            # keep all tasks with annotations in safety
            if task['total_annotations'] + task['cancelled_annotations'] > 0:
                one_task_saved = True
            else:
                new_root.append(task)

        for task in new_root:
            # keep the first task in safety
            if not one_task_saved:
                one_task_saved = True
            # remove all other tasks
            else:
                removing.append(task['id'])

    # get the final queryset for removing tasks
    queryset = queryset.filter(id__in=removing, annotations__isnull=True)
    kept = queryset.exclude(id__in=removing, annotations__isnull=True)

    # check that we don't remove tasks with annotations
    if queryset.count() != len(removing):
        raise Exception(
            f'Remove duplicates failed, operation is not finished: '
            f'queryset count {queryset.count()} != removing {len(removing)}. '
            'It means that some of duplicated tasks have been annotated twice or more.'
        )

    delete_tasks(project, queryset)
    logger.info(f'Removed {len(removing)} duplicated tasks')
    return kept


def move_annotations(duplicates):
    """Move annotations to the first task from duplicated tasks"""
    total_moved_annotations = 0

    for data in duplicates:
        root = duplicates[data]
        if len(root) == 1:
            continue

        # find a task with annotations, make it as "first" main one
        i, first = 0, root[0]
        for i, task in enumerate(root):
            first = task
            if task['total_annotations'] + task['cancelled_annotations'] > 0:
                break

        # move annotations to the first task
        for task in root[i + 1 :]:
            if task['total_annotations'] + task['cancelled_annotations'] > 0:
                Task.objects.get(id=task['id']).annotations.update(task_id=first['id'])
                total_moved_annotations += task['total_annotations'] + task['cancelled_annotations']
                logger.info(
                    f"Moved {task['total_annotations']} annotations from task {task['id']} to task {first['id']}"
                )
                task['total_annotations'] = 0
                task['cancelled_annotations'] = 0


def restore_storage_links_for_duplicated_tasks(duplicates) -> None:
    """Build storage links for duplicated tasks and save them to Task in DB"""

    # storage classes
    classes = {
        'io_storages_s3importstoragelink': S3ImportStorageLink,
        'io_storages_gcsimportstoragelink': GCSImportStorageLink,
        'io_storages_azureblobimportstoragelink': AzureBlobImportStorageLink,
        'io_storages_localfilesimportstoragelink': LocalFilesImportStorageLink,
        'io_storages_redisimportstoragelink': RedisImportStorageLink,
        # 'lse_io_storages_lses3importstoragelink'  # not supported yet
    }

    total_restored_links = 0
    for data in list(duplicates):
        tasks = duplicates[data]
        source = None

        # find first task with existing StorageLink
        for task in tasks:
            for link in classes:
                if link in task and task[link] is not None:
                    # we don't support case when there are many storage links in duplicated tasks
                    if source is not None:
                        source = None
                        break
                    source = (
                        task,
                        classes[link],
                        task[link],
                    )  # last arg is a storage link id

        # add storage links to duplicates
        if source:
            storage_link_class = source[1]  # get link name
            for task in tasks:
                if task['id'] != source[0]['id']:
                    # get already existing StorageLink
                    link_instance = storage_link_class.objects.get(id=source[2])

                    # assign existing StorageLink to other duplicated tasks
                    link = storage_link_class(
                        task_id=task['id'],
                        key=link_instance.key,
                        storage=link_instance.storage,
                    )
                    link.save()
                    total_restored_links += 1
                    logger.info(f"Restored storage link for task {task['id']} from source task {source[0]['id']}")

    logger.info(f'Restored {total_restored_links} storage links for duplicated tasks')


def find_duplicated_tasks_by_data(project, queryset):
    """Find duplicated tasks by `task.data` and return them as a dict"""

    # get io_storage_* links for tasks, we need to copy them
    storages = []
    for field in dir(Task):
        if field.startswith('io_storages_'):
            storages += [field]

    groups = defaultdict(list)
    tasks = list(queryset.values('data', 'id', 'total_annotations', 'cancelled_annotations', *storages))
    logger.info(f'Retrieved {len(tasks)} tasks from queryset')

    for task in list(tasks):
        replace_task_data_undefined_with_config_field(task['data'], project)
        task['data'] = json.dumps(task['data'])
        groups[task['data']].append(task)

    # make groups of duplicated ids for info print
    duplicates = {d: groups[d] for d in groups if len(groups[d]) > 1}
    info = {d: [task['id'] for task in duplicates[d]] for d in duplicates}

    logger.info(f'Found {len(duplicates)} duplicated tasks')
    logger.info(f'Duplicated tasks: {info}')
    return duplicates


actions = [
    {
        'entry_point': remove_duplicates,
        'permission': all_permissions.projects_change,
        'title': 'Remove Duplicated Tasks',
        'order': 95,
        'experimental': False,
        'dialog': {
            'text': (
                'Confirm that you want to remove duplicated tasks with the same data fields. '
                'Duplicated tasks will be deleted and all annotations will be moved to the first task from duplicated tasks. '
                'Also Source Storage Links will be restored if at least one duplicated task has a storage link. '
                "Warning: Task assignments (enterprise only) won't be saved."
            ),
            'type': 'confirm',
        },
    },
]
</file>

<file path="label_studio/data_manager/migrations/__init__.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
</file>

<file path="label_studio/data_manager/migrations/0001_squashed_0005_view_user.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
# Generated by Django 3.1.4 on 2021-03-03 07:29

from django.conf import settings
from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    replaces = [('data_manager', '0001_initial'), ('data_manager', '0002_auto_20210201_1316'), ('data_manager', '0003_filter_index'), ('data_manager', '0004_auto_20210204_1231'), ('data_manager', '0005_view_user')]

    initial = True

    dependencies = [
        ('projects', '0001_squashed_0065_auto_20210223_2014'),
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
    ]

    operations = [
        migrations.CreateModel(
            name='Filter',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('column', models.CharField(help_text='Field name', max_length=1024, verbose_name='column')),
                ('type', models.CharField(help_text='Field type', max_length=1024, verbose_name='type')),
                ('operator', models.CharField(help_text='Filter operator', max_length=1024, verbose_name='operator')),
                ('value', models.JSONField(default=dict, help_text='Filter value', null=True, verbose_name='value')),
                ('index', models.IntegerField(default=0, help_text='To keep filter order', verbose_name='index')),
            ],
        ),
        migrations.CreateModel(
            name='FilterGroup',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('conjunction', models.CharField(help_text='Type of conjunction', max_length=1024, verbose_name='conjunction')),
                ('filters', models.ManyToManyField(help_text='Connected filters', related_name='filter_groups', to='data_manager.Filter')),
            ],
        ),
        migrations.CreateModel(
            name='View',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('data', models.JSONField(default=dict, help_text='Custom view data', null=True, verbose_name='data')),
                ('project', models.ForeignKey(help_text='Project ID', on_delete=django.db.models.deletion.CASCADE, related_name='views', to='projects.project')),
                ('ordering', models.JSONField(default=dict, help_text='Ordering parameters', null=True, verbose_name='ordering')),
                ('filter_group', models.ForeignKey(help_text='Groups of filters', null=True, on_delete=django.db.models.deletion.SET_NULL, to='data_manager.filtergroup')),
                ('selected_items', models.JSONField(default=dict, help_text='Selected items', null=True, verbose_name='selected items')),
                ('user', models.ForeignKey(help_text='User who made this view', null=True, on_delete=django.db.models.deletion.CASCADE, related_name='views', to=settings.AUTH_USER_MODEL)),
            ],
        ),
    ]
</file>

<file path="label_studio/data_manager/migrations/0002_remove_annotations_ids.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
from django.db import migrations


def remove(apps, schema_editor):
    View = apps.get_model('data_manager', 'View')
    views = View.objects.all()

    for view in views:
        if 'hiddenColumns' in view.data:
            if 'explore' in view.data['hiddenColumns']:
                view.data['hiddenColumns']['explore'].append('tasks:annotations_ids')
                view.data['hiddenColumns']['explore'] = list(set(view.data['hiddenColumns']['explore']))
            if 'labeling' in view.data['hiddenColumns']:
                view.data['hiddenColumns']['labeling'].append('tasks:annotations_ids')
                view.data['hiddenColumns']['labeling'] = list(set(view.data['hiddenColumns']['labeling']))

        view.save()


class Migration(migrations.Migration):
    dependencies = [
        ('data_manager', '0001_squashed_0005_view_user'),
    ]

    operations = [
        migrations.RunPython(remove),
    ]
</file>

<file path="label_studio/data_manager/migrations/0003_remove_predictions_model_versions.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
from django.db import migrations


def remove(apps, schema_editor):
    View = apps.get_model('data_manager', 'View')
    views = View.objects.all()

    for view in views:
        if 'hiddenColumns' in view.data:
            if 'explore' in view.data['hiddenColumns']:
                view.data['hiddenColumns']['explore'].append('tasks:predictions_model_versions')
                view.data['hiddenColumns']['explore'] = list(set(view.data['hiddenColumns']['explore']))
            if 'labeling' in view.data['hiddenColumns']:
                view.data['hiddenColumns']['labeling'].append('tasks:predictions_model_versions')
                view.data['hiddenColumns']['labeling'] = list(set(view.data['hiddenColumns']['labeling']))

        view.save()


class Migration(migrations.Migration):
    dependencies = [
        ('data_manager', '0002_remove_annotations_ids'),
    ]

    operations = [
        migrations.RunPython(remove),
    ]
</file>

<file path="label_studio/data_manager/migrations/0004_remove_avg_lead_time.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
from django.db import migrations


def remove(apps, schema_editor):
    View = apps.get_model('data_manager', 'View')
    views = View.objects.all()

    for view in views:
        if 'hiddenColumns' in view.data:
            if 'explore' in view.data['hiddenColumns']:
                view.data['hiddenColumns']['explore'].append('tasks:avg_lead_time')
                view.data['hiddenColumns']['explore'] = list(set(view.data['hiddenColumns']['explore']))
            if 'labeling' in view.data['hiddenColumns']:
                view.data['hiddenColumns']['labeling'].append('tasks:avg_lead_time')
                view.data['hiddenColumns']['labeling'] = list(set(view.data['hiddenColumns']['labeling']))

        view.save()


def backwards(apps, schema_editor):
    View = apps.get_model('data_manager', 'View')
    views = View.objects.all()

    for view in views:
        if 'hiddenColumns' in view.data:
            if 'explore' in view.data['hiddenColumns']:
                view.data['hiddenColumns']['explore'].remove('tasks:avg_lead_time')
                view.data['hiddenColumns']['explore'] = list(set(view.data['hiddenColumns']['explore']))
            if 'labeling' in view.data['hiddenColumns']:
                view.data['hiddenColumns']['labeling'].remove('tasks:avg_lead_time')
                view.data['hiddenColumns']['labeling'] = list(set(view.data['hiddenColumns']['labeling']))

        view.save()


class Migration(migrations.Migration):
    dependencies = [
        ('data_manager', '0003_remove_predictions_model_versions'),
    ]

    operations = [
        migrations.RunPython(remove, backwards),
    ]
</file>

<file path="label_studio/data_manager/migrations/0005_remove_updated_by.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
from django.db import migrations


def remove(apps, schema_editor):
    View = apps.get_model('data_manager', 'View')
    views = View.objects.all()

    for view in views:
        if 'hiddenColumns' in view.data:
            if 'explore' in view.data['hiddenColumns']:
                view.data['hiddenColumns']['explore'].append('tasks:updated_by')
                view.data['hiddenColumns']['explore'] = list(set(view.data['hiddenColumns']['explore']))
            if 'labeling' in view.data['hiddenColumns']:
                view.data['hiddenColumns']['labeling'].append('tasks:updated_by')
                view.data['hiddenColumns']['labeling'] = list(set(view.data['hiddenColumns']['labeling']))

        view.save()


def backwards(apps, schema_editor):
    View = apps.get_model('data_manager', 'View')
    views = View.objects.all()

    for view in views:
        if 'hiddenColumns' in view.data:
            if 'explore' in view.data['hiddenColumns']:
                view.data['hiddenColumns']['explore'].remove('tasks:updated_by')
                view.data['hiddenColumns']['explore'] = list(set(view.data['hiddenColumns']['explore']))
            if 'labeling' in view.data['hiddenColumns']:
                view.data['hiddenColumns']['labeling'].remove('tasks:updated_by')
                view.data['hiddenColumns']['labeling'] = list(set(view.data['hiddenColumns']['labeling']))

        view.save()


class Migration(migrations.Migration):
    dependencies = [
        ('data_manager', '0004_remove_avg_lead_time'),
    ]

    operations = [
        migrations.RunPython(remove, backwards),
    ]
</file>

<file path="label_studio/data_manager/migrations/0006_remove_inner_id.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
from django.db import migrations


def remove(apps, schema_editor):
    View = apps.get_model('data_manager', 'View')
    views = View.objects.all()

    for view in views:
        if 'hiddenColumns' in view.data:
            if 'explore' in view.data['hiddenColumns']:
                view.data['hiddenColumns']['explore'].append('tasks:inner_id')
                view.data['hiddenColumns']['explore'] = list(set(view.data['hiddenColumns']['explore']))
            if 'labeling' in view.data['hiddenColumns']:
                view.data['hiddenColumns']['labeling'].append('tasks:inner_id')
                view.data['hiddenColumns']['labeling'] = list(set(view.data['hiddenColumns']['labeling']))

        view.save()


def backwards(apps, schema_editor):
    View = apps.get_model('data_manager', 'View')
    views = View.objects.all()

    for view in views:
        if 'hiddenColumns' in view.data:
            if 'explore' in view.data['hiddenColumns']:
                view.data['hiddenColumns']['explore'].remove('tasks:inner_id')
                view.data['hiddenColumns']['explore'] = list(set(view.data['hiddenColumns']['explore']))
            if 'labeling' in view.data['hiddenColumns']:
                view.data['hiddenColumns']['labeling'].remove('tasks:inner_id')
                view.data['hiddenColumns']['labeling'] = list(set(view.data['hiddenColumns']['labeling']))

        view.save()


class Migration(migrations.Migration):
    dependencies = [
        ('data_manager', '0005_remove_updated_by'),
    ]

    operations = [
        migrations.RunPython(remove, backwards),
    ]
</file>

<file path="label_studio/data_manager/migrations/0007_auto_20220708_0832.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
from django.db import migrations


def remove(apps, schema_editor):
    View = apps.get_model('data_manager', 'View')
    views = View.objects.all()

    for view in views:
        if 'hiddenColumns' in view.data:
            if 'explore' in view.data['hiddenColumns']:
                view.data['hiddenColumns']['explore'].append('tasks:storage_filename')
                view.data['hiddenColumns']['explore'] = list(set(view.data['hiddenColumns']['explore']))
            if 'labeling' in view.data['hiddenColumns']:
                view.data['hiddenColumns']['labeling'].append('tasks:storage_filename')
                view.data['hiddenColumns']['labeling'] = list(set(view.data['hiddenColumns']['labeling']))

        view.save()


def backwards(apps, schema_editor):
    View = apps.get_model('data_manager', 'View')
    views = View.objects.all()

    for view in views:
        if 'hiddenColumns' in view.data:
            if 'explore' in view.data['hiddenColumns']:
                view.data['hiddenColumns']['explore'].remove('tasks:storage_filename')
                view.data['hiddenColumns']['explore'] = list(set(view.data['hiddenColumns']['explore']))
            if 'labeling' in view.data['hiddenColumns']:
                view.data['hiddenColumns']['labeling'].remove('tasks:storage_filename')
                view.data['hiddenColumns']['labeling'] = list(set(view.data['hiddenColumns']['labeling']))

        view.save()


class Migration(migrations.Migration):
    dependencies = [
        ('data_manager', '0006_remove_inner_id'),
    ]

    operations = [
        migrations.RunPython(remove, backwards),
    ]
</file>

<file path="label_studio/data_manager/migrations/0008_manual_counters_update.py">
import sys
import logging

from django.db import migrations

logger = logging.getLogger(__name__)


def forwards(apps, schema_editor):
    from tasks.functions import calculate_stats_all_orgs
    from django.conf import settings

    if settings.VERSION_EDITION == 'Community':
        run_command = 'label-studio calculate_stats_all_orgs'
    else:
        run_command = 'cd /label-studio-enterprise/label_studio_enterprise && ' \
                      'python3 manage.py calculate_stats_all_orgs'

    if '--skip-long-migrations' in sys.argv:
        logger.error(
            f"You used --skip-long-migrations, so you should run the migration manually as a separate process "
            f"to recalculate task counters, please use Django command `{run_command}`"
        )
        return

    logger.debug('=> Starting calculate_stats_all_orgs for task counters again')
    calculate_stats_all_orgs(from_scratch=True, redis=True, migration_name='data_manager-0008_manual_counters_update')


def backwards(apps, schema_editor):
    pass


class Migration(migrations.Migration):
    atomic = False

    dependencies = [
        ('data_manager', '0007_auto_20220708_0832'),
        ('tasks', '0023_auto_20220620_1007'),
        ('core', '0001_initial'),
        ('projects', '0017_project_pinned_at'),
    ]

    operations = [
        migrations.RunPython(forwards, backwards),
    ]
</file>

<file path="label_studio/data_manager/migrations/0009_alter_view_user.py">
# Generated by Django 3.2.16 on 2023-03-10 12:46

from django.conf import settings
from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    dependencies = [
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
        ('data_manager', '0008_manual_counters_update'),
    ]

    operations = [
        migrations.AlterField(
            model_name='view',
            name='user',
            field=models.ForeignKey(help_text='User who made this view', null=True, on_delete=django.db.models.deletion.CASCADE, related_name='data_manager_views', to=settings.AUTH_USER_MODEL),
        ),
    ]
</file>

<file path="label_studio/data_manager/migrations/0010_auto_20230718_1423.py">
# Generated by Django 3.2.16 on 2023-07-18 14:23

from django.db import migrations


def remove(apps, schema_editor):
    View = apps.get_model('data_manager', 'View')
    views = View.objects.all()

    for view in views:
        if 'hiddenColumns' in view.data:
            if 'explore' in view.data['hiddenColumns']:
                view.data['hiddenColumns']['explore'].append('tasks:draft_exists')
                view.data['hiddenColumns']['explore'] = list(set(view.data['hiddenColumns']['explore']))
            if 'labeling' in view.data['hiddenColumns']:
                view.data['hiddenColumns']['labeling'].append('tasks:draft_exists')
                view.data['hiddenColumns']['labeling'] = list(set(view.data['hiddenColumns']['labeling']))

        view.save()


def backwards(apps, schema_editor):
    View = apps.get_model('data_manager', 'View')
    views = View.objects.all()

    for view in views:
        if 'hiddenColumns' in view.data:
            if 'explore' in view.data['hiddenColumns']:
                view.data['hiddenColumns']['explore'].remove('tasks:draft_exists')
                view.data['hiddenColumns']['explore'] = list(set(view.data['hiddenColumns']['explore']))
            if 'labeling' in view.data['hiddenColumns']:
                view.data['hiddenColumns']['labeling'].remove('tasks:draft_exists')
                view.data['hiddenColumns']['labeling'] = list(set(view.data['hiddenColumns']['labeling']))

        view.save()


class Migration(migrations.Migration):

    dependencies = [('data_manager', '0009_alter_view_user')]

    operations = [migrations.RunPython(remove, backwards)]
</file>

<file path="label_studio/data_manager/migrations/0011_auto_20240718_1355.py">
# Generated by Django 3.2.25 on 2024-07-18 13:55

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('data_manager', '0010_auto_20230718_1423'),
    ]

    operations = [
        migrations.AlterModelOptions(
            name='view',
            options={'ordering': ['order']},
        ),
        migrations.AddField(
            model_name='view',
            name='order',
            field=models.IntegerField(default=0, help_text='Position of the tab, starting at the left in data manager and increasing as the tabs go left to right', null=True, verbose_name='order'),
        ),
        migrations.AddIndex(
            model_name='view',
            index=models.Index(fields=['project', 'order'], name='data_manage_project_69b96e_idx'),
        ),
    ]
</file>

<file path="label_studio/data_manager/migrations/0012_alter_view_user.py">
# Generated by Django 4.2.13 on 2024-08-13 19:51

from django.conf import settings
from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    dependencies = [
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
        ("data_manager", "0011_auto_20240718_1355"),
    ]

    operations = [
        migrations.AlterField(
            model_name="view",
            name="user",
            field=models.ForeignKey(
                help_text="User who made this view",
                null=True,
                on_delete=django.db.models.deletion.CASCADE,
                related_name="%(app_label)s_%(class)ss",
                to=settings.AUTH_USER_MODEL,
            ),
        )
    ]
</file>

<file path="label_studio/data_manager/__init__.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
</file>

<file path="label_studio/data_manager/api.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import logging

from asgiref.sync import async_to_sync, sync_to_async
from core.feature_flags import flag_set
from core.permissions import ViewClassPermission, all_permissions
from core.utils.common import int_from_request, load_func
from core.utils.params import bool_from_request
from data_manager.actions import get_all_actions, perform_action
from data_manager.functions import evaluate_predictions, get_prepare_params, get_prepared_queryset
from data_manager.managers import get_fields_for_evaluation
from data_manager.models import View
from data_manager.prepare_params import filters_schema, ordering_schema, prepare_params_schema
from data_manager.serializers import (
    DataManagerTaskSerializer,
    ViewOrderSerializer,
    ViewResetSerializer,
    ViewSerializer,
)
from django.conf import settings
from django.db.models import Sum
from django.db.models.functions import Coalesce
from django.utils.decorators import method_decorator
from django_filters.rest_framework import DjangoFilterBackend
from drf_yasg import openapi
from drf_yasg.utils import swagger_auto_schema
from projects.models import Project
from projects.serializers import ProjectSerializer
from rest_framework import generics, viewsets
from rest_framework.decorators import action
from rest_framework.pagination import PageNumberPagination
from rest_framework.response import Response
from rest_framework.views import APIView
from tasks.models import Annotation, Prediction, Task

logger = logging.getLogger(__name__)

_view_request_body = openapi.Schema(
    type=openapi.TYPE_OBJECT,
    properties={
        'data': openapi.Schema(
            type=openapi.TYPE_OBJECT,
            description='Custom view data',
            properties={'filters': filters_schema, 'ordering': ordering_schema},
        ),
        'project': openapi.Schema(type=openapi.TYPE_INTEGER, description='Project ID'),
    },
)


@method_decorator(
    name='list',
    decorator=swagger_auto_schema(
        tags=['Data Manager'],
        x_fern_sdk_group_name='views',
        x_fern_sdk_method_name='list',
        x_fern_audiences=['public'],
        operation_summary='List views',
        operation_description='List all views for a specific project.',
        manual_parameters=[
            openapi.Parameter(
                name='project', type=openapi.TYPE_INTEGER, in_=openapi.IN_QUERY, description='Project ID'
            ),
        ],
    ),
)
@method_decorator(
    name='create',
    decorator=swagger_auto_schema(
        tags=['Data Manager'],
        x_fern_sdk_group_name='views',
        x_fern_sdk_method_name='create',
        x_fern_audiences=['public'],
        operation_summary='Create view',
        operation_description='Create a view for a specific project.',
        request_body=_view_request_body,
        responses={201: ViewSerializer},
    ),
)
@method_decorator(
    name='retrieve',
    decorator=swagger_auto_schema(
        tags=['Data Manager'],
        x_fern_sdk_group_name='views',
        x_fern_sdk_method_name='get',
        x_fern_audiences=['public'],
        operation_summary='Get view details',
        operation_description='Get the details about a specific view in the data manager',
        manual_parameters=[
            openapi.Parameter(name='id', type=openapi.TYPE_STRING, in_=openapi.IN_PATH, description='View ID'),
        ],
    ),
)
@method_decorator(
    name='update',
    decorator=swagger_auto_schema(
        tags=['Data Manager'],
        x_fern_audiences=['internal'],
        operation_summary='Put view',
        operation_description='Overwrite view data with updated filters and other information for a specific project.',
        request_body=_view_request_body,
        manual_parameters=[
            openapi.Parameter(name='id', type=openapi.TYPE_STRING, in_=openapi.IN_PATH, description='View ID'),
        ],
    ),
)
@method_decorator(
    name='partial_update',
    decorator=swagger_auto_schema(
        tags=['Data Manager'],
        x_fern_sdk_group_name='views',
        x_fern_sdk_method_name='update',
        x_fern_audiences=['public'],
        operation_summary='Update view',
        operation_description='Update view data with additional filters and other information for a specific project.',
        manual_parameters=[
            openapi.Parameter(name='id', type=openapi.TYPE_STRING, in_=openapi.IN_PATH, description='View ID'),
        ],
        request_body=_view_request_body,
        responses={200: ViewSerializer},
    ),
)
@method_decorator(
    name='destroy',
    decorator=swagger_auto_schema(
        tags=['Data Manager'],
        x_fern_sdk_group_name='views',
        x_fern_sdk_method_name='delete',
        x_fern_audiences=['public'],
        operation_summary='Delete view',
        operation_description='Delete a specific view by ID.',
        manual_parameters=[
            openapi.Parameter(name='id', type=openapi.TYPE_STRING, in_=openapi.IN_PATH, description='View ID'),
        ],
    ),
)
class ViewAPI(viewsets.ModelViewSet):
    serializer_class = ViewSerializer
    filter_backends = [DjangoFilterBackend]
    filterset_fields = ['project']
    permission_required = ViewClassPermission(
        GET=all_permissions.tasks_view,
        POST=all_permissions.tasks_change,
        PATCH=all_permissions.tasks_change,
        PUT=all_permissions.tasks_change,
        DELETE=all_permissions.tasks_delete,
    )

    def perform_create(self, serializer):
        serializer.save(user=self.request.user)

    @swagger_auto_schema(
        tags=['Data Manager'],
        x_fern_sdk_group_name='views',
        x_fern_sdk_method_name='delete_all',
        x_fern_audiences=['public'],
        operation_summary='Delete all project views',
        operation_description='Delete all views for a specific project',
        request_body=ViewResetSerializer,
    )
    @action(detail=False, methods=['delete'])
    def reset(self, request):
        serializer = ViewResetSerializer(data=request.data)
        serializer.is_valid(raise_exception=True)
        project = generics.get_object_or_404(
            Project.objects.for_user(request.user), pk=serializer.validated_data['project'].id
        )
        queryset = self.filter_queryset(self.get_queryset()).filter(project=project)
        queryset.all().delete()
        return Response(status=204)

    @swagger_auto_schema(
        method='post',
        tags=['Data Manager'],
        operation_summary='Update order of views',
        operation_description='Update the order field of views based on the provided list of view IDs',
        request_body=ViewOrderSerializer,
    )
    @action(detail=False, methods=['post'], url_path='order')
    def update_order(self, request):
        serializer = ViewOrderSerializer(data=request.data)
        serializer.is_valid(raise_exception=True)

        project_id = serializer.validated_data['project']
        view_ids = serializer.validated_data['ids']

        project = generics.get_object_or_404(Project.objects.for_user(request.user), pk=project_id)

        queryset = self.filter_queryset(self.get_queryset()).filter(project=project)
        views = list(queryset.filter(id__in=view_ids))

        # Update the order field for each view
        view_dict = {view.id: view for view in views}
        for order, view_id in enumerate(view_ids):
            if view_id in view_dict:
                view_dict[view_id].order = order

        # Bulk update views
        View.objects.bulk_update(views, ['order'])

        return Response(status=200)

    def get_queryset(self):
        return View.objects.filter(project__organization=self.request.user.active_organization).order_by('order', 'id')


class TaskPagination(PageNumberPagination):
    page_size = 100
    page_size_query_param = 'page_size'
    total_annotations = 0
    total_predictions = 0
    max_page_size = settings.TASK_API_PAGE_SIZE_MAX

    @async_to_sync
    async def async_paginate_queryset(self, queryset, request, view=None):
        predictions_count_qs = Prediction.objects.filter(task_id__in=queryset)
        self.total_predictions = await sync_to_async(predictions_count_qs.count, thread_sensitive=True)()

        annotations_count_qs = Annotation.objects.filter(task_id__in=queryset, was_cancelled=False)
        self.total_annotations = await sync_to_async(annotations_count_qs.count, thread_sensitive=True)()
        return await sync_to_async(super().paginate_queryset, thread_sensitive=True)(queryset, request, view)

    def sync_paginate_queryset(self, queryset, request, view=None):
        self.total_predictions = Prediction.objects.filter(task_id__in=queryset).count()
        self.total_annotations = Annotation.objects.filter(task_id__in=queryset, was_cancelled=False).count()
        return super().paginate_queryset(queryset, request, view)

    def paginate_totals_queryset(self, queryset, request, view=None):
        totals = queryset.values('id').aggregate(
            total_annotations=Coalesce(Sum('total_annotations'), 0),
            total_predictions=Coalesce(Sum('total_predictions'), 0),
        )
        self.total_annotations = totals['total_annotations']
        self.total_predictions = totals['total_predictions']
        return super().paginate_queryset(queryset, request, view)

    def paginate_queryset(self, queryset, request, view=None):
        if flag_set('fflag_fix_back_optic_1407_optimize_tasks_api_pagination_counts'):
            return self.paginate_totals_queryset(queryset, request, view)
        return self.sync_paginate_queryset(queryset, request, view)

    def get_paginated_response(self, data):
        return Response(
            {
                'total_annotations': self.total_annotations,
                'total_predictions': self.total_predictions,
                'total': self.page.paginator.count,
                'tasks': data,
            }
        )


class TaskListAPI(generics.ListCreateAPIView):
    task_serializer_class = DataManagerTaskSerializer
    permission_required = ViewClassPermission(
        GET=all_permissions.tasks_view,
        POST=all_permissions.tasks_change,
        PATCH=all_permissions.tasks_change,
        PUT=all_permissions.tasks_change,
        DELETE=all_permissions.tasks_delete,
    )
    pagination_class = TaskPagination

    @staticmethod
    def get_task_serializer_context(request, project):
        all_fields = request.GET.get('fields', None) == 'all'  # false by default

        return {
            'resolve_uri': bool_from_request(request.GET, 'resolve_uri', True),
            'request': request,
            'project': project,
            'drafts': all_fields,
            'predictions': all_fields,
            'annotations': all_fields,
        }

    def get_task_queryset(self, request, prepare_params):
        return Task.prepared.only_filtered(prepare_params=prepare_params)

    @staticmethod
    def prefetch(queryset):
        return queryset.prefetch_related(
            'annotations',
            'predictions',
            'annotations__completed_by',
            'project',
            'io_storages_azureblobimportstoragelink',
            'io_storages_gcsimportstoragelink',
            'io_storages_localfilesimportstoragelink',
            'io_storages_redisimportstoragelink',
            'io_storages_s3importstoragelink',
            'file_upload',
        )

    def get(self, request):
        # get project
        view_pk = int_from_request(request.GET, 'view', 0) or int_from_request(request.data, 'view', 0)
        project_pk = int_from_request(request.GET, 'project', 0) or int_from_request(request.data, 'project', 0)
        if project_pk:
            project = generics.get_object_or_404(Project, pk=project_pk)
            self.check_object_permissions(request, project)
        elif view_pk:
            view = generics.get_object_or_404(View, pk=view_pk)
            project = view.project
            self.check_object_permissions(request, project)
        else:
            return Response({'detail': 'Neither project nor view id specified'}, status=404)
        # get prepare params (from view or from payload directly)
        prepare_params = get_prepare_params(request, project)
        queryset = self.get_task_queryset(request, prepare_params)
        context = self.get_task_serializer_context(self.request, project)

        # paginated tasks
        page = self.paginate_queryset(queryset)

        # get request params
        all_fields = 'all' if request.GET.get('fields', None) == 'all' else None
        fields_for_evaluation = get_fields_for_evaluation(prepare_params, request.user)
        review = bool_from_request(self.request.GET, 'review', False)

        if review:
            fields_for_evaluation = ['annotators', 'reviewed']
            all_fields = None
        if page is not None:
            ids = [task.id for task in page]  # page is a list already
            tasks = list(
                self.prefetch(
                    Task.prepared.annotate_queryset(
                        Task.objects.filter(id__in=ids),
                        fields_for_evaluation=fields_for_evaluation,
                        all_fields=all_fields,
                        request=request,
                    )
                )
            )
            tasks_by_ids = {task.id: task for task in tasks}
            # keep ids ordering
            page = [tasks_by_ids[_id] for _id in ids]

            # retrieve ML predictions if tasks don't have them
            if not review and project.evaluate_predictions_automatically:
                # TODO MM TODO this needs a discussion, because I'd expect
                # people to retrieve manually instead on DM load, plus it
                # will slow down initial DM load
                # if project.retrieve_predictions_automatically is deprecated now and no longer used
                tasks_for_predictions = Task.objects.filter(id__in=ids, predictions__isnull=True)
                evaluate_predictions(tasks_for_predictions)
                [tasks_by_ids[_id].refresh_from_db() for _id in ids]

            serializer = self.task_serializer_class(page, many=True, context=context)
            return self.get_paginated_response(serializer.data)
        # all tasks
        if project.evaluate_predictions_automatically:
            evaluate_predictions(queryset.filter(predictions__isnull=True))
        queryset = Task.prepared.annotate_queryset(
            queryset, fields_for_evaluation=fields_for_evaluation, all_fields=all_fields, request=request
        )
        serializer = self.task_serializer_class(queryset, many=True, context=context)
        return Response(serializer.data)


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Data Manager'],
        x_fern_audiences=['internal'],
        operation_summary='Get data manager columns',
        operation_description=(
            'Retrieve the data manager columns available for the tasks in a specific project. '
            'For more details, see [GET api/actions](#/Data%20Manager/get_api_actions).'
        ),
        manual_parameters=[
            openapi.Parameter(
                name='project',
                type=openapi.TYPE_INTEGER,
                in_=openapi.IN_QUERY,
                description='Project ID',
                required=True,
            )
        ],
        responses={
            200: openapi.Response(
                description='Columns retrieved successfully',
                examples={
                    'application/json': {
                        'columns': [
                            {
                                'id': 'id',
                                'title': 'ID',
                                'type': 'Number',
                                'help': 'Task ID',
                                'target': 'tasks',
                                'visibility_defaults': {'explore': True, 'labeling': False},
                                'project_defined': False,
                            },
                            {
                                'id': 'completed_at',
                                'title': 'Completed',
                                'type': 'Datetime',
                                'target': 'tasks',
                                'help': 'Last annotation date',
                                'visibility_defaults': {'explore': True, 'labeling': False},
                                'project_defined': False,
                            },
                            # ... other columns ...
                        ]
                    }
                },
            ),
            400: openapi.Response(description='Invalid project ID supplied'),
            404: openapi.Response(description='Project not found'),
        },
    ),
)
class ProjectColumnsAPI(APIView):
    permission_required = all_permissions.projects_view

    def get(self, request):
        pk = int_from_request(request.GET, 'project', 1)
        project = generics.get_object_or_404(Project, pk=pk)
        self.check_object_permissions(request, project)
        GET_ALL_COLUMNS = load_func(settings.DATA_MANAGER_GET_ALL_COLUMNS)
        data = GET_ALL_COLUMNS(project, request.user)
        return Response(data)


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Data Manager'],
        x_fern_audiences=['internal'],
        operation_summary='Get project state',
        operation_description='Retrieve the project state for the data manager.',
    ),
)
class ProjectStateAPI(APIView):
    permission_required = all_permissions.projects_view

    def get(self, request):
        pk = int_from_request(request.GET, 'project', 1)  # replace 1 to None, it's for debug only
        project = generics.get_object_or_404(Project, pk=pk)
        self.check_object_permissions(request, project)
        data = ProjectSerializer(project).data

        data.update(
            {
                'can_delete_tasks': True,
                'can_manage_annotations': True,
                'can_manage_tasks': True,
                'source_syncing': False,
                'target_syncing': False,
                'task_count': project.tasks.count(),
                'annotation_count': Annotation.objects.filter(project=project).count(),
                'config_has_control_tags': len(project.get_parsed_config()) > 0,
            }
        )
        return Response(data)


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Data Manager'],
        x_fern_sdk_group_name='actions',
        x_fern_sdk_method_name='list',
        x_fern_audiences=['public'],
        operation_summary='Get actions',
        operation_description='Retrieve all the registered actions with descriptions that data manager can use.',
    ),
)
@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Data Manager'],
        x_fern_sdk_group_name='actions',
        x_fern_sdk_method_name='create',
        x_fern_audiences=['public'],
        operation_summary='Post actions',
        operation_description=(
            'Perform a Data Manager action with the selected tasks and filters. '
            'Note: More complex actions require additional parameters in the request body. '
            'Call `GET api/actions?project=<id>` to explore them. <br>'
            'Example: `GET api/actions?id=delete_tasks&project=1`'
        ),
        request_body=prepare_params_schema,
        manual_parameters=[
            openapi.Parameter(
                name='id',
                type=openapi.TYPE_STRING,
                in_=openapi.IN_QUERY,
                description='Action name ID, see the full list of actions in the `GET api/actions` request',
                enum=[
                    'retrieve_tasks_predictions',
                    'predictions_to_annotations',
                    'remove_duplicates',
                    'delete_tasks',
                    'delete_ground_truths',
                    'delete_tasks_annotations',
                    'delete_tasks_reviews',
                    'delete_tasks_predictions',
                    'delete_reviewers',
                    'delete_annotators',
                ],
                example='delete_tasks',
                required=True,
            ),
            openapi.Parameter(
                name='project',
                type=openapi.TYPE_INTEGER,
                in_=openapi.IN_QUERY,
                description='Project ID',
                required=True,
            ),
            openapi.Parameter(
                name='view',
                type=openapi.TYPE_INTEGER,
                in_=openapi.IN_QUERY,
                description='View ID (optional, it has higher priority than filters, '
                'selectedItems and ordering from the request body payload)',
            ),
        ],
        responses={200: openapi.Response(description='Action performed successfully')},
    ),
)
class ProjectActionsAPI(APIView):
    permission_required = ViewClassPermission(
        GET=all_permissions.projects_view,
        POST=all_permissions.projects_view,
    )

    def get(self, request):
        pk = int_from_request(request.GET, 'project', 1)  # replace 1 to None, it's for debug only
        project = generics.get_object_or_404(Project, pk=pk)
        self.check_object_permissions(request, project)
        return Response(get_all_actions(request.user, project))

    def post(self, request):
        pk = int_from_request(request.GET, 'project', None)
        project = generics.get_object_or_404(Project, pk=pk)
        self.check_object_permissions(request, project)

        queryset = get_prepared_queryset(request, project)

        # wrong action id
        action_id = request.GET.get('id', None)
        if action_id is None:
            response = {'detail': 'No action id "' + str(action_id) + '", use ?id=<action-id>'}
            return Response(response, status=422)

        # perform action and return the result dict
        kwargs = {'request': request}  # pass advanced params to actions
        result = perform_action(action_id, project, queryset, request.user, **kwargs)
        code = result.pop('response_code', 200)

        return Response(result, status=code)
</file>

<file path="label_studio/data_manager/apps.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
from django.apps import AppConfig


class DataManagerConfig(AppConfig):
    name = 'data_manager'
</file>

<file path="label_studio/data_manager/functions.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import logging
from collections import OrderedDict
from typing import Any, Iterable, Tuple
from urllib.parse import unquote

import ujson as json
from core.utils.common import int_from_request
from data_manager.models import View
from data_manager.prepare_params import PrepareParams
from django.conf import settings
from rest_framework.generics import get_object_or_404
from tasks.models import Task

TASKS = 'tasks:'
logger = logging.getLogger(__name__)


class DataManagerException(Exception):
    pass


def get_all_columns(project, *_):
    """Make columns info for the frontend data manager"""
    result = {'columns': []}

    # frontend uses MST data model, so we need two directional referencing parent <-> child
    task_data_children = []
    i = 0

    data_types = OrderedDict()

    # add data types from config again
    project_data_types = {}
    for key, value in project.data_types.items():
        # skip keys from Repeater tag, because we already have its base data,
        # e.g.: skip 'image[{{idx}}]' because we have 'image' list already
        if '[' not in key:
            project_data_types[key] = value
    data_types.update(project_data_types.items())

    # all data types from import data
    all_data_columns = project.summary.all_data_columns
    logger.info(f'get_all_columns: project_id={project.id} {all_data_columns=} {data_types=}')
    if all_data_columns:
        data_types.update({key: 'Unknown' for key in all_data_columns if key not in data_types})
    logger.info(f'get_all_columns: project_id={project.id} {data_types=}')

    # remove $undefined$ if there is one type at least in labeling config, because it will be resolved automatically
    if len(project_data_types) > 0:
        data_types.pop(settings.DATA_UNDEFINED_NAME, None)
    logger.info(f'get_all_columns: project_id={project.id} {data_types=} {project_data_types=}')

    for key, data_type in list(data_types.items()):  # make data types from labeling config first
        column = {
            'id': key,
            'title': key if key != settings.DATA_UNDEFINED_NAME else 'data',
            'type': data_type if data_type in ['Image', 'Audio', 'AudioPlus', 'Video', 'Unknown'] else 'String',
            'target': 'tasks',
            'parent': 'data',
            'visibility_defaults': {
                'explore': True,
                'labeling': key in project_data_types or key == settings.DATA_UNDEFINED_NAME,
            },
            'project_defined': True,
        }
        result['columns'].append(column)
        task_data_children.append(column['id'])
        i += 1

    # --- Data root ---
    data_root = {
        'id': 'data',
        'title': 'data',
        'type': 'List',
        'target': 'tasks',
        'children': task_data_children,
        'project_defined': False,
    }

    result['columns'] += [
        # --- Tasks ---
        {
            'id': 'id',
            'title': 'ID',
            'type': 'Number',
            'help': 'Task ID',
            'target': 'tasks',
            'visibility_defaults': {'explore': True, 'labeling': False},
            'project_defined': False,
        }
    ]

    result['columns'] += [
        {
            'id': 'inner_id',
            'title': 'Inner ID',
            'type': 'Number',
            'help': 'Internal task ID starting from 1 for the current project',
            'target': 'tasks',
            'visibility_defaults': {'explore': False, 'labeling': False},
            'project_defined': False,
        }
    ]

    project_members = project.all_members.values_list('id', flat=True)

    result['columns'] += [
        {
            'id': 'completed_at',
            'title': 'Completed',
            'type': 'Datetime',
            'target': 'tasks',
            'help': 'Last annotation date',
            'visibility_defaults': {'explore': True, 'labeling': False},
            'project_defined': False,
        },
        {
            'id': 'total_annotations',
            'title': 'Annotations',
            'type': 'Number',
            'target': 'tasks',
            'help': 'Total annotations per task',
            'visibility_defaults': {'explore': True, 'labeling': True},
            'project_defined': False,
        },
        {
            'id': 'cancelled_annotations',
            'title': 'Cancelled',
            'type': 'Number',
            'target': 'tasks',
            'help': 'Total cancelled (skipped) annotations',
            'visibility_defaults': {'explore': True, 'labeling': False},
            'project_defined': False,
        },
        {
            'id': 'total_predictions',
            'title': 'Predictions',
            'type': 'Number',
            'target': 'tasks',
            'help': 'Total predictions per task',
            'visibility_defaults': {'explore': True, 'labeling': False},
            'project_defined': False,
        },
        {
            'id': 'annotators',
            'title': 'Annotated by',
            'type': 'List',
            'target': 'tasks',
            'help': 'All users who completed the task',
            'schema': {'items': project_members},
            'visibility_defaults': {'explore': True, 'labeling': False},
            'project_defined': False,
        },
        {
            'id': 'annotations_results',
            'title': 'Annotation results',
            'type': 'String',
            'target': 'tasks',
            'help': 'Annotation results stacked over all annotations',
            'visibility_defaults': {'explore': False, 'labeling': False},
            'project_defined': False,
        },
        {
            'id': 'annotations_ids',
            'title': 'Annotation IDs',
            'type': 'String',
            'target': 'tasks',
            'help': 'Annotation IDs stacked over all annotations',
            'visibility_defaults': {'explore': False, 'labeling': False},
            'project_defined': False,
        },
        {
            'id': 'predictions_score',
            'title': 'Prediction score',
            'type': 'Number',
            'target': 'tasks',
            'help': 'Average prediction score over all task predictions',
            'visibility_defaults': {'explore': False, 'labeling': False},
            'project_defined': False,
        },
        {
            'id': 'predictions_model_versions',
            'title': 'Prediction model versions',
            'type': 'List',
            'target': 'tasks',
            'help': 'Model versions aggregated over all predictions',
            'schema': {'items': project.get_model_versions()},
            'visibility_defaults': {'explore': False, 'labeling': False},
            'project_defined': False,
        },
        {
            'id': 'predictions_results',
            'title': 'Prediction results',
            'type': 'String',
            'target': 'tasks',
            'help': 'Prediction results stacked over all predictions',
            'visibility_defaults': {'explore': False, 'labeling': False},
            'project_defined': False,
        },
        {
            'id': 'file_upload',
            'title': 'Upload filename',
            'type': 'String',
            'target': 'tasks',
            'help': 'Filename of uploaded file',
            'visibility_defaults': {'explore': False, 'labeling': False},
            'project_defined': False,
        },
        {
            'id': 'storage_filename',
            'title': 'Storage filename',
            'type': 'String',
            'target': 'tasks',
            'help': 'Filename from import storage',
            'visibility_defaults': {'explore': False, 'labeling': False},
            'project_defined': False,
        },
        {
            'id': 'created_at',
            'title': 'Created at',
            'type': 'Datetime',
            'target': 'tasks',
            'help': 'Task creation time',
            'visibility_defaults': {'explore': False, 'labeling': False},
            'project_defined': False,
        },
        {
            'id': 'updated_at',
            'title': 'Updated at',
            'type': 'Datetime',
            'target': 'tasks',
            'help': 'Task update time',
            'visibility_defaults': {'explore': False, 'labeling': False},
            'project_defined': False,
        },
        {
            'id': 'updated_by',
            'title': 'Updated by',
            'type': 'List',
            'target': 'tasks',
            'help': 'User who did the last task update',
            'schema': {'items': project_members},
            'visibility_defaults': {'explore': False, 'labeling': False},
            'project_defined': False,
        },
        {
            'id': 'avg_lead_time',
            'title': 'Lead Time',
            'type': 'Number',
            'help': 'Average lead time over all annotations (seconds)',
            'target': 'tasks',
            'visibility_defaults': {'explore': False, 'labeling': False},
            'project_defined': False,
        },
        {
            'id': 'draft_exists',
            'title': 'Drafts',
            'type': 'Boolean',
            'help': 'True if at least one draft exists for the task',
            'target': 'tasks',
            'visibility_defaults': {'explore': False, 'labeling': False},
            'project_defined': False,
        },
    ]

    result['columns'].append(data_root)

    return result


def get_prepare_params(request, project):
    """This function extract prepare_params from
    * view_id if it's inside of request data
    * selectedItems, filters, ordering if they are in request and there is no view id
    """
    # use filters and selected items from view
    view_id = int_from_request(request.GET, 'view', 0) or int_from_request(request.data, 'view', 0)
    if view_id > 0:
        view = get_object_or_404(View, pk=view_id)
        if view.project.pk != project.pk:
            raise DataManagerException('Project and View mismatch')
        prepare_params = view.get_prepare_tasks_params(add_selected_items=True)
        prepare_params.request = request

    # use filters and selected items from request if it's specified
    else:
        # query arguments from url
        if 'query' in request.GET:
            data = json.loads(unquote(request.GET['query']))
        # data payload from body
        else:
            data = request.data

        selected = data.get('selectedItems', {'all': True, 'excluded': []})
        if not isinstance(selected, dict):
            if isinstance(selected, str):
                # try to parse JSON string
                try:
                    selected = json.loads(selected)
                except Exception as e:
                    logger.error(f'Error parsing selectedItems: {e}')
                    raise DataManagerException(
                        'selectedItems must be JSON encoded string for dict: {"all": [true|false], '
                        '"excluded | included": [...task_ids...]}. '
                        f'Found: {selected}'
                    )
            else:
                raise DataManagerException(
                    'selectedItems must be dict: {"all": [true|false], '
                    '"excluded | included": [...task_ids...]}. '
                    f'Found type: {type(selected)} with value: {selected}'
                )
        filters = data.get('filters', None)
        ordering = data.get('ordering', [])
        prepare_params = PrepareParams(
            project=project.id, selectedItems=selected, data=data, filters=filters, ordering=ordering, request=request
        )
    return prepare_params


def get_prepared_queryset(request, project):
    prepare_params = get_prepare_params(request, project)
    queryset = Task.prepared.only_filtered(prepare_params=prepare_params)
    return queryset


def evaluate_predictions(tasks):
    """
    Call the given ML backend to retrieve predictions with the task queryset as an input.
    If backend is not specified, we'll assume the tasks' project only has one associated
    ML backend, and use that backend.
    """
    if not tasks:
        return

    project = tasks[0].project

    backend = project.ml_backend

    if backend:
        return backend.predict_tasks(tasks=tasks)


def filters_ordering_selected_items_exist(data):
    return data.get('filters') or data.get('ordering') or data.get('selectedItems')


def custom_filter_expressions(*args, **kwargs):
    pass


def preprocess_filter(_filter, *_):
    return _filter


def preprocess_field_name(raw_field_name, project) -> Tuple[str, bool]:
    """Transform a field name (as specified in the datamanager views endpoint) to
    a django ORM field name. Also handle dotted accesses to task.data.

    Edit with care; it's critical that this function not be changed in ways that
    introduce vulnerabilities in the vein of the ORM Leak (see #5012). In particular
    it is not advisable to use `replace` or other calls that replace all instances
    of a string within this function.

    Returns: Django ORM field name: str, Sort is ascending: bool
    """

    field_name = raw_field_name
    ascending = True

    # Descending marker `-` may come at the beginning of the string
    if field_name.startswith('-'):
        ascending = False
        field_name = field_name[1:]

    # For security reasons, these must only be removed when they fall at the beginning of the string (or after `-`).
    optional_prefixes = ['filter:', 'tasks:']
    for prefix in optional_prefixes:
        if field_name.startswith(prefix):
            field_name = field_name[len(prefix) :]

    # Descending marker may also come after other prefixes. Double negative is not allowed.
    if ascending and field_name.startswith('-'):
        ascending = False
        field_name = field_name[1:]

    if field_name.startswith('data.'):
        # process as $undefined$ only if real_name is from labeling config, not from task.data
        real_name = field_name.replace('data.', '')
        common_data_columns = project.summary.common_data_columns
        real_name_suitable = (
            # there is only one object tag in labeling config
            # and requested filter name == value from object tag
            len(project.data_types.keys()) == 1
            and real_name in project.data_types.keys()
            # file was uploaded before labeling config is set, `data.data` is system predefined name
            or len(project.data_types.keys()) == 0
            and real_name == 'data'
        )
        if (
            real_name_suitable
            # common data columns are not None
            and common_data_columns
            # $undefined$ is in common data columns, in all tasks
            and settings.DATA_UNDEFINED_NAME in common_data_columns
        ):
            field_name = f'data__{settings.DATA_UNDEFINED_NAME}'
        else:
            field_name = field_name.replace('data.', 'data__')
    return field_name, ascending


def intersperse(items: Iterable, separator: Any) -> list:
    """
    Create a list with a separator between each item in the passed iterable `items`

    for example, intersperse(['one', 'two', 'three'], 0) == ['one', 0, 'two', 0, 'three']
    """

    output = []
    for item in items:
        output.append(item)
        output.append(separator)
    # if there are no items, there will be no last separator to remove
    if output:
        output.pop()
    return output
</file>

<file path="label_studio/data_manager/managers.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import logging
import re
from datetime import datetime
from typing import ClassVar

import ujson as json
from core.feature_flags import flag_set
from core.utils.db import fast_first
from data_manager.prepare_params import ConjunctionEnum
from django.conf import settings
from django.contrib.postgres.aggregates import ArrayAgg
from django.db import models
from django.db.models import (
    Aggregate,
    Avg,
    Case,
    DateTimeField,
    Exists,
    F,
    FloatField,
    OuterRef,
    Q,
    Subquery,
    TextField,
    Value,
    When,
)
from django.db.models.fields.json import KeyTextTransform
from django.db.models.functions import Cast, Coalesce, Concat
from pydantic import BaseModel

from label_studio.core.utils.common import load_func
from label_studio.core.utils.params import cast_bool_from_str

logger = logging.getLogger(__name__)

DATETIME_FORMAT = '%Y-%m-%dT%H:%M:%S.%fZ'


class _Operator(BaseModel):
    EQUAL: ClassVar[str] = 'equal'
    NOT_EQUAL: ClassVar[str] = 'not_equal'
    LESS: ClassVar[str] = 'less'
    GREATER: ClassVar[str] = 'greater'
    LESS_OR_EQUAL: ClassVar[str] = 'less_or_equal'
    GREATER_OR_EQUAL: ClassVar[str] = 'greater_or_equal'
    IN: ClassVar[str] = 'in'
    NOT_IN: ClassVar[str] = 'not_in'
    IN_LIST: ClassVar[str] = 'in_list'
    NOT_IN_LIST: ClassVar[str] = 'not_in_list'
    EMPTY: ClassVar[str] = 'empty'
    CONTAINS: ClassVar[str] = 'contains'
    NOT_CONTAINS: ClassVar[str] = 'not_contains'
    REGEX: ClassVar[str] = 'regex'


Operator = _Operator()

operators = {
    Operator.EQUAL: '',
    Operator.NOT_EQUAL: '',
    Operator.LESS: '__lt',
    Operator.GREATER: '__gt',
    Operator.LESS_OR_EQUAL: '__lte',
    Operator.GREATER_OR_EQUAL: '__gte',
    Operator.IN: '',
    Operator.NOT_IN: '',
    Operator.IN_LIST: '',
    Operator.NOT_IN_LIST: '',
    Operator.EMPTY: '__isnull',
    Operator.CONTAINS: '__icontains',
    Operator.NOT_CONTAINS: '__icontains',
    Operator.REGEX: '__regex',
}


def get_fields_for_filter_ordering(prepare_params):
    result = []
    if prepare_params is None:
        return result

    # collect fields from ordering
    if prepare_params.ordering:
        ordering_field_name = prepare_params.ordering[0].replace('tasks:', '').replace('-', '')
        result.append(ordering_field_name)

    # collect fields from filters
    if prepare_params.filters:
        for _filter in prepare_params.filters.items:
            filter_field_name = _filter.filter.replace('filter:tasks:', '')
            result.append(filter_field_name)
    return result


def get_fields_for_evaluation(prepare_params, user, skip_regular=True):
    """Collecting field names to annotate them

    :param prepare_params: structure with filters and ordering
    :param user: user
    :return: list of field names
    """
    from projects.models import Project
    from tasks.models import Task

    result = []
    result += get_fields_for_filter_ordering(prepare_params)

    # visible fields calculation
    fields = prepare_params.data.get('hiddenColumns', None)
    if fields:
        from label_studio.data_manager.functions import TASKS

        GET_ALL_COLUMNS = load_func(settings.DATA_MANAGER_GET_ALL_COLUMNS)
        all_columns = GET_ALL_COLUMNS(Project.objects.get(id=prepare_params.project), user)
        all_columns = set(
            [TASKS + ('data.' if c.get('parent', None) == 'data' else '') + c['id'] for c in all_columns['columns']]
        )
        hidden = set(fields['explore']) & set(fields['labeling'])
        shown = all_columns - hidden
        shown = {c[len(TASKS) :] for c in shown} - {'data'}  # remove tasks:
        result = set(result) | shown

    # remove duplicates
    result = set(result)

    # we don't need to annotate regular model fields, so we skip them
    if skip_regular:
        skipped_fields = [field.attname for field in Task._meta.fields]
        skipped_fields.append('id')
        result = [f for f in result if f not in skipped_fields]
        result = [f for f in result if not f.startswith('data.')]

    return result


def apply_ordering(queryset, ordering, project, request, view_data=None):
    if ordering:

        preprocess_field_name = load_func(settings.PREPROCESS_FIELD_NAME)
        raw_field_name = ordering[0]
        numeric_ordering = False
        unsigned_field_name = raw_field_name.lstrip('-+')
        if (
            view_data is not None
            and 'columnsDisplayType' in view_data
            and unsigned_field_name in view_data['columnsDisplayType']
            and view_data['columnsDisplayType'][unsigned_field_name] == 'Number'
        ):
            numeric_ordering = True
        field_name, ascending = preprocess_field_name(raw_field_name, project=project)

        if field_name.startswith('data__'):
            # annotate task with data field for float/int/bool ordering support
            json_field = field_name.replace('data__', '')
            numeric_ordering_applied = False
            if numeric_ordering is True:
                queryset = queryset.annotate(
                    ordering_field=Cast(KeyTextTransform(json_field, 'data'), output_field=FloatField())
                )
                # for non numeric values we need fallback to string ordering
                try:
                    queryset.first()
                    numeric_ordering_applied = True
                except Exception as e:
                    logger.warning(f'Failed to apply numeric ordering for field {json_field}: {e}')
            if not numeric_ordering_applied:
                queryset = queryset.annotate(ordering_field=KeyTextTransform(json_field, 'data'))
            f = F('ordering_field').asc(nulls_last=True) if ascending else F('ordering_field').desc(nulls_last=True)

        else:
            f = F(field_name).asc(nulls_last=True) if ascending else F(field_name).desc(nulls_last=True)

        queryset = queryset.order_by(f)
    else:
        queryset = queryset.order_by('id')

    return queryset


def cast_value(_filter):
    # range (is between)
    if hasattr(_filter.value, 'max'):
        if _filter.type == 'Number':
            _filter.value.min = float(_filter.value.min)
            _filter.value.max = float(_filter.value.max)
        elif _filter.type == 'Datetime':
            _filter.value.min = datetime.strptime(_filter.value.min, DATETIME_FORMAT)
            _filter.value.max = datetime.strptime(_filter.value.max, DATETIME_FORMAT)
    # one value
    else:
        if _filter.type == 'Number':
            _filter.value = float(_filter.value)
        elif _filter.type == 'Datetime':
            _filter.value = datetime.strptime(_filter.value, DATETIME_FORMAT)
        elif _filter.type == 'Boolean':
            _filter.value = cast_bool_from_str(_filter.value)


def add_result_filter(field_name, _filter, filter_expressions, project):
    from django.db.models.expressions import RawSQL
    from tasks.models import Annotation, Prediction

    _class = Annotation if field_name == 'annotations_results' else Prediction

    # Annotation
    if field_name == 'annotations_results':
        subquery = Q(
            id__in=Annotation.objects.annotate(json_str=RawSQL('cast(result as text)', ''))
            .filter(Q(project=project) & Q(json_str__contains=_filter.value))
            .filter(task=OuterRef('pk'))
            .values_list('task', flat=True)
        )
    # Predictions: they don't have `project` yet
    else:
        subquery = Exists(
            _class.objects.annotate(json_str=RawSQL('cast(result as text)', '')).filter(
                Q(task=OuterRef('pk')) & Q(json_str__contains=_filter.value)
            )
        )

    if _filter.operator in [Operator.EQUAL, Operator.NOT_EQUAL]:
        try:
            value = json.loads(_filter.value)
        except:  # noqa: E722
            return 'exit'

        q = Exists(_class.objects.filter(Q(task=OuterRef('pk')) & Q(result=value)))
        filter_expressions.append(q if _filter.operator == Operator.EQUAL else ~q)
        return 'continue'
    elif _filter.operator == Operator.CONTAINS:
        filter_expressions.append(Q(subquery))
        return 'continue'
    elif _filter.operator == Operator.NOT_CONTAINS:
        filter_expressions.append(~Q(subquery))
        return 'continue'
    elif _filter.operator == Operator.EMPTY:
        if cast_bool_from_str(_filter.value):
            q = Q(annotations__result__isnull=True) | Q(annotations__result=[])
        else:
            q = Q(annotations__result__isnull=False) & ~Q(annotations__result=[])
        filter_expressions.append(q)
        return 'continue'


def add_user_filter(enabled, key, _filter, filter_expressions):
    if enabled and _filter.operator == Operator.CONTAINS:
        filter_expressions.append(Q(**{key: int(_filter.value)}))
        return 'continue'
    elif enabled and _filter.operator == Operator.NOT_CONTAINS:
        filter_expressions.append(~Q(**{key: int(_filter.value)}))
        return 'continue'
    elif enabled and _filter.operator == Operator.EMPTY:
        value = cast_bool_from_str(_filter.value)
        filter_expressions.append(Q(**{key + '__isnull': value}))
        return 'continue'


def apply_filters(queryset, filters, project, request):
    if not filters:
        return queryset

    # convert conjunction to orm statement
    filter_expressions = []
    custom_filter_expressions = load_func(settings.DATA_MANAGER_CUSTOM_FILTER_EXPRESSIONS)

    for _filter in filters.items:

        # we can also have annotations filters
        if not _filter.filter.startswith('filter:tasks:') or _filter.value is None:
            continue

        # django orm loop expression attached to column name
        preprocess_field_name = load_func(settings.PREPROCESS_FIELD_NAME)
        field_name, _ = preprocess_field_name(_filter.filter, project)

        # filter pre-processing, value type conversion, etc..
        preprocess_filter = load_func(settings.DATA_MANAGER_PREPROCESS_FILTER)
        _filter = preprocess_filter(_filter, field_name)

        # custom expressions for enterprise
        filter_expression = custom_filter_expressions(_filter, field_name, project, request=request)
        if filter_expression:
            filter_expressions.append(filter_expression)
            continue

        # annotators
        result = add_user_filter(field_name == 'annotators', 'annotations__completed_by', _filter, filter_expressions)
        if result == 'continue':
            continue

        # updated_by
        result = add_user_filter(field_name == 'updated_by', 'updated_by', _filter, filter_expressions)
        if result == 'continue':
            continue

        # annotations results & predictions results
        if field_name in ['annotations_results', 'predictions_results']:
            result = add_result_filter(field_name, _filter, filter_expressions, project)
            if result == 'exit':
                return queryset.none()
            elif result == 'continue':
                continue

        # annotation ids
        if field_name == 'annotations_ids':
            field_name = 'annotations__id'
            if 'contains' in _filter.operator:
                # convert string like "1 2,3" => [1,2,3]
                _filter.value = [int(value) for value in re.split(',|;| ', _filter.value) if value and value.isdigit()]
                _filter.operator = 'in_list' if _filter.operator == 'contains' else 'not_in_list'
            elif 'equal' in _filter.operator:
                if not _filter.value.isdigit():
                    _filter.value = 0

        # predictions model versions
        if field_name == 'predictions_model_versions' and _filter.operator == Operator.CONTAINS:
            q = Q()
            for value in _filter.value:
                q |= Q(predictions__model_version__contains=value)
            filter_expressions.append(q)
            continue
        elif field_name == 'predictions_model_versions' and _filter.operator == Operator.NOT_CONTAINS:
            q = Q()
            for value in _filter.value:
                q &= ~Q(predictions__model_version__contains=value)
            filter_expressions.append(q)
            continue
        elif field_name == 'predictions_model_versions' and _filter.operator == Operator.EMPTY:
            value = cast_bool_from_str(_filter.value)
            filter_expressions.append(Q(predictions__model_version__isnull=value))
            continue

        # use other name because of model names conflict
        if field_name == 'file_upload':
            field_name = 'file_upload_field'

        # annotate with cast to number if need
        if _filter.type == 'Number' and field_name.startswith('data__'):
            json_field = field_name.replace('data__', '')
            queryset = queryset.annotate(
                **{
                    f'filter_{json_field.replace("$undefined$", "undefined")}': Cast(
                        KeyTextTransform(json_field, 'data'), output_field=FloatField()
                    )
                }
            )
            clean_field_name = f'filter_{json_field.replace("$undefined$", "undefined")}'
        else:
            clean_field_name = field_name

        # special case: predictions, annotations, cancelled --- for them 0 is equal to is_empty=True
        if (
            clean_field_name in ('total_predictions', 'total_annotations', 'cancelled_annotations')
            and _filter.operator == 'empty'
        ):
            _filter.operator = 'equal' if cast_bool_from_str(_filter.value) else 'not_equal'
            _filter.value = 0

        # get type of annotated field
        value_type = 'str'
        if queryset.exists():
            value_type = type(queryset.values_list(field_name, flat=True)[0]).__name__

        if (value_type == 'list' or value_type == 'tuple') and 'equal' in _filter.operator:
            raise Exception('Not supported filter type')

        # special case: for strings empty is "" or null=True
        if _filter.type in ('String', 'Unknown') and _filter.operator == 'empty':
            value = cast_bool_from_str(_filter.value)
            if value:  # empty = true
                q = Q(Q(**{field_name: None}) | Q(**{field_name + '__isnull': True}))
                if value_type == 'str':
                    q |= Q(**{field_name: ''})
                if value_type == 'list':
                    q = Q(**{field_name: [None]})

            else:  # empty = false
                q = Q(~Q(**{field_name: None}) & ~Q(**{field_name + '__isnull': True}))
                if value_type == 'str':
                    q &= ~Q(**{field_name: ''})
                if value_type == 'list':
                    q = ~Q(**{field_name: [None]})

            filter_expressions.append(q)
            continue

        # regex pattern check
        elif _filter.operator == 'regex':
            try:
                re.compile(pattern=str(_filter.value))
            except Exception as e:
                logger.info('Incorrect regex for filter: %s: %s', _filter.value, str(e))
                return queryset.none()

        # append operator
        field_name = f"{clean_field_name}{operators.get(_filter.operator, '')}"

        # in
        if _filter.operator == 'in':
            cast_value(_filter)
            filter_expressions.append(
                Q(
                    **{
                        f'{field_name}__gte': _filter.value.min,
                        f'{field_name}__lte': _filter.value.max,
                    }
                ),
            )

        # not in
        elif _filter.operator == 'not_in':
            cast_value(_filter)
            filter_expressions.append(
                ~Q(
                    **{
                        f'{field_name}__gte': _filter.value.min,
                        f'{field_name}__lte': _filter.value.max,
                    }
                ),
            )

        # in list
        elif _filter.operator == 'in_list':
            filter_expressions.append(
                Q(**{f'{field_name}__in': _filter.value}),
            )

        # not in list
        elif _filter.operator == 'not_in_list':
            filter_expressions.append(
                ~Q(**{f'{field_name}__in': _filter.value}),
            )

        # empty
        elif _filter.operator == 'empty':
            if cast_bool_from_str(_filter.value):
                filter_expressions.append(Q(**{field_name: True}))
            else:
                filter_expressions.append(~Q(**{field_name: True}))

        # starting from not_
        elif _filter.operator.startswith('not_'):
            cast_value(_filter)
            filter_expressions.append(~Q(**{field_name: _filter.value}))

        # all others
        else:
            cast_value(_filter)
            filter_expressions.append(Q(**{field_name: _filter.value}))

    """WARNING: Stringifying filter_expressions will evaluate the (sub)queryset.
        Do not use a log in the following manner:
        logger.debug(f'Apply filter: {filter_expressions}')
        Even in DEBUG mode, a subqueryset that has OuterRef will raise an error
        if evaluated outside a parent queryset.
    """
    if filters.conjunction == ConjunctionEnum.OR:
        result_filter = Q()
        for filter_expression in filter_expressions:
            result_filter.add(filter_expression, Q.OR)
        queryset = queryset.filter(result_filter)
    else:
        for filter_expression in filter_expressions:
            queryset = queryset.filter(filter_expression)
    return queryset


class TaskQuerySet(models.QuerySet):
    def prepared(self, prepare_params=None):
        """Apply filters, ordering and selected items to queryset

        :param prepare_params: prepare params with project, filters, orderings, etc
        :return: ordered and filtered queryset
        """
        from projects.models import Project

        queryset = self

        if prepare_params is None:
            return queryset

        project = Project.objects.get(pk=prepare_params.project)
        request = prepare_params.request
        queryset = apply_filters(queryset, prepare_params.filters, project, request)
        queryset = apply_ordering(queryset, prepare_params.ordering, project, request, view_data=prepare_params.data)

        if not prepare_params.selectedItems:
            return queryset

        # included selected items
        if prepare_params.selectedItems.all is False and prepare_params.selectedItems.included:
            queryset = queryset.filter(id__in=prepare_params.selectedItems.included)

        # excluded selected items
        elif prepare_params.selectedItems.all is True and prepare_params.selectedItems.excluded:
            queryset = queryset.exclude(id__in=prepare_params.selectedItems.excluded)

        return queryset


class GroupConcat(Aggregate):
    function = 'GROUP_CONCAT'
    template = '%(function)s(%(distinct)s%(expressions)s)'

    def __init__(self, expression, distinct=False, output_field=None, **extra):
        output_field = models.JSONField() if output_field is None else output_field
        super().__init__(expression, distinct='DISTINCT ' if distinct else '', output_field=output_field, **extra)


def newest_annotation_subquery() -> Subquery:
    from tasks.models import Annotation

    newest_annotations = Annotation.objects.filter(task=OuterRef('pk')).order_by('-id')[:1]
    return Subquery(newest_annotations.values('created_at'))


def base_annotate_completed_at(queryset: TaskQuerySet) -> TaskQuerySet:
    return queryset.annotate(completed_at=Case(When(is_labeled=True, then=newest_annotation_subquery())))


def annotate_completed_at(queryset: TaskQuerySet) -> TaskQuerySet:
    LseProject = load_func(settings.LSE_PROJECT)
    get_tasks_agreement_queryset = load_func(settings.GET_TASKS_AGREEMENT_QUERYSET)

    is_lse_project = bool(LseProject)
    has_custom_agreement_queryset = bool(get_tasks_agreement_queryset)

    if (
        is_lse_project
        and has_custom_agreement_queryset
        and flag_set('fflag_feat_optic_161_project_settings_for_low_agreement_threshold_score_short', user='auto')
    ):
        return annotated_completed_at_considering_agreement_threshold(queryset)

    return base_annotate_completed_at(queryset)


def annotated_completed_at_considering_agreement_threshold(queryset):
    LseProject = load_func(settings.LSE_PROJECT)
    get_tasks_agreement_queryset = load_func(settings.GET_TASKS_AGREEMENT_QUERYSET)

    is_lse_project = bool(LseProject)
    has_custom_agreement_queryset = bool(get_tasks_agreement_queryset)

    project_exists = is_lse_project and hasattr(queryset, 'project') and queryset.project is not None

    project_id = queryset.project.id if project_exists else None

    if project_id is None or not is_lse_project or not has_custom_agreement_queryset:
        return base_annotate_completed_at(queryset)

    lse_project = fast_first(
        LseProject.objects.filter(project_id=project_id).values(
            'agreement_threshold', 'max_additional_annotators_assignable'
        )
    )

    agreement_threshold = lse_project['agreement_threshold'] if lse_project else None
    if not lse_project or not agreement_threshold:
        # This project doesn't use task_agreement so don't consider it when determining completed_at
        return base_annotate_completed_at(queryset)

    queryset = get_tasks_agreement_queryset(queryset)
    max_additional_annotators_assignable = lse_project['max_additional_annotators_assignable']

    completed_at_case = Case(
        When(
            # If agreement_threshold is set, evaluate all conditions
            Q(is_labeled=True)
            & (
                Q(_agreement__gte=agreement_threshold)
                | Q(annotation_count__gte=(F('overlap') + max_additional_annotators_assignable))
            ),
            then=newest_annotation_subquery(),
        ),
        default=Value(None),
        output_field=DateTimeField(),
    )

    return queryset.annotate(completed_at=completed_at_case)


def annotate_storage_filename(queryset: TaskQuerySet) -> TaskQuerySet:
    from label_studio.data_manager.functions import intersperse

    storage_key_names = [F(s + '__key') for s in settings.IO_STORAGES_IMPORT_LINK_NAMES]
    return queryset.annotate(
        storage_filename=Concat(*intersperse(storage_key_names, Value(';')), output_field=TextField())
    )


def annotate_annotations_results(queryset):
    if settings.DJANGO_DB == settings.DJANGO_DB_SQLITE:
        return queryset.annotate(
            annotations_results=Coalesce(
                GroupConcat('annotations__result'), Value(''), output_field=models.CharField()
            )
        )
    else:
        return queryset.annotate(annotations_results=ArrayAgg('annotations__result', distinct=True, default=Value([])))


def annotate_predictions_results(queryset):
    if settings.DJANGO_DB == settings.DJANGO_DB_SQLITE:
        return queryset.annotate(
            predictions_results=Coalesce(
                GroupConcat('predictions__result'), Value(''), output_field=models.CharField()
            )
        )
    else:
        return queryset.annotate(predictions_results=ArrayAgg('predictions__result', distinct=True, default=Value([])))


def annotate_annotators(queryset):
    if settings.DJANGO_DB == settings.DJANGO_DB_SQLITE:
        return queryset.annotate(
            annotators=Coalesce(GroupConcat('annotations__completed_by'), Value(''), output_field=models.CharField())
        )
    else:
        return queryset.annotate(annotators=ArrayAgg('annotations__completed_by', distinct=True, default=Value([])))


def annotate_predictions_score(queryset):
    first_task = queryset.first()
    if not first_task:
        return queryset

    # new approach with each ML backend contains it's version
    if flag_set('ff_front_dev_1682_model_version_dropdown_070622_short', first_task.project.organization.created_by):
        model_versions = list(
            first_task.project.ml_backends.filter(project=first_task.project).values_list('model_version', flat=True)
        )
        if len(model_versions) == 0:
            return queryset.annotate(predictions_score=Avg('predictions__score'))

        else:
            return queryset.annotate(
                predictions_score=Avg('predictions__score', filter=Q(predictions__model_version__in=model_versions))
            )
    else:
        model_version = first_task.project.model_version
        if model_version is None:
            return queryset.annotate(predictions_score=Avg('predictions__score'))
        else:
            return queryset.annotate(
                predictions_score=Avg('predictions__score', filter=Q(predictions__model_version=model_version))
            )


def annotate_annotations_ids(queryset):
    if settings.DJANGO_DB == settings.DJANGO_DB_SQLITE:
        return queryset.annotate(annotations_ids=GroupConcat('annotations__id', output_field=models.CharField()))
    else:
        return queryset.annotate(annotations_ids=ArrayAgg('annotations__id', default=Value([])))


def annotate_predictions_model_versions(queryset):
    if settings.DJANGO_DB == settings.DJANGO_DB_SQLITE:
        return queryset.annotate(
            predictions_model_versions=GroupConcat('predictions__model_version', output_field=models.CharField())
        )
    else:
        return queryset.annotate(predictions_model_versions=ArrayAgg('predictions__model_version', default=Value([])))


def annotate_avg_lead_time(queryset):
    return queryset.annotate(avg_lead_time=Avg('annotations__lead_time'))


def annotate_draft_exists(queryset):
    from tasks.models import AnnotationDraft

    return queryset.annotate(draft_exists=Exists(AnnotationDraft.objects.filter(task=OuterRef('pk'))))


def file_upload(queryset):
    return queryset.annotate(file_upload_field=F('file_upload__file'))


def dummy(queryset):
    return queryset


settings.DATA_MANAGER_ANNOTATIONS_MAP = {
    'avg_lead_time': annotate_avg_lead_time,
    'completed_at': annotate_completed_at,
    'annotations_results': annotate_annotations_results,
    'predictions_results': annotate_predictions_results,
    'predictions_model_versions': annotate_predictions_model_versions,
    'predictions_score': annotate_predictions_score,
    'annotators': annotate_annotators,
    'annotations_ids': annotate_annotations_ids,
    'file_upload': file_upload,
    'draft_exists': annotate_draft_exists,
    'storage_filename': annotate_storage_filename,
}


def get_annotations_map():
    return settings.DATA_MANAGER_ANNOTATIONS_MAP


def update_annotation_map(obj):
    settings.DATA_MANAGER_ANNOTATIONS_MAP.update(obj)


class PreparedTaskManager(models.Manager):
    @staticmethod
    def annotate_queryset(queryset, fields_for_evaluation=None, all_fields=False, request=None):
        annotations_map = get_annotations_map()

        if fields_for_evaluation is None:
            fields_for_evaluation = []

        first_task = queryset.first()
        project = None if first_task is None else first_task.project

        # db annotations applied only if we need them in ordering or filters
        for field in annotations_map.keys():
            if field in fields_for_evaluation or all_fields:
                queryset.project = project
                queryset.request = request
                function = annotations_map[field]
                queryset = function(queryset)

        return queryset

    def get_queryset(self, fields_for_evaluation=None, prepare_params=None, all_fields=False):
        """
        :param fields_for_evaluation: list of annotated fields in task
        :param prepare_params: filters, ordering, selected items
        :param all_fields: evaluate all fields for task
        :param request: request for user extraction
        :return: task queryset with annotated fields
        """
        queryset = self.only_filtered(prepare_params=prepare_params)
        return self.annotate_queryset(
            queryset,
            fields_for_evaluation=fields_for_evaluation,
            all_fields=all_fields,
            request=prepare_params.request,
        )

    def only_filtered(self, prepare_params=None):
        request = prepare_params.request
        queryset = TaskQuerySet(self.model).filter(project=prepare_params.project)
        fields_for_filter_ordering = get_fields_for_filter_ordering(prepare_params)
        queryset = self.annotate_queryset(queryset, fields_for_evaluation=fields_for_filter_ordering, request=request)
        return queryset.prepared(prepare_params=prepare_params)


class TaskManager(models.Manager):
    def for_user(self, user):
        return self.filter(project__organization=user.active_organization)
</file>

<file path="label_studio/data_manager/models.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
from data_manager.prepare_params import PrepareParams
from django.conf import settings
from django.db import models
from django.utils.translation import gettext_lazy as _


class ViewBaseModel(models.Model):
    data = models.JSONField(_('data'), default=dict, null=True, help_text='Custom view data')
    ordering = models.JSONField(_('ordering'), default=dict, null=True, help_text='Ordering parameters')
    order = models.IntegerField(
        _('order'),
        default=0,
        null=True,
        help_text='Position of the tab, starting at the left in data manager and increasing as the tabs go left to right',
    )
    selected_items = models.JSONField(_('selected items'), default=dict, null=True, help_text='Selected items')
    filter_group = models.ForeignKey(
        'data_manager.FilterGroup', null=True, on_delete=models.SET_NULL, help_text='Groups of filters'
    )
    user = models.ForeignKey(
        settings.AUTH_USER_MODEL,
        related_name='%(app_label)s_%(class)ss',
        on_delete=models.CASCADE,
        help_text='User who made this view',
        null=True,
    )

    class Meta:
        ordering = ['order']
        indexes = [models.Index(fields=['project', 'order'])]
        abstract = True


class ProjectViewMixin(models.Model):
    project = models.ForeignKey(
        'projects.Project', related_name='views', on_delete=models.CASCADE, help_text='Project ID'
    )

    def has_permission(self, user):
        user.project = self.project  # link for activity log
        if self.project.organization == user.active_organization:
            return True
        return False

    class Meta:
        abstract = True


class View(ViewBaseModel, ProjectViewMixin):
    def get_prepare_tasks_params(self, add_selected_items=False):
        # convert filters to PrepareParams structure
        filters = None
        if self.filter_group:
            items = []
            for f in self.filter_group.filters.all():
                items.append(
                    dict(
                        filter=f.column,
                        operator=f.operator,
                        type=f.type,
                        value=f.value,
                    )
                )
            filters = dict(conjunction=self.filter_group.conjunction, items=items)

        ordering = self.ordering
        if not ordering:
            ordering = []  # default empty json field is dict, but we need list

        selected_items = None
        if add_selected_items and self.selected_items:
            selected_items = self.selected_items

        return PrepareParams(
            project=self.project_id, ordering=ordering, filters=filters, data=self.data, selectedItems=selected_items
        )


class FilterGroup(models.Model):
    conjunction = models.CharField(_('conjunction'), max_length=1024, help_text='Type of conjunction')
    filters = models.ManyToManyField(
        'data_manager.Filter', related_name='filter_groups', help_text='Connected filters'
    )


class Filter(models.Model):
    index = models.IntegerField(_('index'), default=0, help_text='To keep filter order')
    column = models.CharField(_('column'), max_length=1024, help_text='Field name')
    type = models.CharField(_('type'), max_length=1024, help_text='Field type')
    operator = models.CharField(_('operator'), max_length=1024, help_text='Filter operator')
    value = models.JSONField(_('value'), default=dict, null=True, help_text='Filter value')
</file>

<file path="label_studio/data_manager/prepare_params.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
from enum import Enum
from typing import Any, List, Optional, Union

from drf_yasg import openapi
from pydantic import BaseModel, StrictBool, StrictFloat, StrictInt, StrictStr


class FilterIn(BaseModel):
    min: Union[StrictInt, StrictFloat, StrictStr]
    max: Union[StrictInt, StrictFloat, StrictStr]


class Filter(BaseModel):
    filter: str
    operator: str
    type: str
    value: Union[StrictInt, StrictFloat, StrictBool, StrictStr, FilterIn, list]


class ConjunctionEnum(Enum):
    OR = 'or'
    AND = 'and'


class Filters(BaseModel):
    conjunction: ConjunctionEnum
    items: List[Filter]


class SelectedItems(BaseModel):
    all: bool
    included: List[int] = []
    excluded: List[int] = []


class PrepareParams(BaseModel):
    project: int
    ordering: List[str] = []
    selectedItems: Optional[SelectedItems] = None
    filters: Optional[Filters] = None
    data: Optional[dict] = None
    request: Optional[Any] = None


class CustomEnum(Enum):
    def __init__(self, value, description):
        self._value_ = value
        self.description = description

    @classmethod
    def enums(cls):
        return sorted([item.value for item in cls])

    @classmethod
    def descriptions(cls):
        return {item.value: item.description for item in sorted(cls, key=lambda x: x.value)}


class Column(Enum):
    ID = 'id', 'Number', 'Task ID'
    INNER_ID = 'inner_id', 'Number', 'Task Inner ID, it starts from 1 for all projects'
    GROUND_TRUTH = 'ground_truth', 'Boolean', 'Ground truth status of the tasks'
    ANNOTATIONS_RESULTS = 'annotations_results', 'String', 'Annotation results for the tasks'
    REVIEWED = 'reviewed', 'Boolean', 'Whether the tasks have been reviewed (Enterprise only)'
    PREDICTIONS_SCORE = 'predictions_score', 'Number', 'Prediction score for the task'
    PREDICTIONS_MODEL_VERSIONS = 'predictions_model_versions', 'String', 'Model version used for the predictions'
    PREDICTIONS_RESULTS = 'predictions_results', 'String', 'Prediction results for the tasks'
    FILE_UPLOAD = 'file_upload', 'String', 'Name of the file uploaded to create the tasks'
    CREATED_AT = 'created_at', 'Datetime', 'Time the task was created at'
    UPDATED_AT = (
        'updated_at',
        'Datetime',
        'Time the task was updated at (e.g. new annotation was created, review added, etc)',
    )
    ANNOTATORS = (
        'annotators',
        'List',
        'Annotators that completed the task (Community). Can include assigned annotators (Enterprise only). '
        'Important note: the filter `type` should be List, but the filter `value` is integer',
    )
    TOTAL_PREDICTIONS = 'total_predictions', 'Number', 'Total number of predictions for the task'
    CANCELLED_ANNOTATIONS = (
        'cancelled_annotations',
        'Number',
        'Number of cancelled or skipped annotations for the task',
    )
    TOTAL_ANNOTATIONS = 'total_annotations', 'Number', 'Total number of annotations on a task'
    COMPLETED_AT = 'completed_at', 'Datetime', 'Time when a task was fully annotated'
    AGREEMENT = 'agreement', 'Number', 'Agreement for annotation results for a specific task (Enterprise only)'
    REVIEWERS = (
        'reviewers',
        'String',
        'Reviewers that reviewed the task, or assigned reviewers (Enterprise only). '
        'Important note: the filter `type` should be List, but the filter `value` is integer',
    )
    REVIEWS_REJECTED = (
        'reviews_rejected',
        'Number',
        'Number of annotations rejected for a task in review (Enterprise only)',
    )
    REVIEWS_ACCEPTED = (
        'reviews_accepted',
        'Number',
        'Number of annotations accepted for a task in review (Enterprise only)',
    )
    COMMENTS = 'comments', 'Number', 'Number of comments in a task'
    UNRESOLVED_COMMENT_COUNT = 'unresolved_comment_count', 'Number', 'Number of unresolved comments in a task'

    def __init__(self, value, value_type, description):
        self._value_ = value
        self.type = value_type
        self.description = description

    @classmethod
    def enums_for_filters(cls):
        return sorted(['filter:tasks:' + str(item.value) for item in cls])

    @classmethod
    def enums_for_ordering(cls):
        return sorted([('tasks:' + str(item.value)) for item in cls])

    @classmethod
    def descriptions_for_filters(cls):
        return {
            'filter:tasks:' + item.value: f'({item.type}) ' + item.description
            for item in sorted(cls, key=lambda x: x.value)
        }


class Operator(CustomEnum):
    EQUAL = 'equal', 'Equal to'
    NOT_EQUAL = 'not_equal', 'Not equal to'
    GREATER = 'greater', 'Greater than'
    GREATER_OR_EQUAL = 'greater_or_equal', 'Greater than or equal to'
    LESS = 'less', 'Less than'
    LESS_OR_EQUAL = 'less_or_equal', 'Less than or equal to'
    CONTAINS = 'contains', 'Contains'
    NOT_CONTAINS = 'not_contains', 'Does not contain'
    EXISTS = 'exists', 'Exists'
    NOT_EXISTS = 'not_exists', 'Does not exist'
    STARTS_WITH = 'starts_with', 'Starts with'
    ENDS_WITH = 'ends_with', 'Ends with'
    IS_BETWEEN = 'in', 'Is between min and max values, so the filter `value` should be e.g. `{"min": 1, "max": 7}`'
    NOT_BETWEEN = (
        'not_in',
        'Is not between min and max values, so the filter `value` should be e.g. `{"min": 1, "max": 7}`',
    )


class Type(CustomEnum):
    Number = 'Number', 'Float or Integer'
    Datetime = 'Datetime', "Datetime string in `strftime('%Y-%m-%dT%H:%M:%S.%fZ')` format"
    Boolean = 'Boolean', 'Boolean'
    String = 'String', 'String'
    List = 'List', 'List of items'
    Unknown = 'Unknown', 'Unknown is explicitly converted to string format'


# Example request and response
example_request_1 = {
    'filters': {
        'conjunction': 'or',
        'items': [{'filter': 'filter:tasks:id', 'operator': 'greater', 'type': 'Number', 'value': 123}],
    },
    'selectedItems': {'all': True, 'excluded': [124, 125, 126]},
    'ordering': ['tasks:total_annotations'],
}

example_request_2 = {
    'filters': {
        'conjunction': 'or',
        'items': [
            {
                'filter': 'filter:tasks:completed_at',
                'operator': 'in',
                'type': 'Datetime',
                'value': {'min': '2021-01-01T00:00:00.000Z', 'max': '2025-01-01T00:00:00.000Z'},
            }
        ],
    },
    'selectedItems': {'all': False, 'included': [1, 2, 3]},
    'ordering': ['-tasks:completed_at'],
}

# Define the schemas for filters and selectedItems
filters_schema = openapi.Schema(
    type=openapi.TYPE_OBJECT,
    properties={
        'conjunction': openapi.Schema(
            type=openapi.TYPE_STRING,
            enum=['or', 'and'],
            description=(
                'Logical conjunction for the filters. This conjunction (either "or" or "and") '
                'will be applied to all items in the filters list. It is not possible to combine '
                '"or" and "and" within one list of filters. All filters will be either combined with "or" '
                'or with "and", but not a mix of both.'
            ),
        ),
        'items': openapi.Schema(
            type=openapi.TYPE_ARRAY,
            items=openapi.Schema(
                type=openapi.TYPE_OBJECT,
                properties={
                    'filter': openapi.Schema(
                        type=openapi.TYPE_STRING,
                        enum=Column.enums_for_filters(),
                        description=(
                            'Filter identifier, it should start with `filter:tasks:` prefix, '
                            'e.g. `filter:tasks:agreement`. '
                            'For `task.data` fields it may look like `filter:tasks:data.field_name`. '
                            'If you need more info about columns, check the '
                            '[Get data manager columns](#tag/Data-Manager/operation/api_dm_columns_list) API endpoint. '
                            'Possible values:<br>'
                            + '<br>'.join(
                                [
                                    f'<li>`{key}`<br> {desc}</li>'
                                    for key, desc in Column.descriptions_for_filters().items()
                                ]
                            )
                        ),
                    ),
                    'operator': openapi.Schema(
                        type=openapi.TYPE_STRING,
                        enum=Operator.enums(),
                        description=(
                            'Filter operator. Possible values:<br>'
                            + '<br>'.join(
                                [f'<li>`{key}`<br> {desc}</li>' for key, desc in Operator.descriptions().items()]
                            )
                        ),
                    ),
                    'type': openapi.Schema(
                        type=openapi.TYPE_STRING,
                        description='Type of the filter value. Possible values:<br>'
                        + '<br>'.join([f'<li>`{key}`<br> {desc}</li>' for key, desc in Type.descriptions().items()]),
                    ),
                    'value': openapi.Schema(
                        type=openapi.TYPE_OBJECT,
                        oneOf=[
                            openapi.Schema(type=openapi.TYPE_STRING, title='String', description='String'),
                            openapi.Schema(type=openapi.TYPE_INTEGER, title='Integer', description='Integer'),
                            openapi.Schema(
                                type=openapi.TYPE_NUMBER, title='Float', format='float', description='Float'
                            ),
                            openapi.Schema(type=openapi.TYPE_BOOLEAN, title='Boolean', description='Boolean'),
                            openapi.Schema(
                                type=openapi.TYPE_OBJECT,
                                title='Dictionary',
                                description='Dictionary is used for some operator types, e.g. `in` and `not_in`',
                            ),
                            openapi.Schema(
                                type=openapi.TYPE_OBJECT,
                                title='List',
                                description='List of strings or integers',
                            ),
                        ],
                        description='Value to filter by',
                    ),
                },
                required=['filter', 'operator', 'type', 'value'],
                example=example_request_1['filters']['items'][0],
            ),
            description='List of filter items',
        ),
    },
    required=['conjunction', 'items'],
    description=(
        'Filters to apply on tasks. '
        'You can use [the helper class `Filters` from this page](https://labelstud.io/sdk/data_manager.html) '
        'to create Data Manager Filters.<br>'
        'Example: `{"conjunction": "or", "items": [{"filter": "filter:tasks:completed_at", "operator": "greater", '
        '"type": "Datetime", "value": "2021-01-01T00:00:00.000Z"}]}`'
    ),
)

selected_items_schema = openapi.Schema(
    type=openapi.TYPE_OBJECT,
    required=['all'],
    description='Task selection by IDs. If filters are applied, the selection will be applied to the filtered tasks.'
    'If "all" is `false`, `"included"` must be used. If "all" is `true`, `"excluded"` must be used.<br>'
    'Examples: `{"all": false, "included": [1, 2, 3]}` or `{"all": true, "excluded": [4, 5]}`',
    oneOf=[
        openapi.Schema(
            title='all: false',
            type=openapi.TYPE_OBJECT,
            properties={
                'all': openapi.Schema(type=openapi.TYPE_BOOLEAN, enum=[False], description='No tasks are selected'),
                'included': openapi.Schema(
                    type=openapi.TYPE_ARRAY,
                    items=openapi.Schema(type=openapi.TYPE_INTEGER),
                    description='List of included task IDs',
                ),
            },
            required=['all'],
        ),
        openapi.Schema(
            title='all: true',
            type=openapi.TYPE_OBJECT,
            properties={
                'all': openapi.Schema(type=openapi.TYPE_BOOLEAN, enum=[True], description='All tasks are selected'),
                'excluded': openapi.Schema(
                    type=openapi.TYPE_ARRAY,
                    items=openapi.Schema(type=openapi.TYPE_INTEGER),
                    description='List of excluded task IDs',
                ),
            },
            required=['all'],
        ),
    ],
)

# Define ordering schema
ordering_schema = openapi.Schema(
    type=openapi.TYPE_ARRAY,
    items=openapi.Schema(
        type=openapi.TYPE_STRING,
        enum=Column.enums_for_ordering(),
    ),
    description='List of fields to order by. Fields are similar to filters but without the `filter:` prefix. '
    'To reverse the order, add a minus sign before the field name, e.g. `-tasks:created_at`.',
)

# Define the main schema for the data payload
data_schema = openapi.Schema(
    type=openapi.TYPE_OBJECT,
    properties={'filters': filters_schema, 'selectedItems': selected_items_schema, 'ordering': ordering_schema},
    description='Additional query to filter and order tasks',
)

prepare_params_schema = openapi.Schema(
    type=openapi.TYPE_OBJECT,
    properties={'filters': filters_schema, 'selectedItems': selected_items_schema, 'ordering': ordering_schema},
    description='Data payload containing task filters, selected task items, and ordering',
    example=example_request_1,
)
</file>

<file path="label_studio/data_manager/serializers.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import os

import ujson as json
from data_manager.models import Filter, FilterGroup, View
from django.conf import settings
from django.db import transaction
from drf_yasg import openapi
from projects.models import Project
from rest_framework import serializers
from tasks.models import Task
from tasks.serializers import (
    AnnotationDraftSerializer,
    AnnotationSerializer,
    PredictionSerializer,
    TaskSerializer,
)
from users.models import User

from label_studio.core.utils.common import round_floats


class FilterSerializer(serializers.ModelSerializer):
    class Meta:
        model = Filter
        fields = '__all__'

    def validate_column(self, column: str) -> str:
        """
        Ensure that the passed filter expression starts with 'filter:tasks:' and contains
        no foreign key traversals. This means either the filter expression contains no '__'
        substrings, or that it's the task.data json field that's accessed.

        Users depending on foreign key traversals in views can allowlist them via the
        DATA_MANAGER_FILTER_ALLOWLIST setting in the env.

        Edit with care. The validations below are critical for security.
        """

        column_copy = column

        # We may support 'filter:annotations:' in the future, but we don't as of yet.
        required_prefix = 'filter:tasks:'
        optional_prefix = '-'

        if not column_copy.startswith(required_prefix):
            raise serializers.ValidationError(f'Filter "{column}" should start with "{required_prefix}"')

        column_copy = column_copy[len(required_prefix) :]

        if column_copy.startswith(optional_prefix):
            column_copy = column_copy[len(optional_prefix) :]

        if column_copy.startswith('data.'):
            # Allow underscores if the filter is based on the `task.data` JSONField, because these don't leverage foreign keys.
            return column

        # Specific filters relying on foreign keys can be allowlisted
        if column_copy in settings.DATA_MANAGER_FILTER_ALLOWLIST:
            return column

        # But in general, we don't allow foreign keys
        if '__' in column_copy:
            raise serializers.ValidationError(
                f'"__" is not generally allowed in filters. Consider asking your administrator to add "{column_copy}" '
                'to DATA_MANAGER_FILTER_ALLOWLIST, but note that some filter expressions may pose a security risk'
            )

        return column


class FilterGroupSerializer(serializers.ModelSerializer):
    filters = FilterSerializer(many=True)

    class Meta:
        model = FilterGroup
        fields = '__all__'


class ViewSerializer(serializers.ModelSerializer):
    filter_group = FilterGroupSerializer(required=False)

    class Meta:
        model = View
        fields = '__all__'

    def to_internal_value(self, data):
        """
        map old filters structure to models
        "filters": {  ===> FilterGroup model
            "conjunction": "or",
            "items":[  ===> "filters" in FilterGroup
                 {  ==> Filter model
                   "filter":"filter:tasks:data.image", ==> column
                    "operator":"contains",
                    "type":"Image",
                    "value": <string: "XXX" | int: 123 | dict | list>
                 },
                  {
                    "filter":"filter:tasks:data.image",
                    "operator":"equal",
                    "type":"Image",
                    "value": <string: "XXX" | int: 123 | dict | list>
                 }
              ]
           }
        }
        """
        _data = data.get('data', {})

        filters = _data.pop('filters', {})
        conjunction = filters.get('conjunction')
        if 'filter_group' not in data and conjunction:
            data['filter_group'] = {'conjunction': conjunction, 'filters': []}
            if 'items' in filters:
                for f in filters['items']:
                    data['filter_group']['filters'].append(
                        {
                            'column': f.get('filter', ''),
                            'operator': f.get('operator', ''),
                            'type': f.get('type', ''),
                            'value': f.get('value', {}),
                        }
                    )

        ordering = _data.pop('ordering', {})
        data['ordering'] = ordering

        return super().to_internal_value(data)

    def to_representation(self, instance):
        result = super().to_representation(instance)
        filters = result.pop('filter_group', {})
        if filters:
            filters['items'] = []
            filters.pop('filters', [])
            filters.pop('id', None)

            for f in instance.filter_group.filters.order_by('index'):
                filters['items'].append(
                    {
                        'filter': f.column,
                        'operator': f.operator,
                        'type': f.type,
                        'value': f.value,
                    }
                )
            result['data']['filters'] = filters

        selected_items = result.pop('selected_items', {})
        if selected_items:
            result['data']['selectedItems'] = selected_items

        ordering = result.pop('ordering', {})
        if ordering:
            result['data']['ordering'] = ordering
        return result

    @staticmethod
    def _create_filters(filter_group, filters_data):
        filter_index = 0
        for filter_data in filters_data:
            filter_data['index'] = filter_index
            filter_group.filters.add(Filter.objects.create(**filter_data))
            filter_index += 1

    def create(self, validated_data):
        with transaction.atomic():
            filter_group_data = validated_data.pop('filter_group', None)
            if filter_group_data:
                filters_data = filter_group_data.pop('filters', [])
                filter_group = FilterGroup.objects.create(**filter_group_data)

                self._create_filters(filter_group=filter_group, filters_data=filters_data)

                validated_data['filter_group_id'] = filter_group.id
            view = self.Meta.model.objects.create(**validated_data)

            return view

    def update(self, instance, validated_data):
        with transaction.atomic():
            filter_group_data = validated_data.pop('filter_group', None)
            if filter_group_data:
                filters_data = filter_group_data.pop('filters', [])

                filter_group = instance.filter_group
                if filter_group is None:
                    filter_group = FilterGroup.objects.create(**filter_group_data)

                conjunction = filter_group_data.get('conjunction')
                if conjunction and filter_group.conjunction != conjunction:
                    filter_group.conjunction = conjunction
                    filter_group.save()

                filter_group.filters.clear()
                self._create_filters(filter_group=filter_group, filters_data=filters_data)

            ordering = validated_data.pop('ordering', None)
            if ordering and ordering != instance.ordering:
                instance.ordering = ordering
                instance.save()

            if validated_data['data'] != instance.data:
                instance.data = validated_data['data']
                instance.save()

            return instance


class UpdatedByDMFieldSerializer(serializers.SerializerMethodField):
    # TODO: get_updated_by implementation is weird, but we need to adhere schema to it
    class Meta:
        swagger_schema_fields = {
            'type': openapi.TYPE_ARRAY,
            'title': 'User IDs',
            'description': 'User IDs who updated this task',
            'items': {'type': openapi.TYPE_OBJECT, 'title': 'User IDs'},
        }


class AnnotatorsDMFieldSerializer(serializers.SerializerMethodField):
    # TODO: get_updated_by implementation is weird, but we need to adhere schema to it
    class Meta:
        swagger_schema_fields = {
            'type': openapi.TYPE_ARRAY,
            'title': 'Annotators IDs',
            'description': 'Annotators IDs who annotated this task',
            'items': {'type': openapi.TYPE_INTEGER, 'title': 'User IDs'},
        }


class CompletedByDMSerializerWithGenericSchema(serializers.PrimaryKeyRelatedField):
    # TODO: likely we need to remove full user details from GET /api/tasks/{id} as it non-secure and currently controlled by the export toggle
    class Meta:
        swagger_schema_fields = {
            'type': openapi.TYPE_OBJECT,
            'title': 'User details',
            'description': 'User details who completed this annotation.',
        }


class AnnotationsDMFieldSerializer(AnnotationSerializer):
    completed_by = CompletedByDMSerializerWithGenericSchema(required=False, queryset=User.objects.all())


class AnnotationDraftDMFieldSerializer(serializers.SerializerMethodField):
    class Meta:
        swagger_schema_fields = {
            'type': openapi.TYPE_ARRAY,
            'title': 'Annotation drafts',
            'description': 'Drafts for this task',
            'items': {
                'type': openapi.TYPE_OBJECT,
                'title': 'Draft object',
                'properties': {
                    'result': {
                        'type': openapi.TYPE_ARRAY,
                        'title': 'Draft result',
                        'items': {
                            'type': openapi.TYPE_OBJECT,
                            'title': 'Draft result item',
                        },
                    },
                    'created_at': {
                        'type': openapi.TYPE_STRING,
                        'format': 'date-time',
                        'title': 'Creation time',
                    },
                    'updated_at': {
                        'type': openapi.TYPE_STRING,
                        'format': 'date-time',
                        'title': 'Last update time',
                    },
                },
            },
        }


class PredictionsDMFieldSerializer(serializers.SerializerMethodField):
    class Meta:
        swagger_schema_fields = {
            'type': openapi.TYPE_ARRAY,
            'title': 'Predictions',
            'description': 'Predictions for this task',
            'items': {
                'type': openapi.TYPE_OBJECT,
                'title': 'Prediction object',
                'properties': {
                    'result': {
                        'type': openapi.TYPE_ARRAY,
                        'title': 'Prediction result',
                        'items': {
                            'type': openapi.TYPE_OBJECT,
                            'title': 'Prediction result item',
                        },
                    },
                    'score': {
                        'type': openapi.TYPE_NUMBER,
                        'title': 'Prediction score',
                    },
                    'model_version': {
                        'type': openapi.TYPE_STRING,
                        'title': 'Model version',
                    },
                    'model': {
                        'type': openapi.TYPE_OBJECT,
                        'title': 'ML Backend instance',
                    },
                    'model_run': {
                        'type': openapi.TYPE_OBJECT,
                        'title': 'Model Run instance',
                    },
                    'task': {
                        'type': openapi.TYPE_INTEGER,
                        'title': 'Task ID related to the prediction',
                    },
                    'project': {
                        'type': openapi.TYPE_NUMBER,
                        'title': 'Project ID related to the prediction',
                    },
                    'created_at': {
                        'type': openapi.TYPE_STRING,
                        'format': 'date-time',
                        'title': 'Creation time',
                    },
                    'updated_at': {
                        'type': openapi.TYPE_STRING,
                        'format': 'date-time',
                        'title': 'Last update time',
                    },
                },
            },
        }


class DataManagerTaskSerializer(TaskSerializer):
    predictions = PredictionsDMFieldSerializer(required=False, read_only=True)
    annotations = AnnotationsDMFieldSerializer(required=False, many=True, default=[], read_only=True)
    drafts = AnnotationDraftDMFieldSerializer(required=False, read_only=True)
    annotators = AnnotatorsDMFieldSerializer(required=False, read_only=True)

    inner_id = serializers.IntegerField(required=False)
    cancelled_annotations = serializers.IntegerField(required=False)
    total_annotations = serializers.IntegerField(required=False)
    total_predictions = serializers.IntegerField(required=False)
    completed_at = serializers.DateTimeField(required=False)
    annotations_results = serializers.SerializerMethodField(required=False)
    predictions_results = serializers.SerializerMethodField(required=False)
    predictions_score = serializers.FloatField(required=False)
    file_upload = serializers.SerializerMethodField(required=False)
    storage_filename = serializers.SerializerMethodField(required=False)
    annotations_ids = serializers.SerializerMethodField(required=False)
    predictions_model_versions = serializers.SerializerMethodField(required=False)
    avg_lead_time = serializers.FloatField(required=False)
    draft_exists = serializers.BooleanField(required=False)
    updated_by = UpdatedByDMFieldSerializer(required=False, read_only=True)

    CHAR_LIMITS = 500

    class Meta:
        model = Task
        ref_name = 'data_manager_task_serializer'
        fields = '__all__'
        expandable_fields = {'annotations': (AnnotationSerializer, {'many': True})}

    def to_representation(self, obj):
        """Dynamically manage including of some fields in the API result"""
        ret = super(DataManagerTaskSerializer, self).to_representation(obj)
        if not self.context.get('annotations'):
            ret.pop('annotations', None)
        if not self.context.get('predictions'):
            ret.pop('predictions', None)
        return ret

    def _pretty_results(self, task, field, unique=False):
        if not hasattr(task, field) or getattr(task, field) is None:
            return ''

        result = getattr(task, field)
        if isinstance(result, str):
            output = result
            if unique:
                output = list(set(output.split(',')))
                output = ','.join(output)

        elif isinstance(result, int):
            output = str(result)
        else:
            result = [r for r in result if r is not None]
            if unique:
                result = list(set(result))
            result = round_floats(result)
            output = json.dumps(result, ensure_ascii=False)[1:-1]  # remove brackets [ ]

        return output[: self.CHAR_LIMITS].replace(',"', ', "').replace('],[', '] [').replace('"', '')

    def get_annotations_results(self, task):
        return self._pretty_results(task, 'annotations_results')

    def get_predictions_results(self, task):
        return self._pretty_results(task, 'predictions_results')

    def get_predictions(self, task):
        return PredictionSerializer(task.predictions, many=True, default=[], read_only=True).data

    @staticmethod
    def get_file_upload(task):
        if hasattr(task, 'file_upload_field'):
            file_upload = task.file_upload_field
            return os.path.basename(task.file_upload_field) if file_upload else None
        return None

    @staticmethod
    def get_storage_filename(task):
        return task.get_storage_filename()

    @staticmethod
    def get_updated_by(obj):
        return [{'user_id': obj.updated_by_id}] if obj.updated_by_id else []

    @staticmethod
    def get_annotators(obj):
        if not hasattr(obj, 'annotators'):
            return []

        annotators = obj.annotators
        if not annotators:
            return []
        if isinstance(annotators, str):
            annotators = [int(v) for v in annotators.split(',')]

        annotators = list(set(annotators))
        annotators = [a for a in annotators if a is not None]
        return annotators if hasattr(obj, 'annotators') and annotators else []

    def get_annotations_ids(self, task):
        return self._pretty_results(task, 'annotations_ids', unique=True)

    def get_predictions_model_versions(self, task):
        return self._pretty_results(task, 'predictions_model_versions', unique=True)

    def get_drafts_serializer(self):
        return AnnotationDraftSerializer

    def get_drafts_queryset(self, user, drafts):
        """Get all user's draft"""
        return drafts.filter(user=user)

    def get_drafts(self, task):
        """Return drafts only for the current user"""
        # it's for swagger documentation
        if not isinstance(task, Task) or not self.context.get('drafts'):
            return []

        drafts = task.drafts
        if 'request' in self.context and hasattr(self.context['request'], 'user'):
            user = self.context['request'].user
            drafts = self.get_drafts_queryset(user, drafts)

        serializer_class = self.get_drafts_serializer()
        return serializer_class(drafts, many=True, read_only=True, default=True, context=self.context).data


class SelectedItemsSerializer(serializers.Serializer):
    all = serializers.BooleanField()
    included = serializers.ListField(child=serializers.IntegerField(), required=False)
    excluded = serializers.ListField(child=serializers.IntegerField(), required=False)

    def validate(self, data):
        if data['all'] is True and data.get('included'):
            raise serializers.ValidationError('included not allowed with all==true')
        if data['all'] is False and data.get('excluded'):
            raise serializers.ValidationError('excluded not allowed with all==false')

        view = self.context.get('view')
        request = self.context.get('request')
        if view and request and request.method in ('PATCH', 'DELETE'):
            all_value = view.selected_items.get('all')
            if all_value and all_value != data['all']:
                raise serializers.ValidationError('changing all value possible only with POST method')

        return data


class ViewResetSerializer(serializers.Serializer):
    project = serializers.PrimaryKeyRelatedField(queryset=Project.objects.all())


class ViewOrderSerializer(serializers.Serializer):
    project = serializers.IntegerField()
    ids = serializers.ListField(
        child=serializers.IntegerField(), allow_empty=False, help_text='A list of view IDs in the desired order.'
    )
</file>

<file path="label_studio/data_manager/urls.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
from data_manager import api, views
from django.urls import include, path
from rest_framework.routers import DefaultRouter

app_name = 'data_manager'
router = DefaultRouter()
router.register(r'views', api.ViewAPI, basename='view')

urlpatterns = [
    path('api/dm/', include((router.urls, app_name), namespace='api')),
    path('api/dm/columns/', api.ProjectColumnsAPI.as_view(), name='dm-columns'),
    path('api/dm/project/', api.ProjectStateAPI.as_view(), name='dm-project'),
    path('api/dm/actions/', api.ProjectActionsAPI.as_view(), name='dm-actions'),
    # path("api/dm/tasks/", api.TaskListAPI.as_view()),
    # path("api/dm/tasks/<int:pk>", api.TaskAPI.as_view()),
    path('projects/<int:pk>/', views.task_page, name='project-data'),
    path('projects/<int:pk>/data/', views.task_page, name='project-data'),
    path('projects/<int:pk>/data/import', views.task_page, name='project-import'),
    path('projects/<int:pk>/data/export', views.task_page, name='project-export'),
]
</file>

<file path="label_studio/data_manager/views.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
from core.version import get_short_version
from django.contrib.auth.decorators import login_required
from django.shortcuts import render


@login_required
def task_page(request, pk):
    response = {'version': get_short_version()}
    return render(request, 'base.html', response)
</file>

<file path="label_studio/io_storages/azure_blob/__init__.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
</file>

<file path="label_studio/io_storages/azure_blob/api.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
from django.utils.decorators import method_decorator
from drf_yasg import openapi
from drf_yasg.utils import no_body, swagger_auto_schema
from io_storages.api import (
    ExportStorageDetailAPI,
    ExportStorageFormLayoutAPI,
    ExportStorageListAPI,
    ExportStorageSyncAPI,
    ExportStorageValidateAPI,
    ImportStorageDetailAPI,
    ImportStorageFormLayoutAPI,
    ImportStorageListAPI,
    ImportStorageSyncAPI,
    ImportStorageValidateAPI,
)
from io_storages.azure_blob.models import AzureBlobExportStorage, AzureBlobImportStorage
from io_storages.azure_blob.serializers import AzureBlobExportStorageSerializer, AzureBlobImportStorageSerializer

from .openapi_schema import (
    _azure_blob_export_storage_schema,
    _azure_blob_export_storage_schema_with_id,
    _azure_blob_import_storage_schema,
    _azure_blob_import_storage_schema_with_id,
)


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Storage: Azure'],
        x_fern_sdk_group_name=['import_storage', 'azure'],
        x_fern_sdk_method_name='list',
        x_fern_audiences=['public'],
        operation_summary='Get all import storage',
        operation_description='Get list of all Azure import storage connections.',
        manual_parameters=[
            openapi.Parameter(
                name='project',
                type=openapi.TYPE_INTEGER,
                in_=openapi.IN_QUERY,
                description='Project ID',
            ),
        ],
        request_body=no_body,
    ),
)
@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Storage: Azure'],
        x_fern_sdk_group_name=['import_storage', 'azure'],
        x_fern_sdk_method_name='create',
        x_fern_audiences=['public'],
        operation_summary='Create new storage',
        operation_description='Create new Azure import storage',
        request_body=_azure_blob_import_storage_schema,
    ),
)
class AzureBlobImportStorageListAPI(ImportStorageListAPI):
    queryset = AzureBlobImportStorage.objects.all()
    serializer_class = AzureBlobImportStorageSerializer


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Storage: Azure'],
        x_fern_sdk_group_name=['import_storage', 'azure'],
        x_fern_sdk_method_name='get',
        x_fern_audiences=['public'],
        operation_summary='Get import storage',
        operation_description='Get a specific Azure import storage connection.',
        request_body=no_body,
    ),
)
@method_decorator(
    name='patch',
    decorator=swagger_auto_schema(
        tags=['Storage: Azure'],
        x_fern_sdk_group_name=['import_storage', 'azure'],
        x_fern_sdk_method_name='update',
        x_fern_audiences=['public'],
        operation_summary='Update import storage',
        operation_description='Update a specific Azure import storage connection.',
        request_body=_azure_blob_import_storage_schema,
    ),
)
@method_decorator(
    name='delete',
    decorator=swagger_auto_schema(
        tags=['Storage: Azure'],
        x_fern_sdk_group_name=['import_storage', 'azure'],
        x_fern_sdk_method_name='delete',
        x_fern_audiences=['public'],
        operation_summary='Delete import storage',
        operation_description='Delete a specific Azure import storage connection.',
        request_body=no_body,
    ),
)
class AzureBlobImportStorageDetailAPI(ImportStorageDetailAPI):
    queryset = AzureBlobImportStorage.objects.all()
    serializer_class = AzureBlobImportStorageSerializer


@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Storage: Azure'],
        x_fern_sdk_group_name=['import_storage', 'azure'],
        x_fern_sdk_method_name='sync',
        x_fern_audiences=['public'],
        operation_summary='Sync import storage',
        operation_description='Sync tasks from an Azure import storage connection.',
        manual_parameters=[
            openapi.Parameter(
                name='id',
                type=openapi.TYPE_INTEGER,
                in_=openapi.IN_PATH,
                description='Storage ID',
            ),
        ],
        request_body=no_body,
    ),
)
class AzureBlobImportStorageSyncAPI(ImportStorageSyncAPI):
    serializer_class = AzureBlobImportStorageSerializer


@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Storage: Azure'],
        x_fern_sdk_group_name=['export_storage', 'azure'],
        x_fern_sdk_method_name='sync',
        x_fern_audiences=['public'],
        operation_summary='Sync export storage',
        operation_description='Sync tasks from an Azure export storage connection.',
        request_body=no_body,
    ),
)
class AzureBlobExportStorageSyncAPI(ExportStorageSyncAPI):
    serializer_class = AzureBlobExportStorageSerializer


@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Storage: Azure'],
        x_fern_sdk_group_name=['import_storage', 'azure'],
        x_fern_sdk_method_name='validate',
        x_fern_audiences=['public'],
        operation_summary='Validate import storage',
        operation_description='Validate a specific Azure import storage connection.',
        request_body=_azure_blob_import_storage_schema_with_id,
        # expecting empty response
        responses={200: openapi.Response(description='OK')},
    ),
)
class AzureBlobImportStorageValidateAPI(ImportStorageValidateAPI):
    serializer_class = AzureBlobImportStorageSerializer


@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Storage: Azure'],
        x_fern_sdk_group_name=['export_storage', 'azure'],
        x_fern_sdk_method_name='validate',
        x_fern_audiences=['public'],
        operation_summary='Validate export storage',
        operation_description='Validate a specific Azure export storage connection.',
        request_body=_azure_blob_export_storage_schema_with_id,
        # expecting empty response
        responses={200: openapi.Response(description='OK')},
    ),
)
class AzureBlobExportStorageValidateAPI(ExportStorageValidateAPI):
    serializer_class = AzureBlobExportStorageSerializer


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Storage: Azure'],
        x_fern_sdk_group_name=['export_storage', 'azure'],
        x_fern_sdk_method_name='list',
        x_fern_audiences=['public'],
        operation_summary='Get all export storage',
        operation_description='Get a list of all Azure export storage connections.',
        manual_parameters=[
            openapi.Parameter(
                name='project',
                type=openapi.TYPE_INTEGER,
                in_=openapi.IN_QUERY,
                description='Project ID',
            ),
        ],
        request_body=no_body,
    ),
)
@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Storage: Azure'],
        x_fern_sdk_group_name=['export_storage', 'azure'],
        x_fern_sdk_method_name='create',
        x_fern_audiences=['public'],
        operation_summary='Create export storage',
        operation_description='Create a new Azure export storage connection to store annotations.',
        request_body=_azure_blob_export_storage_schema,
    ),
)
class AzureBlobExportStorageListAPI(ExportStorageListAPI):
    queryset = AzureBlobExportStorage.objects.all()
    serializer_class = AzureBlobExportStorageSerializer


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Storage: Azure'],
        x_fern_sdk_group_name=['export_storage', 'azure'],
        x_fern_sdk_method_name='get',
        x_fern_audiences=['public'],
        operation_summary='Get export storage',
        operation_description='Get a specific Azure export storage connection.',
        request_body=no_body,
    ),
)
@method_decorator(
    name='patch',
    decorator=swagger_auto_schema(
        tags=['Storage: Azure'],
        x_fern_sdk_group_name=['export_storage', 'azure'],
        x_fern_sdk_method_name='update',
        x_fern_audiences=['public'],
        operation_summary='Update export storage',
        operation_description='Update a specific Azure export storage connection.',
        request_body=_azure_blob_export_storage_schema,
    ),
)
@method_decorator(
    name='delete',
    decorator=swagger_auto_schema(
        tags=['Storage: Azure'],
        x_fern_sdk_group_name=['export_storage', 'azure'],
        x_fern_sdk_method_name='delete',
        x_fern_audiences=['public'],
        operation_summary='Delete export storage',
        operation_description='Delete a specific Azure export storage connection.',
        request_body=no_body,
    ),
)
class AzureBlobExportStorageDetailAPI(ExportStorageDetailAPI):
    queryset = AzureBlobExportStorage.objects.all()
    serializer_class = AzureBlobExportStorageSerializer


class AzureBlobImportStorageFormLayoutAPI(ImportStorageFormLayoutAPI):
    pass


class AzureBlobExportStorageFormLayoutAPI(ExportStorageFormLayoutAPI):
    pass
</file>

<file path="label_studio/io_storages/azure_blob/models.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import json
import logging
import re
from datetime import timedelta
from typing import Union
from urllib.parse import urlparse

from azure.core.exceptions import ResourceNotFoundError
from azure.storage.blob import BlobSasPermissions, BlobServiceClient, generate_blob_sas
from core.redis import start_job_async_or_sync
from core.utils.params import get_env
from django.conf import settings
from django.db import models
from django.db.models.signals import post_save
from django.dispatch import receiver
from django.utils import timezone
from django.utils.translation import gettext_lazy as _
from io_storages.base_models import (
    ExportStorage,
    ExportStorageLink,
    ImportStorage,
    ImportStorageLink,
    ProjectStorageMixin,
)
from io_storages.utils import storage_can_resolve_bucket_url
from tasks.models import Annotation

from label_studio.io_storages.azure_blob.utils import AZURE

logger = logging.getLogger(__name__)
logging.getLogger('azure.core.pipeline.policies.http_logging_policy').setLevel(logging.WARNING)


class AzureBlobStorageMixin(models.Model):
    container = models.TextField(_('container'), null=True, blank=True, help_text='Azure blob container')
    prefix = models.TextField(_('prefix'), null=True, blank=True, help_text='Azure blob prefix name')
    regex_filter = models.TextField(
        _('regex_filter'), null=True, blank=True, help_text='Cloud storage regex for filtering objects'
    )
    use_blob_urls = models.BooleanField(
        _('use_blob_urls'), default=False, help_text='Interpret objects as BLOBs and generate URLs'
    )
    account_name = models.TextField(_('account_name'), null=True, blank=True, help_text='Azure Blob account name')
    account_key = models.TextField(_('account_key'), null=True, blank=True, help_text='Azure Blob account key')

    def get_account_name(self):
        return str(self.account_name) if self.account_name else get_env('AZURE_BLOB_ACCOUNT_NAME')

    def get_account_key(self):
        return str(self.account_key) if self.account_key else get_env('AZURE_BLOB_ACCOUNT_KEY')

    def get_client_and_container(self):
        account_name = self.get_account_name()
        account_key = self.get_account_key()
        if not account_name or not account_key:
            raise ValueError(
                'Azure account name and key must be set using '
                'environment variables AZURE_BLOB_ACCOUNT_NAME and AZURE_BLOB_ACCOUNT_KEY '
                'or account_name and account_key fields.'
            )
        connection_string = (
            'DefaultEndpointsProtocol=https;AccountName='
            + account_name
            + ';AccountKey='
            + account_key
            + ';EndpointSuffix=core.windows.net'
        )
        client = BlobServiceClient.from_connection_string(conn_str=connection_string)
        container = client.get_container_client(str(self.container))
        return client, container

    def get_container(self):
        _, container = self.get_client_and_container()
        return container

    def validate_connection(self, **kwargs):
        logger.debug('Validating Azure Blob Storage connection')
        client, container = self.get_client_and_container()

        try:
            container_properties = container.get_container_properties()
            logger.debug(f'Container exists: {container_properties.name}')
        except ResourceNotFoundError:
            raise KeyError(f'Container not found: {self.container}')

        # Check path existence for Import storages only
        if self.prefix and 'Export' not in self.__class__.__name__:
            logger.debug(f'Test connection to container {self.container} with prefix {self.prefix}')
            prefix = str(self.prefix)
            try:
                blob = next(container.list_blob_names(name_starts_with=prefix))
            except StopIteration:
                blob = None

            if not blob:
                raise KeyError(f'{self.url_scheme}://{self.container}/{self.prefix} not found.')


class AzureBlobImportStorageBase(AzureBlobStorageMixin, ImportStorage):
    url_scheme = 'azure-blob'

    presign = models.BooleanField(_('presign'), default=True, help_text='Generate presigned URLs')
    presign_ttl = models.PositiveSmallIntegerField(
        _('presign_ttl'), default=1, help_text='Presigned URLs TTL (in minutes)'
    )

    def iterkeys(self):
        container = self.get_container()
        prefix = str(self.prefix) if self.prefix else ''
        files = container.list_blobs(name_starts_with=prefix)
        regex = re.compile(str(self.regex_filter)) if self.regex_filter else None

        for file in files:
            # skip folder
            if file.name == (prefix.rstrip('/') + '/'):
                continue
            # check regex pattern filter
            if regex and not regex.match(file.name):
                logger.debug(file.name + ' is skipped by regex filter')
                continue
            yield file.name

    def get_data(self, key):
        if self.use_blob_urls:
            data_key = settings.DATA_UNDEFINED_NAME
            return {data_key: f'{self.url_scheme}://{self.container}/{key}'}

        container = self.get_container()
        blob = container.download_blob(key)
        blob_str = blob.content_as_text()
        value = json.loads(blob_str)
        if not isinstance(value, dict):
            raise ValueError(
                f'Error on key {key}: For {self.__class__.__name__} your JSON file must be a dictionary with one task'
            )
        return value

    def scan_and_create_links(self):
        return self._scan_and_create_links(AzureBlobImportStorageLink)

    def generate_http_url(self, url):
        r = urlparse(url, allow_fragments=False)
        container = r.netloc
        blob = r.path.lstrip('/')

        expiry = timezone.now() + timedelta(minutes=self.presign_ttl)

        sas_token = generate_blob_sas(
            account_name=self.get_account_name(),
            container_name=container,
            blob_name=blob,
            account_key=self.get_account_key(),
            permission=BlobSasPermissions(read=True),
            expiry=expiry,
        )
        return (
            'https://' + self.get_account_name() + '.blob.core.windows.net/' + container + '/' + blob + '?' + sas_token
        )

    def can_resolve_url(self, url: Union[str, None]) -> bool:
        return storage_can_resolve_bucket_url(self, url)

    def get_blob_metadata(self, key):
        return AZURE.get_blob_metadata(
            key, self.container, account_name=self.account_name, account_key=self.account_key
        )

    class Meta:
        abstract = True


class AzureBlobImportStorage(ProjectStorageMixin, AzureBlobImportStorageBase):
    class Meta:
        abstract = False


class AzureBlobExportStorage(AzureBlobStorageMixin, ExportStorage):  # note: order is important!
    def save_annotation(self, annotation):
        container = self.get_container()
        logger.debug(f'Creating new object on {self.__class__.__name__} Storage {self} for annotation {annotation}')
        ser_annotation = self._get_serialized_data(annotation)
        # get key that identifies this object in storage
        key = AzureBlobExportStorageLink.get_key(annotation)
        key = str(self.prefix) + '/' + key if self.prefix else key

        # put object into storage
        blob = container.get_blob_client(key)
        blob.upload_blob(json.dumps(ser_annotation), overwrite=True)

        # create link if everything ok
        AzureBlobExportStorageLink.create(annotation, self)


def async_export_annotation_to_azure_storages(annotation):
    project = annotation.project
    if hasattr(project, 'io_storages_azureblobexportstorages'):
        for storage in project.io_storages_azureblobexportstorages.all():
            logger.debug(f'Export {annotation} to Azure Blob storage {storage}')
            storage.save_annotation(annotation)


@receiver(post_save, sender=Annotation)
def export_annotation_to_azure_storages(sender, instance, **kwargs):
    storages = getattr(instance.project, 'io_storages_azureblobexportstorages', None)
    if storages and storages.exists():  # avoid excess jobs in rq
        start_job_async_or_sync(async_export_annotation_to_azure_storages, instance)


class AzureBlobImportStorageLink(ImportStorageLink):
    storage = models.ForeignKey(AzureBlobImportStorage, on_delete=models.CASCADE, related_name='links')


class AzureBlobExportStorageLink(ExportStorageLink):
    storage = models.ForeignKey(AzureBlobExportStorage, on_delete=models.CASCADE, related_name='links')
</file>

<file path="label_studio/io_storages/azure_blob/openapi_schema.py">
from drf_yasg import openapi

_common_storage_schema_properties = {
    'title': openapi.Schema(type=openapi.TYPE_STRING, description='Storage title'),
    'description': openapi.Schema(type=openapi.TYPE_STRING, description='Storage description'),
    'project': openapi.Schema(type=openapi.TYPE_INTEGER, description='Project ID'),
    'container': openapi.Schema(type=openapi.TYPE_STRING, description='Azure blob container'),
    'prefix': openapi.Schema(type=openapi.TYPE_STRING, description='Azure blob prefix name'),
    'account_name': openapi.Schema(type=openapi.TYPE_STRING, description='Azure Blob account name'),
    'account_key': openapi.Schema(type=openapi.TYPE_STRING, description='Azure Blob account key'),
}


_azure_blob_import_storage_schema = openapi.Schema(
    type=openapi.TYPE_OBJECT,
    properties=dict(
        regex_filter=openapi.Schema(
            type=openapi.TYPE_STRING,
            description='Cloud storage regex for filtering objects. '
            'You must specify it otherwise no objects will be imported.',
        ),
        use_blob_urls=openapi.Schema(
            type=openapi.TYPE_BOOLEAN,
            description='Interpret objects as BLOBs and generate URLs. For example, if your bucket contains images, '
            'you can use this option to generate URLs for these images. '
            'If set to False, it will read the content of the file and load it into Label Studio.',
            default=False,
        ),
        presign=openapi.Schema(
            type=openapi.TYPE_BOOLEAN, description='Presign URLs for direct download', default=True
        ),
        presign_ttl=openapi.Schema(type=openapi.TYPE_INTEGER, description='Presign TTL in minutes', default=1),
        **_common_storage_schema_properties,
    ),
    required=[],
)

_azure_blob_import_storage_schema_with_id = openapi.Schema(
    type=openapi.TYPE_OBJECT,
    properties=dict(
        id=openapi.Schema(
            type=openapi.TYPE_INTEGER, description='Storage ID. If set, storage with specified ID will be updated'
        ),
        **_azure_blob_import_storage_schema.properties,
    ),
    required=[],
)

_azure_blob_export_storage_schema = openapi.Schema(
    type=openapi.TYPE_OBJECT,
    properties=dict(
        can_delete_objects=openapi.Schema(
            type=openapi.TYPE_BOOLEAN, description='Deletion from storage enabled', default=False
        ),
        **_common_storage_schema_properties,
    ),
    required=[],
)

_azure_blob_export_storage_schema_with_id = openapi.Schema(
    type=openapi.TYPE_OBJECT,
    properties=dict(
        id=openapi.Schema(
            type=openapi.TYPE_INTEGER, description='Storage ID. If set, storage with specified ID will be updated'
        ),
        **_azure_blob_export_storage_schema.properties,
    ),
    required=[],
)
</file>

<file path="label_studio/io_storages/azure_blob/serializers.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""

from io_storages.azure_blob.models import AzureBlobExportStorage, AzureBlobImportStorage
from io_storages.serializers import ExportStorageSerializer, ImportStorageSerializer
from rest_framework import serializers
from rest_framework.exceptions import ValidationError


class AzureBlobImportStorageSerializer(ImportStorageSerializer):
    type = serializers.ReadOnlyField(default='azure')
    presign = serializers.BooleanField(required=False, default=True)
    secure_fields = ['account_name', 'account_key']

    class Meta:
        model = AzureBlobImportStorage
        fields = '__all__'

    def to_representation(self, instance):
        result = super().to_representation(instance)
        for attr in AzureBlobImportStorageSerializer.secure_fields:
            result.pop(attr)
        return result

    def validate(self, data):
        data = super(AzureBlobImportStorageSerializer, self).validate(data)
        storage = self.instance
        if storage:
            for key, value in data.items():
                setattr(storage, key, value)
        else:
            if 'id' in self.initial_data:
                storage_object = self.Meta.model.objects.get(id=self.initial_data['id'])
                for attr in AzureBlobImportStorageSerializer.secure_fields:
                    data[attr] = data.get(attr) or getattr(storage_object, attr)
            storage = self.Meta.model(**data)
        try:
            storage.validate_connection()
        except Exception as exc:
            raise ValidationError(exc)
        return data


class AzureBlobExportStorageSerializer(ExportStorageSerializer):
    type = serializers.ReadOnlyField(default='azure')

    def to_representation(self, instance):
        result = super().to_representation(instance)
        result.pop('account_name')
        result.pop('account_key')
        return result

    class Meta:
        model = AzureBlobExportStorage
        fields = '__all__'
</file>

<file path="label_studio/io_storages/azure_blob/utils.py">
import fnmatch
import logging
import re

from azure.storage.blob import BlobServiceClient
from core.utils.params import get_env
from django.conf import settings

logger = logging.getLogger(__name__)


class AZURE(object):
    @classmethod
    def get_client_and_container(cls, container, account_name=None, account_key=None):
        # get account name and key from params or from environment variables
        account_name = str(account_name) if account_name else get_env('AZURE_BLOB_ACCOUNT_NAME')
        account_key = str(account_key) if account_key else get_env('AZURE_BLOB_ACCOUNT_KEY')
        # check that both account name and key are set
        if not account_name or not account_key:
            raise ValueError(
                'Azure account name and key must be set using '
                'environment variables AZURE_BLOB_ACCOUNT_NAME and AZURE_BLOB_ACCOUNT_KEY'
            )
        connection_string = (
            'DefaultEndpointsProtocol=https;AccountName='
            + account_name
            + ';AccountKey='
            + account_key
            + ';EndpointSuffix=core.windows.net'
        )
        client = BlobServiceClient.from_connection_string(conn_str=connection_string)
        container = client.get_container_client(str(container))
        return client, container

    @classmethod
    def get_blob_metadata(cls, url: str, container: str, account_name: str = None, account_key: str = None) -> dict:
        """
        Get blob metadata by url
        :param url: Object key
        :param container: Azure container name
        :param account_name: Azure account name
        :param account_key: Azure account key
        :return: Object metadata dict("name": "value")
        """
        _, container = cls.get_client_and_container(container, account_name=account_name, account_key=account_key)
        blob = container.get_blob_client(url)
        return dict(blob.get_blob_properties())

    @classmethod
    def validate_pattern(cls, storage, pattern, glob_pattern=True):
        """
        Validate pattern against Azure Blob Storage
        :param storage: AzureBlobStorage instance
        :param pattern: Pattern to validate
        :param glob_pattern: If True, pattern is a glob pattern, otherwise it is a regex pattern
        :return: Message if pattern is not valid, empty string otherwise
        """
        logger.debug('Validating Azure Blob Storage pattern.')
        client, container = storage.get_client_and_container()
        if storage.prefix:
            generator = container.list_blob_names(
                name_starts_with=storage.prefix,
                results_per_page=settings.CLOUD_STORAGE_CHECK_FOR_RECORDS_PAGE_SIZE,
                timeout=settings.CLOUD_STORAGE_CHECK_FOR_RECORDS_TIMEOUT,
            )
        else:
            generator = container.list_blob_names(
                results_per_page=settings.CLOUD_STORAGE_CHECK_FOR_RECORDS_PAGE_SIZE,
                timeout=settings.CLOUD_STORAGE_CHECK_FOR_RECORDS_TIMEOUT,
            )
        # compile pattern to regex
        if glob_pattern:
            pattern = fnmatch.translate(pattern)
        regex = re.compile(str(pattern))
        # match pattern against all keys in the container
        for index, key in enumerate(generator):
            # skip directories
            if key.endswith('/'):
                logger.debug(key + ' is skipped because it is a folder')
                continue
            if regex and regex.match(key):
                logger.debug(key + ' matches file pattern')
                return ''
        return 'No objects found matching the provided glob pattern'
</file>

<file path="label_studio/io_storages/gcs/__init__.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
</file>

<file path="label_studio/io_storages/gcs/api.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
from django.utils.decorators import method_decorator
from drf_yasg import openapi
from drf_yasg.utils import no_body, swagger_auto_schema
from io_storages.api import (
    ExportStorageDetailAPI,
    ExportStorageFormLayoutAPI,
    ExportStorageListAPI,
    ExportStorageSyncAPI,
    ExportStorageValidateAPI,
    ImportStorageDetailAPI,
    ImportStorageFormLayoutAPI,
    ImportStorageListAPI,
    ImportStorageSyncAPI,
    ImportStorageValidateAPI,
)
from io_storages.gcs.models import GCSExportStorage, GCSImportStorage
from io_storages.gcs.serializers import GCSExportStorageSerializer, GCSImportStorageSerializer

from .openapi_schema import (
    _gcs_export_storage_schema,
    _gcs_export_storage_schema_with_id,
    _gcs_import_storage_schema,
    _gcs_import_storage_schema_with_id,
)


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Storage: GCS'],
        x_fern_sdk_group_name=['import_storage', 'gcs'],
        x_fern_sdk_method_name='list',
        x_fern_audiences=['public'],
        operation_summary='Get all import storage',
        operation_description='Get a list of all GCS import storage connections.',
        manual_parameters=[
            openapi.Parameter(
                name='project',
                type=openapi.TYPE_INTEGER,
                in_=openapi.IN_QUERY,
                description='Project ID',
            ),
        ],
        request_body=no_body,
    ),
)
@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Storage: GCS'],
        x_fern_sdk_group_name=['import_storage', 'gcs'],
        x_fern_sdk_method_name='create',
        x_fern_audiences=['public'],
        operation_summary='Create import storage',
        operation_description='Create a new GCS import storage connection.',
        request_body=_gcs_import_storage_schema,
    ),
)
class GCSImportStorageListAPI(ImportStorageListAPI):
    queryset = GCSImportStorage.objects.all()
    serializer_class = GCSImportStorageSerializer


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Storage: GCS'],
        x_fern_sdk_group_name=['import_storage', 'gcs'],
        x_fern_sdk_method_name='get',
        x_fern_audiences=['public'],
        operation_summary='Get import storage',
        operation_description='Get a specific GCS import storage connection.',
        request_body=no_body,
    ),
)
@method_decorator(
    name='patch',
    decorator=swagger_auto_schema(
        tags=['Storage: GCS'],
        x_fern_sdk_group_name=['import_storage', 'gcs'],
        x_fern_sdk_method_name='update',
        x_fern_audiences=['public'],
        operation_summary='Update import storage',
        operation_description='Update a specific GCS import storage connection.',
        request_body=_gcs_import_storage_schema,
    ),
)
@method_decorator(
    name='delete',
    decorator=swagger_auto_schema(
        tags=['Storage: GCS'],
        x_fern_sdk_group_name=['import_storage', 'gcs'],
        x_fern_sdk_method_name='delete',
        x_fern_audiences=['public'],
        operation_summary='Delete import storage',
        operation_description='Delete a specific GCS import storage connection.',
        request_body=no_body,
    ),
)
class GCSImportStorageDetailAPI(ImportStorageDetailAPI):
    queryset = GCSImportStorage.objects.all()
    serializer_class = GCSImportStorageSerializer


@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Storage: GCS'],
        x_fern_sdk_group_name=['import_storage', 'gcs'],
        x_fern_sdk_method_name='sync',
        x_fern_audiences=['public'],
        operation_summary='Sync import storage',
        operation_description='Sync tasks from an GCS import storage connection.',
        manual_parameters=[
            openapi.Parameter(
                name='id',
                type=openapi.TYPE_INTEGER,
                in_=openapi.IN_PATH,
                description='Storage ID',
            ),
        ],
        request_body=no_body,
    ),
)
class GCSImportStorageSyncAPI(ImportStorageSyncAPI):
    serializer_class = GCSImportStorageSerializer


@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Storage: GCS'],
        x_fern_sdk_group_name=['export_storage', 'gcs'],
        x_fern_sdk_method_name='sync',
        x_fern_audiences=['public'],
        operation_summary='Sync export storage',
        operation_description='Sync tasks from an GCS export storage connection.',
        request_body=no_body,
    ),
)
class GCSExportStorageSyncAPI(ExportStorageSyncAPI):
    serializer_class = GCSExportStorageSerializer


@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Storage: GCS'],
        x_fern_sdk_group_name=['import_storage', 'gcs'],
        x_fern_sdk_method_name='validate',
        x_fern_audiences=['public'],
        operation_summary='Validate import storage',
        operation_description='Validate a specific GCS import storage connection.',
        request_body=_gcs_import_storage_schema_with_id,
        # expecting empty response
        responses={200: openapi.Response(description='OK')},
    ),
)
class GCSImportStorageValidateAPI(ImportStorageValidateAPI):
    serializer_class = GCSImportStorageSerializer


@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Storage: GCS'],
        x_fern_sdk_group_name=['export_storage', 'gcs'],
        x_fern_sdk_method_name='validate',
        x_fern_audiences=['public'],
        operation_summary='Validate export storage',
        operation_description='Validate a specific GCS export storage connection.',
        request_body=_gcs_export_storage_schema_with_id,
        # expecting empty response
        responses={200: openapi.Response(description='OK')},
    ),
)
class GCSExportStorageValidateAPI(ExportStorageValidateAPI):
    serializer_class = GCSExportStorageSerializer


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Storage: GCS'],
        x_fern_sdk_group_name=['export_storage', 'gcs'],
        x_fern_sdk_method_name='list',
        x_fern_audiences=['public'],
        operation_summary='Get all export storage',
        operation_description='Get a list of all GCS export storage connections.',
        manual_parameters=[
            openapi.Parameter(
                name='project',
                type=openapi.TYPE_INTEGER,
                in_=openapi.IN_QUERY,
                description='Project ID',
            ),
        ],
        request_body=no_body,
    ),
)
@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Storage: GCS'],
        x_fern_sdk_group_name=['export_storage', 'gcs'],
        x_fern_sdk_method_name='create',
        x_fern_audiences=['public'],
        operation_summary='Create export storage',
        operation_description='Create a new GCS export storage connection to store annotations.',
        request_body=_gcs_export_storage_schema,
    ),
)
class GCSExportStorageListAPI(ExportStorageListAPI):
    queryset = GCSExportStorage.objects.all()
    serializer_class = GCSExportStorageSerializer


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Storage: GCS'],
        x_fern_sdk_group_name=['export_storage', 'gcs'],
        x_fern_sdk_method_name='get',
        x_fern_audiences=['public'],
        operation_summary='Get export storage',
        operation_description='Get a specific GCS export storage connection.',
        request_body=no_body,
    ),
)
@method_decorator(
    name='patch',
    decorator=swagger_auto_schema(
        tags=['Storage: GCS'],
        x_fern_sdk_group_name=['export_storage', 'gcs'],
        x_fern_sdk_method_name='update',
        x_fern_audiences=['public'],
        operation_summary='Update export storage',
        operation_description='Update a specific GCS export storage connection.',
        request_body=_gcs_export_storage_schema,
    ),
)
@method_decorator(
    name='delete',
    decorator=swagger_auto_schema(
        tags=['Storage: GCS'],
        x_fern_sdk_group_name=['export_storage', 'gcs'],
        x_fern_sdk_method_name='delete',
        x_fern_audiences=['public'],
        operation_summary='Delete export storage',
        operation_description='Delete a specific GCS export storage connection.',
        request_body=no_body,
    ),
)
class GCSExportStorageDetailAPI(ExportStorageDetailAPI):
    queryset = GCSExportStorage.objects.all()
    serializer_class = GCSExportStorageSerializer


class GCSImportStorageFormLayoutAPI(ImportStorageFormLayoutAPI):
    pass


class GCSExportStorageFormLayoutAPI(ExportStorageFormLayoutAPI):
    pass
</file>

<file path="label_studio/io_storages/gcs/models.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import json
import logging
from typing import Union

from core.redis import start_job_async_or_sync
from django.conf import settings
from django.db import models
from django.db.models.signals import post_save
from django.dispatch import receiver
from django.utils.translation import gettext_lazy as _
from io_storages.base_models import (
    ExportStorage,
    ExportStorageLink,
    ImportStorage,
    ImportStorageLink,
    ProjectStorageMixin,
)
from io_storages.gcs.utils import GCS
from io_storages.utils import storage_can_resolve_bucket_url
from tasks.models import Annotation

logger = logging.getLogger(__name__)


class GCSStorageMixin(models.Model):
    bucket = models.TextField(_('bucket'), null=True, blank=True, help_text='GCS bucket name')
    prefix = models.TextField(_('prefix'), null=True, blank=True, help_text='GCS bucket prefix')
    regex_filter = models.TextField(
        _('regex_filter'), null=True, blank=True, help_text='Cloud storage regex for filtering objects'
    )
    use_blob_urls = models.BooleanField(
        _('use_blob_urls'), default=False, help_text='Interpret objects as BLOBs and generate URLs'
    )
    google_application_credentials = models.TextField(
        _('google_application_credentials'),
        null=True,
        blank=True,
        help_text='The content of GOOGLE_APPLICATION_CREDENTIALS json file',
    )
    google_project_id = models.TextField(_('Google Project ID'), null=True, blank=True, help_text='Google project ID')

    def get_client(self):
        return GCS.get_client(
            google_project_id=self.google_project_id,
            google_application_credentials=self.google_application_credentials,
        )

    def get_bucket(self, client=None, bucket_name=None):
        if not client:
            client = self.get_client()
        return client.get_bucket(bucket_name or self.bucket)

    def validate_connection(self):
        GCS.validate_connection(
            self.bucket,
            self.google_project_id,
            self.google_application_credentials,
            # we don't need to validate path for export storage, it will be created automatically
            None if 'Export' in self.__class__.__name__ else self.prefix,
        )


class GCSImportStorageBase(GCSStorageMixin, ImportStorage):
    url_scheme = 'gs'

    presign = models.BooleanField(_('presign'), default=True, help_text='Generate presigned URLs')
    presign_ttl = models.PositiveSmallIntegerField(
        _('presign_ttl'), default=1, help_text='Presigned URLs TTL (in minutes)'
    )

    def iterkeys(self):
        return GCS.iter_blobs(
            client=self.get_client(),
            bucket_name=self.bucket,
            prefix=self.prefix,
            regex_filter=self.regex_filter,
            return_key=True,
        )

    def get_data(self, key):
        if self.use_blob_urls:
            return {settings.DATA_UNDEFINED_NAME: GCS.get_uri(self.bucket, key)}
        return GCS.read_file(
            client=self.get_client(), bucket_name=self.bucket, key=key, convert_to=GCS.ConvertBlobTo.JSON_DICT
        )

    def generate_http_url(self, url):
        return GCS.generate_http_url(
            url=url,
            presign=self.presign,
            google_application_credentials=self.google_application_credentials,
            google_project_id=self.google_project_id,
            presign_ttl=self.presign_ttl,
        )

    def can_resolve_url(self, url: Union[str, None]) -> bool:
        return storage_can_resolve_bucket_url(self, url)

    def scan_and_create_links(self):
        return self._scan_and_create_links(GCSImportStorageLink)

    def get_blob_metadata(self, key):
        return GCS.get_blob_metadata(
            url=key,
            google_application_credentials=self.google_application_credentials,
            google_project_id=self.google_project_id,
        )

    class Meta:
        abstract = True


class GCSImportStorage(ProjectStorageMixin, GCSImportStorageBase):
    class Meta:
        abstract = False


class GCSExportStorage(GCSStorageMixin, ExportStorage):
    def save_annotation(self, annotation):
        bucket = self.get_bucket()
        logger.debug(f'Creating new object on {self.__class__.__name__} Storage {self} for annotation {annotation}')
        ser_annotation = self._get_serialized_data(annotation)

        # get key that identifies this object in storage
        key = GCSExportStorageLink.get_key(annotation)
        key = str(self.prefix) + '/' + key if self.prefix else key

        # put object into storage
        blob = bucket.blob(key)
        blob.upload_from_string(json.dumps(ser_annotation))

        # create link if everything ok
        GCSExportStorageLink.create(annotation, self)


def async_export_annotation_to_gcs_storages(annotation):
    project = annotation.project
    if hasattr(project, 'io_storages_gcsexportstorages'):
        for storage in project.io_storages_gcsexportstorages.all():
            logger.debug(f'Export {annotation} to GCS storage {storage}')
            storage.save_annotation(annotation)


@receiver(post_save, sender=Annotation)
def export_annotation_to_gcs_storages(sender, instance, **kwargs):
    storages = getattr(instance.project, 'io_storages_gcsexportstorages', None)
    if storages and storages.exists():  # avoid excess jobs in rq
        start_job_async_or_sync(async_export_annotation_to_gcs_storages, instance)


class GCSImportStorageLink(ImportStorageLink):
    storage = models.ForeignKey(GCSImportStorage, on_delete=models.CASCADE, related_name='links')


class GCSExportStorageLink(ExportStorageLink):
    storage = models.ForeignKey(GCSExportStorage, on_delete=models.CASCADE, related_name='links')
</file>

<file path="label_studio/io_storages/gcs/openapi_schema.py">
from drf_yasg import openapi

_common_gcs_storage_schema_properties = {
    'title': openapi.Schema(type=openapi.TYPE_STRING, description='Storage title'),
    'description': openapi.Schema(type=openapi.TYPE_STRING, description='Storage description'),
    'project': openapi.Schema(type=openapi.TYPE_INTEGER, description='Project ID'),
    'bucket': openapi.Schema(type=openapi.TYPE_STRING, description='GCS bucket name'),
    'prefix': openapi.Schema(type=openapi.TYPE_STRING, description='GCS bucket prefix'),
    'google_application_credentials': openapi.Schema(
        type=openapi.TYPE_STRING,
        description='The content of GOOGLE_APPLICATION_CREDENTIALS json file. '
        'Check official Google Cloud Authentication documentation for more details.',
    ),
    'google_project_id': openapi.Schema(type=openapi.TYPE_STRING, description='Google project ID'),
}

_gcs_import_storage_schema = openapi.Schema(
    type=openapi.TYPE_OBJECT,
    properties=dict(
        regex_filter=openapi.Schema(
            type=openapi.TYPE_STRING,
            description='Cloud storage regex for filtering objects. '
            'You must specify it otherwise no objects will be imported.',
        ),
        use_blob_urls=openapi.Schema(
            type=openapi.TYPE_BOOLEAN,
            description='Interpret objects as BLOBs and generate URLs. For example, if your bucket contains images, '
            'you can use this option to generate URLs for these images. '
            'If set to False, it will read the content of the file and load it into Label Studio.',
            default=False,
        ),
        presign=openapi.Schema(
            type=openapi.TYPE_BOOLEAN, description='Presign URLs for direct download', default=True
        ),
        presign_ttl=openapi.Schema(type=openapi.TYPE_INTEGER, description='Presign TTL in minutes', default=1),
        **_common_gcs_storage_schema_properties,
    ),
    required=[],
)

_gcs_import_storage_schema_with_id = openapi.Schema(
    type=openapi.TYPE_OBJECT,
    properties=dict(
        id=openapi.Schema(
            type=openapi.TYPE_INTEGER, description='Storage ID. If set, storage with specified ID will be updated'
        ),
        **_gcs_import_storage_schema.properties,
    ),
    required=[],
)

_gcs_export_storage_schema = openapi.Schema(
    type=openapi.TYPE_OBJECT,
    properties=dict(
        can_delete_objects=openapi.Schema(type=openapi.TYPE_BOOLEAN, description='Deletion from storage enabled.'),
        **_common_gcs_storage_schema_properties,
    ),
    required=[],
)

_gcs_export_storage_schema_with_id = openapi.Schema(
    type=openapi.TYPE_OBJECT,
    properties=dict(
        id=openapi.Schema(
            type=openapi.TYPE_INTEGER, description='Storage ID. If set, storage with specified ID will be updated'
        ),
        **_gcs_export_storage_schema.properties,
    ),
    required=[],
)
</file>

<file path="label_studio/io_storages/gcs/serializers.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import os

from io_storages.gcs.models import GCSExportStorage, GCSImportStorage
from io_storages.serializers import ExportStorageSerializer, ImportStorageSerializer
from rest_framework import serializers
from rest_framework.exceptions import ValidationError


class GCSImportStorageSerializer(ImportStorageSerializer):
    type = serializers.ReadOnlyField(default=os.path.basename(os.path.dirname(__file__)))
    presign = serializers.BooleanField(required=False, default=True)
    secure_fields = ['google_application_credentials']

    class Meta:
        model = GCSImportStorage
        fields = '__all__'

    def to_representation(self, instance):
        result = super().to_representation(instance)
        for attr in GCSImportStorageSerializer.secure_fields:
            result.pop(attr)
        return result

    def validate(self, data):
        data = super().validate(data)
        storage = self.instance
        if storage:
            for key, value in data.items():
                setattr(storage, key, value)
        else:
            if 'id' in self.initial_data:
                storage_object = self.Meta.model.objects.get(id=self.initial_data['id'])
                for attr in GCSImportStorageSerializer.secure_fields:
                    data[attr] = data.get(attr) or getattr(storage_object, attr)
            storage = self.Meta.model(**data)
        try:
            storage.validate_connection()
        except Exception as exc:
            raise ValidationError(exc)
        return data


class GCSExportStorageSerializer(ExportStorageSerializer):
    type = serializers.ReadOnlyField(default=os.path.basename(os.path.dirname(__file__)))

    def to_representation(self, instance):
        result = super().to_representation(instance)
        result.pop('google_application_credentials')
        return result

    class Meta:
        model = GCSExportStorage
        fields = '__all__'
</file>

<file path="label_studio/io_storages/gcs/utils.py">
import base64
import fnmatch
import json
import logging
import re
from datetime import timedelta
from enum import Enum
from functools import lru_cache
from json import JSONDecodeError
from typing import Optional, Union
from urllib.parse import urlparse

import google.auth
import google.cloud.storage as gcs
from core.utils.common import get_ttl_hash
from django.conf import settings
from google.auth.exceptions import DefaultCredentialsError
from google.oauth2 import service_account

logger = logging.getLogger(__name__)

Base64 = bytes


class GCS(object):
    _client_cache = {}
    _credentials_cache = None
    DEFAULT_GOOGLE_PROJECT_ID = gcs.client._marker

    class ConvertBlobTo(Enum):
        NOTHING = 1
        JSON = 2
        JSON_DICT = 3
        BASE64 = 4

    @classmethod
    @lru_cache(maxsize=1)
    def get_bucket(
        cls,
        ttl_hash: int,
        google_project_id: Optional[str] = None,
        google_application_credentials: Optional[Union[str, dict]] = None,
        bucket_name: Optional[str] = None,
    ) -> gcs.Bucket:

        client = cls.get_client(
            google_project_id=google_project_id, google_application_credentials=google_application_credentials
        )

        return client.get_bucket(bucket_name)

    @classmethod
    def get_client(
        cls, google_project_id: str = None, google_application_credentials: Union[str, dict] = None
    ) -> gcs.Client:
        """
        :param google_project_id:
        :param google_application_credentials:
        :return:
        """
        google_project_id = google_project_id or GCS.DEFAULT_GOOGLE_PROJECT_ID
        cache_key = google_application_credentials

        if cache_key not in GCS._client_cache:

            # use credentials from LS Cloud Storage settings
            if google_application_credentials:
                if isinstance(google_application_credentials, str):
                    try:
                        google_application_credentials = json.loads(google_application_credentials)
                    except JSONDecodeError as e:
                        # change JSON error to human-readable format
                        raise ValueError(f'Google Application Credentials must be valid JSON string. {e}')
                credentials = service_account.Credentials.from_service_account_info(google_application_credentials)
                GCS._client_cache[cache_key] = gcs.Client(project=google_project_id, credentials=credentials)

            # use Google Application Default Credentials (ADC)
            else:
                GCS._client_cache[cache_key] = gcs.Client(project=google_project_id)

        return GCS._client_cache[cache_key]

    @classmethod
    def validate_connection(
        cls,
        bucket_name: str,
        google_project_id: str = None,
        google_application_credentials: Union[str, dict] = None,
        prefix: str = None,
        use_glob_syntax: bool = False,
    ):
        logger.debug('Validating GCS connection')
        client = cls.get_client(
            google_application_credentials=google_application_credentials, google_project_id=google_project_id
        )
        logger.debug('Validating GCS bucket')
        bucket = client.get_bucket(bucket_name)

        # Dataset storages uses glob syntax and we want to add explicit checks
        # In the future when GCS lib supports it
        if use_glob_syntax:
            pass
        else:
            if prefix:
                blobs = list(bucket.list_blobs(prefix=prefix, max_results=1))
                if not blobs:
                    raise ValueError(f"No blobs found in {bucket_name}/{prefix} or prefix doesn't exist")

    @classmethod
    def iter_blobs(
        cls,
        client: gcs.Client,
        bucket_name: str,
        prefix: str = None,
        regex_filter: str = None,
        limit: int = None,
        return_key: bool = False,
    ):
        """
        Iterate files on the bucket. Optionally return limited number of files that match provided extensions
        :param client: GCS Client obj
        :param bucket_name: bucket name
        :param prefix: bucket prefix
        :param regex_filter: RegEx filter
        :param limit: specify limit for max files
        :param return_key: return object key string instead of gcs.Blob object
        :return: Iterator object
        """
        total_read = 0
        blob_iter = client.list_blobs(bucket_name, prefix=prefix)
        prefix = str(prefix) if prefix else ''
        regex = re.compile(str(regex_filter)) if regex_filter else None
        for blob in blob_iter:
            # skip dir level
            if blob.name == (prefix.rstrip('/') + '/'):
                continue
            # check regex pattern filter
            if regex and not regex.match(blob.name):
                logger.debug(blob.name + ' is skipped by regex filter')
                continue
            if return_key:
                yield blob.name
            else:
                yield blob
            total_read += 1
            if limit and total_read == limit:
                break

    @classmethod
    def _get_default_credentials(cls):
        """Get default GCS credentials for LS Cloud Storages"""
        # TODO: remove this func with fflag_fix_back_lsdv_4902_force_google_adc_16052023_short
        try:
            # check if GCS._credentials_cache is None, we don't want to try getting default credentials again
            credentials = GCS._credentials_cache.get('credentials') if GCS._credentials_cache else None
            if GCS._credentials_cache is None or (credentials and credentials.expired):
                # try to get credentials from the current environment
                credentials, _ = google.auth.default(['https://www.googleapis.com/auth/cloud-platform'])
                # apply & refresh credentials
                auth_req = google.auth.transport.requests.Request()
                credentials.refresh(auth_req)
                # set cache
                GCS._credentials_cache = {
                    'service_account_email': credentials.service_account_email,
                    'access_token': credentials.token,
                    'credentials': credentials,
                }

        except DefaultCredentialsError as exc:
            logger.warning(f'Label studio could not load default GCS credentials from env. {exc}', exc_info=True)
            GCS._credentials_cache = {}

        return GCS._credentials_cache

    @classmethod
    def generate_http_url(
        cls,
        url: str,
        presign: bool,
        google_application_credentials: Union[str, dict] = None,
        google_project_id: str = None,
        presign_ttl: int = 1,
    ) -> str:
        """
        Gets gs:// like URI string and returns presigned https:// URL
        :param url: input URI
        :param presign: Whether to generate presigned URL. If false, will generate base64 encoded data URL
        :param google_application_credentials:
        :param google_project_id:
        :param presign_ttl: Presign TTL in minutes
        :return: Presigned URL string
        """
        r = urlparse(url, allow_fragments=False)
        bucket_name = r.netloc
        blob_name = r.path.lstrip('/')

        """Generates a v4 signed URL for downloading a blob.

        Note that this method requires a service account key file. You can not use
        this if you are using Application Default Credentials from Google Compute
        Engine or from the Google Cloud SDK.
        """
        bucket = cls.get_bucket(
            ttl_hash=get_ttl_hash(),
            google_application_credentials=google_application_credentials,
            google_project_id=google_project_id,
            bucket_name=bucket_name,
        )

        blob = bucket.blob(blob_name)

        # this flag should be OFF, maybe we need to enable it for 1-2 customers, we have to check it
        if settings.GCS_CLOUD_STORAGE_FORCE_DEFAULT_CREDENTIALS:
            # google_application_credentials has higher priority,
            # use Application Default Credentials (ADC) when google_application_credentials is empty only
            maybe_credentials = {} if google_application_credentials else cls._get_default_credentials()
            maybe_client = None if google_application_credentials else cls.get_client()
        else:
            maybe_credentials = {}
            maybe_client = None

        if not presign:
            blob.reload(client=maybe_client)  # needed to know the content type
            blob_bytes = blob.download_as_bytes(client=maybe_client)
            return f'data:{blob.content_type};base64,{base64.b64encode(blob_bytes).decode("utf-8")}'

        url = blob.generate_signed_url(
            version='v4',
            # This URL is valid for 15 minutes
            expiration=timedelta(minutes=presign_ttl),
            # Allow GET requests using this URL.
            method='GET',
            **maybe_credentials,
        )

        logger.debug('Generated GCS signed url: ' + url)
        return url

    @classmethod
    def iter_images_base64(cls, client, bucket_name, max_files):
        for image in cls.iter_blobs(client, bucket_name, max_files):
            yield GCS.read_base64(image)

    @classmethod
    def iter_images_filename(cls, client, bucket_name, max_files):
        for image in cls.iter_blobs(client, bucket_name, max_files):
            yield image.name

    @classmethod
    def get_uri(cls, bucket_name, key):
        return f'gs://{bucket_name}/{key}'

    @classmethod
    def _try_read_json(cls, blob_str):
        try:
            data = json.loads(blob_str)
        except ValueError:
            logger.error(f"Can't parse JSON from {blob_str}")
            return
        return data

    @classmethod
    def read_file(
        cls, client: gcs.Client, bucket_name: str, key: str, convert_to: ConvertBlobTo = ConvertBlobTo.NOTHING
    ):
        bucket = client.get_bucket(bucket_name)
        blob = bucket.blob(key)
        blob_str = blob.download_as_bytes()
        if convert_to == cls.ConvertBlobTo.NOTHING:
            return blob_str
        elif convert_to == cls.ConvertBlobTo.JSON:
            return cls._try_read_json(blob_str)
        elif convert_to == cls.ConvertBlobTo.JSON_DICT:
            json_data = cls._try_read_json(blob_str)
            if not isinstance(json_data, dict):
                raise ValueError(
                    f'Error on key {key}: For {cls.__name__} your JSON file must be a dictionary with one task.'
                )
            return json_data
        elif convert_to == cls.ConvertBlobTo.BASE64:
            return base64.b64encode(blob_str)

        return blob_str

    @classmethod
    def read_base64(cls, f: gcs.Blob) -> Base64:
        return base64.b64encode(f.download_as_bytes())

    @classmethod
    def get_blob_metadata(
        cls,
        url: str,
        google_application_credentials: Union[str, dict] = None,
        google_project_id: str = None,
        properties_name: list = [],
    ) -> dict:
        """
        Gets object metadata like size and updated date from GCS in dict format
        :param url: input URI
        :param google_application_credentials:
        :param google_project_id:
        :return: Object metadata dict("name": "value")
        """
        r = urlparse(url, allow_fragments=False)
        bucket_name = r.netloc
        blob_name = r.path.lstrip('/')

        client = cls.get_client(
            google_application_credentials=google_application_credentials, google_project_id=google_project_id
        )
        bucket = client.get_bucket(bucket_name)
        # Get blob instead of Blob() is used to make an http request and get metadata
        blob = bucket.get_blob(blob_name)
        if not properties_name:
            return blob._properties
        return {key: value for key, value in blob._properties.items() if key in properties_name}

    @classmethod
    def validate_pattern(cls, storage, pattern, glob_pattern=True):
        """
        Validate pattern against Google Cloud Storage
        :param storage: Google Cloud Storage instance
        :param pattern: Pattern to validate
        :param glob_pattern: If True, pattern is a glob pattern, otherwise it is a regex pattern
        :return: Message if pattern is not valid, empty string otherwise
        """
        client = storage.get_client()
        blob_iter = client.list_blobs(
            storage.bucket, prefix=storage.prefix, page_size=settings.CLOUD_STORAGE_CHECK_FOR_RECORDS_PAGE_SIZE
        )
        prefix = str(storage.prefix) if storage.prefix else ''
        # compile pattern to regex
        if glob_pattern:
            pattern = fnmatch.translate(pattern)
        regex = re.compile(str(pattern))
        for index, blob in enumerate(blob_iter):
            # skip directories
            if blob.name == (prefix.rstrip('/') + '/'):
                continue
            # check regex pattern filter
            if pattern and regex.match(blob.name):
                logger.debug(blob.name + ' matches file pattern')
                return ''
        return 'No objects found matching the provided glob pattern'
</file>

<file path="label_studio/io_storages/localfiles/api.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
from django.utils.decorators import method_decorator
from drf_yasg import openapi as openapi
from drf_yasg.utils import no_body, swagger_auto_schema
from io_storages.api import (
    ExportStorageDetailAPI,
    ExportStorageFormLayoutAPI,
    ExportStorageListAPI,
    ExportStorageSyncAPI,
    ExportStorageValidateAPI,
    ImportStorageDetailAPI,
    ImportStorageFormLayoutAPI,
    ImportStorageListAPI,
    ImportStorageSyncAPI,
    ImportStorageValidateAPI,
)
from io_storages.localfiles.models import LocalFilesExportStorage, LocalFilesImportStorage
from io_storages.localfiles.serializers import LocalFilesExportStorageSerializer, LocalFilesImportStorageSerializer

from .openapi_schema import (
    _local_files_export_storage_schema,
    _local_files_export_storage_schema_with_id,
    _local_files_import_storage_schema,
    _local_files_import_storage_schema_with_id,
)


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Storage: Local'],
        x_fern_sdk_group_name=['import_storage', 'local'],
        x_fern_sdk_method_name='list',
        x_fern_audiences=['public'],
        operation_summary='Get all import storage',
        operation_description='Get a list of all local file import storage connections.',
        manual_parameters=[
            openapi.Parameter(
                name='project',
                type=openapi.TYPE_INTEGER,
                in_=openapi.IN_QUERY,
                description='Project ID',
            ),
        ],
        request_body=no_body,
    ),
)
@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Storage: Local'],
        x_fern_sdk_group_name=['import_storage', 'local'],
        x_fern_sdk_method_name='create',
        x_fern_audiences=['public'],
        operation_summary='Create import storage',
        operation_description='Create a new local file import storage connection.',
        request_body=_local_files_import_storage_schema,
    ),
)
class LocalFilesImportStorageListAPI(ImportStorageListAPI):
    queryset = LocalFilesImportStorage.objects.all()
    serializer_class = LocalFilesImportStorageSerializer


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Storage: Local'],
        x_fern_sdk_group_name=['import_storage', 'local'],
        x_fern_sdk_method_name='get',
        x_fern_audiences=['public'],
        operation_summary='Get import storage',
        operation_description='Get a specific local file import storage connection.',
        request_body=no_body,
    ),
)
@method_decorator(
    name='patch',
    decorator=swagger_auto_schema(
        tags=['Storage: Local'],
        x_fern_sdk_group_name=['import_storage', 'local'],
        x_fern_sdk_method_name='update',
        x_fern_audiences=['public'],
        operation_summary='Update import storage',
        operation_description='Update a specific local file import storage connection.',
        request_body=_local_files_import_storage_schema,
    ),
)
@method_decorator(
    name='delete',
    decorator=swagger_auto_schema(
        tags=['Storage: Local'],
        x_fern_sdk_group_name=['import_storage', 'local'],
        x_fern_sdk_method_name='delete',
        x_fern_audiences=['public'],
        operation_summary='Delete import storage',
        operation_description='Delete a specific local import storage connection.',
        request_body=no_body,
    ),
)
class LocalFilesImportStorageDetailAPI(ImportStorageDetailAPI):
    queryset = LocalFilesImportStorage.objects.all()
    serializer_class = LocalFilesImportStorageSerializer


@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Storage: Local'],
        x_fern_sdk_group_name=['import_storage', 'local'],
        x_fern_sdk_method_name='sync',
        x_fern_audiences=['public'],
        operation_summary='Sync import storage',
        operation_description='Sync tasks from a local file import storage connection.',
        manual_parameters=[
            openapi.Parameter(
                name='id',
                type=openapi.TYPE_INTEGER,
                in_=openapi.IN_PATH,
                description='Storage ID',
            ),
        ],
        request_body=no_body,
    ),
)
class LocalFilesImportStorageSyncAPI(ImportStorageSyncAPI):
    serializer_class = LocalFilesImportStorageSerializer


@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Storage: Local'],
        x_fern_sdk_group_name=['export_storage', 'local'],
        x_fern_sdk_method_name='sync',
        x_fern_audiences=['public'],
        operation_summary='Sync export storage',
        operation_description='Sync tasks from a local file export storage connection.',
        request_body=no_body,
    ),
)
class LocalFilesExportStorageSyncAPI(ExportStorageSyncAPI):
    serializer_class = LocalFilesExportStorageSerializer


@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Storage: Local'],
        x_fern_sdk_group_name=['import_storage', 'local'],
        x_fern_sdk_method_name='validate',
        x_fern_audiences=['public'],
        operation_summary='Validate import storage',
        operation_description='Validate a specific local file import storage connection.',
        request_body=_local_files_import_storage_schema_with_id,
        responses={200: openapi.Response(description='Validation successful')},
    ),
)
class LocalFilesImportStorageValidateAPI(ImportStorageValidateAPI):
    serializer_class = LocalFilesImportStorageSerializer


@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Storage: Local'],
        x_fern_sdk_group_name=['export_storage', 'local'],
        x_fern_sdk_method_name='validate',
        x_fern_audiences=['public'],
        operation_summary='Validate export storage',
        operation_description='Validate a specific local file export storage connection.',
        request_body=_local_files_export_storage_schema_with_id,
        responses={200: openapi.Response(description='Validation successful')},
    ),
)
class LocalFilesExportStorageValidateAPI(ExportStorageValidateAPI):
    serializer_class = LocalFilesExportStorageSerializer


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Storage: Local'],
        x_fern_sdk_group_name=['export_storage', 'local'],
        x_fern_sdk_method_name='list',
        x_fern_audiences=['public'],
        operation_summary='Get all export storage',
        operation_description='Get a list of all Local export storage connections.',
        manual_parameters=[
            openapi.Parameter(
                name='project',
                type=openapi.TYPE_INTEGER,
                in_=openapi.IN_QUERY,
                description='Project ID',
            ),
        ],
        request_body=no_body,
    ),
)
@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Storage: Local'],
        x_fern_sdk_group_name=['export_storage', 'local'],
        x_fern_sdk_method_name='create',
        x_fern_audiences=['public'],
        operation_summary='Create export storage',
        operation_description='Create a new local file export storage connection to store annotations.',
        request_body=_local_files_export_storage_schema,
    ),
)
class LocalFilesExportStorageListAPI(ExportStorageListAPI):
    queryset = LocalFilesExportStorage.objects.all()
    serializer_class = LocalFilesExportStorageSerializer


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Storage: Local'],
        x_fern_sdk_group_name=['export_storage', 'local'],
        x_fern_sdk_method_name='get',
        x_fern_audiences=['public'],
        operation_summary='Get export storage',
        operation_description='Get a specific local file export storage connection.',
        request_body=no_body,
    ),
)
@method_decorator(
    name='patch',
    decorator=swagger_auto_schema(
        tags=['Storage: Local'],
        x_fern_sdk_group_name=['export_storage', 'local'],
        x_fern_sdk_method_name='update',
        x_fern_audiences=['public'],
        operation_summary='Update export storage',
        operation_description='Update a specific local file export storage connection.',
        request_body=_local_files_export_storage_schema,
    ),
)
@method_decorator(
    name='delete',
    decorator=swagger_auto_schema(
        tags=['Storage: Local'],
        x_fern_sdk_group_name=['export_storage', 'local'],
        x_fern_sdk_method_name='delete',
        x_fern_audiences=['public'],
        operation_summary='Delete export storage',
        operation_description='Delete a specific local file export storage connection.',
        request_body=no_body,
    ),
)
class LocalFilesExportStorageDetailAPI(ExportStorageDetailAPI):
    queryset = LocalFilesExportStorage.objects.all()
    serializer_class = LocalFilesExportStorageSerializer


class LocalFilesImportStorageFormLayoutAPI(ImportStorageFormLayoutAPI):
    pass


class LocalFilesExportStorageFormLayoutAPI(ExportStorageFormLayoutAPI):
    pass
</file>

<file path="label_studio/io_storages/localfiles/models.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import json
import logging
import os
import re
from pathlib import Path
from urllib.parse import quote

from django.conf import settings
from django.db import models
from django.db.models.signals import post_save
from django.dispatch import receiver
from django.utils.translation import gettext_lazy as _
from io_storages.base_models import (
    ExportStorage,
    ExportStorageLink,
    ImportStorage,
    ImportStorageLink,
    ProjectStorageMixin,
)
from rest_framework.exceptions import ValidationError
from tasks.models import Annotation

logger = logging.getLogger(__name__)


class LocalFilesMixin(models.Model):
    path = models.TextField(_('path'), null=True, blank=True, help_text='Local path')
    regex_filter = models.TextField(_('regex_filter'), null=True, blank=True, help_text='Regex for filtering objects')
    use_blob_urls = models.BooleanField(
        _('use_blob_urls'), default=False, help_text='Interpret objects as BLOBs and generate URLs'
    )

    def validate_connection(self):
        path = Path(self.path)
        document_root = Path(settings.LOCAL_FILES_DOCUMENT_ROOT)
        if not path.exists():
            raise ValidationError(f'Path {self.path} does not exist')
        if document_root not in path.parents:
            raise ValidationError(
                f'Path {self.path} must start with '
                f'LOCAL_FILES_DOCUMENT_ROOT={settings.LOCAL_FILES_DOCUMENT_ROOT} '
                f'and must be a child, e.g.: {Path(settings.LOCAL_FILES_DOCUMENT_ROOT) / "abc"}'
            )
        if settings.LOCAL_FILES_SERVING_ENABLED is False:
            raise ValidationError(
                "Serving local files can be dangerous, so it's disabled by default. "
                'You can enable it with LOCAL_FILES_SERVING_ENABLED environment variable, '
                'please check docs: https://labelstud.io/guide/storage.html#Local-storage'
            )


class LocalFilesImportStorageBase(LocalFilesMixin, ImportStorage):
    url_scheme = 'https'

    def can_resolve_url(self, url):
        return False

    def iterkeys(self):
        path = Path(self.path)
        regex = re.compile(str(self.regex_filter)) if self.regex_filter else None
        # For better control of imported tasks, file reading has been changed to ascending order of filenames.
        # In other words, the task IDs are sorted by filename order.
        for file in sorted(path.rglob('*'), key=os.path.basename):
            if file.is_file():
                key = file.name
                if regex and not regex.match(key):
                    logger.debug(key + ' is skipped by regex filter')
                    continue
                yield str(file)

    def get_data(self, key):
        path = Path(key)
        if self.use_blob_urls:
            # include self-hosted links pointed to local resources via
            # {settings.HOSTNAME}/data/local-files?d=<path/to/local/dir>
            document_root = Path(settings.LOCAL_FILES_DOCUMENT_ROOT)
            relative_path = str(path.relative_to(document_root))
            return {
                settings.DATA_UNDEFINED_NAME: f'{settings.HOSTNAME}/data/local-files/?d={quote(str(relative_path))}'
            }

        try:
            with open(path, encoding='utf8') as f:
                value = json.load(f)
        except (UnicodeDecodeError, json.decoder.JSONDecodeError):
            raise ValueError(
                f"Can't import JSON-formatted tasks from {key}. If you're trying to import binary objects, "
                f'perhaps you\'ve forgot to enable "Treat every bucket object as a source file" option?'
            )

        if not isinstance(value, dict):
            raise ValueError(
                f'Error on key {key}: For {self.__class__.__name__} your JSON file must be a dictionary with one task.'
            )
        return value

    def scan_and_create_links(self):
        return self._scan_and_create_links(LocalFilesImportStorageLink)

    class Meta:
        abstract = True


class LocalFilesImportStorage(ProjectStorageMixin, LocalFilesImportStorageBase):
    class Meta:
        abstract = False


class LocalFilesExportStorage(LocalFilesMixin, ExportStorage):
    def save_annotation(self, annotation):
        logger.debug(f'Creating new object on {self.__class__.__name__} Storage {self} for annotation {annotation}')
        ser_annotation = self._get_serialized_data(annotation)

        # get key that identifies this object in storage
        key = LocalFilesExportStorageLink.get_key(annotation)
        key = os.path.join(self.path, f'{key}')

        # put object into storage
        with open(key, mode='w') as f:
            json.dump(ser_annotation, f, indent=2)

        # Create export storage link
        LocalFilesExportStorageLink.create(annotation, self)


class LocalFilesImportStorageLink(ImportStorageLink):
    storage = models.ForeignKey(LocalFilesImportStorage, on_delete=models.CASCADE, related_name='links')


class LocalFilesExportStorageLink(ExportStorageLink):
    storage = models.ForeignKey(LocalFilesExportStorage, on_delete=models.CASCADE, related_name='links')


@receiver(post_save, sender=Annotation)
def export_annotation_to_local_files(sender, instance, **kwargs):
    project = instance.project
    if hasattr(project, 'io_storages_localfilesexportstorages'):
        for storage in project.io_storages_localfilesexportstorages.all():
            logger.debug(f'Export {instance} to Local Storage {storage}')
            storage.save_annotation(instance)
</file>

<file path="label_studio/io_storages/localfiles/openapi_schema.py">
from drf_yasg import openapi

_common_storage_schema_properties = {
    'title': openapi.Schema(type=openapi.TYPE_STRING, description='Storage title'),
    'description': openapi.Schema(type=openapi.TYPE_STRING, description='Storage description'),
    'project': openapi.Schema(type=openapi.TYPE_INTEGER, description='Project ID'),
    'path': openapi.Schema(type=openapi.TYPE_STRING, description='Path to local directory'),
    'regex_filter': openapi.Schema(type=openapi.TYPE_STRING, description='Regex for filtering objects'),
    'use_blob_urls': openapi.Schema(
        type=openapi.TYPE_BOOLEAN,
        description='Interpret objects as BLOBs and generate URLs. For example, if your directory contains images, '
        'you can use this option to generate URLs for these images. '
        'If set to False, it will read the content of the file and load it into Label Studio.',
        default=False,
    ),
}

_local_files_import_storage_schema = openapi.Schema(
    type=openapi.TYPE_OBJECT,
    properties=_common_storage_schema_properties,
    required=[],
)

_local_files_import_storage_schema_with_id = openapi.Schema(
    type=openapi.TYPE_OBJECT,
    properties=dict(
        id=openapi.Schema(
            type=openapi.TYPE_INTEGER, description='Storage ID. If set, storage with specified ID will be updated'
        ),
        **_local_files_import_storage_schema.properties,
    ),
    required=[],
)

_local_files_export_storage_schema = openapi.Schema(
    type=openapi.TYPE_OBJECT, properties=_common_storage_schema_properties, required=[]
)

_local_files_export_storage_schema_with_id = openapi.Schema(
    type=openapi.TYPE_OBJECT,
    properties=dict(
        id=openapi.Schema(
            type=openapi.TYPE_INTEGER, description='Storage ID. If set, storage with specified ID will be updated'
        ),
        **_local_files_export_storage_schema.properties,
    ),
    required=[],
)
</file>

<file path="label_studio/io_storages/localfiles/serializers.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import os

from io_storages.localfiles.models import LocalFilesExportStorage, LocalFilesImportStorage
from io_storages.serializers import ExportStorageSerializer, ImportStorageSerializer
from rest_framework import serializers
from rest_framework.exceptions import ValidationError


class LocalFilesImportStorageSerializer(ImportStorageSerializer):
    type = serializers.ReadOnlyField(default=os.path.basename(os.path.dirname(__file__)))

    class Meta:
        model = LocalFilesImportStorage
        fields = '__all__'

    def validate(self, data):
        # Validate local file path
        data = super(LocalFilesImportStorageSerializer, self).validate(data)
        storage = LocalFilesImportStorage(**data)
        try:
            storage.validate_connection()
        except Exception as exc:
            raise ValidationError(exc)
        return data


class LocalFilesExportStorageSerializer(ExportStorageSerializer):
    type = serializers.ReadOnlyField(default=os.path.basename(os.path.dirname(__file__)))

    class Meta:
        model = LocalFilesExportStorage
        fields = '__all__'

    def validate(self, data):
        # Validate local file path
        data = super(LocalFilesExportStorageSerializer, self).validate(data)
        return data
</file>

<file path="label_studio/io_storages/migrations/__init__.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
</file>

<file path="label_studio/io_storages/migrations/0001_squashed_0002_auto_20210302_1827.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
# Generated by Django 3.1.4 on 2021-03-03 07:31

from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    replaces = [('io_storages', '0001_initial'), ('io_storages', '0002_auto_20210302_1827')]

    initial = True

    dependencies = [
        ('projects', '0001_squashed_0065_auto_20210223_2014'),
        ('tasks', '__first__'),
    ]

    operations = [
        migrations.CreateModel(
            name='AzureBlobStorageMixin',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('container', models.TextField(blank=True, help_text='Azure blob container', null=True, verbose_name='container')),
                ('prefix', models.TextField(blank=True, help_text='Azure blob prefix name', null=True, verbose_name='prefix')),
                ('regex_filter', models.TextField(blank=True, help_text='Cloud storage regex for filtering objects', null=True, verbose_name='regex_filter')),
                ('use_blob_urls', models.BooleanField(default=False, help_text='Interpret objects as BLOBs and generate URLs', verbose_name='use_blob_urls')),
                ('account_name', models.TextField(blank=True, help_text='Azure Blob account name', null=True, verbose_name='account_name')),
                ('account_key', models.TextField(blank=True, help_text='Azure Blob account key', null=True, verbose_name='account_key')),
            ],
        ),
        migrations.CreateModel(
            name='GCSStorageMixin',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('bucket', models.TextField(blank=True, help_text='GCS bucket name', null=True, verbose_name='bucket')),
                ('prefix', models.TextField(blank=True, help_text='GCS bucket prefix', null=True, verbose_name='prefix')),
                ('regex_filter', models.TextField(blank=True, help_text='Cloud storage regex for filtering objects', null=True, verbose_name='regex_filter')),
                ('use_blob_urls', models.BooleanField(default=False, help_text='Interpret objects as BLOBs and generate URLs', verbose_name='use_blob_urls')),
            ],
        ),
        migrations.CreateModel(
            name='RedisStorageMixin',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('path', models.TextField(blank=True, help_text='Storage prefix (optional)', null=True, verbose_name='path')),
                ('host', models.TextField(blank=True, help_text='Server Host IP (optional)', null=True, verbose_name='host')),
                ('port', models.TextField(blank=True, help_text='Server Port (optional)', null=True, verbose_name='port')),
                ('password', models.TextField(blank=True, help_text='Server Password (optional)', null=True, verbose_name='password')),
                ('regex_filter', models.TextField(blank=True, help_text='Cloud storage regex for filtering objects', null=True, verbose_name='port')),
                ('use_blob_urls', models.BooleanField(default=False, help_text='Interpret objects as BLOBs and generate URLs', verbose_name='use_blob_urls')),
            ],
        ),
        migrations.CreateModel(
            name='S3ExportStorage',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('title', models.CharField(help_text='Cloud storage title', max_length=256, null=True, verbose_name='title')),
                ('description', models.TextField(blank=True, help_text='Cloud storage description', null=True, verbose_name='description')),
                ('created_at', models.DateTimeField(auto_now_add=True, help_text='Creation time', verbose_name='created at')),
                ('bucket', models.TextField(blank=True, help_text='S3 bucket name', null=True, verbose_name='bucket')),
                ('prefix', models.TextField(blank=True, help_text='S3 bucket prefix', null=True, verbose_name='prefix')),
                ('regex_filter', models.TextField(blank=True, help_text='Cloud storage regex for filtering objects', null=True, verbose_name='regex_filter')),
                ('use_blob_urls', models.BooleanField(default=False, help_text='Interpret objects as BLOBs and generate URLs', verbose_name='use_blob_urls')),
                ('aws_access_key_id', models.TextField(blank=True, help_text='AWS_ACCESS_KEY_ID', null=True, verbose_name='aws_access_key_id')),
                ('aws_secret_access_key', models.TextField(blank=True, help_text='AWS_SECRET_ACCESS_KEY', null=True, verbose_name='aws_secret_access_key')),
                ('aws_session_token', models.TextField(blank=True, help_text='AWS_SESSION_TOKEN', null=True, verbose_name='aws_session_token')),
                ('region_name', models.TextField(blank=True, help_text='AWS Region', null=True, verbose_name='region_name')),
                ('s3_endpoint', models.TextField(blank=True, help_text='S3 Endpoint', null=True, verbose_name='s3_endpoint')),
                ('project', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='io_storages_s3exportstorages', to='projects.project')),
            ],
            options={
                'abstract': False,
            },
        ),
        migrations.CreateModel(
            name='S3ImportStorage',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('title', models.CharField(help_text='Cloud storage title', max_length=256, null=True, verbose_name='title')),
                ('description', models.TextField(blank=True, help_text='Cloud storage description', null=True, verbose_name='description')),
                ('created_at', models.DateTimeField(auto_now_add=True, help_text='Creation time', verbose_name='created at')),
                ('bucket', models.TextField(blank=True, help_text='S3 bucket name', null=True, verbose_name='bucket')),
                ('prefix', models.TextField(blank=True, help_text='S3 bucket prefix', null=True, verbose_name='prefix')),
                ('regex_filter', models.TextField(blank=True, help_text='Cloud storage regex for filtering objects', null=True, verbose_name='regex_filter')),
                ('use_blob_urls', models.BooleanField(default=False, help_text='Interpret objects as BLOBs and generate URLs', verbose_name='use_blob_urls')),
                ('aws_access_key_id', models.TextField(blank=True, help_text='AWS_ACCESS_KEY_ID', null=True, verbose_name='aws_access_key_id')),
                ('aws_secret_access_key', models.TextField(blank=True, help_text='AWS_SECRET_ACCESS_KEY', null=True, verbose_name='aws_secret_access_key')),
                ('aws_session_token', models.TextField(blank=True, help_text='AWS_SESSION_TOKEN', null=True, verbose_name='aws_session_token')),
                ('region_name', models.TextField(blank=True, help_text='AWS Region', null=True, verbose_name='region_name')),
                ('s3_endpoint', models.TextField(blank=True, help_text='S3 Endpoint', null=True, verbose_name='s3_endpoint')),
                ('presign', models.BooleanField(default=True, help_text='Generate presigned URLs', verbose_name='presign')),
                ('presign_ttl', models.PositiveSmallIntegerField(default=1, help_text='Presigned URLs TTL (in minutes)', verbose_name='presign_ttl')),
                ('project', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='io_storages_s3importstorages', to='projects.project')),
            ],
            options={
                'abstract': False,
            },
        ),
        migrations.CreateModel(
            name='AzureBlobExportStorage',
            fields=[
                ('azureblobstoragemixin_ptr', models.OneToOneField(auto_created=True, on_delete=django.db.models.deletion.CASCADE, parent_link=True, primary_key=True, serialize=False, to='io_storages.azureblobstoragemixin')),
                ('title', models.CharField(help_text='Cloud storage title', max_length=256, null=True, verbose_name='title')),
                ('description', models.TextField(blank=True, help_text='Cloud storage description', null=True, verbose_name='description')),
                ('created_at', models.DateTimeField(auto_now_add=True, help_text='Creation time', verbose_name='created at')),
                ('project', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='io_storages_azureblobexportstorages', to='projects.project')),
            ],
            options={
                'abstract': False,
            },
            bases=('io_storages.azureblobstoragemixin', models.Model),
        ),
        migrations.CreateModel(
            name='AzureBlobImportStorage',
            fields=[
                ('azureblobstoragemixin_ptr', models.OneToOneField(auto_created=True, on_delete=django.db.models.deletion.CASCADE, parent_link=True, primary_key=True, serialize=False, to='io_storages.azureblobstoragemixin')),
                ('title', models.CharField(help_text='Cloud storage title', max_length=256, null=True, verbose_name='title')),
                ('description', models.TextField(blank=True, help_text='Cloud storage description', null=True, verbose_name='description')),
                ('created_at', models.DateTimeField(auto_now_add=True, help_text='Creation time', verbose_name='created at')),
                ('presign', models.BooleanField(default=True, help_text='Generate presigned URLs', verbose_name='presign')),
                ('presign_ttl', models.PositiveSmallIntegerField(default=1, help_text='Presigned URLs TTL (in minutes)', verbose_name='presign_ttl')),
                ('project', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='io_storages_azureblobimportstorages', to='projects.project')),
            ],
            options={
                'abstract': False,
            },
            bases=('io_storages.azureblobstoragemixin', models.Model),
        ),
        migrations.CreateModel(
            name='GCSExportStorage',
            fields=[
                ('gcsstoragemixin_ptr', models.OneToOneField(auto_created=True, on_delete=django.db.models.deletion.CASCADE, parent_link=True, primary_key=True, serialize=False, to='io_storages.gcsstoragemixin')),
                ('title', models.CharField(help_text='Cloud storage title', max_length=256, null=True, verbose_name='title')),
                ('description', models.TextField(blank=True, help_text='Cloud storage description', null=True, verbose_name='description')),
                ('created_at', models.DateTimeField(auto_now_add=True, help_text='Creation time', verbose_name='created at')),
                ('project', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='io_storages_gcsexportstorages', to='projects.project')),
            ],
            options={
                'abstract': False,
            },
            bases=('io_storages.gcsstoragemixin', models.Model),
        ),
        migrations.CreateModel(
            name='GCSImportStorage',
            fields=[
                ('gcsstoragemixin_ptr', models.OneToOneField(auto_created=True, on_delete=django.db.models.deletion.CASCADE, parent_link=True, primary_key=True, serialize=False, to='io_storages.gcsstoragemixin')),
                ('title', models.CharField(help_text='Cloud storage title', max_length=256, null=True, verbose_name='title')),
                ('description', models.TextField(blank=True, help_text='Cloud storage description', null=True, verbose_name='description')),
                ('created_at', models.DateTimeField(auto_now_add=True, help_text='Creation time', verbose_name='created at')),
                ('presign', models.BooleanField(default=True, help_text='Generate presigned URLs', verbose_name='presign')),
                ('presign_ttl', models.PositiveSmallIntegerField(default=1, help_text='Presigned URLs TTL (in minutes)', verbose_name='presign_ttl')),
                ('project', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='io_storages_gcsimportstorages', to='projects.project')),
            ],
            options={
                'abstract': False,
            },
            bases=('io_storages.gcsstoragemixin', models.Model),
        ),
        migrations.CreateModel(
            name='RedisExportStorage',
            fields=[
                ('redisstoragemixin_ptr', models.OneToOneField(auto_created=True, on_delete=django.db.models.deletion.CASCADE, parent_link=True, primary_key=True, serialize=False, to='io_storages.redisstoragemixin')),
                ('title', models.CharField(help_text='Cloud storage title', max_length=256, null=True, verbose_name='title')),
                ('description', models.TextField(blank=True, help_text='Cloud storage description', null=True, verbose_name='description')),
                ('created_at', models.DateTimeField(auto_now_add=True, help_text='Creation time', verbose_name='created at')),
                ('db', models.PositiveSmallIntegerField(default=2, help_text='Server Database', verbose_name='db')),
                ('project', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='io_storages_redisexportstorages', to='projects.project')),
            ],
            options={
                'abstract': False,
            },
            bases=('io_storages.redisstoragemixin', models.Model),
        ),
        migrations.CreateModel(
            name='RedisImportStorage',
            fields=[
                ('redisstoragemixin_ptr', models.OneToOneField(auto_created=True, on_delete=django.db.models.deletion.CASCADE, parent_link=True, primary_key=True, serialize=False, to='io_storages.redisstoragemixin')),
                ('title', models.CharField(help_text='Cloud storage title', max_length=256, null=True, verbose_name='title')),
                ('description', models.TextField(blank=True, help_text='Cloud storage description', null=True, verbose_name='description')),
                ('created_at', models.DateTimeField(auto_now_add=True, help_text='Creation time', verbose_name='created at')),
                ('db', models.PositiveSmallIntegerField(default=1, help_text='Server Database', verbose_name='db')),
                ('project', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='io_storages_redisimportstorages', to='projects.project')),
            ],
            options={
                'abstract': False,
            },
            bases=('io_storages.redisstoragemixin', models.Model),
        ),
        migrations.CreateModel(
            name='S3ImportStorageLink',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('key', models.TextField(help_text='External link key', verbose_name='key')),
                ('object_exists', models.BooleanField(default=True, help_text='Whether object under external link still exists', verbose_name='object exists')),
                ('created_at', models.DateTimeField(auto_now_add=True, help_text='Creation time', verbose_name='created at')),
                ('storage', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='links', to='io_storages.s3importstorage')),
                ('task', models.OneToOneField(on_delete=django.db.models.deletion.CASCADE, related_name='io_storages_s3importstoragelink', to='tasks.task')),
            ],
            options={
                'abstract': False,
            },
        ),
        migrations.CreateModel(
            name='S3ExportStorageLink',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('object_exists', models.BooleanField(default=True, help_text='Whether object under external link still exists', verbose_name='object exists')),
                ('created_at', models.DateTimeField(auto_now_add=True, help_text='Creation time', verbose_name='created at')),
                ('annotation', models.OneToOneField(on_delete=django.db.models.deletion.CASCADE, related_name='io_storages_s3exportstoragelink', to='tasks.taskcompletion')),
                ('storage', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='links', to='io_storages.s3exportstorage')),
            ],
            options={
                'abstract': False,
            },
        ),
        migrations.CreateModel(
            name='RedisImportStorageLink',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('key', models.TextField(help_text='External link key', verbose_name='key')),
                ('object_exists', models.BooleanField(default=True, help_text='Whether object under external link still exists', verbose_name='object exists')),
                ('created_at', models.DateTimeField(auto_now_add=True, help_text='Creation time', verbose_name='created at')),
                ('task', models.OneToOneField(on_delete=django.db.models.deletion.CASCADE, related_name='io_storages_redisimportstoragelink', to='tasks.task')),
                ('storage', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='links', to='io_storages.redisimportstorage')),
            ],
            options={
                'abstract': False,
            },
        ),
        migrations.CreateModel(
            name='RedisExportStorageLink',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('object_exists', models.BooleanField(default=True, help_text='Whether object under external link still exists', verbose_name='object exists')),
                ('created_at', models.DateTimeField(auto_now_add=True, help_text='Creation time', verbose_name='created at')),
                ('annotation', models.OneToOneField(on_delete=django.db.models.deletion.CASCADE, related_name='io_storages_redisexportstoragelink', to='tasks.taskcompletion')),
                ('storage', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='links', to='io_storages.redisexportstorage')),
            ],
            options={
                'abstract': False,
            },
        ),
        migrations.CreateModel(
            name='GCSImportStorageLink',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('key', models.TextField(help_text='External link key', verbose_name='key')),
                ('object_exists', models.BooleanField(default=True, help_text='Whether object under external link still exists', verbose_name='object exists')),
                ('created_at', models.DateTimeField(auto_now_add=True, help_text='Creation time', verbose_name='created at')),
                ('task', models.OneToOneField(on_delete=django.db.models.deletion.CASCADE, related_name='io_storages_gcsimportstoragelink', to='tasks.task')),
                ('storage', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='links', to='io_storages.gcsimportstorage')),
            ],
            options={
                'abstract': False,
            },
        ),
        migrations.CreateModel(
            name='GCSExportStorageLink',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('object_exists', models.BooleanField(default=True, help_text='Whether object under external link still exists', verbose_name='object exists')),
                ('created_at', models.DateTimeField(auto_now_add=True, help_text='Creation time', verbose_name='created at')),
                ('annotation', models.OneToOneField(on_delete=django.db.models.deletion.CASCADE, related_name='io_storages_gcsexportstoragelink', to='tasks.taskcompletion')),
                ('storage', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='links', to='io_storages.gcsexportstorage')),
            ],
            options={
                'abstract': False,
            },
        ),
        migrations.CreateModel(
            name='AzureBlobImportStorageLink',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('key', models.TextField(help_text='External link key', verbose_name='key')),
                ('object_exists', models.BooleanField(default=True, help_text='Whether object under external link still exists', verbose_name='object exists')),
                ('created_at', models.DateTimeField(auto_now_add=True, help_text='Creation time', verbose_name='created at')),
                ('task', models.OneToOneField(on_delete=django.db.models.deletion.CASCADE, related_name='io_storages_azureblobimportstoragelink', to='tasks.task')),
                ('storage', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='links', to='io_storages.azureblobimportstorage')),
            ],
            options={
                'abstract': False,
            },
        ),
        migrations.CreateModel(
            name='AzureBlobExportStorageLink',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('object_exists', models.BooleanField(default=True, help_text='Whether object under external link still exists', verbose_name='object exists')),
                ('created_at', models.DateTimeField(auto_now_add=True, help_text='Creation time', verbose_name='created at')),
                ('annotation', models.OneToOneField(on_delete=django.db.models.deletion.CASCADE, related_name='io_storages_azureblobexportstoragelink', to='tasks.taskcompletion')),
                ('storage', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='links', to='io_storages.azureblobexportstorage')),
            ],
            options={
                'abstract': False,
            },
        ),
    ]
</file>

<file path="label_studio/io_storages/migrations/0002_auto_20210311_0530.py">
# Generated by Django 3.1.4 on 2021-03-11 05:30

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('io_storages', '0001_squashed_0002_auto_20210302_1827'),
    ]

    operations = [
        migrations.AddField(
            model_name='azureblobexportstorage',
            name='last_sync',
            field=models.DateTimeField(blank=True, help_text='Last sync finished time', null=True, verbose_name='last sync'),
        ),
        migrations.AddField(
            model_name='azureblobexportstorage',
            name='last_sync_count',
            field=models.PositiveIntegerField(blank=True, help_text='Count of tasks synced last time', null=True, verbose_name='last sync count'),
        ),
        migrations.AddField(
            model_name='azureblobimportstorage',
            name='last_sync',
            field=models.DateTimeField(blank=True, help_text='Last sync finished time', null=True, verbose_name='last sync'),
        ),
        migrations.AddField(
            model_name='azureblobimportstorage',
            name='last_sync_count',
            field=models.PositiveIntegerField(blank=True, help_text='Count of tasks synced last time', null=True, verbose_name='last sync count'),
        ),
        migrations.AddField(
            model_name='gcsexportstorage',
            name='last_sync',
            field=models.DateTimeField(blank=True, help_text='Last sync finished time', null=True, verbose_name='last sync'),
        ),
        migrations.AddField(
            model_name='gcsexportstorage',
            name='last_sync_count',
            field=models.PositiveIntegerField(blank=True, help_text='Count of tasks synced last time', null=True, verbose_name='last sync count'),
        ),
        migrations.AddField(
            model_name='gcsimportstorage',
            name='last_sync',
            field=models.DateTimeField(blank=True, help_text='Last sync finished time', null=True, verbose_name='last sync'),
        ),
        migrations.AddField(
            model_name='gcsimportstorage',
            name='last_sync_count',
            field=models.PositiveIntegerField(blank=True, help_text='Count of tasks synced last time', null=True, verbose_name='last sync count'),
        ),
        migrations.AddField(
            model_name='redisexportstorage',
            name='last_sync',
            field=models.DateTimeField(blank=True, help_text='Last sync finished time', null=True, verbose_name='last sync'),
        ),
        migrations.AddField(
            model_name='redisexportstorage',
            name='last_sync_count',
            field=models.PositiveIntegerField(blank=True, help_text='Count of tasks synced last time', null=True, verbose_name='last sync count'),
        ),
        migrations.AddField(
            model_name='redisimportstorage',
            name='last_sync',
            field=models.DateTimeField(blank=True, help_text='Last sync finished time', null=True, verbose_name='last sync'),
        ),
        migrations.AddField(
            model_name='redisimportstorage',
            name='last_sync_count',
            field=models.PositiveIntegerField(blank=True, help_text='Count of tasks synced last time', null=True, verbose_name='last sync count'),
        ),
        migrations.AddField(
            model_name='s3exportstorage',
            name='last_sync',
            field=models.DateTimeField(blank=True, help_text='Last sync finished time', null=True, verbose_name='last sync'),
        ),
        migrations.AddField(
            model_name='s3exportstorage',
            name='last_sync_count',
            field=models.PositiveIntegerField(blank=True, help_text='Count of tasks synced last time', null=True, verbose_name='last sync count'),
        ),
        migrations.AddField(
            model_name='s3importstorage',
            name='last_sync',
            field=models.DateTimeField(blank=True, help_text='Last sync finished time', null=True, verbose_name='last sync'),
        ),
        migrations.AddField(
            model_name='s3importstorage',
            name='last_sync_count',
            field=models.PositiveIntegerField(blank=True, help_text='Count of tasks synced last time', null=True, verbose_name='last sync count'),
        ),
    ]
</file>

<file path="label_studio/io_storages/migrations/0003_localfilesimportstorage.py">
# Generated by Django 3.1.4 on 2021-03-18 14:46

from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    dependencies = [
        ('projects', '0008_auto_20210314_1840'),
        ('tasks', '0005_auto_20210309_1239'),
        ('io_storages', '0002_auto_20210311_0530'),
    ]

    operations = [
        migrations.CreateModel(
            name='LocalFilesMixin',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('path', models.TextField(blank=True, help_text='Local path', null=True, verbose_name='path')),
                ('regex_filter', models.TextField(blank=True, help_text='Regex for filtering objects', null=True, verbose_name='regex_filter')),
                ('use_blob_urls', models.BooleanField(default=False, help_text='Interpret objects as BLOBs and generate URLs', verbose_name='use_blob_urls')),
            ],
        ),
        migrations.CreateModel(
            name='LocalFilesExportStorage',
            fields=[
                ('localfilesmixin_ptr', models.OneToOneField(auto_created=True, on_delete=django.db.models.deletion.CASCADE, parent_link=True, primary_key=True, serialize=False, to='io_storages.localfilesmixin')),
                ('title', models.CharField(help_text='Cloud storage title', max_length=256, null=True, verbose_name='title')),
                ('description', models.TextField(blank=True, help_text='Cloud storage description', null=True, verbose_name='description')),
                ('created_at', models.DateTimeField(auto_now_add=True, help_text='Creation time', verbose_name='created at')),
                ('last_sync', models.DateTimeField(blank=True, help_text='Last sync finished time', null=True, verbose_name='last sync')),
                ('last_sync_count', models.PositiveIntegerField(blank=True, help_text='Count of tasks synced last time', null=True, verbose_name='last sync count')),
                ('project', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='io_storages_localfilesexportstorages', to='projects.project')),
            ],
            options={
                'abstract': False,
            },
            bases=('io_storages.localfilesmixin', models.Model),
        ),
        migrations.CreateModel(
            name='LocalFilesImportStorage',
            fields=[
                ('localfilesmixin_ptr', models.OneToOneField(auto_created=True, on_delete=django.db.models.deletion.CASCADE, parent_link=True, primary_key=True, serialize=False, to='io_storages.localfilesmixin')),
                ('title', models.CharField(help_text='Cloud storage title', max_length=256, null=True, verbose_name='title')),
                ('description', models.TextField(blank=True, help_text='Cloud storage description', null=True, verbose_name='description')),
                ('created_at', models.DateTimeField(auto_now_add=True, help_text='Creation time', verbose_name='created at')),
                ('last_sync', models.DateTimeField(blank=True, help_text='Last sync finished time', null=True, verbose_name='last sync')),
                ('last_sync_count', models.PositiveIntegerField(blank=True, help_text='Count of tasks synced last time', null=True, verbose_name='last sync count')),
                ('project', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='io_storages_localfilesimportstorages', to='projects.project')),
            ],
            options={
                'abstract': False,
            },
            bases=('io_storages.localfilesmixin', models.Model),
        ),
        migrations.CreateModel(
            name='LocalFilesImportStorageLink',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('key', models.TextField(help_text='External link key', verbose_name='key')),
                ('object_exists', models.BooleanField(default=True, help_text='Whether object under external link still exists', verbose_name='object exists')),
                ('created_at', models.DateTimeField(auto_now_add=True, help_text='Creation time', verbose_name='created at')),
                ('task', models.OneToOneField(on_delete=django.db.models.deletion.CASCADE, related_name='io_storages_localfilesimportstoragelink', to='tasks.task')),
                ('storage', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='links', to='io_storages.localfilesimportstorage')),
            ],
            options={
                'abstract': False,
            },
        ),
        migrations.CreateModel(
            name='LocalFilesExportStorageLink',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('object_exists', models.BooleanField(default=True, help_text='Whether object under external link still exists', verbose_name='object exists')),
                ('created_at', models.DateTimeField(auto_now_add=True, help_text='Creation time', verbose_name='created at')),
                ('annotation', models.OneToOneField(on_delete=django.db.models.deletion.CASCADE, related_name='io_storages_localfilesexportstoragelink', to='tasks.annotation')),
                ('storage', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='links', to='io_storages.localfilesexportstorage')),
            ],
            options={
                'abstract': False,
            },
        ),
    ]
</file>

<file path="label_studio/io_storages/migrations/0004_gcsstoragemixin_google_application_credentials.py">
# Generated by Django 3.1.12 on 2021-07-07 16:51

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('io_storages', '0003_localfilesimportstorage'),
    ]

    operations = [
        migrations.AddField(
            model_name='gcsstoragemixin',
            name='google_application_credentials',
            field=models.TextField(blank=True, help_text='The content of GOOGLE_APPLICATION_CREDENTIALS json file', null=True, verbose_name='google_application_credentials'),
        ),
    ]
</file>

<file path="label_studio/io_storages/migrations/0005_s3importstorage_recursive_scan.py">
# Generated by Django 3.1.12 on 2021-08-11 16:29

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('io_storages', '0004_gcsstoragemixin_google_application_credentials'),
    ]

    operations = [
        migrations.AddField(
            model_name='s3importstorage',
            name='recursive_scan',
            field=models.BooleanField(default=False, help_text='Perform recursive scan over the bucket content', verbose_name='recursive scan'),
        ),
    ]
</file>

<file path="label_studio/io_storages/migrations/0006_auto_20210906_1323.py">
# Generated by Django 3.1.12 on 2021-09-06 13:23

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('io_storages', '0005_s3importstorage_recursive_scan'),
    ]

    operations = [
        migrations.AlterField(
            model_name='azureblobexportstorage',
            name='title',
            field=models.CharField(blank=True, help_text='Cloud storage title', max_length=256, null=True, verbose_name='title'),
        ),
        migrations.AlterField(
            model_name='azureblobimportstorage',
            name='title',
            field=models.CharField(blank=True, help_text='Cloud storage title', max_length=256, null=True, verbose_name='title'),
        ),
        migrations.AlterField(
            model_name='gcsexportstorage',
            name='title',
            field=models.CharField(blank=True, help_text='Cloud storage title', max_length=256, null=True, verbose_name='title'),
        ),
        migrations.AlterField(
            model_name='gcsimportstorage',
            name='title',
            field=models.CharField(blank=True, help_text='Cloud storage title', max_length=256, null=True, verbose_name='title'),
        ),
        migrations.AlterField(
            model_name='localfilesexportstorage',
            name='title',
            field=models.CharField(blank=True, help_text='Cloud storage title', max_length=256, null=True, verbose_name='title'),
        ),
        migrations.AlterField(
            model_name='localfilesimportstorage',
            name='title',
            field=models.CharField(blank=True, help_text='Cloud storage title', max_length=256, null=True, verbose_name='title'),
        ),
        migrations.AlterField(
            model_name='redisexportstorage',
            name='title',
            field=models.CharField(blank=True, help_text='Cloud storage title', max_length=256, null=True, verbose_name='title'),
        ),
        migrations.AlterField(
            model_name='redisimportstorage',
            name='title',
            field=models.CharField(blank=True, help_text='Cloud storage title', max_length=256, null=True, verbose_name='title'),
        ),
        migrations.AlterField(
            model_name='s3exportstorage',
            name='title',
            field=models.CharField(blank=True, help_text='Cloud storage title', max_length=256, null=True, verbose_name='title'),
        ),
        migrations.AlterField(
            model_name='s3importstorage',
            name='title',
            field=models.CharField(blank=True, help_text='Cloud storage title', max_length=256, null=True, verbose_name='title'),
        ),
    ]
</file>

<file path="label_studio/io_storages/migrations/0007_auto_20210928_1252.py">
# Generated by Django 3.1.13 on 2021-09-28 12:52

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('io_storages', '0006_auto_20210906_1323'),
    ]

    operations = [
        migrations.AddField(
            model_name='azureblobexportstorage',
            name='can_delete_objects',
            field=models.BooleanField(blank=True, help_text='Deletion from storage enabled', null=True, verbose_name='can_delete_objects'),
        ),
        migrations.AddField(
            model_name='gcsexportstorage',
            name='can_delete_objects',
            field=models.BooleanField(blank=True, help_text='Deletion from storage enabled', null=True, verbose_name='can_delete_objects'),
        ),
        migrations.AddField(
            model_name='localfilesexportstorage',
            name='can_delete_objects',
            field=models.BooleanField(blank=True, help_text='Deletion from storage enabled', null=True, verbose_name='can_delete_objects'),
        ),
        migrations.AddField(
            model_name='redisexportstorage',
            name='can_delete_objects',
            field=models.BooleanField(blank=True, help_text='Deletion from storage enabled', null=True, verbose_name='can_delete_objects'),
        ),
        migrations.AddField(
            model_name='s3exportstorage',
            name='can_delete_objects',
            field=models.BooleanField(blank=True, help_text='Deletion from storage enabled', null=True, verbose_name='can_delete_objects'),
        ),
    ]
</file>

<file path="label_studio/io_storages/migrations/0008_auto_20211129_1132.py">
# Generated by Django 3.1.13 on 2021-11-29 11:32

from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    dependencies = [
        ('projects', '0013_project_reveal_preannotations_interactively'),
        ('io_storages', '0007_auto_20210928_1252'),
    ]

    operations = [
        migrations.AlterField(
            model_name='azureblobexportstorage',
            name='project',
            field=models.ForeignKey(help_text='A unique integer value identifying this project.', on_delete=django.db.models.deletion.CASCADE, related_name='io_storages_azureblobexportstorages', to='projects.project'),
        ),
        migrations.AlterField(
            model_name='azureblobimportstorage',
            name='project',
            field=models.ForeignKey(help_text='A unique integer value identifying this project.', on_delete=django.db.models.deletion.CASCADE, related_name='io_storages_azureblobimportstorages', to='projects.project'),
        ),
        migrations.AlterField(
            model_name='gcsexportstorage',
            name='project',
            field=models.ForeignKey(help_text='A unique integer value identifying this project.', on_delete=django.db.models.deletion.CASCADE, related_name='io_storages_gcsexportstorages', to='projects.project'),
        ),
        migrations.AlterField(
            model_name='gcsimportstorage',
            name='project',
            field=models.ForeignKey(help_text='A unique integer value identifying this project.', on_delete=django.db.models.deletion.CASCADE, related_name='io_storages_gcsimportstorages', to='projects.project'),
        ),
        migrations.AlterField(
            model_name='localfilesexportstorage',
            name='project',
            field=models.ForeignKey(help_text='A unique integer value identifying this project.', on_delete=django.db.models.deletion.CASCADE, related_name='io_storages_localfilesexportstorages', to='projects.project'),
        ),
        migrations.AlterField(
            model_name='localfilesimportstorage',
            name='project',
            field=models.ForeignKey(help_text='A unique integer value identifying this project.', on_delete=django.db.models.deletion.CASCADE, related_name='io_storages_localfilesimportstorages', to='projects.project'),
        ),
        migrations.AlterField(
            model_name='redisexportstorage',
            name='project',
            field=models.ForeignKey(help_text='A unique integer value identifying this project.', on_delete=django.db.models.deletion.CASCADE, related_name='io_storages_redisexportstorages', to='projects.project'),
        ),
        migrations.AlterField(
            model_name='redisimportstorage',
            name='project',
            field=models.ForeignKey(help_text='A unique integer value identifying this project.', on_delete=django.db.models.deletion.CASCADE, related_name='io_storages_redisimportstorages', to='projects.project'),
        ),
        migrations.AlterField(
            model_name='s3exportstorage',
            name='project',
            field=models.ForeignKey(help_text='A unique integer value identifying this project.', on_delete=django.db.models.deletion.CASCADE, related_name='io_storages_s3exportstorages', to='projects.project'),
        ),
        migrations.AlterField(
            model_name='s3importstorage',
            name='project',
            field=models.ForeignKey(help_text='A unique integer value identifying this project.', on_delete=django.db.models.deletion.CASCADE, related_name='io_storages_s3importstorages', to='projects.project'),
        ),
    ]
</file>

<file path="label_studio/io_storages/migrations/0009_auto_20220310_0922.py">
# Generated by Django 3.1.14 on 2022-03-10 09:22

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('io_storages', '0008_auto_20211129_1132'),
    ]

    operations = [
        migrations.AddField(
            model_name='azureblobexportstoragelink',
            name='updated_at',
            field=models.DateTimeField(auto_now=True, help_text='Update time', verbose_name='updated at'),
        ),
        migrations.AddField(
            model_name='gcsexportstoragelink',
            name='updated_at',
            field=models.DateTimeField(auto_now=True, help_text='Update time', verbose_name='updated at'),
        ),
        migrations.AddField(
            model_name='localfilesexportstoragelink',
            name='updated_at',
            field=models.DateTimeField(auto_now=True, help_text='Update time', verbose_name='updated at'),
        ),
        migrations.AddField(
            model_name='redisexportstoragelink',
            name='updated_at',
            field=models.DateTimeField(auto_now=True, help_text='Update time', verbose_name='updated at'),
        ),
        migrations.AddField(
            model_name='s3exportstoragelink',
            name='updated_at',
            field=models.DateTimeField(auto_now=True, help_text='Update time', verbose_name='updated at'),
        ),
    ]
</file>

<file path="label_studio/io_storages/migrations/0010_auto_20221014_1708.py">
# Generated by Django 3.2.14 on 2022-10-14 14:08

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('io_storages', '0009_auto_20220310_0922'),
    ]

    operations = [
        migrations.AddField(
            model_name='azureblobexportstorage',
            name='last_sync_job',
            field=models.CharField(blank=True, help_text='Last sync job ID', max_length=256, null=True, verbose_name='last_sync_job'),
        ),
        migrations.AddField(
            model_name='azureblobimportstorage',
            name='last_sync_job',
            field=models.CharField(blank=True, help_text='Last sync job ID', max_length=256, null=True, verbose_name='last_sync_job'),
        ),
        migrations.AddField(
            model_name='gcsexportstorage',
            name='last_sync_job',
            field=models.CharField(blank=True, help_text='Last sync job ID', max_length=256, null=True, verbose_name='last_sync_job'),
        ),
        migrations.AddField(
            model_name='gcsimportstorage',
            name='last_sync_job',
            field=models.CharField(blank=True, help_text='Last sync job ID', max_length=256, null=True, verbose_name='last_sync_job'),
        ),
        migrations.AddField(
            model_name='localfilesexportstorage',
            name='last_sync_job',
            field=models.CharField(blank=True, help_text='Last sync job ID', max_length=256, null=True, verbose_name='last_sync_job'),
        ),
        migrations.AddField(
            model_name='localfilesimportstorage',
            name='last_sync_job',
            field=models.CharField(blank=True, help_text='Last sync job ID', max_length=256, null=True, verbose_name='last_sync_job'),
        ),
        migrations.AddField(
            model_name='redisexportstorage',
            name='last_sync_job',
            field=models.CharField(blank=True, help_text='Last sync job ID', max_length=256, null=True, verbose_name='last_sync_job'),
        ),
        migrations.AddField(
            model_name='redisimportstorage',
            name='last_sync_job',
            field=models.CharField(blank=True, help_text='Last sync job ID', max_length=256, null=True, verbose_name='last_sync_job'),
        ),
        migrations.AddField(
            model_name='s3exportstorage',
            name='last_sync_job',
            field=models.CharField(blank=True, help_text='Last sync job ID', max_length=256, null=True, verbose_name='last_sync_job'),
        ),
        migrations.AddField(
            model_name='s3importstorage',
            name='last_sync_job',
            field=models.CharField(blank=True, help_text='Last sync job ID', max_length=256, null=True, verbose_name='last_sync_job'),
        ),
    ]
</file>

<file path="label_studio/io_storages/migrations/0011_gcsstoragemixin_google_project_id.py">
# Generated by Django 3.2.16 on 2022-12-16 13:04

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('io_storages', '0010_auto_20221014_1708'),
    ]

    operations = [
        migrations.AddField(
            model_name='gcsstoragemixin',
            name='google_project_id',
            field=models.TextField(blank=True, help_text='Google project ID', null=True, verbose_name='Google Project ID'),
        ),
    ]
</file>

<file path="label_studio/io_storages/migrations/0012_auto_20230418_1510.py">
# Generated by Django 3.2.16 on 2023-04-18 15:10

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('io_storages', '0011_gcsstoragemixin_google_project_id'),
    ]

    operations = [
        migrations.AddField(
            model_name='azureblobexportstorage',
            name='meta',
            field=models.JSONField(default=dict, help_text='Meta and debug information about storage processes', null=True, verbose_name='meta'),
        ),
        migrations.AddField(
            model_name='azureblobexportstorage',
            name='status',
            field=models.CharField(choices=[('created', 'Created'), ('in_progress', 'In progress'), ('failed', 'Failed'), ('completed', 'Completed')], default='created', max_length=64),
        ),
        migrations.AddField(
            model_name='azureblobexportstorage',
            name='traceback',
            field=models.TextField(blank=True, help_text='Traceback report for the last failed sync', null=True),
        ),
        migrations.AddField(
            model_name='azureblobimportstorage',
            name='meta',
            field=models.JSONField(default=dict, help_text='Meta and debug information about storage processes', null=True, verbose_name='meta'),
        ),
        migrations.AddField(
            model_name='azureblobimportstorage',
            name='status',
            field=models.CharField(choices=[('created', 'Created'), ('in_progress', 'In progress'), ('failed', 'Failed'), ('completed', 'Completed')], default='created', max_length=64),
        ),
        migrations.AddField(
            model_name='azureblobimportstorage',
            name='traceback',
            field=models.TextField(blank=True, help_text='Traceback report for the last failed sync', null=True),
        ),
        migrations.AddField(
            model_name='gcsexportstorage',
            name='meta',
            field=models.JSONField(default=dict, help_text='Meta and debug information about storage processes', null=True, verbose_name='meta'),
        ),
        migrations.AddField(
            model_name='gcsexportstorage',
            name='status',
            field=models.CharField(choices=[('created', 'Created'), ('in_progress', 'In progress'), ('failed', 'Failed'), ('completed', 'Completed')], default='created', max_length=64),
        ),
        migrations.AddField(
            model_name='gcsexportstorage',
            name='traceback',
            field=models.TextField(blank=True, help_text='Traceback report for the last failed sync', null=True),
        ),
        migrations.AddField(
            model_name='gcsimportstorage',
            name='meta',
            field=models.JSONField(default=dict, help_text='Meta and debug information about storage processes', null=True, verbose_name='meta'),
        ),
        migrations.AddField(
            model_name='gcsimportstorage',
            name='status',
            field=models.CharField(choices=[('created', 'Created'), ('in_progress', 'In progress'), ('failed', 'Failed'), ('completed', 'Completed')], default='created', max_length=64),
        ),
        migrations.AddField(
            model_name='gcsimportstorage',
            name='traceback',
            field=models.TextField(blank=True, help_text='Traceback report for the last failed sync', null=True),
        ),
        migrations.AddField(
            model_name='localfilesexportstorage',
            name='meta',
            field=models.JSONField(default=dict, help_text='Meta and debug information about storage processes', null=True, verbose_name='meta'),
        ),
        migrations.AddField(
            model_name='localfilesexportstorage',
            name='status',
            field=models.CharField(choices=[('created', 'Created'), ('in_progress', 'In progress'), ('failed', 'Failed'), ('completed', 'Completed')], default='created', max_length=64),
        ),
        migrations.AddField(
            model_name='localfilesexportstorage',
            name='traceback',
            field=models.TextField(blank=True, help_text='Traceback report for the last failed sync', null=True),
        ),
        migrations.AddField(
            model_name='localfilesimportstorage',
            name='meta',
            field=models.JSONField(default=dict, help_text='Meta and debug information about storage processes', null=True, verbose_name='meta'),
        ),
        migrations.AddField(
            model_name='localfilesimportstorage',
            name='status',
            field=models.CharField(choices=[('created', 'Created'), ('in_progress', 'In progress'), ('failed', 'Failed'), ('completed', 'Completed')], default='created', max_length=64),
        ),
        migrations.AddField(
            model_name='localfilesimportstorage',
            name='traceback',
            field=models.TextField(blank=True, help_text='Traceback report for the last failed sync', null=True),
        ),
        migrations.AddField(
            model_name='redisexportstorage',
            name='meta',
            field=models.JSONField(default=dict, help_text='Meta and debug information about storage processes', null=True, verbose_name='meta'),
        ),
        migrations.AddField(
            model_name='redisexportstorage',
            name='status',
            field=models.CharField(choices=[('created', 'Created'), ('in_progress', 'In progress'), ('failed', 'Failed'), ('completed', 'Completed')], default='created', max_length=64),
        ),
        migrations.AddField(
            model_name='redisexportstorage',
            name='traceback',
            field=models.TextField(blank=True, help_text='Traceback report for the last failed sync', null=True),
        ),
        migrations.AddField(
            model_name='redisimportstorage',
            name='meta',
            field=models.JSONField(default=dict, help_text='Meta and debug information about storage processes', null=True, verbose_name='meta'),
        ),
        migrations.AddField(
            model_name='redisimportstorage',
            name='status',
            field=models.CharField(choices=[('created', 'Created'), ('in_progress', 'In progress'), ('failed', 'Failed'), ('completed', 'Completed')], default='created', max_length=64),
        ),
        migrations.AddField(
            model_name='redisimportstorage',
            name='traceback',
            field=models.TextField(blank=True, help_text='Traceback report for the last failed sync', null=True),
        ),
        migrations.AddField(
            model_name='s3exportstorage',
            name='meta',
            field=models.JSONField(default=dict, help_text='Meta and debug information about storage processes', null=True, verbose_name='meta'),
        ),
        migrations.AddField(
            model_name='s3exportstorage',
            name='status',
            field=models.CharField(choices=[('created', 'Created'), ('in_progress', 'In progress'), ('failed', 'Failed'), ('completed', 'Completed')], default='created', max_length=64),
        ),
        migrations.AddField(
            model_name='s3exportstorage',
            name='traceback',
            field=models.TextField(blank=True, help_text='Traceback report for the last failed sync', null=True),
        ),
        migrations.AddField(
            model_name='s3importstorage',
            name='meta',
            field=models.JSONField(default=dict, help_text='Meta and debug information about storage processes', null=True, verbose_name='meta'),
        ),
        migrations.AddField(
            model_name='s3importstorage',
            name='status',
            field=models.CharField(choices=[('created', 'Created'), ('in_progress', 'In progress'), ('failed', 'Failed'), ('completed', 'Completed')], default='created', max_length=64),
        ),
        migrations.AddField(
            model_name='s3importstorage',
            name='traceback',
            field=models.TextField(blank=True, help_text='Traceback report for the last failed sync', null=True),
        ),
    ]
</file>

<file path="label_studio/io_storages/migrations/0013_auto_20230420_0259.py">
# Generated by Django 3.2.16 on 2023-04-20 02:59

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('io_storages', '0012_auto_20230418_1510'),
    ]

    operations = [
        migrations.AlterField(
            model_name='azureblobexportstorage',
            name='status',
            field=models.CharField(choices=[('initialized', 'Initialized'), ('queued', 'Queued'), ('in_progress', 'In progress'), ('failed', 'Failed'), ('completed', 'Completed')], default='initialized', max_length=64),
        ),
        migrations.AlterField(
            model_name='azureblobimportstorage',
            name='status',
            field=models.CharField(choices=[('initialized', 'Initialized'), ('queued', 'Queued'), ('in_progress', 'In progress'), ('failed', 'Failed'), ('completed', 'Completed')], default='initialized', max_length=64),
        ),
        migrations.AlterField(
            model_name='gcsexportstorage',
            name='status',
            field=models.CharField(choices=[('initialized', 'Initialized'), ('queued', 'Queued'), ('in_progress', 'In progress'), ('failed', 'Failed'), ('completed', 'Completed')], default='initialized', max_length=64),
        ),
        migrations.AlterField(
            model_name='gcsimportstorage',
            name='status',
            field=models.CharField(choices=[('initialized', 'Initialized'), ('queued', 'Queued'), ('in_progress', 'In progress'), ('failed', 'Failed'), ('completed', 'Completed')], default='initialized', max_length=64),
        ),
        migrations.AlterField(
            model_name='localfilesexportstorage',
            name='status',
            field=models.CharField(choices=[('initialized', 'Initialized'), ('queued', 'Queued'), ('in_progress', 'In progress'), ('failed', 'Failed'), ('completed', 'Completed')], default='initialized', max_length=64),
        ),
        migrations.AlterField(
            model_name='localfilesimportstorage',
            name='status',
            field=models.CharField(choices=[('initialized', 'Initialized'), ('queued', 'Queued'), ('in_progress', 'In progress'), ('failed', 'Failed'), ('completed', 'Completed')], default='initialized', max_length=64),
        ),
        migrations.AlterField(
            model_name='redisexportstorage',
            name='status',
            field=models.CharField(choices=[('initialized', 'Initialized'), ('queued', 'Queued'), ('in_progress', 'In progress'), ('failed', 'Failed'), ('completed', 'Completed')], default='initialized', max_length=64),
        ),
        migrations.AlterField(
            model_name='redisimportstorage',
            name='status',
            field=models.CharField(choices=[('initialized', 'Initialized'), ('queued', 'Queued'), ('in_progress', 'In progress'), ('failed', 'Failed'), ('completed', 'Completed')], default='initialized', max_length=64),
        ),
        migrations.AlterField(
            model_name='s3exportstorage',
            name='status',
            field=models.CharField(choices=[('initialized', 'Initialized'), ('queued', 'Queued'), ('in_progress', 'In progress'), ('failed', 'Failed'), ('completed', 'Completed')], default='initialized', max_length=64),
        ),
        migrations.AlterField(
            model_name='s3importstorage',
            name='status',
            field=models.CharField(choices=[('initialized', 'Initialized'), ('queued', 'Queued'), ('in_progress', 'In progress'), ('failed', 'Failed'), ('completed', 'Completed')], default='initialized', max_length=64),
        ),
    ]
</file>

<file path="label_studio/io_storages/migrations/0014_init_statuses.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import logging
from django.db import migrations

logger = logging.getLogger(__name__)


def update_storage(storage):
    logger.info(f'=> Migration for {storage._meta.label} statuses started')
    storage.objects.update(status='initialized')
    instances = list(storage.objects.all().only('id', 'meta', 'status', 'last_sync_count'))

    for instance in instances:
        prefix = f'Project ID={instance.project.id} {instance}'

        # import source storages
        if 'import' in storage._meta.label_lower:
            count = instance.links.count() - instance.last_sync_count if instance.last_sync_count else 0
            instance.meta['tasks_existed'] = count if count > 0 else 0
            if instance.meta['tasks_existed'] and instance.meta['tasks_existed'] > 0:
                instance.status = 'completed'
            logger.info(f'{prefix} tasks_existed = {instance.meta["tasks_existed"]}')

        # target export storages
        else:
            instance.meta['total_annotations'] = instance.last_sync_count
            if instance.last_sync_count and instance.last_sync_count > 0:
                instance.status = 'completed'
            logger.info(f'{prefix} total_annotations = {instance.last_sync_count}')

    storage.objects.bulk_update(instances, fields=['meta', 'status'], batch_size=100)
    logger.info(f'=> Migration for {storage._meta.label} statuses finished')


def forwards(apps, schema_editor):
    storages = [
        apps.get_model('io_storages', 'AzureBlobImportStorage'),
        apps.get_model('io_storages', 'AzureBlobExportStorage'),
        apps.get_model('io_storages', 'GCSImportStorage'),
        apps.get_model('io_storages', 'GCSExportStorage'),
        apps.get_model('io_storages', 'LocalFilesImportStorage'),
        apps.get_model('io_storages', 'LocalFilesExportStorage'),
        apps.get_model('io_storages', 'RedisImportStorage'),
        apps.get_model('io_storages', 'RedisExportStorage'),
        apps.get_model('io_storages', 'S3ImportStorage'),
        apps.get_model('io_storages', 'S3ExportStorage'),
    ]

    for storage in storages:
        update_storage(storage)


def backwards(apps, schema_editor):
    pass


class Migration(migrations.Migration):
    dependencies = [
        ('io_storages', '0013_auto_20230420_0259'),
    ]

    operations = [
        migrations.RunPython(forwards, backwards),
    ]
</file>

<file path="label_studio/io_storages/migrations/0015_auto_20230804_1732.py">
# Generated by Django 3.2.19 on 2023-08-04 14:32

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('io_storages', '0014_init_statuses'),
    ]

    operations = [
        migrations.AddField(
            model_name='azureblobexportstorage',
            name='synchronizable',
            field=models.BooleanField(default=True, help_text='If storage can be synced', verbose_name='synchronizable'),
        ),
        migrations.AddField(
            model_name='azureblobimportstorage',
            name='synchronizable',
            field=models.BooleanField(default=True, help_text='If storage can be synced', verbose_name='synchronizable'),
        ),
        migrations.AddField(
            model_name='gcsexportstorage',
            name='synchronizable',
            field=models.BooleanField(default=True, help_text='If storage can be synced', verbose_name='synchronizable'),
        ),
        migrations.AddField(
            model_name='gcsimportstorage',
            name='synchronizable',
            field=models.BooleanField(default=True, help_text='If storage can be synced', verbose_name='synchronizable'),
        ),
        migrations.AddField(
            model_name='localfilesexportstorage',
            name='synchronizable',
            field=models.BooleanField(default=True, help_text='If storage can be synced', verbose_name='synchronizable'),
        ),
        migrations.AddField(
            model_name='localfilesimportstorage',
            name='synchronizable',
            field=models.BooleanField(default=True, help_text='If storage can be synced', verbose_name='synchronizable'),
        ),
        migrations.AddField(
            model_name='redisexportstorage',
            name='synchronizable',
            field=models.BooleanField(default=True, help_text='If storage can be synced', verbose_name='synchronizable'),
        ),
        migrations.AddField(
            model_name='redisimportstorage',
            name='synchronizable',
            field=models.BooleanField(default=True, help_text='If storage can be synced', verbose_name='synchronizable'),
        ),
        migrations.AddField(
            model_name='s3exportstorage',
            name='synchronizable',
            field=models.BooleanField(default=True, help_text='If storage can be synced', verbose_name='synchronizable'),
        ),
        migrations.AddField(
            model_name='s3importstorage',
            name='synchronizable',
            field=models.BooleanField(default=True, help_text='If storage can be synced', verbose_name='synchronizable'),
        ),
    ]
</file>

<file path="label_studio/io_storages/migrations/0016_add_aws_sse_kms_key.py">
# Generated by Django 3.2.19 on 2023-07-22 00:12

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('io_storages', '0015_auto_20230804_1732'),
    ]

    operations = [
        migrations.AddField(
            model_name='s3exportstorage',
            name='aws_sse_kms_key_id',
            field=models.TextField(blank=True, help_text='AWS SSE KMS Key ID', null=True, verbose_name='aws_sse_kms_key_id'),
        ),
        migrations.AddField(
            model_name='s3importstorage',
            name='aws_sse_kms_key_id',
            field=models.TextField(blank=True, help_text='AWS SSE KMS Key ID', null=True, verbose_name='aws_sse_kms_key_id'),
        ),
    ]
</file>

<file path="label_studio/io_storages/migrations/0017_auto_20240731_1638.py">
# Generated by Django 3.2.25 on 2024-07-31 16:38

from django.conf import settings
from django.db import migrations, models
from django.db.migrations.operations.special import RunSQL
from django.db.migrations.operations.base import Operation
from django.db import connection
from core.redis import start_job_async_or_sync
from core.models import AsyncMigrationStatus
import logging
logger = logging.getLogger(__name__)


IS_SQLITE = connection.vendor == 'sqlite'
migration_name = '0017_auto_20240731_1638'

def create_index_sql(table_name, index_name, column_name):
    return f"""
    CREATE INDEX CONCURRENTLY IF NOT EXISTS "{index_name}" ON "{table_name}" ("{column_name}");
    """

def create_fk_sql(table_name, constraint_name, column_name, referenced_table, referenced_column):
    return f"""
    ALTER TABLE "{table_name}" DROP CONSTRAINT IF EXISTS "{constraint_name}";
    ALTER TABLE "{table_name}" ADD CONSTRAINT "{constraint_name}" FOREIGN KEY ("{column_name}") REFERENCES "{referenced_table}" ("{referenced_column}") DEFERRABLE INITIALLY DEFERRED;
    """

def drop_index_sql(table_name, index_name, column_name):
    return f"""
    DROP INDEX CONCURRENTLY IF EXISTS "{index_name}";
    """

tables = [
    {
        "model_name": "azureblobexportstoragelink",
        "table_name": "io_storages_azureblobexportstoragelink",
        "index_name": "io_storages_azureblobexportstoragelink_annotation_id_6cc15c83",
        "fk_constraint": "io_storages_azureblo_annotation_id_6cc15c83_fk_task_comp",
        "column_name": "annotation_id"
    },
    {
        "model_name": "gcsexportstoragelink",
        "table_name": "io_storages_gcsexportstoragelink",
        "index_name": "io_storages_gcsexportstoragelink_annotation_id_2df715a6",
        "fk_constraint": "io_storages_gcsexpor_annotation_id_2df715a6_fk_task_comp",
        "column_name": "annotation_id"
    },
    {
        "model_name": "localfilesexportstoragelink",
        "table_name": "io_storages_localfilesexportstoragelink",
        "index_name": "io_storages_localfilesexportstoragelink_annotation_id_fc4f9825",
        "fk_constraint": "io_storages_localfil_annotation_id_fc4f9825_fk_task_comp",
        "column_name": "annotation_id"
    },
    {
        "model_name": "redisexportstoragelink",
        "table_name": "io_storages_redisexportstoragelink",
        "index_name": "io_storages_redisexportstoragelink_annotation_id_8547e508",
        "fk_constraint": "io_storages_redisexp_annotation_id_8547e508_fk_task_comp",
        "column_name": "annotation_id"
    },
    {
        "model_name": "s3exportstoragelink",
        "table_name": "io_storages_s3exportstoragelink",
        "index_name": "io_storages_s3exportstoragelink_annotation_id_729994fe",
        "fk_constraint": "io_storages_s3export_annotation_id_729994fe_fk_task_comp",
        "column_name": "annotation_id"
    }
]


def forward_migration(migration_name):
    migration = AsyncMigrationStatus.objects.create(
        name=migration_name,
        status=AsyncMigrationStatus.STATUS_STARTED,
    )
    logger.debug(
        f'Start async migration {migration_name}'
    )

    # Get db cursor
    cursor = connection.cursor()
    for table in tables:
        index_sql = create_index_sql(table['table_name'], table['index_name'], table['column_name'])
        fk_sql = create_fk_sql(table['table_name'], table['fk_constraint'], table['column_name'], "task_completion",
                               "id")

        # Run index_sql
        cursor.execute(index_sql)
        cursor.execute(fk_sql)

    migration.status = AsyncMigrationStatus.STATUS_FINISHED
    migration.save()
    logger.debug(
        f'Async migration {migration_name} complete'
    )

def reverse_migration(migration_name):
    migration = AsyncMigrationStatus.objects.create(
        name=migration_name,
        status=AsyncMigrationStatus.STATUS_STARTED,
    )
    logger.debug(
        f'Start async migration {migration_name}'
    )

    # Get db cursor
    cursor = connection.cursor()
    for table in tables:
        reverse_sql = drop_index_sql(table['table_name'], table['index_name'], table['column_name'])
        # Run reverse_sql
        cursor.execute(reverse_sql)

    migration.status = AsyncMigrationStatus.STATUS_FINISHED
    migration.save()
    logger.debug(
        f'Async migration {migration_name} complete'
    )


def forwards(apps, schema_editor):
    # Dispatch migrations to rqworkers
    start_job_async_or_sync(forward_migration, migration_name=migration_name)


def backwards(apps, schema_editor):
    start_job_async_or_sync(reverse_migration, migration_name=migration_name)


def get_operations():
    logger.info(f'IS_SQLITE: {IS_SQLITE}')
    if not IS_SQLITE:
        return [
            migrations.RunPython(forwards, backwards),
        ]

    operations = []
    # Use standard migration for SQLITE
    for table in tables:
        operations.append(
            migrations.AlterField(
                model_name=table['model_name'],
                name='annotation',
                field=models.ForeignKey(on_delete=models.deletion.CASCADE,
                                        related_name=table['table_name'],
                                        to='tasks.annotation'),
            ),
        )
    return operations


class Migration(migrations.Migration):
    atomic = False

    dependencies = [
        ('tasks', '0047_merge_20240318_2210'),
        ('io_storages', '0016_add_aws_sse_kms_key'),
    ]

    operations = get_operations()
</file>

<file path="label_studio/io_storages/migrations/0018_alter_azureblobexportstorage_project_and_more.py">
# Generated by Django 4.2.13 on 2024-08-13 19:51
import logging

from django.db import migrations, models
import django.db.models.deletion
from django.db import connection
from django.conf import settings

logger = logging.getLogger(__name__)

IS_SQLITE = connection.vendor == 'sqlite'


class Migration(migrations.Migration):

    dependencies = [
        ("tasks", "0047_merge_20240318_2210"),
        ("projects", "0026_auto_20231103_0020"),
        ("io_storages", "0017_auto_20240731_1638"),
    ]

    operations = [
        migrations.AlterField(
            model_name="azureblobexportstorage",
            name="project",
            field=models.ForeignKey(
                help_text="A unique integer value identifying this project.",
                on_delete=django.db.models.deletion.CASCADE,
                related_name="%(app_label)s_%(class)ss",
                to="projects.project",
            ),
        ),
        migrations.AlterField(
            model_name="azureblobexportstoragelink",
            name="annotation",
            field=models.ForeignKey(
                on_delete=django.db.models.deletion.CASCADE,
                related_name="%(app_label)s_%(class)s",
                to="tasks.annotation",
            ),
        ),
        migrations.AlterField(
            model_name="azureblobimportstorage",
            name="project",
            field=models.ForeignKey(
                help_text="A unique integer value identifying this project.",
                on_delete=django.db.models.deletion.CASCADE,
                related_name="%(app_label)s_%(class)ss",
                to="projects.project",
            ),
        ),
        migrations.AlterField(
            model_name="azureblobimportstoragelink",
            name="task",
            field=models.OneToOneField(
                on_delete=django.db.models.deletion.CASCADE,
                related_name="%(app_label)s_%(class)s",
                to="tasks.task",
            ),
        ),
        migrations.AlterField(
            model_name="gcsexportstorage",
            name="project",
            field=models.ForeignKey(
                help_text="A unique integer value identifying this project.",
                on_delete=django.db.models.deletion.CASCADE,
                related_name="%(app_label)s_%(class)ss",
                to="projects.project",
            ),
        ),
        migrations.AlterField(
            model_name="gcsexportstoragelink",
            name="annotation",
            field=models.ForeignKey(
                on_delete=django.db.models.deletion.CASCADE,
                related_name="%(app_label)s_%(class)s",
                to="tasks.annotation",
            ),
        ),
        migrations.AlterField(
            model_name="gcsimportstorage",
            name="project",
            field=models.ForeignKey(
                help_text="A unique integer value identifying this project.",
                on_delete=django.db.models.deletion.CASCADE,
                related_name="%(app_label)s_%(class)ss",
                to="projects.project",
            ),
        ),
        migrations.AlterField(
            model_name="gcsimportstoragelink",
            name="task",
            field=models.OneToOneField(
                on_delete=django.db.models.deletion.CASCADE,
                related_name="%(app_label)s_%(class)s",
                to="tasks.task",
            ),
        ),
        migrations.AlterField(
            model_name="localfilesexportstorage",
            name="project",
            field=models.ForeignKey(
                help_text="A unique integer value identifying this project.",
                on_delete=django.db.models.deletion.CASCADE,
                related_name="%(app_label)s_%(class)ss",
                to="projects.project",
            ),
        ),
        migrations.AlterField(
            model_name="localfilesexportstoragelink",
            name="annotation",
            field=models.ForeignKey(
                on_delete=django.db.models.deletion.CASCADE,
                related_name="%(app_label)s_%(class)s",
                to="tasks.annotation",
            ),
        ),
        migrations.AlterField(
            model_name="localfilesimportstorage",
            name="project",
            field=models.ForeignKey(
                help_text="A unique integer value identifying this project.",
                on_delete=django.db.models.deletion.CASCADE,
                related_name="%(app_label)s_%(class)ss",
                to="projects.project",
            ),
        ),
        migrations.AlterField(
            model_name="localfilesimportstoragelink",
            name="task",
            field=models.OneToOneField(
                on_delete=django.db.models.deletion.CASCADE,
                related_name="%(app_label)s_%(class)s",
                to="tasks.task",
            ),
        ),
        migrations.AlterField(
            model_name="redisexportstorage",
            name="project",
            field=models.ForeignKey(
                help_text="A unique integer value identifying this project.",
                on_delete=django.db.models.deletion.CASCADE,
                related_name="%(app_label)s_%(class)ss",
                to="projects.project",
            ),
        ),
        migrations.AlterField(
            model_name="redisexportstoragelink",
            name="annotation",
            field=models.ForeignKey(
                on_delete=django.db.models.deletion.CASCADE,
                related_name="%(app_label)s_%(class)s",
                to="tasks.annotation",
            ),
        ),
        migrations.AlterField(
            model_name="redisimportstorage",
            name="project",
            field=models.ForeignKey(
                help_text="A unique integer value identifying this project.",
                on_delete=django.db.models.deletion.CASCADE,
                related_name="%(app_label)s_%(class)ss",
                to="projects.project",
            ),
        ),
        migrations.AlterField(
            model_name="redisimportstoragelink",
            name="task",
            field=models.OneToOneField(
                on_delete=django.db.models.deletion.CASCADE,
                related_name="%(app_label)s_%(class)s",
                to="tasks.task",
            ),
        ),
        migrations.AlterField(
            model_name="s3exportstorage",
            name="project",
            field=models.ForeignKey(
                help_text="A unique integer value identifying this project.",
                on_delete=django.db.models.deletion.CASCADE,
                related_name="%(app_label)s_%(class)ss",
                to="projects.project",
            ),
        ),
        migrations.AlterField(
            model_name="s3exportstoragelink",
            name="annotation",
            field=models.ForeignKey(
                on_delete=django.db.models.deletion.CASCADE,
                related_name="%(app_label)s_%(class)s",
                to="tasks.annotation",
            ),
        ),
        migrations.AlterField(
            model_name="s3importstorage",
            name="project",
            field=models.ForeignKey(
                help_text="A unique integer value identifying this project.",
                on_delete=django.db.models.deletion.CASCADE,
                related_name="%(app_label)s_%(class)ss",
                to="projects.project",
            ),
        ),
        migrations.AlterField(
            model_name="s3importstoragelink",
            name="task",
            field=models.OneToOneField(
                on_delete=django.db.models.deletion.CASCADE,
                related_name="%(app_label)s_%(class)s",
                to="tasks.task",
            ),
        ),
    ]

    def apply(self, project_state, schema_editor, collect_sql=False):
        if IS_SQLITE and not settings.LSE_PROJECT:
            # Migration should be a no op after 0017 but we're forcing the no-op here because
            # The migration thinks there's a difference
            logger.info('Trying to apply default migrations')
            return super().apply(project_state, schema_editor, collect_sql)
        logger.info('Skipping migrations for LSE project or Postgres database')
        return project_state

    def unapply(self, project_state, schema_editor, collect_sql=False):
        if not IS_SQLITE:
            # Same as above. Reverting a noop migration is noop
            return project_state

        return super().unapply(project_state, schema_editor, collect_sql)
</file>

<file path="label_studio/io_storages/redis/__init__.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
</file>

<file path="label_studio/io_storages/redis/api.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
from django.utils.decorators import method_decorator
from drf_yasg import openapi as openapi
from drf_yasg.utils import no_body, swagger_auto_schema
from io_storages.api import (
    ExportStorageDetailAPI,
    ExportStorageFormLayoutAPI,
    ExportStorageListAPI,
    ExportStorageSyncAPI,
    ExportStorageValidateAPI,
    ImportStorageDetailAPI,
    ImportStorageFormLayoutAPI,
    ImportStorageListAPI,
    ImportStorageValidateAPI,
)
from io_storages.redis.models import RedisExportStorage, RedisImportStorage
from io_storages.redis.serializers import RedisExportStorageSerializer, RedisImportStorageSerializer

from .openapi_schema import (
    _redis_export_storage_schema,
    _redis_export_storage_schema_with_id,
    _redis_import_storage_schema,
    _redis_import_storage_schema_with_id,
)


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Storage: Redis'],
        x_fern_sdk_group_name=['import_storage', 'redis'],
        x_fern_sdk_method_name='list',
        x_fern_audiences=['public'],
        operation_summary='Get all import storage',
        operation_description='Get a list of all Redis import storage connections.',
        manual_parameters=[
            openapi.Parameter(
                name='project',
                type=openapi.TYPE_INTEGER,
                in_=openapi.IN_QUERY,
                description='Project ID',
            ),
        ],
        request_body=no_body,
    ),
)
@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Storage: Redis'],
        x_fern_sdk_group_name=['import_storage', 'redis'],
        x_fern_sdk_method_name='create',
        x_fern_audiences=['public'],
        operation_summary='Create import storage',
        operation_description='Create a new Redis import storage connection.',
        request_body=_redis_import_storage_schema,
    ),
)
class RedisImportStorageListAPI(ImportStorageListAPI):
    queryset = RedisImportStorage.objects.all()
    serializer_class = RedisImportStorageSerializer


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Storage: Redis'],
        x_fern_sdk_group_name=['import_storage', 'redis'],
        x_fern_sdk_method_name='get',
        x_fern_audiences=['public'],
        operation_summary='Get import storage',
        operation_description='Get a specific Redis import storage connection.',
        request_body=no_body,
    ),
)
@method_decorator(
    name='patch',
    decorator=swagger_auto_schema(
        tags=['Storage: Redis'],
        x_fern_sdk_group_name=['import_storage', 'redis'],
        x_fern_sdk_method_name='update',
        x_fern_audiences=['public'],
        operation_summary='Update import storage',
        operation_description='Update a specific Redis import storage connection.',
        request_body=_redis_import_storage_schema,
    ),
)
@method_decorator(
    name='delete',
    decorator=swagger_auto_schema(
        tags=['Storage: Redis'],
        x_fern_sdk_group_name=['import_storage', 'redis'],
        x_fern_sdk_method_name='delete',
        x_fern_audiences=['public'],
        operation_summary='Delete import storage',
        operation_description='Delete a specific Redis import storage connection.',
        request_body=no_body,
    ),
)
class RedisImportStorageDetailAPI(ImportStorageDetailAPI):
    queryset = RedisImportStorage.objects.all()
    serializer_class = RedisImportStorageSerializer


@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Storage: Redis'],
        x_fern_sdk_group_name=['import_storage', 'redis'],
        x_fern_sdk_method_name='sync',
        x_fern_audiences=['public'],
        operation_summary='Sync import storage',
        operation_description='Sync tasks from a specific Redis import storage connection.',
        manual_parameters=[
            openapi.Parameter(
                name='id',
                type=openapi.TYPE_INTEGER,
                in_=openapi.IN_PATH,
                description='Storage ID',
            ),
        ],
        request_body=no_body,
    ),
)
class RedisImportStorageSyncAPI(ExportStorageSyncAPI):
    serializer_class = RedisImportStorageSerializer


@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Storage: Redis'],
        x_fern_sdk_group_name=['export_storage', 'redis'],
        x_fern_sdk_method_name='sync',
        x_fern_audiences=['public'],
        operation_summary='Sync export storage',
        operation_description='Sync tasks from a specific Redis export storage connection.',
        request_body=no_body,
    ),
)
class RedisExportStorageSyncAPI(ExportStorageSyncAPI):
    serializer_class = RedisExportStorageSerializer


@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Storage: Redis'],
        x_fern_sdk_group_name=['import_storage', 'redis'],
        x_fern_sdk_method_name='validate',
        x_fern_audiences=['public'],
        operation_summary='Validate import storage',
        operation_description='Validate a specific Redis import storage connection.',
        request_body=_redis_import_storage_schema_with_id,
        responses={200: openapi.Response(description='Validation successful')},
    ),
)
class RedisImportStorageValidateAPI(ImportStorageValidateAPI):
    serializer_class = RedisImportStorageSerializer


@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Storage: Redis'],
        x_fern_sdk_group_name=['export_storage', 'redis'],
        x_fern_sdk_method_name='validate',
        x_fern_audiences=['public'],
        operation_summary='Validate export storage',
        operation_description='Validate a specific Redis export storage connection.',
        request_body=_redis_export_storage_schema_with_id,
        responses={200: openapi.Response(description='Validation successful')},
    ),
)
class RedisExportStorageValidateAPI(ExportStorageValidateAPI):
    serializer_class = RedisExportStorageSerializer


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Storage: Redis'],
        x_fern_sdk_group_name=['export_storage', 'redis'],
        x_fern_sdk_method_name='list',
        x_fern_audiences=['public'],
        operation_summary='Get all export storage',
        operation_description='Get a list of all Redis export storage connections.',
        manual_parameters=[
            openapi.Parameter(
                name='project',
                type=openapi.TYPE_INTEGER,
                in_=openapi.IN_QUERY,
                description='Project ID',
            ),
        ],
        request_body=no_body,
    ),
)
@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Storage: Redis'],
        x_fern_sdk_group_name=['export_storage', 'redis'],
        x_fern_sdk_method_name='create',
        x_fern_audiences=['public'],
        operation_summary='Create export storage',
        operation_description='Create a new Redis export storage connection to store annotations.',
        request_body=_redis_export_storage_schema,
    ),
)
class RedisExportStorageListAPI(ExportStorageListAPI):
    queryset = RedisExportStorage.objects.all()
    serializer_class = RedisExportStorageSerializer


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Storage: Redis'],
        x_fern_sdk_group_name=['export_storage', 'redis'],
        x_fern_sdk_method_name='get',
        x_fern_audiences=['public'],
        operation_summary='Get export storage',
        operation_description='Get a specific Redis export storage connection.',
        request_body=no_body,
    ),
)
@method_decorator(
    name='patch',
    decorator=swagger_auto_schema(
        tags=['Storage: Redis'],
        x_fern_sdk_group_name=['export_storage', 'redis'],
        x_fern_sdk_method_name='update',
        x_fern_audiences=['public'],
        operation_summary='Update export storage',
        operation_description='Update a specific Redis export storage connection.',
        request_body=_redis_export_storage_schema,
    ),
)
@method_decorator(
    name='delete',
    decorator=swagger_auto_schema(
        tags=['Storage: Redis'],
        x_fern_sdk_group_name=['export_storage', 'redis'],
        x_fern_sdk_method_name='delete',
        x_fern_audiences=['public'],
        operation_summary='Delete export storage',
        operation_description='Delete a specific Redis export storage connection.',
        request_body=no_body,
    ),
)
class RedisExportStorageDetailAPI(ExportStorageDetailAPI):
    queryset = RedisExportStorage.objects.all()
    serializer_class = RedisExportStorageSerializer


class RedisImportStorageFormLayoutAPI(ImportStorageFormLayoutAPI):
    pass


class RedisExportStorageFormLayoutAPI(ExportStorageFormLayoutAPI):
    pass
</file>

<file path="label_studio/io_storages/redis/models.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import json
import logging

import redis
from django.db import models
from django.db.models.signals import post_save
from django.dispatch import receiver
from django.utils.translation import gettext_lazy as _
from io_storages.base_models import (
    ExportStorage,
    ExportStorageLink,
    ImportStorage,
    ImportStorageLink,
    ProjectStorageMixin,
)
from tasks.models import Annotation

logger = logging.getLogger(__name__)


class RedisStorageMixin(models.Model):
    path = models.TextField(_('path'), null=True, blank=True, help_text='Storage prefix (optional)')
    host = models.TextField(_('host'), null=True, blank=True, help_text='Server Host IP (optional)')
    port = models.TextField(_('port'), null=True, blank=True, help_text='Server Port (optional)')
    password = models.TextField(_('password'), null=True, blank=True, help_text='Server Password (optional)')
    regex_filter = models.TextField(
        _('port'), null=True, blank=True, help_text='Cloud storage regex for filtering objects'
    )
    use_blob_urls = models.BooleanField(
        _('use_blob_urls'), default=False, help_text='Interpret objects as BLOBs and generate URLs'
    )

    def get_redis_connection(self, db=None, redis_config={}):
        """Get a redis connection from the provided arguments.

        Args:
            db (int): Database ID of database to use. This needs to
                      always be provided to prevent accidental overwrite
                      to a default value. Therefore, the default is None,
                      but raises an error if not provided.
            redis_config (dict, optional): Further redis configuration.

        Returns:
            redis.StrictRedis object with connection to database.
        """
        if not db:
            # This should never happen, but better to check than to accidentally
            # overwrite an existing database by choosing a wrong default:
            raise ValueError(
                'Please explicitly pass a redis db id to prevent accidentally overwriting existing database!'
            )

        # Since tasks are always text, we use StrictRedis with utf-8 decoding.
        r = redis.StrictRedis(db=db, charset='utf-8', decode_responses=True, **redis_config)
        # Test connection
        # (this will raise redis.exceptions.ConnectionError if it cannot connect)
        r.ping()
        return r

    def get_client(self):
        redis_config = {}
        if self.host:
            redis_config['host'] = self.host
        if self.port:
            redis_config['port'] = self.port
        if self.password:
            redis_config['password'] = self.password

        return self.get_redis_connection(db=self.db, redis_config=redis_config)


class RedisImportStorageBase(ImportStorage, RedisStorageMixin):
    db = models.PositiveSmallIntegerField(_('db'), default=1, help_text='Server Database')

    def can_resolve_url(self, url):
        return False

    def iterkeys(self):
        client = self.get_client()
        path = str(self.path)
        for key in client.keys(path + '*'):
            yield key

    def get_data(self, key):
        client = self.get_client()
        value = client.get(key)
        if not value:
            return
        return json.loads(value)

    def scan_and_create_links(self):
        return self._scan_and_create_links(RedisImportStorageLink)

    def validate_connection(self, client=None):
        if client is None:
            client = self.get_client()
        client.ping()

    class Meta:
        abstract = True


class RedisImportStorage(ProjectStorageMixin, RedisImportStorageBase):
    class Meta:
        abstract = False


class RedisExportStorage(RedisStorageMixin, ExportStorage):
    db = models.PositiveSmallIntegerField(_('db'), default=2, help_text='Server Database')

    def save_annotation(self, annotation):
        client = self.get_client()
        logger.debug(f'Creating new object on {self.__class__.__name__} Storage {self} for annotation {annotation}')
        ser_annotation = self._get_serialized_data(annotation)

        # get key that identifies this object in storage
        key = RedisExportStorageLink.get_key(annotation)

        # put object into storage
        client.set(key, json.dumps(ser_annotation))

        # create link if everything ok
        RedisExportStorageLink.create(annotation, self)

    def validate_connection(self, client=None):
        if client is None:
            client = self.get_client()
        client.ping()


@receiver(post_save, sender=Annotation)
def export_annotation_to_redis_storages(sender, instance, **kwargs):
    project = instance.project
    if hasattr(project, 'io_storages_redisexportstorages'):
        for storage in project.io_storages_redisexportstorages.all():
            logger.debug(f'Export {instance} to Redis storage {storage}')
            storage.save_annotation(instance)


class RedisImportStorageLink(ImportStorageLink):
    storage = models.ForeignKey(RedisImportStorage, on_delete=models.CASCADE, related_name='links')


class RedisExportStorageLink(ExportStorageLink):
    storage = models.ForeignKey(RedisExportStorage, on_delete=models.CASCADE, related_name='links')
</file>

<file path="label_studio/io_storages/redis/openapi_schema.py">
from drf_yasg import openapi

_common_redis_storage_schema_properties = {
    'title': openapi.Schema(type=openapi.TYPE_STRING, description='Storage title'),
    'description': openapi.Schema(type=openapi.TYPE_STRING, description='Storage description'),
    'project': openapi.Schema(type=openapi.TYPE_INTEGER, description='Project ID'),
    'path': openapi.Schema(type=openapi.TYPE_STRING, description='Storage prefix (optional)'),
    'host': openapi.Schema(type=openapi.TYPE_STRING, description='Server Host IP (optional)'),
    'port': openapi.Schema(type=openapi.TYPE_STRING, description='Server Port (optional)'),
    'password': openapi.Schema(type=openapi.TYPE_STRING, description='Server Password (optional)'),
}


_redis_import_storage_schema = openapi.Schema(
    type=openapi.TYPE_OBJECT,
    properties=dict(
        regex_filter=openapi.Schema(
            type=openapi.TYPE_STRING,
            description='Cloud storage regex for filtering objects. '
            'You must specify it otherwise no objects will be imported.',
        ),
        use_blob_urls=openapi.Schema(
            type=openapi.TYPE_BOOLEAN,
            description='Interpret objects as BLOBs and generate URLs. For example, if your bucket contains images, '
            'you can use this option to generate URLs for these images. '
            'If set to False, it will read the content of the file and load it into Label Studio.',
            default=False,
        ),
        **_common_redis_storage_schema_properties,
    ),
    required=[],
)

_redis_import_storage_schema_with_id = openapi.Schema(
    type=openapi.TYPE_OBJECT,
    properties=dict(
        id=openapi.Schema(
            type=openapi.TYPE_INTEGER, description='Storage ID. If set, storage with specified ID will be updated'
        ),
        **_redis_import_storage_schema.properties,
    ),
    required=[],
)


_redis_export_storage_schema = openapi.Schema(
    type=openapi.TYPE_OBJECT,
    properties=dict(
        db=openapi.Schema(type=openapi.TYPE_INTEGER, description='Database ID of database to use'),
        can_delete_objects=openapi.Schema(type=openapi.TYPE_BOOLEAN, description='Deletion from storage enabled.'),
        **_common_redis_storage_schema_properties,
    ),
    required=[],
)

_redis_export_storage_schema_with_id = openapi.Schema(
    type=openapi.TYPE_OBJECT,
    properties=dict(
        id=openapi.Schema(
            type=openapi.TYPE_INTEGER, description='Storage ID. If set, storage with specified ID will be updated'
        ),
        **_redis_export_storage_schema.properties,
    ),
    required=[],
)
</file>

<file path="label_studio/io_storages/redis/serializers.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import os

from io_storages.redis.models import RedisExportStorage, RedisImportStorage
from io_storages.serializers import ExportStorageSerializer, ImportStorageSerializer
from rest_framework import serializers
from rest_framework.exceptions import ValidationError


class RedisImportStorageSerializer(ImportStorageSerializer):
    type = serializers.ReadOnlyField(default=os.path.basename(os.path.dirname(__file__)))

    class Meta:
        model = RedisImportStorage
        fields = '__all__'

    def to_representation(self, instance):
        result = super().to_representation(instance)
        result.pop('password')
        return result

    def validate(self, data):
        data = super(RedisImportStorageSerializer, self).validate(data)

        storage = RedisImportStorage(**data)
        try:
            storage.validate_connection()
        except:  # noqa: E722
            raise ValidationError("Can't connect to Redis server.")
        return data


class RedisExportStorageSerializer(ExportStorageSerializer):
    type = serializers.ReadOnlyField(default=os.path.basename(os.path.dirname(__file__)))

    def to_representation(self, instance):
        result = super().to_representation(instance)
        result.pop('password')
        return result

    class Meta:
        model = RedisExportStorage
        fields = '__all__'
</file>

<file path="label_studio/io_storages/s3/__init__.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
</file>

<file path="label_studio/io_storages/s3/api.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
from django.utils.decorators import method_decorator
from drf_yasg import openapi as openapi
from drf_yasg.utils import no_body, swagger_auto_schema
from io_storages.api import (
    ExportStorageDetailAPI,
    ExportStorageFormLayoutAPI,
    ExportStorageListAPI,
    ExportStorageSyncAPI,
    ExportStorageValidateAPI,
    ImportStorageDetailAPI,
    ImportStorageFormLayoutAPI,
    ImportStorageListAPI,
    ImportStorageSyncAPI,
    ImportStorageValidateAPI,
)
from io_storages.s3.models import S3ExportStorage, S3ImportStorage
from io_storages.s3.serializers import S3ExportStorageSerializer, S3ImportStorageSerializer

from .openapi_schema import (
    _s3_export_storage_schema,
    _s3_export_storage_schema_with_id,
    _s3_import_storage_schema,
    _s3_import_storage_schema_with_id,
)


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Storage:S3'],
        x_fern_sdk_group_name=['import_storage', 's3'],
        x_fern_sdk_method_name='list',
        x_fern_audiences=['public'],
        operation_summary='List S3 import storage',
        operation_description='Get a list of all S3 import storage connections.',
        manual_parameters=[
            openapi.Parameter(
                name='project',
                type=openapi.TYPE_INTEGER,
                in_=openapi.IN_QUERY,
                description='Project ID',
            ),
        ],
    ),
)
@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Storage:S3'],
        x_fern_sdk_group_name=['import_storage', 's3'],
        x_fern_sdk_method_name='create',
        x_fern_audiences=['public'],
        operation_summary='Create new S3 storage',
        operation_description='Create new S3 import storage',
        request_body=_s3_import_storage_schema,
    ),
)
class S3ImportStorageListAPI(ImportStorageListAPI):
    queryset = S3ImportStorage.objects.all()
    serializer_class = S3ImportStorageSerializer


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Storage:S3'],
        x_fern_sdk_group_name=['import_storage', 's3'],
        x_fern_sdk_method_name='get',
        x_fern_audiences=['public'],
        operation_summary='Get import storage',
        operation_description='Get a specific S3 import storage connection.',
        request_body=no_body,
    ),
)
@method_decorator(
    name='patch',
    decorator=swagger_auto_schema(
        tags=['Storage:S3'],
        x_fern_sdk_group_name=['import_storage', 's3'],
        x_fern_sdk_method_name='update',
        x_fern_audiences=['public'],
        operation_summary='Update import storage',
        operation_description='Update a specific S3 import storage connection.',
        request_body=_s3_import_storage_schema,
    ),
)
@method_decorator(
    name='delete',
    decorator=swagger_auto_schema(
        tags=['Storage:S3'],
        x_fern_sdk_group_name=['import_storage', 's3'],
        x_fern_sdk_method_name='delete',
        x_fern_audiences=['public'],
        operation_summary='Delete import storage',
        operation_description='Delete a specific S3 import storage connection.',
        request_body=no_body,
    ),
)
class S3ImportStorageDetailAPI(ImportStorageDetailAPI):
    queryset = S3ImportStorage.objects.all()
    serializer_class = S3ImportStorageSerializer


@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Storage:S3'],
        x_fern_sdk_group_name=['import_storage', 's3'],
        x_fern_sdk_method_name='sync',
        x_fern_audiences=['public'],
        operation_summary='Sync import storage',
        operation_description='Sync tasks from an S3 import storage connection.',
        manual_parameters=[
            openapi.Parameter(
                name='id',
                type=openapi.TYPE_INTEGER,
                in_=openapi.IN_PATH,
                description='Storage ID',
            ),
        ],
        request_body=no_body,
    ),
)
class S3ImportStorageSyncAPI(ImportStorageSyncAPI):
    serializer_class = S3ImportStorageSerializer


@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Storage:S3'],
        x_fern_sdk_group_name=['import_storage', 's3'],
        x_fern_sdk_method_name='validate',
        x_fern_audiences=['public'],
        operation_summary='Validate import storage',
        operation_description='Validate a specific S3 import storage connection.',
        request_body=_s3_import_storage_schema_with_id,
        responses={200: openapi.Response(description='Validation successful')},
    ),
)
class S3ImportStorageValidateAPI(ImportStorageValidateAPI):
    serializer_class = S3ImportStorageSerializer


@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Storage:S3'],
        x_fern_sdk_group_name=['export_storage', 's3'],
        x_fern_sdk_method_name='validate',
        x_fern_audiences=['public'],
        operation_summary='Validate export storage',
        operation_description='Validate a specific S3 export storage connection.',
        request_body=_s3_export_storage_schema_with_id,
        responses={200: openapi.Response(description='Validation successful')},
    ),
)
class S3ExportStorageValidateAPI(ExportStorageValidateAPI):
    serializer_class = S3ExportStorageSerializer


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Storage:S3'],
        x_fern_sdk_group_name=['export_storage', 's3'],
        x_fern_sdk_method_name='list',
        x_fern_audiences=['public'],
        operation_summary='Get all export storage',
        operation_description='Get a list of all S3 export storage connections.',
        manual_parameters=[
            openapi.Parameter(
                name='project',
                type=openapi.TYPE_INTEGER,
                in_=openapi.IN_QUERY,
                description='Project ID',
            ),
        ],
    ),
)
@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Storage:S3'],
        x_fern_sdk_group_name=['export_storage', 's3'],
        x_fern_sdk_method_name='create',
        x_fern_audiences=['public'],
        operation_summary='Create export storage',
        operation_description='Create a new S3 export storage connection to store annotations.',
        request_body=_s3_export_storage_schema,
    ),
)
class S3ExportStorageListAPI(ExportStorageListAPI):
    queryset = S3ExportStorage.objects.all()
    serializer_class = S3ExportStorageSerializer


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Storage:S3'],
        x_fern_sdk_group_name=['export_storage', 's3'],
        x_fern_sdk_method_name='get',
        x_fern_audiences=['public'],
        operation_summary='Get export storage',
        operation_description='Get a specific S3 export storage connection.',
        request_body=no_body,
    ),
)
@method_decorator(
    name='patch',
    decorator=swagger_auto_schema(
        tags=['Storage:S3'],
        x_fern_sdk_group_name=['export_storage', 's3'],
        x_fern_sdk_method_name='update',
        x_fern_audiences=['public'],
        operation_summary='Update export storage',
        operation_description='Update a specific S3 export storage connection.',
        request_body=_s3_export_storage_schema,
    ),
)
@method_decorator(
    name='delete',
    decorator=swagger_auto_schema(
        tags=['Storage:S3'],
        x_fern_sdk_group_name=['export_storage', 's3'],
        x_fern_sdk_method_name='delete',
        x_fern_audiences=['public'],
        operation_summary='Delete export storage',
        operation_description='Delete a specific S3 export storage connection.',
        request_body=no_body,
    ),
)
class S3ExportStorageDetailAPI(ExportStorageDetailAPI):
    queryset = S3ExportStorage.objects.all()
    serializer_class = S3ExportStorageSerializer


@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Storage:S3'],
        x_fern_sdk_group_name=['export_storage', 's3'],
        x_fern_sdk_method_name='sync',
        x_fern_audiences=['public'],
        operation_summary='Sync export storage',
        operation_description='Sync tasks from an S3 export storage connection.',
        request_body=no_body,
    ),
)
class S3ExportStorageSyncAPI(ExportStorageSyncAPI):
    serializer_class = S3ExportStorageSerializer


class S3ImportStorageFormLayoutAPI(ImportStorageFormLayoutAPI):
    pass


class S3ExportStorageFormLayoutAPI(ExportStorageFormLayoutAPI):
    pass
</file>

<file path="label_studio/io_storages/s3/models.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import json
import logging
import re
from typing import Union

import boto3
from core.feature_flags import flag_set
from core.redis import start_job_async_or_sync
from django.conf import settings
from django.db import models
from django.db.models.signals import post_save, pre_delete
from django.dispatch import receiver
from django.utils.translation import gettext_lazy as _
from io_storages.base_models import (
    ExportStorage,
    ExportStorageLink,
    ImportStorage,
    ImportStorageLink,
    ProjectStorageMixin,
)
from io_storages.s3.utils import catch_and_reraise_from_none, get_client_and_resource, resolve_s3_url
from io_storages.utils import storage_can_resolve_bucket_url
from tasks.models import Annotation
from tasks.validation import ValidationError as TaskValidationError

from label_studio.io_storages.s3.utils import AWS

logger = logging.getLogger(__name__)
logging.getLogger('botocore').setLevel(logging.CRITICAL)
boto3.set_stream_logger(level=logging.INFO)

clients_cache = {}


class S3StorageMixin(models.Model):
    bucket = models.TextField(_('bucket'), null=True, blank=True, help_text='S3 bucket name')
    prefix = models.TextField(_('prefix'), null=True, blank=True, help_text='S3 bucket prefix')
    regex_filter = models.TextField(
        _('regex_filter'), null=True, blank=True, help_text='Cloud storage regex for filtering objects'
    )
    use_blob_urls = models.BooleanField(
        _('use_blob_urls'), default=False, help_text='Interpret objects as BLOBs and generate URLs'
    )
    aws_access_key_id = models.TextField(_('aws_access_key_id'), null=True, blank=True, help_text='AWS_ACCESS_KEY_ID')
    aws_secret_access_key = models.TextField(
        _('aws_secret_access_key'), null=True, blank=True, help_text='AWS_SECRET_ACCESS_KEY'
    )
    aws_session_token = models.TextField(_('aws_session_token'), null=True, blank=True, help_text='AWS_SESSION_TOKEN')
    aws_sse_kms_key_id = models.TextField(
        _('aws_sse_kms_key_id'), null=True, blank=True, help_text='AWS SSE KMS Key ID'
    )
    region_name = models.TextField(_('region_name'), null=True, blank=True, help_text='AWS Region')
    s3_endpoint = models.TextField(_('s3_endpoint'), null=True, blank=True, help_text='S3 Endpoint')

    @catch_and_reraise_from_none
    def get_client_and_resource(self):
        # s3 client initialization ~ 100 ms, for 30 tasks it's a 3 seconds, so we need to cache it
        cache_key = f'{self.aws_access_key_id}:{self.aws_secret_access_key}:{self.aws_session_token}:{self.region_name}:{self.s3_endpoint}'
        if cache_key in clients_cache:
            return clients_cache[cache_key]

        result = get_client_and_resource(
            self.aws_access_key_id,
            self.aws_secret_access_key,
            self.aws_session_token,
            self.region_name,
            self.s3_endpoint,
        )
        clients_cache[cache_key] = result
        return result

    def get_client(self):
        client, _ = self.get_client_and_resource()
        return client

    def get_client_and_bucket(self, validate_connection=True):
        client, s3 = self.get_client_and_resource()
        if validate_connection:
            self.validate_connection(client)
        return client, s3.Bucket(self.bucket)

    @catch_and_reraise_from_none
    def validate_connection(self, client=None):
        logger.debug('validate_connection')
        if client is None:
            client = self.get_client()
        # TODO(jo): add check for write access for .*Export.* classes
        is_export = 'Export' in self.__class__.__name__
        if self.prefix:
            logger.debug(
                f'[Class {self.__class__.__name__}]: Test connection to bucket {self.bucket} with prefix {self.prefix} using ListObjectsV2 operation'
            )
            result = client.list_objects_v2(Bucket=self.bucket, Prefix=self.prefix, MaxKeys=1)
            # We expect 1 key with the prefix for imports. For exports it's okay if there are 0 with the prefix.
            expected_keycount = 0 if is_export else 1
            if (keycount := result.get('KeyCount')) is None or keycount < expected_keycount:
                raise KeyError(f'{self.url_scheme}://{self.bucket}/{self.prefix} not found.')
        else:
            logger.debug(
                f'[Class {self.__class__.__name__}]: Test connection to bucket {self.bucket} using HeadBucket operation'
            )
            client.head_bucket(Bucket=self.bucket)

    @property
    def path_full(self):
        prefix = self.prefix or ''
        return f'{self.url_scheme}://{self.bucket}/{prefix}'

    @property
    def type_full(self):
        return 'Amazon AWS S3'

    class Meta:
        abstract = True


class S3ImportStorageBase(S3StorageMixin, ImportStorage):

    url_scheme = 's3'

    presign = models.BooleanField(_('presign'), default=True, help_text='Generate presigned URLs')
    presign_ttl = models.PositiveSmallIntegerField(
        _('presign_ttl'), default=1, help_text='Presigned URLs TTL (in minutes)'
    )
    recursive_scan = models.BooleanField(
        _('recursive scan'), default=False, help_text=_('Perform recursive scan over the bucket content')
    )

    @catch_and_reraise_from_none
    def iterkeys(self):
        client, bucket = self.get_client_and_bucket()
        if self.prefix:
            list_kwargs = {'Prefix': self.prefix.rstrip('/') + '/'}
            if not self.recursive_scan:
                list_kwargs['Delimiter'] = '/'
            bucket_iter = bucket.objects.filter(**list_kwargs).all()
        else:
            bucket_iter = bucket.objects.all()
        regex = re.compile(str(self.regex_filter)) if self.regex_filter else None
        for obj in bucket_iter:
            key = obj.key
            if key.endswith('/'):
                logger.debug(key + ' is skipped because it is a folder')
                continue
            if regex and not regex.match(key):
                logger.debug(key + ' is skipped by regex filter')
                continue
            yield key

    @catch_and_reraise_from_none
    def scan_and_create_links(self):
        return self._scan_and_create_links(S3ImportStorageLink)

    def _get_validated_task(self, parsed_data, key):
        """Validate parsed data with labeling config and task structure"""
        if not isinstance(parsed_data, dict):
            raise TaskValidationError(
                'Error at ' + str(key) + ':\n' 'Cloud storage supports one task (one dict object) per JSON file only. '
            )
        return parsed_data

    @catch_and_reraise_from_none
    def get_data(self, key):
        uri = f'{self.url_scheme}://{self.bucket}/{key}'
        if self.use_blob_urls:
            data_key = settings.DATA_UNDEFINED_NAME
            return {data_key: uri}

        # read task json from bucket and validate it
        _, s3 = self.get_client_and_resource()
        bucket = s3.Bucket(self.bucket)
        obj = s3.Object(bucket.name, key).get()['Body'].read().decode('utf-8')
        value = json.loads(obj)
        if not isinstance(value, dict):
            raise ValueError(f'Error on key {key}: For S3 your JSON file must be a dictionary with one task')

        value = self._get_validated_task(value, key)
        return value

    @catch_and_reraise_from_none
    def generate_http_url(self, url):
        return resolve_s3_url(url, self.get_client(), self.presign, expires_in=self.presign_ttl * 60)

    @catch_and_reraise_from_none
    def can_resolve_url(self, url: Union[str, None]) -> bool:
        return storage_can_resolve_bucket_url(self, url)

    @catch_and_reraise_from_none
    def get_blob_metadata(self, key):
        return AWS.get_blob_metadata(
            key,
            self.bucket,
            aws_access_key_id=self.aws_access_key_id,
            aws_secret_access_key=self.aws_secret_access_key,
            aws_session_token=self.aws_session_token,
            region_name=self.region_name,
            s3_endpoint=self.s3_endpoint,
        )

    class Meta:
        abstract = True


class S3ImportStorage(ProjectStorageMixin, S3ImportStorageBase):
    class Meta:
        abstract = False


class S3ExportStorage(S3StorageMixin, ExportStorage):
    @catch_and_reraise_from_none
    def save_annotation(self, annotation):
        client, s3 = self.get_client_and_resource()
        logger.debug(f'Creating new object on {self.__class__.__name__} Storage {self} for annotation {annotation}')
        ser_annotation = self._get_serialized_data(annotation)

        # get key that identifies this object in storage
        key = S3ExportStorageLink.get_key(annotation)
        key = str(self.prefix) + '/' + key if self.prefix else key

        # put object into storage
        additional_params = {}

        self.cached_user = getattr(self, 'cached_user', self.project.organization.created_by)
        if flag_set(
            'fflag_feat_back_lsdv_3958_server_side_encryption_for_target_storage_short', user=self.cached_user
        ):
            if self.aws_sse_kms_key_id:
                additional_params['SSEKMSKeyId'] = self.aws_sse_kms_key_id
                additional_params['ServerSideEncryption'] = 'aws:kms'
            else:
                additional_params['ServerSideEncryption'] = 'AES256'

        s3.Object(self.bucket, key).put(Body=json.dumps(ser_annotation), **additional_params)

        # create link if everything ok
        S3ExportStorageLink.create(annotation, self)

    @catch_and_reraise_from_none
    def delete_annotation(self, annotation):
        client, s3 = self.get_client_and_resource()
        logger.debug(f'Deleting object on {self.__class__.__name__} Storage {self} for annotation {annotation}')

        # get key that identifies this object in storage
        key = S3ExportStorageLink.get_key(annotation)
        key = str(self.prefix) + '/' + key if self.prefix else key

        # delete object from storage
        s3.Object(self.bucket, key).delete()

        # delete link if everything ok
        S3ExportStorageLink.objects.filter(storage=self, annotation=annotation).delete()


def async_export_annotation_to_s3_storages(annotation):
    project = annotation.project
    if hasattr(project, 'io_storages_s3exportstorages'):
        for storage in project.io_storages_s3exportstorages.all():
            logger.debug(f'Export {annotation} to S3 storage {storage}')
            storage.save_annotation(annotation)


@receiver(post_save, sender=Annotation)
def export_annotation_to_s3_storages(sender, instance, **kwargs):
    storages = getattr(instance.project, 'io_storages_s3exportstorages', None)
    if storages and storages.exists():  # avoid excess jobs in rq
        start_job_async_or_sync(async_export_annotation_to_s3_storages, instance)


@receiver(pre_delete, sender=Annotation)
def delete_annotation_from_s3_storages(sender, instance, **kwargs):
    links = S3ExportStorageLink.objects.filter(annotation=instance)
    for link in links:
        storage = link.storage
        if storage.can_delete_objects:
            logger.debug(f'Delete {instance} from S3 storage {storage}')  # nosec
            storage.delete_annotation(instance)


class S3ImportStorageLink(ImportStorageLink):
    storage = models.ForeignKey(S3ImportStorage, on_delete=models.CASCADE, related_name='links')

    @classmethod
    def exists(cls, key, storage):
        storage_link_exists = super(S3ImportStorageLink, cls).exists(key, storage)
        # TODO: this is a workaround to be compatible with old keys version - remove it later
        prefix = str(storage.prefix) or ''
        return (
            storage_link_exists
            or cls.objects.filter(key=prefix + key, storage=storage.id).exists()
            or cls.objects.filter(key=prefix + '/' + key, storage=storage.id).exists()
        )


class S3ExportStorageLink(ExportStorageLink):
    storage = models.ForeignKey(S3ExportStorage, on_delete=models.CASCADE, related_name='links')
</file>

<file path="label_studio/io_storages/s3/openapi_schema.py">
from drf_yasg import openapi

_common_s3_storage_schema_properties = {
    'title': openapi.Schema(type=openapi.TYPE_STRING, description='Storage title'),
    'description': openapi.Schema(type=openapi.TYPE_STRING, description='Storage description'),
    'project': openapi.Schema(type=openapi.TYPE_INTEGER, description='Project ID'),
    'bucket': openapi.Schema(type=openapi.TYPE_STRING, description='S3 bucket name'),
    'prefix': openapi.Schema(type=openapi.TYPE_STRING, description='S3 bucket prefix'),
    'aws_access_key_id': openapi.Schema(type=openapi.TYPE_STRING, description='AWS_ACCESS_KEY_ID'),
    'aws_secret_access_key': openapi.Schema(type=openapi.TYPE_STRING, description='AWS_SECRET_ACCESS_KEY'),
    'aws_session_token': openapi.Schema(type=openapi.TYPE_STRING, description='AWS_SESSION_TOKEN'),
    'aws_sse_kms_key_id': openapi.Schema(type=openapi.TYPE_STRING, description='AWS SSE KMS Key ID'),
    'region_name': openapi.Schema(type=openapi.TYPE_STRING, description='AWS Region'),
    's3_endpoint': openapi.Schema(type=openapi.TYPE_STRING, description='S3 Endpoint'),
}

_s3_import_storage_schema = openapi.Schema(
    type=openapi.TYPE_OBJECT,
    properties=dict(
        regex_filter=openapi.Schema(
            type=openapi.TYPE_STRING,
            description='Cloud storage regex for filtering objects. '
            'You must specify it otherwise no objects will be imported.',
        ),
        use_blob_urls=openapi.Schema(
            type=openapi.TYPE_BOOLEAN,
            description='Interpret objects as BLOBs and generate URLs. For example, if your bucket contains images, '
            'you can use this option to generate URLs for these images. '
            'If set to False, it will read the content of the file and load it into Label Studio.',
            default=False,
        ),
        presign=openapi.Schema(type=openapi.TYPE_BOOLEAN, description='Presign URLs for download', default=True),
        presign_ttl=openapi.Schema(type=openapi.TYPE_INTEGER, description='Presign TTL in minutes', default=1),
        recursive_scan=openapi.Schema(type=openapi.TYPE_BOOLEAN, description='Scan recursively'),
        **_common_s3_storage_schema_properties,
    ),
)

_s3_import_storage_schema_with_id = openapi.Schema(
    type=openapi.TYPE_OBJECT,
    properties=dict(
        id=openapi.Schema(
            type=openapi.TYPE_INTEGER, description='Storage ID. If set, storage with specified ID will be updated'
        ),
        **_s3_import_storage_schema.properties,
    ),
)

_s3_export_storage_schema = openapi.Schema(
    type=openapi.TYPE_OBJECT,
    properties=dict(
        can_delete_objects=openapi.Schema(type=openapi.TYPE_BOOLEAN, description='Deletion from storage enabled.'),
        **_common_s3_storage_schema_properties,
    ),
)

_s3_export_storage_schema_with_id = openapi.Schema(
    type=openapi.TYPE_OBJECT,
    properties=dict(
        id=openapi.Schema(
            type=openapi.TYPE_INTEGER, description='Storage ID. If set, storage with specified ID will be updated'
        ),
        **_s3_export_storage_schema.properties,
    ),
)
</file>

<file path="label_studio/io_storages/s3/serializers.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import os

from botocore.exceptions import ClientError, ParamValidationError
from botocore.handlers import validate_bucket_name
from io_storages.s3.models import S3ExportStorage, S3ImportStorage
from io_storages.serializers import ExportStorageSerializer, ImportStorageSerializer
from rest_framework import serializers
from rest_framework.exceptions import ValidationError


class S3StorageSerializerMixin:
    secure_fields = ['aws_access_key_id', 'aws_secret_access_key']

    def to_representation(self, instance):
        result = super().to_representation(instance)
        for attr in self.secure_fields:
            result.pop(attr)
        return result

    def validate_bucket(self, value):
        if not value:
            return value
        try:
            validate_bucket_name({'Bucket': value})
        except ParamValidationError as exc:
            raise ValidationError(exc.kwargs['report']) from exc
        return value

    def validate(self, data):
        data = super().validate(data)
        if not data.get('bucket', None):
            return data

        storage = self.instance
        if storage:
            for key, value in data.items():
                setattr(storage, key, value)
        else:
            if 'id' in self.initial_data:
                storage_object = self.Meta.model.objects.get(id=self.initial_data['id'])
                for attr in self.secure_fields:
                    data[attr] = data.get(attr) or getattr(storage_object, attr)
            storage = self.Meta.model(**data)
        try:
            storage.validate_connection()
        except ParamValidationError:
            raise ValidationError('Wrong credentials for S3 {bucket_name}'.format(bucket_name=storage.bucket))
        except ClientError as e:
            if (
                e.response.get('Error').get('Code') in ['SignatureDoesNotMatch', '403']
                or e.response.get('ResponseMetadata').get('HTTPStatusCode') == 403
            ):
                raise ValidationError(
                    'Cannot connect to S3 {bucket_name} with specified AWS credentials'.format(
                        bucket_name=storage.bucket
                    )
                )
            if (
                e.response.get('Error').get('Code') in ['NoSuchBucket', '404']
                or e.response.get('ResponseMetadata').get('HTTPStatusCode') == 404
            ):
                raise ValidationError('Cannot find bucket {bucket_name} in S3'.format(bucket_name=storage.bucket))
        except TypeError as e:
            raise ValidationError(f'It seems access keys are incorrect: {e}')
        return data


class S3ImportStorageSerializer(S3StorageSerializerMixin, ImportStorageSerializer):
    type = serializers.ReadOnlyField(default=os.path.basename(os.path.dirname(__file__)))
    presign = serializers.BooleanField(required=False, default=True)

    class Meta:
        model = S3ImportStorage
        fields = '__all__'


class S3ExportStorageSerializer(S3StorageSerializerMixin, ExportStorageSerializer):
    type = serializers.ReadOnlyField(default=os.path.basename(os.path.dirname(__file__)))

    class Meta:
        model = S3ExportStorage
        fields = '__all__'
</file>

<file path="label_studio/io_storages/s3/utils.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import base64
import fnmatch
import logging
import re
from urllib.parse import urlparse

import boto3
from botocore.exceptions import ClientError
from core.utils.params import get_env
from django.conf import settings
from tldextract import TLDExtract

logger = logging.getLogger(__name__)


def get_client_and_resource(
    aws_access_key_id=None, aws_secret_access_key=None, aws_session_token=None, region_name=None, s3_endpoint=None
):
    aws_access_key_id = aws_access_key_id or get_env('AWS_ACCESS_KEY_ID')
    aws_secret_access_key = aws_secret_access_key or get_env('AWS_SECRET_ACCESS_KEY')
    aws_session_token = aws_session_token or get_env('AWS_SESSION_TOKEN')
    logger.debug(
        f'Create boto3 session with '
        f'access key id={aws_access_key_id}, '
        f'secret key={aws_secret_access_key[:4] + "..." if aws_secret_access_key else None}, '
        f'session token={aws_session_token}'
    )
    session = boto3.Session(
        aws_access_key_id=aws_access_key_id,
        aws_secret_access_key=aws_secret_access_key,
        aws_session_token=aws_session_token,
    )
    settings = {'region_name': region_name or get_env('S3_region') or 'us-east-1'}
    s3_endpoint = s3_endpoint or get_env('S3_ENDPOINT')
    if s3_endpoint:
        settings['endpoint_url'] = s3_endpoint
    client = session.client('s3', config=boto3.session.Config(signature_version='s3v4'), **settings)
    resource = session.resource('s3', config=boto3.session.Config(signature_version='s3v4'), **settings)
    return client, resource


def resolve_s3_url(url, client, presign=True, expires_in=3600):
    r = urlparse(url, allow_fragments=False)
    bucket_name = r.netloc
    key = r.path.lstrip('/')

    # Return blob as base64 encoded string if presigned urls are disabled
    if not presign:
        object = client.get_object(Bucket=bucket_name, Key=key)
        content_type = object['ResponseMetadata']['HTTPHeaders']['content-type']
        object_b64 = 'data:' + content_type + ';base64,' + base64.b64encode(object['Body'].read()).decode('utf-8')
        return object_b64

    # Otherwise try to generate presigned url
    try:
        presigned_url = client.generate_presigned_url(
            ClientMethod='get_object', Params={'Bucket': bucket_name, 'Key': key}, ExpiresIn=expires_in
        )
    except ClientError as exc:
        logger.warning(f"Can't generate presigned URL. Reason: {exc}")
        return url
    else:
        logger.debug('Presigned URL {presigned_url} generated for {url}'.format(presigned_url=presigned_url, url=url))
        return presigned_url


class AWS(object):
    @classmethod
    def get_blob_metadata(
        cls,
        url: str,
        bucket_name: str,
        client=None,
        aws_access_key_id=None,
        aws_secret_access_key=None,
        aws_session_token=None,
        region_name=None,
        s3_endpoint=None,
    ):
        """
        Get blob metadata by url
        :param url: Object key
        :param bucket_name: AWS bucket name
        :param client: AWS client for batch processing
        :param account_key: Azure account key
        :return: Object metadata dict("name": "value")
        """
        if client is None:
            client, _ = get_client_and_resource(
                aws_access_key_id=aws_access_key_id,
                aws_secret_access_key=aws_secret_access_key,
                aws_session_token=aws_session_token,
                region_name=region_name,
                s3_endpoint=s3_endpoint,
            )
        object = client.get_object(Bucket=bucket_name, Key=url)
        metadata = dict(object)
        # remove unused fields
        metadata.pop('Body', None)
        metadata.pop('ResponseMetadata', None)
        return metadata

    @classmethod
    def validate_pattern(cls, storage, pattern, glob_pattern=True):
        """
        Validate pattern against S3 Storage
        :param storage: S3 Storage instance
        :param pattern: Pattern to validate
        :param glob_pattern: If True, pattern is a glob pattern, otherwise it is a regex pattern
        :return: Message if pattern is not valid, empty string otherwise
        """
        client, bucket = storage.get_client_and_bucket()
        if glob_pattern:
            pattern = fnmatch.translate(pattern)
        regex = re.compile(pattern)

        if storage.prefix:
            list_kwargs = {'Prefix': storage.prefix.rstrip('/') + '/'}
            if not storage.recursive_scan:
                list_kwargs['Delimiter'] = '/'
            bucket_iter = bucket.objects.filter(**list_kwargs)
        else:
            bucket_iter = bucket.objects

        bucket_iter = bucket_iter.page_size(settings.CLOUD_STORAGE_CHECK_FOR_RECORDS_PAGE_SIZE).all()

        for index, obj in enumerate(bucket_iter):
            key = obj.key
            # skip directories
            if key.endswith('/'):
                logger.debug(key + ' is skipped because it is a folder')
                continue
            if regex and regex.match(key):
                logger.debug(key + ' matches file pattern')
                return ''
        return 'No objects found matching the provided glob pattern'


class S3StorageError(Exception):
    pass


# see https://github.com/john-kurkowski/tldextract?tab=readme-ov-file#note-about-caching
# prevents network call on first use
extractor = TLDExtract(suffix_list_urls=())


def catch_and_reraise_from_none(func):
    """
    For S3 storages - if s3_endpoint is not on a known domain, catch exception and
    raise a new one with the previous context suppressed. See also: https://peps.python.org/pep-0409/
    """

    def wrapper(self, *args, **kwargs):
        try:
            return func(self, *args, **kwargs)
        except Exception as e:
            if self.s3_endpoint and (
                domain := extractor.extract_urllib(urlparse(self.s3_endpoint)).registered_domain.lower()
            ) not in [trusted_domain.lower() for trusted_domain in settings.S3_TRUSTED_STORAGE_DOMAINS]:
                logger.error(f'Exception from unrecognized S3 domain: {e}', exc_info=True)
                raise S3StorageError(
                    f'Debugging info is not available for s3 endpoints on domain: {domain}. '
                    'Please contact your Label Studio devops team if you require detailed error reporting for this domain.'
                ) from None
            else:
                raise e

    return wrapper
</file>

<file path="label_studio/io_storages/__init__.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
</file>

<file path="label_studio/io_storages/all_api.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import logging

from core.permissions import all_permissions
from django.conf import settings
from django.utils.decorators import method_decorator
from drf_yasg import openapi
from drf_yasg.utils import swagger_auto_schema
from rest_framework import generics
from rest_framework.parsers import FormParser, JSONParser, MultiPartParser
from rest_framework.response import Response
from rest_framework.views import APIView

from label_studio.core.utils.common import load_func

from .localfiles.api import LocalFilesExportStorageListAPI, LocalFilesImportStorageListAPI

logger = logging.getLogger(__name__)
# TODO: replace hardcoded apps lists with search over included storage apps


get_storage_list = load_func(settings.GET_STORAGE_LIST)


def _get_common_storage_list():
    storage_list = get_storage_list()
    if settings.ENABLE_LOCAL_FILES_STORAGE:
        storage_list += [
            {
                'name': 'localfiles',
                'title': 'Local files',
                'import_list_api': LocalFilesImportStorageListAPI,
                'export_list_api': LocalFilesExportStorageListAPI,
            }
        ]

    return storage_list


_common_storage_list = _get_common_storage_list()


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Storage'],
        x_fern_sdk_group_name='import_storage',
        x_fern_sdk_method_name='list_types',
        x_fern_audiences=['public'],
        operation_summary='List all import storages types',
        operation_description='Retrieve a list of the import storages types.',
        responses={
            200: openapi.Schema(
                title='ImportStorageTypes',
                description='List of import storage types',
                type=openapi.TYPE_ARRAY,
                items=openapi.Schema(
                    type=openapi.TYPE_OBJECT,
                    properties={
                        'name': openapi.Schema(type=openapi.TYPE_STRING),
                        'title': openapi.Schema(type=openapi.TYPE_STRING),
                    },
                ),
            )
        },
    ),
)
class AllImportStorageTypesAPI(APIView):
    permission_required = all_permissions.projects_change

    def get(self, request, **kwargs):
        return Response([{'name': s['name'], 'title': s['title']} for s in _common_storage_list])


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Storage'],
        x_fern_sdk_group_name='export_storage',
        x_fern_sdk_method_name='list_types',
        x_fern_audiences=['public'],
        operation_summary='List all export storages types',
        operation_description='Retrieve a list of the export storages types.',
        responses={
            200: openapi.Schema(
                title='ExportStorageTypes',
                description='List of export storage types',
                type=openapi.TYPE_ARRAY,
                items=openapi.Schema(
                    type=openapi.TYPE_OBJECT,
                    properties={
                        'name': openapi.Schema(type=openapi.TYPE_STRING),
                        'title': openapi.Schema(type=openapi.TYPE_STRING),
                    },
                ),
            )
        },
    ),
)
class AllExportStorageTypesAPI(APIView):
    permission_required = all_permissions.projects_change

    def get(self, request, **kwargs):
        return Response([{'name': s['name'], 'title': s['title']} for s in _common_storage_list])


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Storage'],
        x_fern_sdk_group_name='import_storage',
        x_fern_sdk_method_name='list',
        x_fern_audiences=['internal'],
        operation_summary='List all import storages from the project',
        operation_description='Retrieve a list of the import storages of all types with their IDs.',
        responses={200: 'List of ImportStorageSerializer'},
    ),
)
class AllImportStorageListAPI(generics.ListAPIView):
    parser_classes = (JSONParser, FormParser, MultiPartParser)
    permission_required = all_permissions.projects_change

    def _get_response(self, api, request, *args, **kwargs):
        try:
            view = api.as_view()
            response = view(request._request, *args, **kwargs)
            payload = response.data
            if not isinstance(payload, list):
                raise ValueError('Response is not list')
            return response.data
        except Exception:
            logger.error(f"Can't process {api.__class__.__name__}", exc_info=True)
            return []

    def list(self, request, *args, **kwargs):
        list_responses = sum(
            [self._get_response(s['import_list_api'], request, *args, **kwargs) for s in _common_storage_list], []
        )
        return Response(list_responses)


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Storage'],
        x_fern_sdk_group_name='export_storage',
        x_fern_sdk_method_name='list',
        x_fern_audiences=['internal'],
        operation_summary='List all export storages from the project',
        operation_description='Retrieve a list of the export storages of all types with their IDs.',
        responses={200: 'List of ExportStorageSerializer'},
    ),
)
class AllExportStorageListAPI(generics.ListAPIView):

    parser_classes = (JSONParser, FormParser, MultiPartParser)
    permission_required = all_permissions.projects_change

    def _get_response(self, api, request, *args, **kwargs):
        view = api.as_view()
        response = view(request._request, *args, **kwargs)
        return response.data

    def list(self, request, *args, **kwargs):
        list_responses = sum(
            [self._get_response(s['export_list_api'], request, *args, **kwargs) for s in _common_storage_list], []
        )
        return Response(list_responses)
</file>

<file path="label_studio/io_storages/api.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import inspect
import logging
import os

from core.permissions import all_permissions
from core.utils.io import read_yaml
from django.conf import settings
from drf_yasg import openapi as openapi
from drf_yasg.utils import swagger_auto_schema
from io_storages.serializers import ExportStorageSerializer, ImportStorageSerializer
from projects.models import Project
from rest_framework import generics, status
from rest_framework.exceptions import NotFound, PermissionDenied, ValidationError
from rest_framework.parsers import FormParser, JSONParser, MultiPartParser
from rest_framework.response import Response
from rest_framework.settings import api_settings

from label_studio.core.utils.common import load_func

logger = logging.getLogger(__name__)

StoragePermission = load_func(settings.STORAGE_PERMISSION)


class ImportStorageListAPI(generics.ListCreateAPIView):
    permission_required = all_permissions.projects_change
    permission_classes = api_settings.DEFAULT_PERMISSION_CLASSES + [StoragePermission]
    parser_classes = (JSONParser, FormParser, MultiPartParser)

    serializer_class = ImportStorageSerializer

    def get_queryset(self):
        project_pk = self.request.query_params.get('project')
        project = generics.get_object_or_404(Project, pk=project_pk)
        self.check_object_permissions(self.request, project)
        StorageClass = self.serializer_class.Meta.model
        storages = StorageClass.objects.filter(project_id=project.id)

        # check failed jobs and sync their statuses
        StorageClass.ensure_storage_statuses(storages)
        return storages


class ImportStorageDetailAPI(generics.RetrieveUpdateDestroyAPIView):
    """RUD storage by pk specified in URL"""

    permission_required = all_permissions.projects_change
    permission_classes = api_settings.DEFAULT_PERMISSION_CLASSES + [StoragePermission]
    parser_classes = (JSONParser, FormParser, MultiPartParser)
    serializer_class = ImportStorageSerializer

    @swagger_auto_schema(auto_schema=None)
    def put(self, request, *args, **kwargs):
        return super(ImportStorageDetailAPI, self).put(request, *args, **kwargs)


class ExportStorageListAPI(generics.ListCreateAPIView):

    permission_required = all_permissions.projects_change
    permission_classes = api_settings.DEFAULT_PERMISSION_CLASSES + [StoragePermission]
    parser_classes = (JSONParser, FormParser, MultiPartParser)
    serializer_class = ExportStorageSerializer

    def get_queryset(self):
        project_pk = self.request.query_params.get('project')
        project = generics.get_object_or_404(Project, pk=project_pk)
        self.check_object_permissions(self.request, project)
        StorageClass = self.serializer_class.Meta.model
        storages = StorageClass.objects.filter(project_id=project.id)

        # check failed jobs and sync their statuses
        StorageClass.ensure_storage_statuses(storages)
        return storages

    def perform_create(self, serializer):
        # double check: not export storages don't validate connection in serializer,
        # just make another explicit check here, note: in this create API we have credentials in request.data
        instance = serializer.Meta.model(**serializer.validated_data)
        try:
            instance.validate_connection()
        except Exception as exc:
            raise ValidationError(exc)

        storage = serializer.save()
        if settings.SYNC_ON_TARGET_STORAGE_CREATION:
            storage.sync()


class ExportStorageDetailAPI(generics.RetrieveUpdateDestroyAPIView):
    """RUD storage by pk specified in URL"""

    permission_required = all_permissions.projects_change
    permission_classes = api_settings.DEFAULT_PERMISSION_CLASSES + [StoragePermission]
    parser_classes = (JSONParser, FormParser, MultiPartParser)
    serializer_class = ExportStorageSerializer

    @swagger_auto_schema(auto_schema=None)
    def put(self, request, *args, **kwargs):
        return super(ExportStorageDetailAPI, self).put(request, *args, **kwargs)


class ImportStorageSyncAPI(generics.GenericAPIView):

    permission_required = all_permissions.projects_change
    parser_classes = (JSONParser, FormParser, MultiPartParser)
    serializer_class = ImportStorageSerializer

    def get_queryset(self):
        ImportStorageClass = self.serializer_class.Meta.model
        return ImportStorageClass.objects.all()

    def post(self, request, *args, **kwargs):
        storage = self.get_object()
        # check connectivity & access, raise an exception if not satisfied
        if not storage.synchronizable:
            response_data = {'message': f'Storage {str(storage.id)} is not synchronizable'}
            return Response(status=status.HTTP_400_BAD_REQUEST, data=response_data)
        storage.validate_connection()
        storage.sync()
        storage.refresh_from_db()
        return Response(self.serializer_class(storage).data)


class ExportStorageSyncAPI(generics.GenericAPIView):

    permission_required = all_permissions.projects_change
    parser_classes = (JSONParser, FormParser, MultiPartParser)
    serializer_class = ExportStorageSerializer

    def get_queryset(self):
        ExportStorageClass = self.serializer_class.Meta.model
        return ExportStorageClass.objects.all()

    def post(self, request, *args, **kwargs):
        storage = self.get_object()
        # check connectivity & access, raise an exception if not satisfied
        if not storage.synchronizable:
            response_data = {'message': f'Storage {str(storage.id)} is not synchronizable'}
            return Response(status=status.HTTP_400_BAD_REQUEST, data=response_data)
        storage.validate_connection()
        storage.sync()
        storage.refresh_from_db()
        return Response(self.serializer_class(storage).data)


class StorageValidateAPI(generics.CreateAPIView):

    permission_required = all_permissions.projects_change
    parser_classes = (JSONParser, FormParser, MultiPartParser)

    def create(self, request, *args, **kwargs):
        storage_id = request.data.get('id')
        instance = None
        if storage_id:
            instance = generics.get_object_or_404(self.serializer_class.Meta.model.objects.all(), pk=storage_id)
            if not instance.has_permission(request.user):
                raise PermissionDenied()

        # combine instance fields with request.data
        serializer = self.get_serializer(data=request.data)
        serializer.is_valid(raise_exception=True)
        # if storage exists, we have to use instance from DB,
        # because instance from serializer won't have credentials, they were popped intentionally
        if instance:
            instance = serializer.update(instance, serializer.validated_data)
        else:
            instance = serializer.Meta.model(**serializer.validated_data)

        # double check: not all storages validate connection in serializer, just make another explicit check here
        try:
            instance.validate_connection()
        except Exception as exc:
            raise ValidationError(exc)
        return Response()


class StorageFormLayoutAPI(generics.RetrieveAPIView):

    permission_required = all_permissions.projects_change
    parser_classes = (JSONParser, FormParser, MultiPartParser)
    swagger_schema = None
    storage_type = None

    @swagger_auto_schema(auto_schema=None)
    def get(self, request, *args, **kwargs):
        form_layout_file = os.path.join(os.path.dirname(inspect.getfile(self.__class__)), 'form_layout.yml')
        if not os.path.exists(form_layout_file):
            raise NotFound(f'"form_layout.yml" is not found for {self.__class__.__name__}')

        form_layout = read_yaml(form_layout_file)
        form_layout = self.post_process_form(form_layout)
        return Response(form_layout[self.storage_type])

    def post_process_form(self, form_layout):
        return form_layout


class ImportStorageValidateAPI(StorageValidateAPI):
    serializer_class = ImportStorageSerializer


class ExportStorageValidateAPI(StorageValidateAPI):
    serializer_class = ExportStorageSerializer


class ImportStorageFormLayoutAPI(StorageFormLayoutAPI):
    storage_type = 'ImportStorage'


class ExportStorageFormLayoutAPI(StorageFormLayoutAPI):
    storage_type = 'ExportStorage'
</file>

<file path="label_studio/io_storages/base_models.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import base64
import json
import logging
import traceback as tb
from datetime import datetime
from typing import Union
from urllib.parse import urljoin

import django_rq
import rq
import rq.exceptions
from core.feature_flags import flag_set
from core.redis import is_job_in_queue, is_job_on_worker, redis_connected
from core.utils.common import load_func
from data_export.serializers import ExportDataSerializer
from django.conf import settings
from django.contrib.auth.models import AnonymousUser
from django.db import models, transaction
from django.db.models import JSONField
from django.shortcuts import reverse
from django.utils import timezone
from django.utils.translation import gettext_lazy as _
from django_rq import job
from io_storages.utils import get_uri_via_regex
from rq.job import Job
from tasks.models import Annotation, Task
from tasks.serializers import AnnotationSerializer, PredictionSerializer
from webhooks.models import WebhookAction
from webhooks.utils import emit_webhooks_for_instance

logger = logging.getLogger(__name__)


class StorageInfo(models.Model):
    """
    StorageInfo helps to understand storage status and progress
    that happens in background jobs
    """

    class Status(models.TextChoices):
        INITIALIZED = 'initialized', _('Initialized')
        QUEUED = 'queued', _('Queued')
        IN_PROGRESS = 'in_progress', _('In progress')
        FAILED = 'failed', _('Failed')
        COMPLETED = 'completed', _('Completed')

    class Meta:
        abstract = True

    last_sync = models.DateTimeField(_('last sync'), null=True, blank=True, help_text='Last sync finished time')
    last_sync_count = models.PositiveIntegerField(
        _('last sync count'), null=True, blank=True, help_text='Count of tasks synced last time'
    )
    last_sync_job = models.CharField(
        _('last_sync_job'), null=True, blank=True, max_length=256, help_text='Last sync job ID'
    )

    status = models.CharField(
        max_length=64,
        choices=Status.choices,
        default=Status.INITIALIZED,
    )
    traceback = models.TextField(null=True, blank=True, help_text='Traceback report for the last failed sync')
    meta = JSONField('meta', null=True, default=dict, help_text='Meta and debug information about storage processes')

    def info_set_job(self, job_id):
        self.last_sync_job = job_id
        self.save(update_fields=['last_sync_job'])

    def info_set_queued(self):
        self.last_sync = None
        self.last_sync_count = None
        self.last_sync_job = None
        self.status = self.Status.QUEUED

        # reset and init meta
        self.meta = {'attempts': self.meta.get('attempts', 0) + 1, 'time_queued': str(timezone.now())}

        self.save(update_fields=['last_sync_job', 'last_sync', 'last_sync_count', 'status', 'meta'])

    def info_set_in_progress(self):
        # only QUEUED => IN_PROGRESS transition is possible, because in QUEUED we reset states
        if self.status != self.Status.QUEUED:
            raise ValueError(f'Storage status ({self.status}) must be QUEUED to move it IN_PROGRESS')
        self.status = self.Status.IN_PROGRESS

        dt = timezone.now()
        self.meta['time_in_progress'] = str(dt)
        # at the very beginning it's the same as in progress time
        self.meta['time_last_ping'] = str(dt)
        self.save(update_fields=['status', 'meta'])

    @property
    def time_in_progress(self):
        return datetime.fromisoformat(self.meta['time_in_progress'])

    def info_set_completed(self, last_sync_count, **kwargs):
        self.status = self.Status.COMPLETED
        self.last_sync = timezone.now()
        self.last_sync_count = last_sync_count

        time_completed = timezone.now()

        self.meta['time_completed'] = str(time_completed)
        self.meta['duration'] = (time_completed - self.time_in_progress).total_seconds()
        self.meta.update(kwargs)
        self.save(update_fields=['status', 'meta', 'last_sync', 'last_sync_count'])

    def info_set_failed(self):
        self.status = self.Status.FAILED
        self.traceback = str(tb.format_exc())

        time_failure = timezone.now()

        self.meta['time_failure'] = str(time_failure)
        self.meta['duration'] = (time_failure - self.time_in_progress).total_seconds()
        self.save(update_fields=['status', 'traceback', 'meta'])

    def info_update_progress(self, last_sync_count, **kwargs):
        # update db counter once per 5 seconds to avid db overloads
        now = timezone.now()
        last_ping = datetime.fromisoformat(self.meta['time_last_ping'])
        delta = (now - last_ping).total_seconds()

        if delta > settings.STORAGE_IN_PROGRESS_TIMER:
            self.last_sync_count = last_sync_count
            self.meta['time_last_ping'] = str(now)
            self.meta['duration'] = (now - self.time_in_progress).total_seconds()
            self.meta.update(kwargs)
            self.save(update_fields=['last_sync_count', 'meta'])

    @staticmethod
    def ensure_storage_statuses(storages):
        """Check failed jobs and set storage status as failed if job is failed

        :param storages: Import or Export storages
        """
        # iterate over all storages
        storages = storages.only('id', 'last_sync_job', 'status', 'meta')
        for storage in storages:
            storage.health_check()

    def health_check(self):
        # get duration between last ping time and now
        now = timezone.now()
        last_ping = datetime.fromisoformat(self.meta.get('time_last_ping', str(now)))
        delta = (now - last_ping).total_seconds()

        # check redis connection
        if redis_connected():
            self.job_health_check()

        # in progress last ping time, job is not needed here
        if self.status == self.Status.IN_PROGRESS and delta > settings.STORAGE_IN_PROGRESS_TIMER * 5:
            self.status = self.Status.FAILED
            self.traceback = (
                'It appears the job was failed because the last ping time is too old, '
                'and no traceback information is available.\n'
                'This typically occurs if job was manually removed '
                'or workers reloaded unexpectedly.'
            )
            self.save(update_fields=['status', 'traceback'])
            logger.info(
                f'Storage {self} status moved to `failed` '
                f'because the job {self.last_sync_job} has too old ping time'
            )

    def job_health_check(self):
        Status = self.Status
        if self.status not in [Status.IN_PROGRESS, Status.QUEUED]:
            return

        queue = django_rq.get_queue('low')
        try:
            sync_job = Job.fetch(self.last_sync_job, connection=queue.connection)
            job_status = sync_job.get_status()
        except rq.exceptions.NoSuchJobError:
            job_status = 'not found'

        # broken synchronization between storage and job
        # this might happen when job was stopped because of OOM and on_failure wasn't called
        if job_status == 'failed':
            self.status = Status.FAILED
            self.traceback = (
                'It appears the job was terminated unexpectedly, '
                'and no traceback information is available.\n'
                'This typically occurs due to an out-of-memory (OOM) error.'
            )
            self.save(update_fields=['status', 'traceback'])
            logger.info(f'Storage {self} status moved to `failed` ' f'because of the failed job {self.last_sync_job}')

        # job is not found in redis (maybe deleted while redeploy), storage status is still active
        elif job_status == 'not found':
            self.status = Status.FAILED
            self.traceback = (
                'It appears the job was not found in redis, '
                'and no traceback information is available.\n'
                'This typically occurs if job was manually removed '
                'or workers reloaded unexpectedly.'
            )
            self.save(update_fields=['status', 'traceback'])
            logger.info(
                f'Storage {self} status moved to `failed` ' f'because the job {self.last_sync_job} was not found'
            )


class Storage(StorageInfo):
    url_scheme = ''

    title = models.CharField(_('title'), null=True, blank=True, max_length=256, help_text='Cloud storage title')
    description = models.TextField(_('description'), null=True, blank=True, help_text='Cloud storage description')
    created_at = models.DateTimeField(_('created at'), auto_now_add=True, help_text='Creation time')

    synchronizable = models.BooleanField(_('synchronizable'), default=True, help_text='If storage can be synced')

    def validate_connection(self, client=None):
        raise NotImplementedError('validate_connection is not implemented')

    class Meta:
        abstract = True


class ImportStorage(Storage):
    def iterkeys(self):
        return iter(())

    def get_data(self, key):
        raise NotImplementedError

    def generate_http_url(self, url):
        raise NotImplementedError

    def can_resolve_url(self, url: Union[str, None]) -> bool:
        return self.can_resolve_scheme(url)

    def can_resolve_scheme(self, url: Union[str, None]) -> bool:
        if not url:
            return False
        # TODO: Search for occurrences inside string, e.g. for cases like "gs://bucket/file.pdf" or "<embed src='gs://bucket/file.pdf'/>"
        _, prefix = get_uri_via_regex(url, prefixes=(self.url_scheme,))
        if prefix == self.url_scheme:
            return True
        # if not found any occurrences - this Storage can't resolve url
        return False

    def resolve_uri(self, uri, task=None):
        #  list of objects
        if isinstance(uri, list):
            resolved = []
            for item in uri:
                result = self.resolve_uri(item, task)
                resolved.append(result if result else item)
            return resolved

        # dict of objects
        elif isinstance(uri, dict):
            resolved = {}
            for key in uri.keys():
                result = self.resolve_uri(uri[key], task)
                resolved[key] = result if result else uri[key]
            return resolved

        # string: process one url
        elif isinstance(uri, str) and self.url_scheme in uri:
            try:
                # extract uri first from task data
                extracted_uri, _ = get_uri_via_regex(uri, prefixes=(self.url_scheme,))
                if not self.can_resolve_url(extracted_uri):
                    logger.debug(f'No storage info found for URI={uri}')
                    return

                if self.presign and task is not None:
                    proxy_url = urljoin(
                        settings.HOSTNAME,
                        reverse('data_import:task-storage-data-presign', kwargs={'task_id': task.id})
                        + f'?fileuri={base64.urlsafe_b64encode(extracted_uri.encode()).decode()}',
                    )
                    return uri.replace(extracted_uri, proxy_url)
                else:
                    # resolve uri to url using storages
                    http_url = self.generate_http_url(extracted_uri)

                return uri.replace(extracted_uri, http_url)
            except Exception:
                logger.info(f"Can't resolve URI={uri}", exc_info=True)

    def _scan_and_create_links_v2(self):
        # Async job execution for batch of objects:
        # e.g. GCS example
        # | "GetKey" >>  --> read file content into label_studio_semantic_search.indexer.RawDataObject repr
        # | "AggregateBatch" >> beam.Combine      --> combine read objects into a batch
        # | "AddObjects" >> label_studio_semantic_search.indexer.add_objects_from_bucket
        # --> add objects from batch to Vector DB
        # or for project task creation last step would be
        # | "AddObject" >> ImportStorage.add_task

        raise NotImplementedError

    @classmethod
    def add_task(cls, data, project, maximum_annotations, max_inner_id, storage, key, link_class):
        # predictions
        predictions = data.get('predictions', [])
        if predictions:
            if 'data' not in data:
                raise ValueError(
                    'If you use "predictions" field in the task, ' 'you must put "data" field in the task too'
                )

        # annotations
        annotations = data.get('annotations', [])
        cancelled_annotations = 0
        if annotations:
            if 'data' not in data:
                raise ValueError(
                    'If you use "annotations" field in the task, ' 'you must put "data" field in the task too'
                )
            cancelled_annotations = len([a for a in annotations if a.get('was_cancelled', False)])

        if 'data' in data and isinstance(data['data'], dict):
            data = data['data']

        with transaction.atomic():
            task = Task.objects.create(
                data=data,
                project=project,
                overlap=maximum_annotations,
                is_labeled=len(annotations) >= maximum_annotations,
                total_predictions=len(predictions),
                total_annotations=len(annotations) - cancelled_annotations,
                cancelled_annotations=cancelled_annotations,
                inner_id=max_inner_id,
            )

            link_class.create(task, key, storage)
            logger.debug(f'Create {storage.__class__.__name__} link with key={key} for task={task}')

            raise_exception = not flag_set(
                'ff_fix_back_dev_3342_storage_scan_with_invalid_annotations', user=AnonymousUser()
            )

            # add predictions
            logger.debug(f'Create {len(predictions)} predictions for task={task}')
            for prediction in predictions:
                prediction['task'] = task.id
                prediction['project'] = project.id
            prediction_ser = PredictionSerializer(data=predictions, many=True)
            if prediction_ser.is_valid(raise_exception=raise_exception):
                prediction_ser.save()

            # add annotations
            logger.debug(f'Create {len(annotations)} annotations for task={task}')
            for annotation in annotations:
                annotation['task'] = task.id
                annotation['project'] = project.id
            annotation_ser = AnnotationSerializer(data=annotations, many=True)
            if annotation_ser.is_valid(raise_exception=raise_exception):
                annotation_ser.save()
        return task
        # FIXME: add_annotation_history / post_process_annotations should be here

    def _scan_and_create_links(self, link_class):
        """
        TODO: deprecate this function and transform it to "pipeline" version  _scan_and_create_links_v2,
        TODO: it must be compatible with opensource, so old version is needed as well
        """
        # set in progress status for storage info
        self.info_set_in_progress()

        tasks_existed = tasks_created = 0
        maximum_annotations = self.project.maximum_annotations
        task = self.project.tasks.order_by('-inner_id').first()
        max_inner_id = (task.inner_id + 1) if task else 1

        tasks_for_webhook = []
        for key in self.iterkeys():
            # w/o Dataflow
            # pubsub.push(topic, key)
            # -> GF.pull(topic, key) + env -> add_task()
            logger.debug(f'Scanning key {key}')
            self.info_update_progress(last_sync_count=tasks_created, tasks_existed=tasks_existed)

            # skip if task already exists
            if link_class.exists(key, self):
                logger.debug(f'{self.__class__.__name__} link {key} already exists')
                tasks_existed += 1  # update progress counter
                continue

            logger.debug(f'{self}: found new key {key}')
            try:
                data = self.get_data(key)
            except (UnicodeDecodeError, json.decoder.JSONDecodeError) as exc:
                logger.debug(exc, exc_info=True)
                raise ValueError(
                    f'Error loading JSON from file "{key}".\nIf you\'re trying to import non-JSON data '
                    f'(images, audio, text, etc.), edit storage settings and enable '
                    f'"Treat every bucket object as a source file"'
                )

            task = self.add_task(data, self.project, maximum_annotations, max_inner_id, self, key, link_class)
            max_inner_id += 1

            # update progress counters for storage info
            tasks_created += 1

            # add task to webhook list
            tasks_for_webhook.append(task)

            # settings.WEBHOOK_BATCH_SIZE
            # `WEBHOOK_BATCH_SIZE` sets the maximum number of tasks sent in a single webhook call, ensuring manageable payload sizes.
            # When `tasks_for_webhook` accumulates tasks equal to/exceeding `WEBHOOK_BATCH_SIZE`, they're sent in a webhook via
            # `emit_webhooks_for_instance`, and `tasks_for_webhook` is cleared for new tasks.
            # If tasks remain in `tasks_for_webhook` at process end (less than `WEBHOOK_BATCH_SIZE`), they're sent in a final webhook
            # call to ensure all tasks are processed and no task is left unreported in the webhook.
            if len(tasks_for_webhook) >= settings.WEBHOOK_BATCH_SIZE:
                emit_webhooks_for_instance(
                    self.project.organization, self.project, WebhookAction.TASKS_CREATED, tasks_for_webhook
                )
                tasks_for_webhook = []
        if tasks_for_webhook:
            emit_webhooks_for_instance(
                self.project.organization, self.project, WebhookAction.TASKS_CREATED, tasks_for_webhook
            )

        self.project.update_tasks_states(
            maximum_annotations_changed=False, overlap_cohort_percentage_changed=False, tasks_number_changed=True
        )

        # sync is finished, set completed status for storage info
        self.info_set_completed(last_sync_count=tasks_created, tasks_existed=tasks_existed)

    def scan_and_create_links(self):
        """This is proto method - you can override it, or just replace ImportStorageLink by your own model"""
        self._scan_and_create_links(ImportStorageLink)

    def sync(self):
        if redis_connected():
            queue = django_rq.get_queue('low')
            meta = {'project': self.project.id, 'storage': self.id}
            if not is_job_in_queue(queue, 'import_sync_background', meta=meta) and not is_job_on_worker(
                job_id=self.last_sync_job, queue_name='low'
            ):
                self.info_set_queued()
                sync_job = queue.enqueue(
                    import_sync_background,
                    self.__class__,
                    self.id,
                    meta=meta,
                    project_id=self.project.id,
                    organization_id=self.project.organization.id,
                    on_failure=storage_background_failure,
                    job_timeout=settings.RQ_LONG_JOB_TIMEOUT,
                )
                self.info_set_job(sync_job.id)
                logger.info(f'Storage sync background job {sync_job.id} for storage {self} has been started')
        else:
            try:
                logger.info(f'Start syncing storage {self}')
                self.info_set_queued()
                import_sync_background(self.__class__, self.id)
            except Exception:
                storage_background_failure(self)

    class Meta:
        abstract = True


class ProjectStorageMixin(models.Model):
    project = models.ForeignKey(
        'projects.Project',
        related_name='%(app_label)s_%(class)ss',
        on_delete=models.CASCADE,
        help_text='A unique integer value identifying this project.',
    )

    def has_permission(self, user):
        user.project = self.project  # link for activity log
        if self.project.has_permission(user):
            return True
        return False

    class Meta:
        abstract = True


@job('low')
def import_sync_background(storage_class, storage_id, timeout=settings.RQ_LONG_JOB_TIMEOUT, **kwargs):
    storage = storage_class.objects.get(id=storage_id)
    storage.scan_and_create_links()


@job('low', timeout=settings.RQ_LONG_JOB_TIMEOUT)
def export_sync_background(storage_class, storage_id, **kwargs):
    storage = storage_class.objects.get(id=storage_id)
    storage.save_all_annotations()


def storage_background_failure(*args, **kwargs):
    # job is used in rqworker failure, extract storage id from job arguments
    if isinstance(args[0], rq.job.Job):
        sync_job = args[0]
        _class = sync_job.args[0]
        storage_id = sync_job.args[1]
        storage = _class.objects.filter(id=storage_id).first()
        if storage is None:
            logger.info(f'Storage {_class} {storage_id} not found at job {sync_job} failure')
            return

    # storage is used when redis and rqworkers are not available (e.g. in opensource)
    elif isinstance(args[0], Storage):
        # we have to load storage with the last states from DB
        # the current args[0] instance might be outdated
        storage_id = args[0].id
        storage = args[0].__class__.objects.filter(id=storage_id).first()
    else:
        raise ValueError(f'Unknown storage in {args}')

    # save info about failure for storage info
    storage.info_set_failed()


class ExportStorage(Storage, ProjectStorageMixin):
    can_delete_objects = models.BooleanField(
        _('can_delete_objects'), null=True, blank=True, help_text='Deletion from storage enabled'
    )

    def _get_serialized_data(self, annotation):
        user = self.project.organization.created_by
        flag = flag_set(
            'fflag_feat_optic_650_target_storage_task_format_long', user=user, override_system_default=False
        )
        if settings.FUTURE_SAVE_TASK_TO_STORAGE or flag:
            # export task with annotations
            # TODO: we have to rewrite save_all_annotations, because this func will be called for each annotation
            # TODO: instead of each task, however, we have to call it only once per task
            expand = ['annotations.reviews', 'annotations.completed_by']
            context = {'project': self.project}
            return ExportDataSerializer(annotation.task, context=context, expand=expand).data
        else:
            serializer_class = load_func(settings.STORAGE_ANNOTATION_SERIALIZER)
            # deprecated functionality - save only annotation
            return serializer_class(annotation, context={'project': self.project}).data

    def save_annotation(self, annotation):
        raise NotImplementedError

    def save_all_annotations(self):
        annotation_exported = 0
        total_annotations = Annotation.objects.filter(project=self.project).count()
        self.info_set_in_progress()
        self.cached_user = self.project.organization.created_by

        for annotation in Annotation.objects.filter(project=self.project).iterator(
            chunk_size=settings.STORAGE_EXPORT_CHUNK_SIZE
        ):
            annotation.cached_user = self.cached_user
            self.save_annotation(annotation)

            # update progress counters
            annotation_exported += 1
            self.info_update_progress(last_sync_count=annotation_exported, total_annotations=total_annotations)

        self.info_set_completed(last_sync_count=annotation_exported, total_annotations=total_annotations)

    def sync(self):
        if redis_connected():
            queue = django_rq.get_queue('low')
            self.info_set_queued()
            sync_job = queue.enqueue(
                export_sync_background,
                self.__class__,
                self.id,
                job_timeout=settings.RQ_LONG_JOB_TIMEOUT,
                project_id=self.project.id,
                organization_id=self.project.organization.id,
                on_failure=storage_background_failure,
            )
            self.info_set_job(sync_job.id)
            logger.info(f'Storage sync background job {sync_job.id} for storage {self} has been queued')
        else:
            try:
                logger.info(f'Start syncing storage {self}')
                self.info_set_queued()
                export_sync_background(self.__class__, self.id)
            except Exception:
                storage_background_failure(self)

    class Meta:
        abstract = True


class ImportStorageLink(models.Model):

    task = models.OneToOneField('tasks.Task', on_delete=models.CASCADE, related_name='%(app_label)s_%(class)s')
    key = models.TextField(_('key'), null=False, help_text='External link key')
    object_exists = models.BooleanField(
        _('object exists'), help_text='Whether object under external link still exists', default=True
    )
    created_at = models.DateTimeField(_('created at'), auto_now_add=True, help_text='Creation time')

    @classmethod
    def exists(cls, key, storage):
        return cls.objects.filter(key=key, storage=storage.id).exists()

    @classmethod
    def create(cls, task, key, storage):
        link, created = cls.objects.get_or_create(task_id=task.id, key=key, storage=storage, object_exists=True)
        return link

    class Meta:
        abstract = True


class ExportStorageLink(models.Model):

    annotation = models.ForeignKey(
        'tasks.Annotation', on_delete=models.CASCADE, related_name='%(app_label)s_%(class)s'
    )
    object_exists = models.BooleanField(
        _('object exists'), help_text='Whether object under external link still exists', default=True
    )
    created_at = models.DateTimeField(_('created at'), auto_now_add=True, help_text='Creation time')
    updated_at = models.DateTimeField(_('updated at'), auto_now=True, help_text='Update time')

    @staticmethod
    def get_key(annotation):
        # get user who created the organization explicitly using filter/values_list to avoid prefetching
        user = getattr(annotation, 'cached_user', None)
        # when signal for annotation save is called, user is not cached
        if user is None:
            user = annotation.project.organization.created_by
        flag = flag_set('fflag_feat_optic_650_target_storage_task_format_long', user=user)

        if settings.FUTURE_SAVE_TASK_TO_STORAGE or flag:
            ext = '.json' if settings.FUTURE_SAVE_TASK_TO_STORAGE_JSON_EXT or flag else ''
            return str(annotation.task.id) + ext
        else:
            return str(annotation.id)

    @property
    def key(self):
        return self.get_key(self.annotation)

    @classmethod
    def exists(cls, annotation, storage):
        return cls.objects.filter(annotation=annotation.id, storage=storage.id).exists()

    @classmethod
    def create(cls, annotation, storage):
        link, created = cls.objects.get_or_create(annotation=annotation, storage=storage, object_exists=True)
        if not created:
            # update updated_at field
            link.save()
        return link

    def has_permission(self, user):
        user.project = self.annotation.project  # link for activity log
        if self.annotation.has_permission(user):
            return True
        return False

    class Meta:
        abstract = True
</file>

<file path="label_studio/io_storages/filesystem.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import logging
import os
from copy import deepcopy

import ujson as json
from core.utils.io import delete_dir_content, iter_files, json_load, remove_file_or_dir

from .base import BaseForm, BaseStorage, CloudStorage

logger = logging.getLogger(__name__)


class JSONStorage(BaseStorage):

    description = 'JSON task file'

    def __init__(self, **kwargs):
        super(JSONStorage, self).__init__(**kwargs)
        tasks = {}
        if os.path.exists(self.path):
            tasks = json_load(self.path, int_keys=True)
        if len(tasks) == 0:
            self.data = {}
        elif isinstance(tasks, dict):
            self.data = tasks
        elif isinstance(self.data, list):
            self.data = {int(task['id']): task for task in tasks}
        self._save()

    def _save(self):
        with open(self.path, mode='w', encoding='utf8') as fout:
            json.dump(self.data, fout, ensure_ascii=False)

    @property
    def readable_path(self):
        return self.path

    def get(self, id):
        return self.data.get(int(id))

    def set(self, id, value):
        self.data[int(id)] = value
        self._save()

    def __contains__(self, id):
        return id in self.data

    def set_many(self, ids, values):
        for id, value in zip(ids, values):
            self.data[int(id)] = value
        self._save()

    def ids(self):
        return self.data.keys()

    def max_id(self):
        return max(self.ids(), default=-1)

    def items(self):
        return self.data.items()

    def remove(self, key):
        self.data.pop(int(key), None)
        self._save()

    def remove_all(self, ids=None):
        if ids is None:
            self.data = {}
        else:
            [self.data.pop(i, None) for i in ids]
        self._save()

    def empty(self):
        return len(self.data) == 0

    def sync(self):
        pass


def already_exists_error(what, path):
    raise RuntimeError(
        '{path} {what} already exists. Use "--force" option to recreate it.'.format(path=path, what=what)
    )


class DirJSONsStorage(BaseStorage):

    description = 'Directory with JSON task files'

    def __init__(self, **kwargs):
        super(DirJSONsStorage, self).__init__(**kwargs)
        os.makedirs(self.path, exist_ok=True)
        self.cache = {}

    @property
    def readable_path(self):
        return self.path

    def get(self, id):
        if id in self.cache:
            return self.cache[id]
        else:
            filename = os.path.join(self.path, str(id) + '.json')
            if os.path.exists(filename):
                data = json_load(filename)
                self.cache[id] = data
                return data

    def __contains__(self, id):
        return id in set(self.ids())

    def set(self, id, value):
        filename = os.path.join(self.path, str(id) + '.json')
        with open(filename, 'w', encoding='utf8') as fout:
            json.dump(value, fout, indent=2, sort_keys=True)
        self.cache[id] = value

    def set_many(self, keys, values):
        self.cache.clear()
        raise NotImplementedError

    def ids(self):
        for f in iter_files(self.path, '.json'):
            yield int(os.path.splitext(os.path.basename(f))[0])

    def max_id(self):
        return max(self.ids(), default=-1)

    def sync(self):
        pass

    def items(self):
        for id in self.ids():
            filename = os.path.join(self.path, str(id) + '.json')
            yield id, self.cache[id] if id in self.cache else json_load(filename)

    def remove(self, id):
        filename = os.path.join(self.path, str(id) + '.json')
        if os.path.exists(filename):
            os.remove(filename)
            self.cache.pop(id, None)

    def remove_all(self, ids=None):
        if ids is None:
            self.cache.clear()
            delete_dir_content(self.path)
        else:
            for i in ids:
                self.cache.pop(i, None)
                path = os.path.join(self.path, str(i) + '.json')
                try:
                    remove_file_or_dir(path)
                except OSError:
                    logger.warning('Storage file already removed: ' + path)

    def empty(self):
        return next(self.ids(), None) is None


class TasksJSONStorage(JSONStorage):

    form = BaseForm
    description = 'Local [loading tasks from "tasks.json" file]'

    def __init__(self, path, project_path, **kwargs):
        super(TasksJSONStorage, self).__init__(
            project_path=project_path, path=os.path.join(project_path, 'tasks.json')
        )


class ExternalTasksJSONStorage(CloudStorage):

    form = BaseForm
    description = 'Local [loading tasks from "tasks.json" file]'

    def __init__(self, name, path, project_path, prefix=None, create_local_copy=False, regex='.*', **kwargs):
        super(ExternalTasksJSONStorage, self).__init__(
            name=name,
            project_path=project_path,
            path=os.path.join(project_path, 'tasks.json'),
            use_blob_urls=False,
            prefix=None,
            regex=None,
            create_local_copy=False,
            sync_in_thread=False,
            **kwargs,
        )
        # data is used as a local cache for tasks.json file
        self.data = {}

    def _save(self):
        with open(self.path, mode='w', encoding='utf8') as fout:
            json.dump(self.data, fout, ensure_ascii=False)

    def _get_client(self):
        pass

    def validate_connection(self):
        pass

    @property
    def url_prefix(self):
        return ''

    @property
    def readable_path(self):
        return self.path

    def _get_value(self, key, inplace=False):
        return self.data[int(key)] if inplace else deepcopy(self.data[int(key)])

    def _set_value(self, key, value):
        self.data[int(key)] = value

    def set(self, id, value):
        with self.thread_lock:
            super(ExternalTasksJSONStorage, self).set(id, value)
            self._save()

    def set_many(self, ids, values):
        with self.thread_lock:
            for id, value in zip(ids, values):
                super(ExternalTasksJSONStorage, self)._pre_set(id, value)
            self._save_ids()
            self._save()

    def _extract_task_id(self, full_key):
        return int(full_key.split(self.key_prefix, 1)[-1])

    def iter_full_keys(self):
        return (self.key_prefix + key for key in self._get_objects())

    def _get_objects(self):
        self.data = json_load(self.path, int_keys=True)
        return (str(id) for id in self.data)

    def _remove_id_from_keys_map(self, id):
        full_key = self.key_prefix + str(id)
        assert id in self._ids_keys_map, 'No such task id: ' + str(id)
        assert self._ids_keys_map[id]['key'] == full_key, (self._ids_keys_map[id]['key'], full_key)
        self._selected_ids.remove(id)
        self._ids_keys_map.pop(id)
        self._keys_ids_map.pop(full_key)

    def remove(self, id):
        with self.thread_lock:
            id = int(id)

            logger.debug('Remove id=' + str(id) + ' from ids.json')
            self._remove_id_from_keys_map(id)
            self._save_ids()

            logger.debug('Remove id=' + str(id) + ' from tasks.json')
            self.data.pop(id, None)
            self._save()

    def remove_all(self, ids=None):
        with self.thread_lock:
            remove_ids = self.data if ids is None else ids

            logger.debug('Remove ' + str(len(remove_ids)) + ' records from ids.json')
            for id in remove_ids:
                self._remove_id_from_keys_map(id)
            self._save_ids()

            logger.debug('Remove all data from tasks.json')
            # remove record from tasks.json
            if ids is None:
                self.data = {}
            else:
                for id in remove_ids:
                    self.data.pop(id, None)
            self._save()


class AnnotationsDirStorage(DirJSONsStorage):

    form = BaseForm
    description = 'Local [annotations are in "annotations" directory]'

    def __init__(self, name, path, project_path, **kwargs):
        super(AnnotationsDirStorage, self).__init__(
            name=name, project_path=project_path, path=os.path.join(project_path, 'annotations')
        )
</file>

<file path="label_studio/io_storages/functions.py">
import logging
from typing import Dict, Iterable, List, Union

from io_storages.base_models import ImportStorage

from .azure_blob.api import AzureBlobExportStorageListAPI, AzureBlobImportStorageListAPI
from .gcs.api import GCSExportStorageListAPI, GCSImportStorageListAPI
from .redis.api import RedisExportStorageListAPI, RedisImportStorageListAPI
from .s3.api import S3ExportStorageListAPI, S3ImportStorageListAPI

logger = logging.getLogger(__name__)


def get_storage_list():
    return [
        {
            'name': 's3',
            'title': 'AWS S3',
            'import_list_api': S3ImportStorageListAPI,
            'export_list_api': S3ExportStorageListAPI,
        },
        {
            'name': 'gcs',
            'title': 'Google Cloud Storage',
            'import_list_api': GCSImportStorageListAPI,
            'export_list_api': GCSExportStorageListAPI,
        },
        {
            'name': 'azure',
            'title': 'Microsoft Azure',
            'import_list_api': AzureBlobImportStorageListAPI,
            'export_list_api': AzureBlobExportStorageListAPI,
        },
        {
            'name': 'redis',
            'title': 'Redis',
            'import_list_api': RedisImportStorageListAPI,
            'export_list_api': RedisExportStorageListAPI,
        },
    ]


def get_storage_by_url(url: Union[str, List, Dict], storage_objects: Iterable[ImportStorage]) -> ImportStorage:
    """Find the first compatible storage and returns storage that can emit pre-signed URL"""

    for storage_object in storage_objects:
        # check url is string because task can have int, float, dict, list
        # and 'can_resolve_url' will fail
        if isinstance(url, str) and storage_object.can_resolve_url(url):
            return storage_object

    # url is list or dict
    if isinstance(url, dict) or isinstance(url, list):
        for storage_object in storage_objects:
            if storage_object.can_resolve_url(url):
                # note: only first found storage_object will be used for link resolving
                # probably we need to use more advanced can_resolve_url mechanics
                # that takes into account not only prefixes, but bucket path too
                return storage_object
</file>

<file path="label_studio/io_storages/models.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
from django.conf import settings  # noqa: I001

from .azure_blob.models import (  # noqa: F401
    AzureBlobImportStorage,
    AzureBlobImportStorageLink,
    AzureBlobExportStorage,
    AzureBlobExportStorageLink,
)
from .s3.models import (  # noqa: F401
    S3ImportStorage,
    S3ImportStorageLink,
    S3ExportStorage,
    S3ExportStorageLink,
)
from .gcs.models import (  # noqa: F401
    GCSImportStorage,
    GCSImportStorageLink,
    GCSExportStorage,
    GCSExportStorageLink,
)
from .redis.models import (  # noqa: F401
    RedisImportStorage,
    RedisImportStorageLink,
    RedisExportStorage,
    RedisExportStorageLink,
)

from label_studio.core.utils.common import load_func


def get_storage_classes(storage_type='import'):
    """Helper function to return all registered ***ImportStorage classes.
    It's been made through the APIViews rather than using models directly to make it consistent with what we expose.
    Note: this func doesn't include LocalFiles storages!
    storage_type: import, export
    """
    storage_list = load_func(settings.GET_STORAGE_LIST)
    storage_classes = []
    for storage_decl in storage_list():
        storage_api_class = storage_decl[f'{storage_type}_list_api']
        storage_classes.append(storage_api_class.serializer_class.Meta.model)
    return storage_classes
</file>

<file path="label_studio/io_storages/permissions.py">
from rest_framework.permissions import BasePermission


class StoragePermission(BasePermission):
    """
    Checks if the user has access to the storage apis
    Default case is always true
    """

    def has_permission(self, request, view):
        return True
</file>

<file path="label_studio/io_storages/serializers.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import os

from io_storages.base_models import ExportStorage, ImportStorage
from rest_framework import serializers
from tasks.models import Task
from tasks.serializers import AnnotationSerializer, TaskSerializer
from users.models import User


class ImportStorageSerializer(serializers.ModelSerializer):
    type = serializers.ReadOnlyField(default=os.path.basename(os.path.dirname(__file__)))
    synchronizable = serializers.BooleanField(required=False, default=True)

    class Meta:
        model = ImportStorage
        fields = '__all__'


class ExportStorageSerializer(serializers.ModelSerializer):
    type = serializers.ReadOnlyField(default=os.path.basename(os.path.dirname(__file__)))
    synchronizable = serializers.BooleanField(required=False, default=True)

    class Meta:
        model = ExportStorage
        fields = '__all__'


class StorageTaskSerializer(TaskSerializer):
    def __init__(self, *args, **kwargs):
        # task is nested into the annotation, we don't need annotations in the task again
        kwargs['context'] = {'resolve_uri': False}
        super().__init__(*args, **kwargs)

    class Meta:
        model = Task
        fields = '__all__'


class StorageCompletedBySerializer(serializers.ModelSerializer):
    class Meta:
        model = User
        fields = ('id', 'first_name', 'last_name', 'email')


class StorageAnnotationSerializer(AnnotationSerializer):
    task = StorageTaskSerializer(read_only=True, omit=['annotations'])
    completed_by = StorageCompletedBySerializer(read_only=True)
</file>

<file path="label_studio/io_storages/urls.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
from django.conf import settings
from django.urls import include, path
from io_storages.all_api import (
    AllExportStorageListAPI,
    AllExportStorageTypesAPI,
    AllImportStorageListAPI,
    AllImportStorageTypesAPI,
)
from io_storages.azure_blob.api import (
    AzureBlobExportStorageDetailAPI,
    AzureBlobExportStorageFormLayoutAPI,
    AzureBlobExportStorageListAPI,
    AzureBlobExportStorageSyncAPI,
    AzureBlobExportStorageValidateAPI,
    AzureBlobImportStorageDetailAPI,
    AzureBlobImportStorageFormLayoutAPI,
    AzureBlobImportStorageListAPI,
    AzureBlobImportStorageSyncAPI,
    AzureBlobImportStorageValidateAPI,
)
from io_storages.gcs.api import (
    GCSExportStorageDetailAPI,
    GCSExportStorageFormLayoutAPI,
    GCSExportStorageListAPI,
    GCSExportStorageSyncAPI,
    GCSExportStorageValidateAPI,
    GCSImportStorageDetailAPI,
    GCSImportStorageFormLayoutAPI,
    GCSImportStorageListAPI,
    GCSImportStorageSyncAPI,
    GCSImportStorageValidateAPI,
)
from io_storages.localfiles.api import (
    LocalFilesExportStorageDetailAPI,
    LocalFilesExportStorageFormLayoutAPI,
    LocalFilesExportStorageListAPI,
    LocalFilesExportStorageSyncAPI,
    LocalFilesExportStorageValidateAPI,
    LocalFilesImportStorageDetailAPI,
    LocalFilesImportStorageFormLayoutAPI,
    LocalFilesImportStorageListAPI,
    LocalFilesImportStorageSyncAPI,
    LocalFilesImportStorageValidateAPI,
)
from io_storages.redis.api import (
    RedisExportStorageDetailAPI,
    RedisExportStorageFormLayoutAPI,
    RedisExportStorageListAPI,
    RedisExportStorageSyncAPI,
    RedisExportStorageValidateAPI,
    RedisImportStorageDetailAPI,
    RedisImportStorageFormLayoutAPI,
    RedisImportStorageListAPI,
    RedisImportStorageSyncAPI,
    RedisImportStorageValidateAPI,
)
from io_storages.s3.api import (
    S3ExportStorageDetailAPI,
    S3ExportStorageFormLayoutAPI,
    S3ExportStorageListAPI,
    S3ExportStorageSyncAPI,
    S3ExportStorageValidateAPI,
    S3ImportStorageDetailAPI,
    S3ImportStorageFormLayoutAPI,
    S3ImportStorageListAPI,
    S3ImportStorageSyncAPI,
    S3ImportStorageValidateAPI,
)

app_name = 'storages'

# IO Storages CRUD
_api_urlpatterns = [
    # All storages
    path('', AllImportStorageListAPI.as_view(), name='storage-list'),
    path('export', AllExportStorageListAPI.as_view(), name='export-storage-list'),
    path('types', AllImportStorageTypesAPI.as_view(), name='storage-types'),
    path('export/types', AllExportStorageTypesAPI.as_view(), name='export-storage-types'),
    # Amazon S3
    path('s3/', S3ImportStorageListAPI.as_view(), name='storage-s3-list'),
    path('s3/<int:pk>', S3ImportStorageDetailAPI.as_view(), name='storage-s3-detail'),
    path('s3/<int:pk>/sync', S3ImportStorageSyncAPI.as_view(), name='storage-s3-sync'),
    path('s3/validate', S3ImportStorageValidateAPI.as_view(), name='storage-s3-validate'),
    path('s3/form', S3ImportStorageFormLayoutAPI.as_view(), name='storage-s3-form'),
    path('export/s3', S3ExportStorageListAPI.as_view(), name='export-storage-s3-list'),
    path('export/s3/<int:pk>', S3ExportStorageDetailAPI.as_view(), name='export-storage-s3-detail'),
    path('export/s3/<int:pk>/sync', S3ExportStorageSyncAPI.as_view(), name='export-storage-s3-sync'),
    path('export/s3/validate', S3ExportStorageValidateAPI.as_view(), name='export-storage-s3-validate'),
    path('export/s3/form', S3ExportStorageFormLayoutAPI.as_view(), name='export-storage-s3-form'),
    # Microsoft Azure
    path('azure/', AzureBlobImportStorageListAPI.as_view(), name='storage-azure-list'),
    path('azure/<int:pk>', AzureBlobImportStorageDetailAPI.as_view(), name='storage-azure-detail'),
    path('azure/<int:pk>/sync', AzureBlobImportStorageSyncAPI.as_view(), name='storage-azure-sync'),
    path('azure/validate', AzureBlobImportStorageValidateAPI.as_view(), name='storage-azure-validate'),
    path('azure/form', AzureBlobImportStorageFormLayoutAPI.as_view(), name='storage-azure-form'),
    path('export/azure', AzureBlobExportStorageListAPI.as_view(), name='export-storage-azure-list'),
    path('export/azure/<int:pk>', AzureBlobExportStorageDetailAPI.as_view(), name='export-storage-azure-detail'),
    path('export/azure/<int:pk>/sync', AzureBlobExportStorageSyncAPI.as_view(), name='export-storage-azure-sync'),
    path('export/azure/validate', AzureBlobExportStorageValidateAPI.as_view(), name='export-storage-azure-validate'),
    path('export/azure/form', AzureBlobExportStorageFormLayoutAPI.as_view(), name='export-storage-azure-form'),
    # Google Cloud Storage
    path('gcs/', GCSImportStorageListAPI.as_view(), name='storage-gcs-list'),
    path('gcs/<int:pk>', GCSImportStorageDetailAPI.as_view(), name='storage-gcs-detail'),
    path('gcs/<int:pk>/sync', GCSImportStorageSyncAPI.as_view(), name='storage-gcs-sync'),
    path('gcs/validate', GCSImportStorageValidateAPI.as_view(), name='storage-gcs-validate'),
    path('gcs/form', GCSImportStorageFormLayoutAPI.as_view(), name='storage-gcs-form'),
    path('export/gcs', GCSExportStorageListAPI.as_view(), name='export-storage-gcs-list'),
    path('export/gcs/<int:pk>', GCSExportStorageDetailAPI.as_view(), name='export-storage-gcs-detail'),
    path('export/gcs/<int:pk>/sync', GCSExportStorageSyncAPI.as_view(), name='export-storage-gcs-sync'),
    path('export/gcs/validate', GCSExportStorageValidateAPI.as_view(), name='export-storage-gcs-validate'),
    path('export/gcs/form', GCSExportStorageFormLayoutAPI.as_view(), name='export-storage-gcs-form'),
    # Redis DB
    path('redis/', RedisImportStorageListAPI.as_view(), name='storage-redis-list'),
    path('redis/<int:pk>', RedisImportStorageDetailAPI.as_view(), name='storage-redis-detail'),
    path('redis/<int:pk>/sync', RedisImportStorageSyncAPI.as_view(), name='storage-redis-sync'),
    path('redis/validate', RedisImportStorageValidateAPI.as_view(), name='storage-redis-validate'),
    path('redis/form', RedisImportStorageFormLayoutAPI.as_view(), name='storage-redis-form'),
    path('export/redis', RedisExportStorageListAPI.as_view(), name='export-storage-redis-list'),
    path('export/redis/<int:pk>', RedisExportStorageDetailAPI.as_view(), name='export-storage-redis-detail'),
    path('export/redis/<int:pk>/sync', RedisExportStorageSyncAPI.as_view(), name='export-storage-redis-sync'),
    path('export/redis/validate', RedisExportStorageValidateAPI.as_view(), name='export-storage-redis-validate'),
    path('export/redis/form', RedisExportStorageFormLayoutAPI.as_view(), name='export-storage-redis-form'),
]
if settings.ENABLE_LOCAL_FILES_STORAGE:
    _api_urlpatterns += [
        # Local files
        path('localfiles/', LocalFilesImportStorageListAPI.as_view(), name='storage-localfiles-list'),
        path('localfiles/<int:pk>', LocalFilesImportStorageDetailAPI.as_view(), name='storage-localfiles-detail'),
        path('localfiles/<int:pk>/sync', LocalFilesImportStorageSyncAPI.as_view(), name='storage-localfiles-sync'),
        path('localfiles/validate', LocalFilesImportStorageValidateAPI.as_view(), name='storage-localfiles-validate'),
        path('localfiles/form', LocalFilesImportStorageFormLayoutAPI.as_view(), name='storage-localfiles-form'),
        path('export/localfiles', LocalFilesExportStorageListAPI.as_view(), name='export-storage-localfiles-list'),
        path(
            'export/localfiles/<int:pk>',
            LocalFilesExportStorageDetailAPI.as_view(),
            name='export-storage-localfiles-detail',
        ),
        path(
            'export/localfiles/<int:pk>/sync',
            LocalFilesExportStorageSyncAPI.as_view(),
            name='export-storage-localfiles-sync',
        ),
        path(
            'export/localfiles/validate',
            LocalFilesExportStorageValidateAPI.as_view(),
            name='export-storage-localfiles-validate',
        ),
        path(
            'export/localfiles/form',
            LocalFilesExportStorageFormLayoutAPI.as_view(),
            name='export-storage-localfiles-form',
        ),
    ]

urlpatterns = [
    path('api/storages/', include((_api_urlpatterns, app_name), namespace='api')),
]
</file>

<file path="label_studio/io_storages/utils.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import logging
import re
from dataclasses import dataclass
from typing import Union

logger = logging.getLogger(__name__)

# Put storage prefixes here
uri_regex = r"([\"'])(?P<uri>(?P<storage>{})://[^\1=]*)\1"


@dataclass
class BucketURI:
    bucket: str
    path: str
    scheme: str


def get_uri_via_regex(data, prefixes=('s3', 'gs')) -> tuple[Union[str, None], Union[str, None]]:
    data = str(data).strip()
    middle_check = False

    # make the fastest startswith check first
    for prefix in prefixes:
        if data.startswith(prefix):
            return data, prefix

        # another fast middle-check before regex run
        if prefix + ':' in data:
            middle_check = True

    # no prefixes in data, exit
    if middle_check is False:
        return None, None

    # make complex regex check for data like <a href="s3://test/123.jpg">
    try:
        uri_regex_prepared = uri_regex.format('|'.join(prefixes))
        r_match = re.search(uri_regex_prepared, data)
    except Exception as exc:
        logger.error(f"Can't parse task.data to match URI. Reason: {exc}", exc_info=True)
        return None, None
    else:
        if r_match is None:
            logger.warning("Can't parse task.data to match URI. Reason: Match is not found.")
            return None, None
    return r_match.group('uri'), r_match.group('storage')


def parse_bucket_uri(value: object, storage) -> Union[BucketURI, None]:
    if not value:
        return None

    uri, _ = get_uri_via_regex(value, prefixes=(storage.url_scheme,))
    if not uri:
        return None

    try:
        scheme, rest = uri.split('://', 1)
        bucket, path = rest.split('/', 1)
    except ValueError:
        return None

    return BucketURI(bucket=bucket, path=path, scheme=scheme)


def storage_can_resolve_bucket_url(storage, url) -> bool:
    if not storage.can_resolve_scheme(url):
        return False

    uri = parse_bucket_uri(url, storage)
    if not uri:
        return False

    storage_bucket: str | None = getattr(storage, 'bucket', None) or getattr(storage, 'container', None)
    if storage_bucket != uri.bucket:
        return False

    return True
</file>

<file path="label_studio/jwt_auth/migrations/0001_initial.py">
# Generated by Django 5.1.5 on 2025-02-11 21:16

import annoying.fields
import django.db.models.deletion
from django.db import migrations, models


class Migration(migrations.Migration):

    initial = True

    dependencies = [
        ('organizations', '0006_alter_organizationmember_deleted_at'),
    ]

    operations = [
        migrations.CreateModel(
            name='JWTSettings',
            fields=[
                ('organization', annoying.fields.AutoOneToOneField(on_delete=django.db.models.deletion.DO_NOTHING, primary_key=True, related_name='jwt', serialize=False, to='organizations.organization')),
                ('api_tokens_enabled', models.BooleanField(default=False, help_text='Enable JWT API token authentication for this organization', verbose_name='JWT API tokens enabled')),
                ('api_token_ttl_days', models.IntegerField(default=73000, help_text='Number of days before JWT API tokens expire', verbose_name='JWT API token time to live (days)')),
                ('legacy_api_tokens_enabled', models.BooleanField(default=True, help_text='Enable legacy API token authentication for this organization', verbose_name='legacy API tokens enabled')),
                ('created_at', models.DateTimeField(auto_now_add=True, verbose_name='created at')),
                ('updated_at', models.DateTimeField(auto_now=True, verbose_name='updated at')),
            ],
        ),
    ]
</file>

<file path="label_studio/jwt_auth/admin.py">
from django.contrib import admin
from rest_framework_simplejwt.token_blacklist.models import BlacklistedToken, OutstandingToken

# don't allow token management from admin console
admin.site.unregister(BlacklistedToken)
admin.site.unregister(OutstandingToken)
</file>

<file path="label_studio/jwt_auth/apps.py">
from django.apps import AppConfig


class JWTAuthConfig(AppConfig):
    name = 'jwt_auth'
</file>

<file path="label_studio/jwt_auth/auth.py">
import logging

from rest_framework.authentication import TokenAuthentication
from rest_framework.exceptions import AuthenticationFailed

logger = logging.getLogger(__name__)


class TokenAuthenticationPhaseout(TokenAuthentication):
    """TokenAuthentication with features to help phase out legacy token auth

    Logs usage and triggers a 401 if legacy token auth is not enabled for the organization."""

    def authenticate(self, request):
        """Authenticate the request and log if successful."""
        from core.feature_flags import flag_set

        auth_result = super().authenticate(request)
        JWT_ACCESS_TOKEN_ENABLED = flag_set('fflag__feature_develop__prompts__dia_1829_jwt_token_auth')
        if JWT_ACCESS_TOKEN_ENABLED and (auth_result is not None):
            user, _ = auth_result
            org = user.active_organization
            org_id = org.id if org else None

            # raise 401 if legacy API token auth disabled (i.e. this token is no longer valid)
            if org and (not org.jwt.legacy_api_tokens_enabled):
                raise AuthenticationFailed(
                    'Authentication token no longer valid: legacy token authentication has been disabled for this organization'
                )

            logger.info(
                'Legacy token authentication used',
                extra={'user_id': user.id, 'organization_id': org_id, 'endpoint': request.path},
            )
        return auth_result
</file>

<file path="label_studio/jwt_auth/middleware.py">
import logging

from django.contrib.auth import get_user_model
from django.http import JsonResponse
from rest_framework import status

logger = logging.getLogger(__name__)

User = get_user_model()


class JWTAuthenticationMiddleware:
    def __init__(self, get_response):
        self.get_response = get_response

    def __call__(self, request):
        from core.feature_flags import flag_set
        from rest_framework_simplejwt.authentication import JWTAuthentication
        from rest_framework_simplejwt.exceptions import AuthenticationFailed, InvalidToken, TokenError

        try:
            user_and_token = JWTAuthentication().authenticate(request)
            if user_and_token:
                user = User.objects.get(pk=user_and_token[0].pk)
                JWT_ACCESS_TOKEN_ENABLED = flag_set(
                    'fflag__feature_develop__prompts__dia_1829_jwt_token_auth', user=user
                )
                if JWT_ACCESS_TOKEN_ENABLED and user.active_organization.jwt.api_tokens_enabled:
                    request.user = user
                    request.is_jwt = True
        except User.DoesNotExist:
            logger.info('JWT authentication failed: User no longer exists')
            return JsonResponse({'detail': 'User not found'}, status=status.HTTP_401_UNAUTHORIZED)
        except (AuthenticationFailed, InvalidToken, TokenError) as e:
            logger.info('JWT authentication failed: %s', e)
            # don't raise 401 here, fallback to other auth methods (in case token is valid for them)
            # (have unit tests verifying that this still results in a 401 if other auth mechanisms fail)
        return self.get_response(request)
</file>

<file path="label_studio/jwt_auth/models.py">
from datetime import timedelta
from typing import Any

from annoying.fields import AutoOneToOneField
from django.db import models
from django.utils.translation import gettext_lazy as _
from organizations.models import Organization
from rest_framework_simplejwt.backends import TokenBackend
from rest_framework_simplejwt.exceptions import TokenError
from rest_framework_simplejwt.tokens import RefreshToken
from rest_framework_simplejwt.tokens import api_settings as simple_jwt_settings


class JWTSettings(models.Model):
    """Organization-specific JWT settings for authentication"""

    organization = AutoOneToOneField(Organization, related_name='jwt', primary_key=True, on_delete=models.DO_NOTHING)
    api_tokens_enabled = models.BooleanField(
        _('JWT API tokens enabled'),
        default=False,
        help_text='Enable JWT API token authentication for this organization',
    )
    api_token_ttl_days = models.IntegerField(
        _('JWT API token time to live (days)'),
        default=(200 * 365),  # "eternity", 200 years
        help_text='Number of days before JWT API tokens expire',
    )
    legacy_api_tokens_enabled = models.BooleanField(
        _('legacy API tokens enabled'),
        default=True,
        help_text='Enable legacy API token authentication for this organization',
    )

    created_at = models.DateTimeField(_('created at'), auto_now_add=True)
    updated_at = models.DateTimeField(_('updated at'), auto_now=True)

    def has_view_permission(self, user):
        """Any member of the organization can view JWT settings."""
        return self.organization.has_permission(user)

    def has_modify_permission(self, user):
        """Only organization owners/admins can modify JWT settings."""
        if not self.organization.has_permission(user):
            return False
        return user.is_owner or (hasattr(user, 'is_administrator') and user.is_administrator)

    def has_permission(self, user):
        """Only organization owners/admins can modify JWT settings."""
        return self.has_modify_permission(user)


class LSTokenBackend(TokenBackend):
    """A custom JWT token backend that truncates tokens before storing in the database.

    Extends simlpe jwt's TokenBackend to provide methods for generating both
    truncated tokens (header + payload only) and full tokens (header + payload + signature).
    This preserves privacy of the token by not exposing the signature to the frontend.
    """

    def encode(self, payload: dict[str, Any]) -> str:
        """Encode a payload into a truncated JWT token string.

        Args:
            payload: Dictionary containing the JWT claims to encode

        Returns:
            A truncated JWT string containing only the header and payload portions,
            with the signature section removed
        """
        header, payload, signature = super().encode(payload).split('.')
        return '.'.join([header, payload])

    def encode_full(self, payload: dict[str, Any]) -> str:
        """Encode a payload into a complete JWT token string.

        Args:
            payload: Dictionary containing the JWT claims to encode

        Returns:
            A complete JWT string containing header, payload and signature portions
        """
        return super().encode(payload)


class LSAPIToken(RefreshToken):
    """API token that utilizes JWT, but stores a truncated version and expires
    based on user settings

    This token class extends RefreshToken to provide organization-specific token
    lifetimes and support for truncated tokens. It uses the LSTokenBackend to
    securely store the token (without the signature).
    """

    lifetime = timedelta(days=365 * 200)  # "eternity" (200 years)

    _token_backend = LSTokenBackend(
        simple_jwt_settings.ALGORITHM,
        simple_jwt_settings.SIGNING_KEY,
        simple_jwt_settings.VERIFYING_KEY,
        simple_jwt_settings.AUDIENCE,
        simple_jwt_settings.ISSUER,
        simple_jwt_settings.JWK_URL,
        simple_jwt_settings.LEEWAY,
        simple_jwt_settings.JSON_ENCODER,
    )

    def get_full_jwt(self) -> str:
        """Get the complete JWT token string (including the signature).

        Returns:
            The full JWT token string with header, payload and signature
        """
        return self.get_token_backend().encode_full(self.payload)

    def blacklist(self):
        """Blacklist this token.

        Raises:
            rest_framework_simplejwt.exceptions.TokenError: If the token is already blacklisted.
        """
        self.check_blacklist()
        return super().blacklist()


class TruncatedLSAPIToken(LSAPIToken):
    """Handles JWT tokens that contain only header and payload (no signature).
    Used when frontend has access to truncated refresh tokens only."""

    def __init__(self, token, *args, **kwargs):
        """Initialize a truncated token, ensuring it has exactly 2 parts before adding a dummy signature."""
        # Ensure we have exactly 2 parts (header and payload)
        parts = token.split('.')
        if len(parts) > 2:
            token = '.'.join(parts[:2])
        elif len(parts) < 2:
            raise TokenError('Invalid Label Studio token')

        # Add dummy signature with exactly 43 'x' characters to match expected JWT signature length
        token = token + '.' + ('x' * 43)
        super().__init__(token, verify=False, *args, **kwargs)
</file>

<file path="label_studio/jwt_auth/serializers.py">
from jwt_auth.models import JWTSettings, LSAPIToken, TruncatedLSAPIToken
from rest_framework import serializers
from rest_framework_simplejwt.serializers import TokenBlacklistSerializer


# Recommended implementation from JWT to support drf-yasg:
# https://django-rest-framework-simplejwt.readthedocs.io/en/latest/drf_yasg_integration.html
class TokenRefreshResponseSerializer(serializers.Serializer):
    access = serializers.CharField()


class JWTSettingsSerializer(serializers.ModelSerializer):
    class Meta:
        model = JWTSettings
        fields = ('api_tokens_enabled', 'legacy_api_tokens_enabled')


class LSAPITokenCreateSerializer(serializers.Serializer):
    token = serializers.SerializerMethodField()

    def get_token(self, obj):
        return obj.get_full_jwt()

    class Meta:
        model = LSAPIToken
        fields = ['token']


class LSAPITokenListSerializer(LSAPITokenCreateSerializer):
    def get_token(self, obj):
        # only return header/payload portion of token, using LSTokenBackend
        return str(obj)


class LSAPITokenBlacklistSerializer(TokenBlacklistSerializer):
    token_class = TruncatedLSAPIToken
</file>

<file path="label_studio/jwt_auth/urls.py">
from django.urls import path

from . import views

app_name = 'jwt_auth'

urlpatterns = [
    path('api/jwt/settings', views.JWTSettingsAPI.as_view(), name='api-jwt-settings'),
    path('api/token/', views.LSAPITokenView.as_view(), name='token_manage'),
    path('api/token/refresh/', views.DecoratedTokenRefreshView.as_view(), name='token_refresh'),
    path('api/token/blacklist/', views.LSTokenBlacklistView.as_view(), name='token_blacklist'),
]
</file>

<file path="label_studio/jwt_auth/views.py">
import logging
from datetime import datetime

from core.permissions import all_permissions
from django.utils.decorators import method_decorator
from drf_yasg.utils import swagger_auto_schema
from jwt_auth.models import JWTSettings, LSAPIToken, TruncatedLSAPIToken
from jwt_auth.serializers import (
    JWTSettingsSerializer,
    LSAPITokenCreateSerializer,
    LSAPITokenListSerializer,
    TokenRefreshResponseSerializer,
)
from rest_framework import generics, status
from rest_framework.exceptions import APIException
from rest_framework.generics import CreateAPIView
from rest_framework.permissions import IsAuthenticated
from rest_framework.response import Response
from rest_framework_simplejwt.exceptions import TokenBackendError, TokenError
from rest_framework_simplejwt.token_blacklist.models import BlacklistedToken, OutstandingToken
from rest_framework_simplejwt.views import TokenRefreshView, TokenViewBase

logger = logging.getLogger(__name__)


class TokenExistsError(APIException):
    status_code = status.HTTP_409_CONFLICT
    default_detail = 'You already have a valid token. Please revoke it before creating a new one.'
    default_code = 'token_exists'


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['JWT'],
        operation_summary='Retrieve JWT Settings',
        operation_description='Retrieve JWT settings for the currently active organization.',
    ),
)
@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['JWT'],
        operation_summary='Update JWT Settings',
        operation_description='Update JWT settings for the currently active organization.',
    ),
)
class JWTSettingsAPI(CreateAPIView):
    queryset = JWTSettings.objects.all()
    serializer_class = JWTSettingsSerializer
    permission_required = all_permissions.organizations_view

    def get(self, request, *args, **kwargs):
        jwt_settings = request.user.active_organization.jwt
        # Check if user has view permission
        if not jwt_settings.has_view_permission(request.user):
            return Response(
                {'detail': 'You do not have permission to view JWT settings'}, status=status.HTTP_403_FORBIDDEN
            )
        return Response(self.get_serializer(jwt_settings).data)

    def post(self, request, *args, **kwargs):
        jwt_settings = request.user.active_organization.jwt
        # Check if user has modify permission
        if not jwt_settings.has_modify_permission(request.user):
            return Response(
                {'detail': 'You do not have permission to modify JWT settings'}, status=status.HTTP_403_FORBIDDEN
            )
        serializer = self.get_serializer(data=request.data, instance=jwt_settings)
        serializer.is_valid(raise_exception=True)
        serializer.save()
        return Response(serializer.data)


# Recommended implementation from JWT to support drf-yasg:
# https://django-rest-framework-simplejwt.readthedocs.io/en/latest/drf_yasg_integration.html
class DecoratedTokenRefreshView(TokenRefreshView):
    @swagger_auto_schema(
        tags=['JWT'],
        responses={
            status.HTTP_200_OK: TokenRefreshResponseSerializer,
        },
    )
    def post(self, request, *args, **kwargs):
        return super().post(request, *args, **kwargs)


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['JWT'],
        operation_summary='List API tokens',
        operation_description='List all API tokens for the current user.',
    ),
)
@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['JWT'],
        operation_summary='Create API token',
        operation_description='Create a new API token for the current user.',
    ),
)
class LSAPITokenView(generics.ListCreateAPIView):
    permission_classes = [IsAuthenticated]
    token_class = LSAPIToken

    def get_queryset(self):
        """Returns all non-expired non-blacklisted tokens for the current user.

        The `list` method handles filtering for refresh tokens (as opposed to access tokens),
        since simple-jwt makes it hard to do this at the DB level."""
        # Notably, if the list of non-expired blacklisted tokens ever gets too long
        # (e.g. users from orgs who have not set a token expiration for their org
        # revoke enough tokens for this to blow up), this will become inefficient.
        # Would be ideal to just add a "blacklisted" attr to our own subclass of
        # OutstandingToken so we can check at that level, or just clean up
        # OutstandingTokens that have been blacklisted every so often.
        current_blacklisted_tokens = BlacklistedToken.objects.filter(token__expires_at__gt=datetime.now()).values_list(
            'token_id', flat=True
        )
        return OutstandingToken.objects.filter(user_id=self.request.user.id, expires_at__gt=datetime.now()).exclude(
            id__in=current_blacklisted_tokens
        )

    def list(self, request, *args, **kwargs):
        all_tokens = self.get_queryset()

        def _maybe_get_token(token: OutstandingToken):
            try:
                return TruncatedLSAPIToken(str(token.token))
            except (TokenError, TokenBackendError) as e:  # expired/invalid token
                logger.debug('JWT API token validation failed: %s', e)
                return None

        # Annoyingly, token_type not stored directly so we have to filter it here.
        # Shouldn't be many unexpired tokens to iterate through.
        token_objects = list(filter(None, [_maybe_get_token(token) for token in all_tokens]))
        refresh_tokens = [tok for tok in token_objects if tok['token_type'] == 'refresh']

        serializer = self.get_serializer(refresh_tokens, many=True)
        data = serializer.data
        return Response(data)

    def get_serializer_class(self):
        if self.request.method == 'POST':
            return LSAPITokenCreateSerializer
        return LSAPITokenListSerializer

    def perform_create(self, serializer):
        # Check for existing valid tokens
        existing_tokens = self.get_queryset()
        if existing_tokens.exists():
            raise TokenExistsError()

        token = self.token_class.for_user(self.request.user)
        serializer.instance = token


class LSTokenBlacklistView(TokenViewBase):
    _serializer_class = 'jwt_auth.serializers.LSAPITokenBlacklistSerializer'

    @swagger_auto_schema(
        tags=['JWT'],
        operation_summary='Blacklist a JWT refresh token',
        operation_description='Adds a JWT refresh token to the blacklist, preventing it from being used to obtain new access tokens.',
        responses={
            status.HTTP_204_NO_CONTENT: 'Token was successfully blacklisted',
            status.HTTP_404_NOT_FOUND: 'Token is already blacklisted',
        },
    )
    def post(self, request, *args, **kwargs):
        serializer = self.get_serializer(data=request.data)
        try:
            # Notably, simple jwt's serializer (which we inherit from) calls
            # .blacklist() on the token under the hood
            serializer.is_valid(raise_exception=True)
        except TokenError as e:
            logger.error('Token error occurred while trying to blacklist a token: %s', str(e), exc_info=True)
            return Response({'detail': 'Token is invalid or already blacklisted.'}, status=status.HTTP_404_NOT_FOUND)

        return Response(status=status.HTTP_204_NO_CONTENT)
</file>

<file path="label_studio/labels_manager/migrations/0001_initial.py">
# Generated by Django 3.1.14 on 2022-01-31 09:56

import django.contrib.postgres.fields
import django.db.models.deletion
from django.conf import settings
from django.db import migrations, models


class Migration(migrations.Migration):

    initial = True

    dependencies = [
        ('projects', '0015_merge_20220117_0749'),
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
        ('organizations', '0003_auto_20211010_1339'),
    ]

    operations = [
        migrations.CreateModel(
            name='Label',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('created_at', models.DateTimeField(auto_now_add=True, help_text='Time of label creation', verbose_name='Created at')),
                ('updated_at', models.DateTimeField(auto_now=True, verbose_name='Updated at')),
                ('value', models.JSONField(verbose_name='value')),
                ('title', models.CharField(help_text='Label title', max_length=2048, verbose_name='Title')),
                ('description', models.TextField(blank=True, help_text='Label description', null=True, verbose_name='Description')),
                ('approved', models.BooleanField(default=False, help_text='Status of label')),
                ('approved_by', models.ForeignKey(help_text='User who approved this label', null=True, on_delete=django.db.models.deletion.CASCADE, related_name='labels_approved', to=settings.AUTH_USER_MODEL)),
                ('created_by', models.ForeignKey(help_text='User who made this label', on_delete=django.db.models.deletion.CASCADE, related_name='labels', to=settings.AUTH_USER_MODEL)),
                ('organization', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='labels', to='organizations.organization')),
            ],
        ),
        migrations.CreateModel(
            name='LabelLink',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('from_name', models.CharField(help_text='Label title', max_length=2048, verbose_name='Title')),
                ('label', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='links', to='labels_manager.label')),
                ('project', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='projects.project')),
            ],
        ),
        migrations.AddField(
            model_name='label',
            name='projects',
            field=models.ManyToManyField(through='labels_manager.LabelLink', to='projects.Project'),
        ),
        migrations.AddConstraint(
            model_name='label',
            constraint=models.UniqueConstraint(fields=('title', 'organization'), name='unique_title'),
        ),
    ]
</file>

<file path="label_studio/labels_manager/migrations/0002_auto_20220131_1325.py">
# Generated by Django 3.1.14 on 2022-01-31 13:25

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('labels_manager', '0001_initial'),
    ]

    operations = [
        migrations.AddConstraint(
            model_name='labellink',
            constraint=models.UniqueConstraint(fields=('project', 'label'), name='unique_label_project'),
        ),
    ]
</file>

<file path="label_studio/labels_manager/migrations/0003_auto_20221213_1612.py">
# Generated by Django 3.2.16 on 2022-12-13 16:12

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('labels_manager', '0002_auto_20220131_1325'),
    ]

    operations = [
        migrations.AlterField(
            model_name='label',
            name='updated_at',
            field=models.DateTimeField(auto_now=True, help_text='Time of label modification', verbose_name='Updated at'),
        ),
        migrations.AlterField(
            model_name='label',
            name='value',
            field=models.JSONField(help_text='Label value', verbose_name='value'),
        ),
        migrations.AlterField(
            model_name='labellink',
            name='from_name',
            field=models.CharField(help_text='Tag name', max_length=2048, verbose_name='Tag name'),
        ),
    ]
</file>

<file path="label_studio/labels_manager/api.py">
import logging

from core.permissions import ViewClassPermission, all_permissions
from django.db.models import CharField, Count, Q
from django.db.models.functions import Cast
from django.utils.decorators import method_decorator
from django_filters.rest_framework import DjangoFilterBackend
from drf_yasg.utils import swagger_auto_schema
from labels_manager.serializers import (
    LabelBulkUpdateSerializer,
    LabelCreateSerializer,
    LabelLinkSerializer,
    LabelSerializer,
)
from rest_framework import views, viewsets
from rest_framework.pagination import PageNumberPagination
from rest_framework.response import Response
from webhooks.utils import api_webhook, api_webhook_for_delete

from .functions import bulk_update_label
from .models import Label, LabelLink

logger = logging.getLogger(__name__)


@method_decorator(
    name='create',
    decorator=swagger_auto_schema(
        tags=['Labels'],
        x_fern_sdk_group_name='labels',
        x_fern_sdk_method_name='create',
        x_fern_audiences=['internal'],
        operation_summary='Create labels',
        operation_description='Add labels to your project without updating the labeling configuration.',
    ),
)
@method_decorator(
    name='destroy',
    decorator=swagger_auto_schema(
        tags=['Labels'],
        x_fern_sdk_group_name='labels',
        x_fern_sdk_method_name='delete',
        x_fern_audiences=['internal'],
        operation_summary='Remove labels',
        operation_description='Remove labels from your project without updating the labeling configuration.',
    ),
)
@method_decorator(
    name='partial_update',
    decorator=swagger_auto_schema(
        tags=['Labels'],
        x_fern_sdk_group_name='labels',
        x_fern_sdk_method_name='update',
        x_fern_audiences=['internal'],
        operation_summary='Update labels',
        operation_description='Update labels used for your project without updating the labeling configuration.',
    ),
)
@method_decorator(
    name='retrieve',
    decorator=swagger_auto_schema(
        tags=['Labels'],
        x_fern_sdk_group_name='labels',
        x_fern_sdk_method_name='get',
        x_fern_audiences=['internal'],
        operation_summary='Get label',
        operation_description="""
        Retrieve a specific custom label used for your project by its ID.
        """,
    ),
)
@method_decorator(
    name='list',
    decorator=swagger_auto_schema(
        tags=['Labels'],
        x_fern_sdk_group_name='labels',
        x_fern_sdk_method_name='list',
        x_fern_audiences=['internal'],
        operation_summary='List labels',
        operation_description='List all custom labels added to your project separately from the labeling configuration.',
    ),
)
@method_decorator(name='update', decorator=swagger_auto_schema(auto_schema=None))
class LabelAPI(viewsets.ModelViewSet):
    pagination_class = PageNumberPagination
    serializer_class = LabelSerializer
    permission_required = ViewClassPermission(
        GET=all_permissions.labels_view,
        POST=all_permissions.labels_create,
        PATCH=all_permissions.labels_change,
        DELETE=all_permissions.labels_delete,
    )

    def get_serializer(self, *args, **kwargs):
        """POST request is bulk by default"""
        if self.action == 'create':
            kwargs['many'] = True
        return super().get_serializer(*args, **kwargs)

    def perform_create(self, serializer):
        serializer.save(created_by=self.request.user, organization=self.request.user.active_organization)

    def get_queryset(self):
        return Label.objects.filter(organization=self.request.user.active_organization).prefetch_related('links')

    def get_serializer_class(self):
        if self.request.method == 'POST':
            return LabelCreateSerializer

        return self.serializer_class


@method_decorator(
    name='create',
    decorator=swagger_auto_schema(
        tags=['Labels'],
        x_fern_sdk_group_name=['projects', 'labels'],
        x_fern_sdk_method_name='create',
        x_fern_audiences=['internal'],
        operation_summary='Create label links',
        operation_description='Create label links to link new custom labels to your project labeling configuration.',
    ),
)
@method_decorator(
    name='destroy',
    decorator=swagger_auto_schema(
        tags=['Labels'],
        x_fern_sdk_group_name=['projects', 'labels'],
        x_fern_sdk_method_name='delete',
        x_fern_audiences=['internal'],
        operation_summary='Remove label link',
        operation_description="""
        Remove a label link that links custom labels to your project labeling configuration. If you remove a label link,
        the label stops being available for the project it was linked to. You can add a new label link at any time. 
        """,
    ),
)
@method_decorator(
    name='partial_update',
    decorator=swagger_auto_schema(
        tags=['Labels'],
        x_fern_sdk_group_name=['projects', 'labels'],
        x_fern_sdk_method_name='update',
        x_fern_audiences=['internal'],
        operation_summary='Update label link',
        operation_description="""
        Update a label link that links custom labels to a project labeling configuration, for example if the fromName,  
        toName, or name parameters for a tag in the labeling configuration change. 
        """,
    ),
)
@method_decorator(
    name='retrieve',
    decorator=swagger_auto_schema(
        tags=['Labels'],
        x_fern_sdk_group_name=['projects', 'labels'],
        x_fern_sdk_method_name='get',
        x_fern_audiences=['internal'],
        operation_summary='Get label link',
        operation_description='Get label links for a specific project configuration. ',
    ),
)
@method_decorator(
    name='list',
    decorator=swagger_auto_schema(
        tags=['Labels'],
        x_fern_sdk_group_name=['projects', 'labels'],
        x_fern_sdk_method_name='list',
        x_fern_audiences=['internal'],
        operation_summary='List label links',
        operation_description='List label links for a specific label and project.',
    ),
)
@method_decorator(name='update', decorator=swagger_auto_schema(auto_schema=None))
class LabelLinkAPI(viewsets.ModelViewSet):
    filter_backends = [DjangoFilterBackend]
    filterset_fields = {
        'project': ['exact'],
        'label__created_at': ['exact', 'gte', 'lte'],
        'label__created_by': ['exact'],
    }
    pagination_class = PageNumberPagination
    serializer_class = LabelLinkSerializer
    permission_required = ViewClassPermission(
        GET=all_permissions.labels_view,
        POST=all_permissions.labels_create,
        PATCH=all_permissions.labels_change,
        DELETE=all_permissions.labels_delete,
    )

    def get_queryset(self):
        return LabelLink.objects.filter(label__organization=self.request.user.active_organization).annotate(
            annotations_count=Count(
                'project__tasks__annotations',
                filter=Q(
                    project__tasks__annotations__result__icontains=Cast('label__value', output_field=CharField())
                ),
            )
        )

    @api_webhook('LABEL_LINK_UPDATED')
    def update(self, request, *args, **kwargs):
        return super().update(request, *args, **kwargs)

    @api_webhook_for_delete('LABEL_LINK_DELETED')
    def destroy(self, request, *args, **kwargs):
        return super().destroy(request, *args, **kwargs)


@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Labels'],
        x_fern_sdk_group_name=['projects', 'labels'],
        x_fern_sdk_method_name='update_many',
        x_fern_audiences=['internal'],
        operation_summary='Bulk update labels',
        operation_description="""
        If you want to update the labels in saved annotations, use this endpoint.
        """,
    ),
)
class LabelBulkUpdateAPI(views.APIView):
    permission_required = all_permissions.labels_change

    def post(self, request):
        serializer = LabelBulkUpdateSerializer(data=request.data)
        serializer.is_valid(raise_exception=True)
        project = serializer.validated_data['project']
        if project is not None:
            self.check_object_permissions(self.request, project)

        updated_count = bulk_update_label(
            old_label=serializer.validated_data['old_label'],
            new_label=serializer.validated_data['new_label'],
            organization=self.request.user.active_organization,
            project=project,
        )
        return Response({'annotations_updated': updated_count})
</file>

<file path="label_studio/labels_manager/apps.py">
from django.apps import AppConfig


class LabelsManagerConfig(AppConfig):
    name = 'labels_manager'
</file>

<file path="label_studio/labels_manager/exceptions.py">
from core.utils.exceptions import LabelStudioAPIException
from rest_framework import status


class LabelBulkUpdateError(LabelStudioAPIException):
    status_code = status.HTTP_422_UNPROCESSABLE_ENTITY
</file>

<file path="label_studio/labels_manager/functions.py">
from django.db import transaction
from tasks.models import Annotation


def bulk_update_label(old_label, new_label, organization, project=None):
    annotations = Annotation.objects.filter(project__organization=organization)
    if project is not None:
        annotations = annotations.filter(project=project)

    updated_count = 0
    with transaction.atomic():
        update_annotations = []
        for annotation in annotations.only('result').all():
            result = annotation.result

            updated_result = []
            need_update = False
            for region in result:
                result_type = region.get('type')
                if result_type is not None:
                    label = region['value'].get(result_type)
                    if label is not None and label == old_label:
                        region['value'][result_type] = new_label
                        updated_count += 1
                        need_update = True
                updated_result.append(region)

            if need_update:
                annotation.result = updated_result
                update_annotations.append(annotation)

        if update_annotations:
            Annotation.objects.bulk_update(update_annotations, ['result'])
    return updated_count
</file>

<file path="label_studio/labels_manager/models.py">
from django.conf import settings
from django.db import models
from django.utils.translation import gettext_lazy as _


class Label(models.Model):
    created_at = models.DateTimeField(_('Created at'), auto_now_add=True, help_text='Time of label creation')
    updated_at = models.DateTimeField(_('Updated at'), auto_now=True, help_text='Time of label modification')
    created_by = models.ForeignKey(
        settings.AUTH_USER_MODEL, related_name='labels', on_delete=models.CASCADE, help_text='User who made this label'
    )
    value = models.JSONField('value', null=False, help_text='Label value')
    title = models.CharField(_('Title'), max_length=2048, help_text='Label title')
    description = models.TextField(_('Description'), help_text='Label description', blank=True, null=True)
    approved_by = models.ForeignKey(
        settings.AUTH_USER_MODEL,
        related_name='labels_approved',
        on_delete=models.CASCADE,
        help_text='User who approved this label',
        null=True,
    )
    approved = models.BooleanField(default=False, help_text='Status of label')
    projects = models.ManyToManyField('projects.Project', through='LabelLink')
    organization = models.ForeignKey('organizations.Organization', related_name='labels', on_delete=models.CASCADE)

    def has_permission(self, user):
        return self.organization_id == user.active_organization_id

    class Meta:
        constraints = [models.UniqueConstraint(fields=['title', 'organization'], name='unique_title')]


class LabelLink(models.Model):
    project = models.ForeignKey('projects.Project', on_delete=models.CASCADE)
    label = models.ForeignKey(Label, on_delete=models.CASCADE, related_name='links')
    from_name = models.CharField(_('Tag name'), max_length=2048, help_text='Tag name')

    class Meta:
        constraints = [models.UniqueConstraint(fields=['project', 'label'], name='unique_label_project')]

    def has_permission(self, user):
        user.project = self.project  # link for activity log
        return self.project.has_permission(user)
</file>

<file path="label_studio/labels_manager/serializers.py">
from django.conf import settings
from django.db import transaction
from projects.models import Project
from rest_flex_fields import FlexFieldsModelSerializer
from rest_framework import serializers
from rest_framework.exceptions import ValidationError

from .models import Label, LabelLink


class LabelListSerializer(serializers.ListSerializer):
    def validate(self, items):
        if len(set(item['project'] for item in items)) > 1:
            raise ValidationError('Creating labels for different projects in one request not allowed')
        return items

    def create(self, validated_data):
        """Bulk creation objects of Label model with related LabelLink
        reusing already existing labels
        """
        from webhooks.utils import emit_webhooks_for_instance

        with transaction.atomic():
            # loading already existing labels
            titles = [item['title'] for item in validated_data]
            existing_labels = Label.objects.filter(
                organization=self.context['request'].user.active_organization, title__in=titles
            ).all()
            existing_labels_map = {label.title: label for label in existing_labels}

            # create objects for labels, that we need to create
            labels_data = []
            labels = []
            labels_create = []
            for item in validated_data:
                project = item.pop('project')
                from_name = item.pop('from_name')
                if item['title'] in existing_labels_map:
                    label = existing_labels_map[item['title']]
                else:
                    label = Label(**item)
                    labels_create.append(label)
                labels.append(label)
                labels_data.append(dict(project=project, from_name=from_name))

            if labels_create:
                if settings.DJANGO_DB == settings.DJANGO_DB_SQLITE:
                    created_labels = {}
                    for label in labels_create:
                        label.save()
                        created_labels[label.title] = label
                else:
                    created_labels = {label.title: label for label in Label.objects.bulk_create(labels_create)}

            # connect existing and created labels to project with LabelLink
            links = []
            result = []
            for index, label in enumerate(labels):
                if label.id is None:
                    label = created_labels[label.title]
                label.project = labels_data[index]['project']
                label.from_name = labels_data[index]['from_name']
                result.append(label)
                links.append(
                    LabelLink(
                        **{
                            'label': label,
                            'project': labels_data[index]['project'],
                            'from_name': labels_data[index]['from_name'],
                        }
                    )
                )

            links = LabelLink.objects.bulk_create(links, ignore_conflicts=True)
            # webhooks processing
            # bulk_create with ignore_conflicts doesn't return ids, reloading links
            project = labels[0].project
            label_ids = [label.id for label in result]
            links = LabelLink.objects.filter(label_id__in=label_ids, project=project).all()
            if links:
                emit_webhooks_for_instance(
                    self.context['request'].user.active_organization, links[0].project, 'LABEL_LINK_CREATED', links
                )

        return result


class LabelCreateSerializer(serializers.ModelSerializer):
    created_by = serializers.PrimaryKeyRelatedField(required=False, read_only=True)
    organization = serializers.PrimaryKeyRelatedField(required=False, read_only=True)
    project = serializers.PrimaryKeyRelatedField(queryset=Project.objects.all())
    from_name = serializers.CharField()

    class Meta:
        model = Label
        list_serializer_class = LabelListSerializer
        fields = '__all__'


class LabelLinkSerializer(FlexFieldsModelSerializer):
    annotations_count = serializers.IntegerField(read_only=True)

    class Meta:
        model = LabelLink
        fields = '__all__'
        expandable_fields = {'label': ('labels_manager.serializers.LabelSerializer', {'omit': ['links', 'projects']})}


class LabelSerializer(FlexFieldsModelSerializer):
    links = serializers.PrimaryKeyRelatedField(many=True, read_only=True)

    class Meta:
        model = Label
        fields = '__all__'
        expandable_fields = {'links': ('labels_manager.serializers.LabelLinkSerializer', {'many': True})}


class LabelBulkUpdateSerializer(serializers.Serializer):
    project = serializers.PrimaryKeyRelatedField(queryset=Project.objects.all(), required=False, default=None)
    old_label = serializers.JSONField()
    new_label = serializers.JSONField()
</file>

<file path="label_studio/labels_manager/urls.py">
from django.urls import include, path
from rest_framework.routers import DefaultRouter

from . import api

app_name = 'labels_manager'

router = DefaultRouter()
router.register(r'labels', api.LabelAPI, basename='label')
router.register(r'label_links', api.LabelLinkAPI, basename='label_link')
api_urlpatterns = router.urls

urlpatterns = [
    path('api/', include((api_urlpatterns, app_name), namespace='api-labels')),
    path('api/labels/bulk', api.LabelBulkUpdateAPI.as_view(), name='api-labels-bulk'),
]
</file>

<file path="label_studio/ml/migrations/__init__.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
</file>

<file path="label_studio/ml/migrations/0001_initial.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
# Generated by Django 3.1.4 on 2021-02-23 07:32

from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    initial = True

    dependencies = [
        ('projects', '0001_squashed_0065_auto_20210223_2014'),
    ]

    operations = [
        migrations.CreateModel(
            name='MLBackend',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('state', models.CharField(choices=[('CO', 'Connected'), ('DI', 'Disconnected'), ('ER', 'Error'), ('TR', 'Training'), ('PR', 'Predicting')], default='DI', max_length=2)),
                ('url', models.TextField(help_text='Model server URL', verbose_name='url')),
                ('error_message', models.TextField(blank=True, help_text='Error message in error state', null=True, verbose_name='url')),
                ('title', models.TextField(blank=True, default='default', help_text='Machine Learning backend title', null=True, verbose_name='title')),
                ('description', models.TextField(blank=True, default='', help_text='Machine Learning backend description', null=True, verbose_name='description')),
                ('model_version', models.TextField(blank=True, default='', help_text='Current model version associated with this ML backend', null=True, verbose_name='model version')),
                ('created_at', models.DateTimeField(auto_now_add=True, verbose_name='created at')),
                ('updated_at', models.DateTimeField(auto_now=True, verbose_name='updated at')),
                ('project', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='ml_backends', to='projects.project')),
            ],
        ),
        migrations.CreateModel(
            name='MLBackendTrainJob',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('job_id', models.CharField(max_length=128)),
                ('model_version', models.TextField(blank=True, help_text='Model version this job associated with', null=True, verbose_name='model version')),
                ('created_at', models.DateTimeField(auto_now_add=True, verbose_name='created at')),
                ('updated_at', models.DateTimeField(auto_now=True, verbose_name='updated at')),
                ('ml_backend', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='train_jobs', to='ml.mlbackend')),
            ],
        ),
        migrations.CreateModel(
            name='MLBackendPredictionJob',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('job_id', models.CharField(max_length=128)),
                ('model_version', models.TextField(blank=True, help_text='Model version this job associated with', null=True, verbose_name='model version')),
                ('batch_size', models.PositiveSmallIntegerField(default=100, help_text='Number of tasks processed per batch', verbose_name='batch size')),
                ('created_at', models.DateTimeField(auto_now_add=True, verbose_name='created at')),
                ('updated_at', models.DateTimeField(auto_now=True, verbose_name='updated at')),
                ('ml_backend', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='prediction_jobs', to='ml.mlbackend')),
            ],
        ),
    ]
</file>

<file path="label_studio/ml/migrations/0002_auto_20210308_1559.py">
# Generated by Django 3.1.4 on 2021-03-08 15:59

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('ml', '0001_initial'),
    ]

    operations = [
        migrations.AlterField(
            model_name='mlbackend',
            name='description',
            field=models.TextField(blank=True, default='', help_text='Description for the Machine Learning backend', null=True, verbose_name='description'),
        ),
        migrations.AlterField(
            model_name='mlbackend',
            name='title',
            field=models.TextField(blank=True, default='default', help_text='Name of the Machine Learning backend', null=True, verbose_name='title'),
        ),
        migrations.AlterField(
            model_name='mlbackend',
            name='url',
            field=models.TextField(help_text='URL for the ML model server', verbose_name='url'),
        ),
        migrations.AlterField(
            model_name='mlbackendtrainjob',
            name='model_version',
            field=models.TextField(blank=True, help_text='Model version this job is associated with', null=True, verbose_name='model version'),
        ),
    ]
</file>

<file path="label_studio/ml/migrations/0003_auto_20210309_1239.py">
# Generated by Django 3.1.4 on 2021-03-09 12:39

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('ml', '0002_auto_20210308_1559'),
    ]

    operations = [
        migrations.AlterField(
            model_name='mlbackend',
            name='description',
            field=models.TextField(blank=True, default='', help_text='Description for the machine learning backend', null=True, verbose_name='description'),
        ),
        migrations.AlterField(
            model_name='mlbackend',
            name='model_version',
            field=models.TextField(blank=True, default='', help_text='Current model version associated with this machine learning backend', null=True, verbose_name='model version'),
        ),
        migrations.AlterField(
            model_name='mlbackend',
            name='title',
            field=models.TextField(blank=True, default='default', help_text='Name of the machine learning backend', null=True, verbose_name='title'),
        ),
        migrations.AlterField(
            model_name='mlbackend',
            name='url',
            field=models.TextField(help_text='URL for the machine learning model server', verbose_name='url'),
        ),
    ]
</file>

<file path="label_studio/ml/migrations/0004_auto_20210820_1610.py">
# Generated by Django 3.1.12 on 2021-08-20 16:10

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('ml', '0003_auto_20210309_1239'),
    ]

    operations = [
        migrations.AddField(
            model_name='mlbackend',
            name='is_interactive',
            field=models.BooleanField(default=False, help_text="It's used for interactive annotating. If true, model has to return one-length list with results", verbose_name='is_interactive'),
        ),
        migrations.AddField(
            model_name='mlbackend',
            name='timeout',
            field=models.FloatField(blank=True, default=100.0, help_text='Responce model timeout', verbose_name='timeout'),
        ),
        migrations.AlterField(
            model_name='mlbackend',
            name='error_message',
            field=models.TextField(blank=True, help_text='Error message in error state', null=True, verbose_name='error_message'),
        ),
    ]
</file>

<file path="label_studio/ml/migrations/0005_auto_20211010_1344.py">
# Generated by Django 3.1.13 on 2021-10-10 13:44

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('ml', '0004_auto_20210820_1610'),
    ]

    operations = [
        migrations.AlterField(
            model_name='mlbackend',
            name='is_interactive',
            field=models.BooleanField(default=False, help_text='Used to interactively annotate tasks. If true, model returns one list with results', verbose_name='is_interactive'),
        ),
        migrations.AlterField(
            model_name='mlbackend',
            name='timeout',
            field=models.FloatField(blank=True, default=100.0, help_text='Response model timeout', verbose_name='timeout'),
        ),
        migrations.AlterField(
            model_name='mlbackendpredictionjob',
            name='model_version',
            field=models.TextField(blank=True, help_text='Model version this job is associated with', null=True, verbose_name='model version'),
        ),
    ]
</file>

<file path="label_studio/ml/migrations/0006_mlbackend_auto_update.py">
# Generated by Django 3.1.14 on 2022-02-14 15:32

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('ml', '0005_auto_20211010_1344'),
    ]

    operations = [
        migrations.AddField(
            model_name='mlbackend',
            name='auto_update',
            field=models.BooleanField(default=True, help_text='If false, model version is set by the user, if true - getting latest version from backend.', verbose_name='auto_update'),
        ),
    ]
</file>

<file path="label_studio/ml/migrations/0007_auto_20240314_1957.py">
# Generated by Django 3.2.24 on 2024-03-14 19:57

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('ml', '0006_mlbackend_auto_update'),
    ]

    operations = [
        migrations.AddField(
            model_name='mlbackend',
            name='auth_method',
            field=models.CharField(choices=[('NONE', 'None'), ('BASIC_AUTH', 'Basic Auth')], default='NONE', max_length=255),
        ),
        migrations.AddField(
            model_name='mlbackend',
            name='basic_auth_pass',
            field=models.TextField(blank=True, default='', help_text='HTTP Basic Auth password', null=True, verbose_name='basic auth password'),
        ),
        migrations.AddField(
            model_name='mlbackend',
            name='basic_auth_user',
            field=models.TextField(blank=True, default='', help_text='HTTP Basic Auth user', null=True, verbose_name='basic auth user'),
        ),
        migrations.AddField(
            model_name='mlbackend',
            name='extra_params',
            field=models.JSONField(help_text='Any extra parameters passed to the ML Backend during the setup', null=True, verbose_name='extra params'),
        ),
    ]
</file>

<file path="label_studio/ml/__init__.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
</file>

<file path="label_studio/ml/api_connector.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import logging
import os
import urllib

import requests
from core.feature_flags import flag_set
from core.utils.common import load_func
from core.version import get_git_version
from data_export.serializers import ExportDataSerializer
from django.conf import settings
from django.contrib.auth.models import AnonymousUser
from django.db.models import Count
from requests.adapters import HTTPAdapter
from requests.auth import HTTPBasicAuth

from label_studio.core.utils.params import get_env

version = get_git_version()
logger = logging.getLogger(__name__)

CONNECTION_TIMEOUT = float(get_env('ML_CONNECTION_TIMEOUT', 1))  # seconds
TIMEOUT_DEFAULT = float(get_env('ML_TIMEOUT_DEFAULT', 100))  # seconds

TIMEOUT_TRAIN = float(get_env('ML_TIMEOUT_TRAIN', 30))
TIMEOUT_PREDICT = float(get_env('ML_TIMEOUT_PREDICT', 100))
TIMEOUT_HEALTH = float(get_env('ML_TIMEOUT_HEALTH', 1))
TIMEOUT_SETUP = float(get_env('ML_TIMEOUT_SETUP', 3))
TIMEOUT_DUPLICATE_MODEL = float(get_env('ML_TIMEOUT_DUPLICATE_MODEL', 1))
TIMEOUT_DELETE = float(get_env('ML_TIMEOUT_DELETE', 1))
TIMEOUT_TRAIN_JOB_STATUS = float(get_env('ML_TIMEOUT_TRAIN_JOB_STATUS', 1))

# TODO
# we would need to make it configurable on the ML backend side too
PREDICT_URL = 'predict'
HEALTH_URL = 'health'
VALIDATE_URL = 'validate'
SETUP_URL = 'setup'
DUPLICATE_URL = 'duplicate_model'
DELETE_URL = 'delete'
JOB_STATUS_URL = 'job_status'
VERSIONS_URL = 'versions'


class BaseHTTPAPI(object):
    MAX_RETRIES = 2
    HEADERS = {
        'User-Agent': 'heartex/' + (version or ''),
    }

    def __init__(
        self, url, timeout=None, connection_timeout=None, max_retries=None, headers=None, auth_method=None, **kwargs
    ):
        self._url = url
        self._timeout = timeout or TIMEOUT_DEFAULT
        self._connection_timeout = connection_timeout or CONNECTION_TIMEOUT
        self._headers = headers or {}
        self._auth_method = auth_method

        # TODO basic auth parameters must be required for auth_method == 'basic'
        self._basic_auth = (kwargs.get('basic_auth_user'), kwargs.get('basic_auth_pass'))

        self._max_retries = max_retries or self.MAX_RETRIES
        self._sessions = {self._session_key(): self.create_session()}

    def create_session(self):
        session = requests.Session()
        session.headers.update(self.HEADERS)
        session.headers.update(self._headers)
        session.mount('http://', HTTPAdapter(max_retries=self._max_retries))
        session.mount('https://', HTTPAdapter(max_retries=self._max_retries))
        return session

    def _session_key(self):
        return os.getpid()

    @property
    def http(self):
        key = self._session_key()
        if key in self._sessions:
            return self._sessions[key]
        else:
            session = self.create_session()
            self._sessions[key] = session
            return session

    def _prepare_kwargs(self, kwargs):
        # add timeout if it's not presented
        if 'timeout' not in kwargs:
            kwargs['timeout'] = self._connection_timeout, self._timeout

        if self._basic_auth[0] and self._basic_auth[1]:
            kwargs['auth'] = HTTPBasicAuth(*self._basic_auth)

        # add connection timeout if it's not presented
        elif isinstance(kwargs['timeout'], float) or isinstance(kwargs['timeout'], int):
            kwargs['timeout'] = (self._connection_timeout, kwargs['timeout'])

    def request(self, method, *args, **kwargs):
        self._prepare_kwargs(kwargs)
        return self.http.request(method, *args, **kwargs)

    def get(self, *args, **kwargs):
        return self.request('GET', *args, **kwargs)

    def post(self, *args, **kwargs):
        return self.request('POST', *args, **kwargs)


class MLApiResult:
    """
    Class for storing the result of ML API request
    """

    def __init__(self, url='', request='', response=None, headers=None, type='ok', status_code=200):
        self.url = url
        self.request = request
        self.response = {} if response is None else response
        self.headers = {} if headers is None else headers
        self.type = type
        self.status_code = status_code

    @property
    def is_error(self):
        return self.type == 'error'

    @property
    def error_message(self):
        return self.response.get('error')


class MLApi(BaseHTTPAPI):
    """
    Class for ML API connector
    """

    def __init__(self, **kwargs):
        super(MLApi, self).__init__(**kwargs)
        self._validate_request_timeout = 10

    def _get_url(self, url_suffix):
        url = self._url
        if url[-1] != '/':
            url += '/'
        return urllib.parse.urljoin(url, url_suffix)

    def _request(self, url_suffix, request=None, verbose=True, method='POST', *args, **kwargs):
        assert method in ('POST', 'GET')
        url = self._get_url(url_suffix)
        request = request or {}
        headers = dict(self.http.headers)

        response = None
        try:
            if method == 'POST':
                response = self.post(url=url, json=request, *args, **kwargs)
            else:
                response = self.get(url=url, *args, **kwargs)
            response.raise_for_status()
        except requests.exceptions.RequestException as e:
            error_string = str(e)
            status_code = response.status_code if response is not None else 0
            return MLApiResult(url, request, {'error': error_string}, headers, 'error', status_code=status_code)
        status_code = response.status_code
        try:
            response = response.json()
        except ValueError as e:
            return MLApiResult(
                url=url,
                request=request,
                response={'error': str(e), 'response': response.content},
                headers=headers,
                type='error',
                status_code=status_code,
            )

        return MLApiResult(url=url, request=request, response=response, headers=headers, status_code=status_code)

    def _create_project_uid(self, project):
        time_id = int(project.created_at.timestamp())
        return f'{project.id}.{time_id}'

    def train(self, project, use_ground_truth=False):
        # TODO Replace AnonymousUser with real user from request
        user = AnonymousUser()
        # Identify if feature flag is turned on
        if flag_set('ff_back_dev_1417_start_training_mlbackend_webhooks_250122_long', user):
            request = {
                'action': 'START_TRAINING',
                'project': load_func(settings.WEBHOOK_SERIALIZERS['project'])(instance=project).data,
            }
            return self._request('webhook', request, verbose=False, timeout=TIMEOUT_PREDICT)
        else:
            # get only tasks with annotations
            tasks = project.tasks.annotate(num_annotations=Count('annotations')).filter(num_annotations__gt=0)
            # create serialized tasks with annotations: {"data": {...}, "annotations": [{...}], "predictions": [{...}]}
            tasks_ser = ExportDataSerializer(tasks, many=True).data
            logger.debug(f'{len(tasks_ser)} tasks with annotations are sent to ML backend for training.')
            request = {
                'annotations': tasks_ser,
                'project': self._create_project_uid(project),
                'label_config': project.label_config,
                'params': {'login': project.task_data_login, 'password': project.task_data_password},
            }
            return self._request('train', request, verbose=False, timeout=TIMEOUT_PREDICT)

    def _prep_prediction_req(self, tasks, project, context=None):
        request = {
            'tasks': tasks,
            'project': self._create_project_uid(project),
            'label_config': project.label_config,
            'params': {
                'login': project.task_data_login,
                'password': project.task_data_password,
                'context': context,
            },
        }

        return request

    def make_predictions(self, tasks, project, context=None):
        request = self._prep_prediction_req(tasks, project, context=context)
        return self._request(PREDICT_URL, request, verbose=False, timeout=TIMEOUT_PREDICT)

    def health(self):
        return self._request(HEALTH_URL, method='GET', timeout=TIMEOUT_HEALTH)

    def validate(self, config):
        return self._request(VALIDATE_URL, request={'config': config}, timeout=self._validate_request_timeout)

    def setup(self, project, extra_params=None, **kwargs):
        return self._request(
            SETUP_URL,
            request={
                'project': self._create_project_uid(project),
                'schema': project.label_config,
                'hostname': settings.HOSTNAME if settings.HOSTNAME else ('http://localhost:' + settings.INTERNAL_PORT),
                'access_token': project.created_by.auth_token.key,
                'extra_params': extra_params,
            },
            timeout=TIMEOUT_SETUP,
        )

    def duplicate_model(self, project_src, project_dst):
        return self._request(
            DUPLICATE_URL,
            request={
                'project_src': self._create_project_uid(project_src),
                'project_dst': self._create_project_uid(project_dst),
            },
            timeout=TIMEOUT_DUPLICATE_MODEL,
        )

    def delete(self, project):
        return self._request(
            DELETE_URL, request={'project': self._create_project_uid(project)}, timeout=TIMEOUT_DELETE
        )

    def get_train_job_status(self, train_job):
        return self._request(JOB_STATUS_URL, request={'job': train_job.job_id}, timeout=TIMEOUT_TRAIN_JOB_STATUS)

    def get_versions(self, project):
        return self._request(
            VERSIONS_URL, request={'project': self._create_project_uid(project)}, timeout=TIMEOUT_SETUP, method='GET'
        )


def get_ml_api(project):
    if project.ml_backend_active_connection is None:
        return None
    if project.ml_backend_active_connection.ml_backend is None:
        return None
    return MLApi(
        url=project.ml_backend_active_connection.ml_backend.url,
        timeout=project.ml_backend_active_connection.ml_backend.timeout,
    )
</file>

<file path="label_studio/ml/api.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import logging

import drf_yasg.openapi as openapi
from core.feature_flags import flag_set
from core.permissions import ViewClassPermission, all_permissions
from django.conf import settings
from django.utils.decorators import method_decorator
from django_filters.rest_framework import DjangoFilterBackend
from drf_yasg.utils import no_body, swagger_auto_schema
from ml.models import MLBackend
from ml.serializers import MLBackendSerializer, MLInteractiveAnnotatingRequest
from projects.models import Project, Task
from rest_framework import generics, status
from rest_framework.parsers import FormParser, JSONParser, MultiPartParser
from rest_framework.response import Response
from rest_framework.views import APIView

logger = logging.getLogger(__name__)

_ml_backend_schema = openapi.Schema(
    type=openapi.TYPE_OBJECT,
    properties={
        'url': openapi.Schema(type=openapi.TYPE_STRING, description='ML backend URL'),
        'project': openapi.Schema(type=openapi.TYPE_INTEGER, description='Project ID'),
        'is_interactive': openapi.Schema(type=openapi.TYPE_BOOLEAN, description='Is interactive'),
        'title': openapi.Schema(type=openapi.TYPE_STRING, description='Title'),
        'description': openapi.Schema(type=openapi.TYPE_STRING, description='Description'),
        'auth_method': openapi.Schema(
            type=openapi.TYPE_STRING, description='Auth method', enum=['NONE', 'BASIC_AUTH']
        ),
        'basic_auth_user': openapi.Schema(type=openapi.TYPE_STRING, description='Basic auth user'),
        'basic_auth_pass': openapi.Schema(type=openapi.TYPE_STRING, description='Basic auth password'),
        'extra_params': openapi.Schema(type=openapi.TYPE_OBJECT, description='Extra parameters'),
        'timeout': openapi.Schema(type=openapi.TYPE_INTEGER, description='Response model timeout'),
    },
    required=[],
)


@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Machine Learning'],
        x_fern_sdk_group_name='ml',
        x_fern_sdk_method_name='create',
        x_fern_audiences=['public'],
        operation_summary='Add ML Backend',
        operation_description="""
    Add an ML backend to a project using the Label Studio UI or by sending a POST request using the following cURL 
    command:
    ```bash
    curl -X POST -H 'Content-type: application/json' {host}/api/ml -H 'Authorization: Token abc123'\\
    --data '{{"url": "http://localhost:9090", "project": {{project_id}}}}' 
    """.format(
            host=(settings.HOSTNAME or 'https://localhost:8080')
        ),
        request_body=_ml_backend_schema,
    ),
)
@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Machine Learning'],
        x_fern_sdk_group_name='ml',
        x_fern_sdk_method_name='list',
        x_fern_audiences=['public'],
        operation_summary='List ML backends',
        operation_description="""
    List all configured ML backends for a specific project by ID.
    Use the following cURL command:
    ```bash
    curl {host}/api/ml?project={{project_id}} -H 'Authorization: Token abc123'
    """.format(
            host=(settings.HOSTNAME or 'https://localhost:8080')
        ),
        manual_parameters=[
            openapi.Parameter(
                name='project', type=openapi.TYPE_INTEGER, in_=openapi.IN_QUERY, description='Project ID'
            ),
        ],
        request_body=no_body,
    ),
)
class MLBackendListAPI(generics.ListCreateAPIView):
    parser_classes = (JSONParser, FormParser, MultiPartParser)
    permission_required = ViewClassPermission(
        GET=all_permissions.projects_view,
        POST=all_permissions.projects_change,
    )
    serializer_class = MLBackendSerializer
    filter_backends = [DjangoFilterBackend]
    filterset_fields = ['is_interactive']

    def get_queryset(self):
        project_pk = self.request.query_params.get('project')
        project = generics.get_object_or_404(Project, pk=project_pk)

        self.check_object_permissions(self.request, project)

        ml_backends = project.update_ml_backends_state()

        return ml_backends

    def perform_create(self, serializer):
        ml_backend = serializer.save()
        ml_backend.update_state()

        project = ml_backend.project

        # In case we are adding the model, let's set it as the default
        # to obtain predictions. This approach is consistent with uploading
        # offline predictions, which would be set automatically.
        if project.show_collab_predictions and not project.model_version:
            project.model_version = ml_backend.title
            project.save(update_fields=['model_version'])


@method_decorator(
    name='patch',
    decorator=swagger_auto_schema(
        tags=['Machine Learning'],
        x_fern_sdk_group_name='ml',
        x_fern_sdk_method_name='update',
        x_fern_audiences=['public'],
        operation_summary='Update ML Backend',
        operation_description="""
    Update ML backend parameters using the Label Studio UI or by sending a PATCH request using the following cURL command:
    ```bash
    curl -X PATCH -H 'Content-type: application/json' {host}/api/ml/{{ml_backend_ID}} -H 'Authorization: Token abc123'\\
    --data '{{"url": "http://localhost:9091"}}' 
    """.format(
            host=(settings.HOSTNAME or 'https://localhost:8080')
        ),
        request_body=_ml_backend_schema,
    ),
)
@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Machine Learning'],
        x_fern_sdk_group_name='ml',
        x_fern_sdk_method_name='get',
        x_fern_audiences=['public'],
        operation_summary='Get ML Backend',
        operation_description="""
    Get details about a specific ML backend connection by ID. For example, make a GET request using the
    following cURL command:
    ```bash
    curl {host}/api/ml/{{ml_backend_ID}} -H 'Authorization: Token abc123'
    """.format(
            host=(settings.HOSTNAME or 'https://localhost:8080')
        ),
        request_body=no_body,
    ),
)
@method_decorator(
    name='delete',
    decorator=swagger_auto_schema(
        tags=['Machine Learning'],
        x_fern_sdk_group_name='ml',
        x_fern_sdk_method_name='delete',
        x_fern_audiences=['public'],
        operation_summary='Remove ML Backend',
        operation_description="""
    Remove an existing ML backend connection by ID. For example, use the
    following cURL command:
    ```bash
    curl -X DELETE {host}/api/ml/{{ml_backend_ID}} -H 'Authorization: Token abc123'
    """.format(
            host=(settings.HOSTNAME or 'https://localhost:8080')
        ),
        request_body=no_body,
    ),
)
@method_decorator(name='put', decorator=swagger_auto_schema(auto_schema=None))
class MLBackendDetailAPI(generics.RetrieveUpdateDestroyAPIView):
    parser_classes = (JSONParser, FormParser, MultiPartParser)
    serializer_class = MLBackendSerializer
    permission_required = all_permissions.projects_change
    queryset = MLBackend.objects.all()

    def get_object(self):
        ml_backend = super(MLBackendDetailAPI, self).get_object()
        ml_backend.update_state()
        return ml_backend

    def perform_update(self, serializer):
        ml_backend = serializer.save()
        ml_backend.update_state()


@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Machine Learning'],
        x_fern_sdk_group_name='ml',
        x_fern_sdk_method_name='train',
        x_fern_audiences=['public'],
        operation_summary='Train',
        operation_description="""
        After you add an ML backend, call this API with the ML backend ID to start training with 
        already-labeled tasks. 
        
        Get the ML backend ID by [listing the ML backends for a project](https://labelstud.io/api/#operation/api_ml_list).
        """,
        manual_parameters=[
            openapi.Parameter(
                name='id',
                type=openapi.TYPE_INTEGER,
                in_=openapi.IN_PATH,
                description='A unique integer value identifying this ML backend.',
            ),
        ],
        request_body=openapi.Schema(
            type=openapi.TYPE_OBJECT,
            properties={
                'use_ground_truth': openapi.Schema(
                    type=openapi.TYPE_BOOLEAN, description='Whether to include ground truth annotations in training'
                )
            },
        ),
        responses={
            200: openapi.Response(description='Training has successfully started.'),
            500: openapi.Response(
                description='Training error',
                schema=openapi.Schema(
                    description='Error message',
                    type=openapi.TYPE_STRING,
                    example='Server responded with an error.',
                ),
            ),
        },
    ),
)
class MLBackendTrainAPI(APIView):

    permission_required = all_permissions.projects_change

    def post(self, request, *args, **kwargs):
        ml_backend = generics.get_object_or_404(MLBackend, pk=self.kwargs['pk'])
        self.check_object_permissions(self.request, ml_backend)

        ml_backend.train()
        return Response(status=status.HTTP_200_OK)


@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Machine Learning'],
        x_fern_sdk_group_name='ml',
        x_fern_sdk_method_name='test_predict',
        x_fern_audiences=['internal'],
        operation_summary='Test prediction',
        operation_description="""
        After you add an ML backend, call this API with the ML backend ID to run a test prediction on specific task data               
        """,
        manual_parameters=[
            openapi.Parameter(
                name='id',
                type=openapi.TYPE_INTEGER,
                in_=openapi.IN_PATH,
                description='A unique integer value identifying this ML backend.',
            ),
        ],
        responses={
            200: openapi.Response(description='Predicting has successfully started.'),
            500: openapi.Response(
                description='Predicting error',
                schema=openapi.Schema(
                    description='Error message',
                    type=openapi.TYPE_STRING,
                    example='Server responded with an error.',
                ),
            ),
        },
    ),
)
class MLBackendPredictTestAPI(APIView):
    serializer_class = MLBackendSerializer
    permission_required = all_permissions.projects_change

    def post(self, request, *args, **kwargs):
        ml_backend = generics.get_object_or_404(MLBackend, pk=self.kwargs['pk'])
        self.check_object_permissions(self.request, ml_backend)

        random = request.query_params.get('random', False)
        if random:
            task = Task.get_random(project=ml_backend.project)
            if not task:
                return Response(
                    status=status.HTTP_500_INTERNAL_SERVER_ERROR,
                    data={
                        'detail': 'Project has no tasks to run prediction on, import at least 1 task to run prediction'
                    },
                )

            kwargs = ml_backend._predict(task)
            if not kwargs:
                return Response(
                    status=status.HTTP_500_INTERNAL_SERVER_ERROR,
                    data={
                        'detail': 'ML backend did not return any predictions, check ML backend logs for more details'
                    },
                )
            return Response(**kwargs)

        else:
            return Response(
                status=status.HTTP_501_NOT_IMPLEMENTED,
                data={'error': 'Not implemented - you must provide random=true query parameter'},
            )


@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Machine Learning'],
        x_fern_sdk_group_name='ml',
        x_fern_sdk_method_name='predict_interactive',
        x_fern_audiences=['public'],
        operation_summary='Request Interactive Annotation',
        operation_description="""
        Send a request to the machine learning backend set up to be used for interactive preannotations to retrieve a
        predicted region based on annotator input. 
        See [set up machine learning](https://labelstud.io/guide/ml.html#Get-interactive-preannotations) for more.
        """,
        manual_parameters=[
            openapi.Parameter(
                name='id',
                type=openapi.TYPE_INTEGER,
                in_=openapi.IN_PATH,
                description='A unique integer value identifying this ML backend.',
            ),
        ],
        request_body=MLInteractiveAnnotatingRequest,
        responses={
            200: openapi.Response(description='Interactive annotation has succeeded.'),
        },
    ),
)
class MLBackendInteractiveAnnotating(APIView):
    """
    Send a request to the machine learning backend set up to be used for interactive preannotations to retrieve a
    predicted region based on annotator input.
    """

    permission_required = all_permissions.tasks_view

    def _error_response(self, message, log_function=logger.info):
        log_function(message)
        return Response({'errors': [message]}, status=status.HTTP_200_OK)

    def _get_task(self, ml_backend, validated_data):
        return generics.get_object_or_404(Task, pk=validated_data['task'], project=ml_backend.project)

    def _get_credentials(self, request, context, project):
        if flag_set('ff_back_dev_2362_project_credentials_060722_short', request.user):
            context.update(
                project_credentials_login=project.task_data_login,
                project_credentials_password=project.task_data_password,
            )
        return context

    def post(self, request, *args, **kwargs):
        """
        Send a request to the machine learning backend set up to be used for interactive preannotations to retrieve a
        predicted region based on annotator input.
        """
        ml_backend = generics.get_object_or_404(MLBackend, pk=self.kwargs['pk'])
        self.check_object_permissions(self.request, ml_backend)
        serializer = MLInteractiveAnnotatingRequest(data=request.data)
        serializer.is_valid(raise_exception=True)

        task = self._get_task(ml_backend, serializer.validated_data)
        context = self._get_credentials(request, serializer.validated_data.get('context', {}), task.project)

        result = ml_backend.interactive_annotating(task, context, user=request.user)

        return Response(
            result,
            status=status.HTTP_200_OK,
        )


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Machine Learning'],
        x_fern_sdk_group_name='ml',
        x_fern_sdk_method_name='list_model_versions',
        x_fern_audiences=['public'],
        operation_summary='Get model versions',
        operation_description='Get available versions of the model.',
        responses={'200': 'List of available versions.'},
    ),
)
class MLBackendVersionsAPI(generics.RetrieveAPIView):

    permission_required = all_permissions.projects_change

    def get(self, request, *args, **kwargs):
        ml_backend = generics.get_object_or_404(MLBackend, pk=self.kwargs['pk'])
        self.check_object_permissions(self.request, ml_backend)
        versions_response = ml_backend.get_versions()
        if versions_response.status_code == 200:
            result = {'versions': versions_response.response.get('versions', [])}
            return Response(data=result, status=200)
        elif versions_response.status_code == 404:
            result = {'versions': [ml_backend.model_version], 'message': 'Upgrade your ML backend version to latest.'}
            return Response(data=result, status=200)
        else:
            result = {'error': str(versions_response.error_message)}
            status_code = versions_response.status_code if versions_response.status_code > 0 else 500
            return Response(data=result, status=status_code)
</file>

<file path="label_studio/ml/mixins.py">
from tasks.models import AnnotationDraft
from tasks.serializers import AnnotationDraftSerializer


class InteractiveMixin:
    def to_representation(self, task):
        user = self.context.get('user')
        drafts = AnnotationDraft.objects.filter(task=task, user=user)
        drafts_ser = AnnotationDraftSerializer(drafts, many=True, default=[], read_only=True).data
        data = super().to_representation(task)
        data['drafts'] = drafts_ser
        return data
</file>

<file path="label_studio/ml/models.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import logging
from typing import Dict, List

from core.utils.common import conditional_atomic, db_is_not_sqlite, load_func
from django.conf import settings
from django.db import models, transaction
from django.db.models import Count, JSONField, Q
from django.db.models.signals import post_save, pre_delete
from django.dispatch import receiver
from django.utils.translation import gettext_lazy as _
from ml.api_connector import PREDICT_URL, TIMEOUT_PREDICT, MLApi
from projects.models import Project
from tasks.serializers import PredictionSerializer, TaskSimpleSerializer
from webhooks.serializers import Webhook, WebhookSerializer

logger = logging.getLogger(__name__)

MAX_JOBS_PER_PROJECT = 1

InteractiveAnnotatingDataSerializer = load_func(settings.INTERACTIVE_DATA_SERIALIZER)


class MLBackendState(models.TextChoices):
    CONNECTED = 'CO', _('Connected')
    DISCONNECTED = 'DI', _('Disconnected')
    ERROR = 'ER', _('Error')
    TRAINING = 'TR', _('Training')
    PREDICTING = 'PR', _('Predicting')


class MLBackendAuth(models.TextChoices):
    NONE = 'NONE', _('None')
    BASIC_AUTH = 'BASIC_AUTH', _('Basic Auth')


class MLBackend(models.Model):
    """ """

    state = models.CharField(
        max_length=2,
        choices=MLBackendState.choices,
        default=MLBackendState.DISCONNECTED,
    )
    is_interactive = models.BooleanField(
        _('is_interactive'),
        default=False,
        help_text=('Used to interactively annotate tasks. ' 'If true, model returns one list with results'),
    )
    url = models.TextField(
        _('url'),
        help_text='URL for the machine learning model server',
    )
    error_message = models.TextField(
        _('error_message'),
        blank=True,
        null=True,
        help_text='Error message in error state',
    )
    title = models.TextField(
        _('title'),
        blank=True,
        null=True,
        default='default',
        help_text='Name of the machine learning backend',
    )

    auth_method = models.CharField(
        max_length=255,
        choices=MLBackendAuth.choices,
        default=MLBackendAuth.NONE,
    )

    basic_auth_user = models.TextField(
        _('basic auth user'),
        blank=True,
        null=True,
        default='',
        help_text='HTTP Basic Auth user',
    )

    basic_auth_pass = models.TextField(
        _('basic auth password'),
        blank=True,
        null=True,
        default='',
        help_text='HTTP Basic Auth password',
    )

    description = models.TextField(
        _('description'),
        blank=True,
        null=True,
        default='',
        help_text='Description for the machine learning backend',
    )

    extra_params = JSONField(
        _('extra params'),
        null=True,
        help_text='Any extra parameters passed to the ML Backend during the setup',
    )

    model_version = models.TextField(
        _('model version'),
        blank=True,
        null=True,
        default='',
        help_text='Current model version associated with this machine learning backend',
    )
    timeout = models.FloatField(
        _('timeout'),
        blank=True,
        default=100.0,
        help_text='Response model timeout',
    )
    project = models.ForeignKey(
        Project,
        on_delete=models.CASCADE,
        related_name='ml_backends',
    )
    created_at = models.DateTimeField(_('created at'), auto_now_add=True)
    updated_at = models.DateTimeField(_('updated at'), auto_now=True)
    auto_update = models.BooleanField(
        _('auto_update'),
        default=True,
        help_text='If false, model version is set by the user, if true - getting latest version from backend.',
    )

    def __str__(self):
        return f'{self.title} (id={self.id}, url={self.url})'

    def __init__(self, *args, **kwargs):
        super(MLBackend, self).__init__(*args, **kwargs)
        self.__original_title = self.title

    def save(self, *args, **kwargs):
        """
        Overrides the save() method to update the associated project's model_version field.
        If the title of the model instance is changed and the model_version
        of the related project is currently the same as the original title,
        the project's model_version is updated to the new title.
        """
        p = self.project

        if self.title != self.__original_title and p.model_version == self.__original_title:
            with transaction.atomic():
                p.model_version = self.title
                p.save(update_fields=['model_version'])
                super().save(*args, **kwargs)
                # reset original field to current field after save
                self.__original_title = self.title
        else:
            super().save(*args, **kwargs)

    @staticmethod
    def healthcheck_(url, auth_method=None, **kwargs):
        return MLApi(url=url, auth_method=auth_method, **kwargs).health()

    def has_permission(self, user):
        user.project = self.project  # link for activity log
        return self.project.has_permission(user)

    @staticmethod
    def setup_(url, project, auth_method=None, **kwargs):
        api = MLApi(url=url, auth_method=auth_method, **kwargs)

        if not isinstance(project, Project):
            project = Project.objects.get(pk=project)
        return api.setup(project, **kwargs)

    def healthcheck(self):
        return self.healthcheck_(
            self.url, self.auth_method, basic_auth_user=self.basic_auth_user, basic_auth_pass=self.basic_auth_pass
        )

    def setup(self):
        return self.setup_(
            self.url,
            self.project,
            self.auth_method,
            extra_params=self.extra_params,
            basic_auth_user=self.basic_auth_user,
            basic_auth_pass=self.basic_auth_pass,
        )

    @property
    def api(self):
        return MLApi(
            url=self.url,
            timeout=self.timeout,
            auth_method=self.auth_method,
            basic_auth_user=self.basic_auth_user,
            basic_auth_pass=self.basic_auth_pass,
        )

    @property
    def not_ready(self):
        return self.state in (MLBackendState.DISCONNECTED, MLBackendState.ERROR)

    def update_state(self):
        model_version = None
        if self.healthcheck().is_error:
            self.state = MLBackendState.DISCONNECTED
        else:
            setup_response = self.setup()
            if setup_response.is_error:
                logger.info(f'ML backend responds with error: {setup_response.error_message}')
                self.state = MLBackendState.ERROR
                self.error_message = setup_response.error_message
            else:
                self.state = MLBackendState.CONNECTED
                model_version = setup_response.response.get('model_version')
                logger.info(f'ML backend responds with success: {setup_response.response}')
                if self.auto_update:
                    logger.debug(f'Changing model version: {self.model_version} -> {model_version}')
                    self.model_version = model_version
                self.error_message = None
        self.save()
        return model_version

    def train(self):
        train_response = self.api.train(self.project)
        if train_response.is_error:
            self.state = MLBackendState.ERROR
            self.error_message = train_response.error_message
        else:
            self.state = MLBackendState.TRAINING
            current_train_job = train_response.response.get('job')
            if current_train_job:
                MLBackendTrainJob.objects.create(job_id=current_train_job, ml_backend=self)
        self.save()

    def _predict(self, task):
        """This is low level prediction method that is used for debugging"""
        ml_api = self.api
        task_ser = TaskSimpleSerializer(task).data

        request_params = ml_api._prep_prediction_req([task_ser], self.project)
        ml_api_result = ml_api._request(PREDICT_URL, request_params, verbose=False, timeout=TIMEOUT_PREDICT)

        if ml_api_result.is_error:
            logger.info(f'Prediction not created for project {self}: {ml_api_result.error_message}')
            return

        results = ml_api_result.response.get('results', None)

        return {
            'status': 200,
            'data': {
                'status': ml_api_result.status_code,
                'error_message': ml_api_result.error_message,
                'url': ml_api._get_url(PREDICT_URL),
                'task': task_ser,
                'request': request_params,
                'response': results,
            },
        }

    def _get_predictions_from_ml_backend_one_by_one(
        self, serialized_tasks: List[Dict], current_responses: List[Dict]
    ) -> List[Dict]:
        """
        This is helper method to get predictions from ML backend one by one
        in case when tasks length doesn't match responses length
        Note: don't use this function outside of this class
        """

        if len(current_responses) == 1:
            # In case ML backend doesn't support batch of tasks, do it one by one
            # TODO: remove this block after all ML backends will support batch processing
            logger.warning(
                f"'ML backend '{self.title}' doesn't support batch processing of tasks, "
                f'switched to one-by-one task retrieval'
            )
            predictions = []
            for serialized_task in serialized_tasks:
                # get predictions per task
                predictions.extend(self._get_predictions_from_ml_backend([serialized_task]))

            return predictions
        else:
            # complete failure - likely ML backend skipped some tasks, we can't match them
            logger.error(
                f'Number of tasks and responses are not equal: '
                f'{len(serialized_tasks)} tasks != {len(current_responses)} responses. '
                f'Returning empty predictions.'
            )
            return []

    def _get_predictions_from_ml_backend(self, serialized_tasks: List[Dict]) -> List[Dict]:
        result = self.api.make_predictions(serialized_tasks, self.project)

        # response validation
        if result.is_error:
            logger.error(f'Error occurred: {result.error_message}')
            return []
        elif not isinstance(result.response, dict) or 'results' not in result.response:
            logger.error(f'ML backend returns an incorrect response, it must be a dict: {result.response}')
            return []
        elif not isinstance(result.response['results'], list) or len(result.response['results']) == 0:
            logger.error(
                'ML backend returns an incorrect response, results field must be a list with at least one item'
            )
            return []

        responses = result.response['results']

        predictions = []
        if len(serialized_tasks) != len(responses):
            # Number of tasks and responses are not equal
            # It can happen if ML backend doesn't support batch processing but only process one task at a time
            # In the future versions, we may better consider this as an error and deprecate this code branch
            return self._get_predictions_from_ml_backend_one_by_one(serialized_tasks, responses)

        # ML backend supports batch processing
        for task, response in zip(serialized_tasks, responses):
            if isinstance(response, dict):
                # ML backend can return single prediction per task or multiple predictions
                response = [response]

            # get all predictions per task
            for r in response:
                if 'result' not in r:
                    logger.error(
                        f"ML backend returns an incorrect prediction, it should be a dict with the 'result' field:"
                        f' {r}'
                    )
                    continue
                predictions.append(
                    {
                        'task': task['id'],
                        'result': r['result'],
                        'score': r.get('score'),
                        'model_version': r.get('model_version', self.model_version),
                        'project': task['project'],
                    }
                )
        return predictions

    def predict_tasks(self, tasks):
        model_version = self.update_state()
        if self.not_ready:
            logger.debug(f'ML backend {self} is not ready')
            return

        if isinstance(tasks, list):
            from tasks.models import Task

            tasks = Task.objects.filter(id__in=[task.id for task in tasks])

        # Filter tasks that already contain the current model version in predictions
        tasks = tasks.annotate(predictions_count=Count('predictions')).exclude(
            Q(predictions_count__gt=0) & Q(predictions__model_version=model_version)
        )
        if not tasks.exists():
            logger.debug(f'All tasks already have prediction from model version={self.model_version}')
            return model_version
        tasks_ser = TaskSimpleSerializer(tasks, many=True).data
        predictions = self._get_predictions_from_ml_backend(tasks_ser)
        with conditional_atomic(predicate=db_is_not_sqlite):
            prediction_ser = PredictionSerializer(data=predictions, many=True)
            prediction_ser.is_valid(raise_exception=True)
            instances = prediction_ser.save()
        return instances

    def interactive_annotating(self, task, context=None, user=None):
        result = {}
        options = {}
        if user:
            options = {'user': user}
        if not self.is_interactive:
            result['errors'] = ['Model is not set to be used for interactive preannotations']
            return result

        tasks_ser = InteractiveAnnotatingDataSerializer(
            [task], many=True, expand=['drafts', 'predictions', 'annotations'], context=options
        ).data
        ml_api_result = self.api.make_predictions(
            tasks=tasks_ser,
            project=self.project,
            context=context,
        )
        if ml_api_result.is_error:
            logger.info(f'Prediction not created for project {self}: {ml_api_result.error_message}')
            result['errors'] = [ml_api_result.error_message]
            return result

        if not (isinstance(ml_api_result.response, dict) and 'results' in ml_api_result.response):
            logger.info(f'ML backend returns an incorrect response, it must be a dict: {ml_api_result.response}')
            result['errors'] = [
                'Incorrect response from ML service: ' 'ML backend returns an incorrect response, it must be a dict.'
            ]
            return result

        ml_results = ml_api_result.response.get(
            'results',
            [
                None,
            ],
        )
        if not isinstance(ml_results, list) or len(ml_results) < 1:
            logger.warning(f'ML backend has to return list with 1 annotation but it returned: {type(ml_results)}')
            result['errors'] = [
                'Incorrect response from ML service: ' 'ML backend has to return list with more than 1 result.'
            ]
            return result
        result['data'] = ml_results[0]
        return result

    @staticmethod
    def get_versions_(url, project, auth_method, **kwargs):
        api = MLApi(url=url, auth_method=auth_method, **kwargs)
        if not isinstance(project, Project):
            project = Project.objects.get(pk=project)
        return api.get_versions(project)

    def get_versions(self):
        return self.get_versions_(
            self.url,
            self.project,
            self.auth_method,
            basic_auth_user=self.basic_auth_user,
            basic_auth_pass=self.basic_auth_pass,
        )


class MLBackendPredictionJob(models.Model):

    job_id = models.CharField(max_length=128)
    ml_backend = models.ForeignKey(MLBackend, related_name='prediction_jobs', on_delete=models.CASCADE)
    model_version = models.TextField(
        _('model version'), blank=True, null=True, help_text='Model version this job is associated with'
    )
    batch_size = models.PositiveSmallIntegerField(
        _('batch size'), default=100, help_text='Number of tasks processed per batch'
    )

    created_at = models.DateTimeField(_('created at'), auto_now_add=True)
    updated_at = models.DateTimeField(_('updated at'), auto_now=True)


class MLBackendTrainJob(models.Model):

    job_id = models.CharField(max_length=128)
    ml_backend = models.ForeignKey(MLBackend, related_name='train_jobs', on_delete=models.CASCADE)
    model_version = models.TextField(
        _('model version'),
        blank=True,
        null=True,
        help_text='Model version this job is associated with',
    )
    created_at = models.DateTimeField(_('created at'), auto_now_add=True)
    updated_at = models.DateTimeField(_('updated at'), auto_now=True)

    def get_status(self):
        project = self.ml_backend.project
        ml_api = project.get_ml_api()
        if not ml_api:
            logger.error(
                f"Training job {self.id}: Can't collect training jobs for project {project.id}: ML API is null"
            )
            return None
        ml_api_result = ml_api.get_train_job_status(self)
        if ml_api_result.is_error:
            if ml_api_result.status_code == 410:
                return {'job_status': 'removed'}
            logger.info(
                f"Training job {self.id}: Can't collect training jobs for project {project}: "
                f'ML API returns error {ml_api_result.error_message}'
            )
            return None
        return ml_api_result.response

    @property
    def is_running(self):
        status = self.get_status()
        return status['job_status'] in ('queued', 'started')


def _validate_ml_api_result(ml_api_result, tasks, curr_logger):
    if ml_api_result.is_error:
        curr_logger.info(ml_api_result.error_message)
        return False

    results = ml_api_result.response['results']
    if not isinstance(results, list) or len(results) != len(tasks):
        curr_logger.warning('Num input tasks is %d but ML API returns %d results', len(tasks), len(results))
        return False

    return True


@receiver(pre_delete, sender=MLBackend)
def modify_project_model_version(sender, instance, **kwargs):
    project = instance.project

    if project.model_version == instance.title:
        project.model_version = ''
        project.save(update_fields=['model_version'])


@receiver(post_save, sender=MLBackend)
def create_ml_webhook(sender, instance, created, **kwargs):
    if not created:
        return
    ml_backend = instance
    webhook_url = ml_backend.url.rstrip('/') + '/webhook'
    project = ml_backend.project
    if Webhook.objects.filter(project=project, url=webhook_url).exists():
        logger.info(f'Webhook {webhook_url} already exists for project {project}: skip creating new one.')
        return
    logger.info(f'Create ML backend webhook {webhook_url}')
    ser = WebhookSerializer(
        data=dict(project=project.id, url=webhook_url, send_payload=True, send_for_all_actions=True)
    )
    if ser.is_valid():
        ser.save(organization=project.organization)
</file>

<file path="label_studio/ml/serializers.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
from core.utils.io import validate_upload_url
from django.conf import settings
from ml.models import MLBackend, MLBackendAuth
from rest_framework import serializers


class MLBackendSerializer(serializers.ModelSerializer):
    """
    Serializer for MLBackend model.
    """

    readable_state = serializers.SerializerMethodField()
    basic_auth_pass = serializers.CharField(write_only=True, required=False, allow_null=True, allow_blank=True)
    basic_auth_pass_is_set = serializers.SerializerMethodField()

    def get_basic_auth_pass_is_set(self, obj):
        return bool(obj.basic_auth_pass)

    def get_readable_state(self, obj):
        return obj.get_state_display()

    def validate_basic_auth_pass(self, value):
        # Checks if the new password and old password are non-existent.
        if not value:
            if not self.instance.basic_auth_pass:
                raise serializers.ValidationError('Authentication password is required for Basic Authentication.')
            else:
                # If user is not changing the password, return the old password.
                return self.instance.basic_auth_pass
        return value

    def validate_url(self, value):
        validate_upload_url(value, block_local_urls=settings.ML_BLOCK_LOCAL_IP)

        return value

    def _validate_authentication(self, attrs):
        if attrs.get('auth_method') == MLBackendAuth.BASIC_AUTH:
            required_fields = ['basic_auth_user', 'basic_auth_pass']

            if any(field not in attrs for field in required_fields):
                raise serializers.ValidationError(
                    'Authentication username and password is required for Basic Authentication.'
                )

    def _validate_healthcheck(self, attrs):
        healthcheck_response = MLBackend.healthcheck_(**attrs)

        if healthcheck_response.is_error:
            if healthcheck_response.status_code == 401:
                message = (
                    'Able to connect to ML Server, but authentication parameters were '
                    'either not provided or are incorrect.'
                )
            else:
                message = (
                    f"Can't connect to ML backend {attrs['url']}, health check failed. "
                    'Make sure it is up and your firewall is properly configured. '
                    f'<a href="https://labelstud.io/guide/ml.html">Learn more</a> '
                    f'about how to set up an ML backend. Additional info: {healthcheck_response.error_message}'
                )

            raise serializers.ValidationError(message)

    def _validate_setup(self, attrs):
        setup_response = MLBackend.setup_(**attrs)

        if setup_response.is_error:
            message = (
                f"Successfully connected to {attrs['url']} but it doesn't look like a valid ML backend. "
                f'Reason: {setup_response.error_message}.\n'
                'Check the ML backend server console logs to check the status.'
                'There might be something wrong with your model or it might be incompatible with the current labeling configuration.'
            )

            raise serializers.ValidationError(message)

    def validate(self, attrs):
        attrs = super().validate(attrs)

        self._validate_authentication(attrs)
        self._validate_healthcheck(attrs)
        self._validate_setup(attrs)

        return attrs

    class Meta:
        model = MLBackend
        fields = [
            'id',
            'state',
            'readable_state',
            'is_interactive',
            'url',
            'error_message',
            'title',
            'auth_method',
            'basic_auth_user',
            'basic_auth_pass',
            'basic_auth_pass_is_set',
            'description',
            'extra_params',
            'model_version',
            'timeout',
            'created_at',
            'updated_at',
            'auto_update',
            'project',
        ]


class MLInteractiveAnnotatingRequest(serializers.Serializer):
    """
    Serializer for ML interactive annotating request.
    """

    task = serializers.IntegerField(help_text='ID of task to annotate', required=True)
    context = serializers.JSONField(help_text='Context for ML model', allow_null=True, default=None)
</file>

<file path="label_studio/ml/urls.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
from django.urls import include, path

from . import api

app_name = 'ml'

# ML backend CRUD
_api_urlpatterns = [
    # All ml backends
    path('', api.MLBackendListAPI.as_view(), name='ml-list'),
    path('<int:pk>', api.MLBackendDetailAPI.as_view(), name='ml-detail'),
    path('<int:pk>/train', api.MLBackendTrainAPI.as_view(), name='ml-train'),
    path('<int:pk>/predict/test', api.MLBackendPredictTestAPI.as_view(), name='ml-predict-test'),
    path(
        '<int:pk>/interactive-annotating',
        api.MLBackendInteractiveAnnotating.as_view(),
        name='ml-interactive-annotating',
    ),
    path('<int:pk>/versions', api.MLBackendVersionsAPI.as_view(), name='ml-versions'),
]

urlpatterns = [
    path('api/ml/', include((_api_urlpatterns, app_name), namespace='api')),
]
</file>

<file path="label_studio/ml_model_providers/migrations/0001_initial.py">
# Generated by Django 3.2.23 on 2024-01-31 16:55

from django.conf import settings
from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    initial = True

    dependencies = [
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
        ('organizations', '0006_alter_organizationmember_deleted_at'),
    ]

    operations = [
        migrations.CreateModel(
            name='ModelProviderConnection',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('provider', models.CharField(choices=[('OpenAI', 'OpenAI')], default='OpenAI', max_length=255)),
                ('api_key', models.TextField(blank=True, help_text='Model provider API key', null=True, verbose_name='api_key')),
                ('scope', models.CharField(choices=[('Organization', 'Organization'), ('User', 'User'), ('Model', 'Model')], default='Organization', max_length=255)),
                ('created_at', models.DateTimeField(auto_now_add=True, verbose_name='created at')),
                ('updated_at', models.DateTimeField(auto_now=True, verbose_name='updated at')),
                ('created_by', models.ForeignKey(null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='created_model_provider_connections', to=settings.AUTH_USER_MODEL)),
                ('organization', models.ForeignKey(null=True, on_delete=django.db.models.deletion.CASCADE, related_name='model_provider_connections', to='organizations.organization')),
            ],
        ),
    ]
</file>

<file path="label_studio/ml_model_providers/migrations/0002_auto_20240722_2054.py">
# Generated by Django 3.2.25 on 2024-07-22 20:54

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('ml_model_providers', '0001_initial'),
    ]

    operations = [
        migrations.AddField(
            model_name='modelproviderconnection',
            name='deployment_name',
            field=models.CharField(blank=True, help_text='Azure OpenAI deployment name', max_length=512, null=True),
        ),
        migrations.AddField(
            model_name='modelproviderconnection',
            name='endpoint',
            field=models.CharField(blank=True, help_text='Azure OpenAI endpoint', max_length=512, null=True),
        ),
        migrations.AlterField(
            model_name='modelproviderconnection',
            name='provider',
            field=models.CharField(choices=[('OpenAI', 'OpenAI'), ('AzureOpenAI', 'AzureOpenAI')], default='OpenAI', max_length=255),
        ),
    ]
</file>

<file path="label_studio/ml_model_providers/migrations/0003_modelproviderconnection_cached_available_models.py">
# Generated by Django 3.2.25 on 2024-08-20 22:40

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('ml_model_providers', '0002_auto_20240722_2054'),
    ]

    operations = [
        migrations.AddField(
            model_name='modelproviderconnection',
            name='cached_available_models',
            field=models.CharField(blank=True, help_text='List of available models from the provider', max_length=4096, null=True),
        ),
    ]
</file>

<file path="label_studio/ml_model_providers/migrations/0004_auto_20240830_1206.py">
# Generated by Django 3.2.25 on 2024-08-30 12:06

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('ml_model_providers', '0003_modelproviderconnection_cached_available_models'),
    ]

    operations = [
        migrations.AddField(
            model_name='modelproviderconnection',
            name='auth_token',
            field=models.TextField(blank=True, help_text='Model provider Auth token', null=True, verbose_name='auth_token'),
        ),
        migrations.AlterField(
            model_name='modelproviderconnection',
            name='provider',
            field=models.CharField(choices=[('OpenAI', 'OpenAI'), ('AzureOpenAI', 'AzureOpenAI'), ('Custom', 'Custom')], default='OpenAI', max_length=255),
        ),
    ]
</file>

<file path="label_studio/ml_model_providers/migrations/0005_modelproviderconnection_budget_alert_threshold_and_more.py">
# Generated by Django 4.2.15 on 2024-11-15 16:49

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('ml_model_providers', '0004_auto_20240830_1206'),
    ]

    operations = [
        migrations.AddField(
            model_name='modelproviderconnection',
            name='budget_alert_threshold',
            field=models.FloatField(blank=True, default=None, help_text='Budget alert threshold for the given provider connection', null=True, verbose_name='budget_alert_threshold'),
        ),
        migrations.AddField(
            model_name='modelproviderconnection',
            name='budget_last_reset_date',
            field=models.DateTimeField(blank=True, default=None, help_text='Date and time the budget was last reset', null=True, verbose_name='budget_last_reset_date'),
        ),
        migrations.AddField(
            model_name='modelproviderconnection',
            name='budget_limit',
            field=models.FloatField(blank=True, default=None, help_text='Budget limit for the model provider connection (null if unlimited)', null=True, verbose_name='budget_limit'),
        ),
        migrations.AddField(
            model_name='modelproviderconnection',
            name='budget_reset_period',
            field=models.CharField(blank=True, choices=[('Monthly', 'Monthly'), ('Yearly', 'Yearly')], default=None, help_text='Budget reset period for the model provider connection (null if not reset)', max_length=20, null=True, verbose_name='budget_reset_period'),
        ),
        migrations.AddField(
            model_name='modelproviderconnection',
            name='budget_total_spent',
            field=models.FloatField(blank=True, default=None, help_text='Tracked total budget spent for the given provider connection within the current budget period', null=True, verbose_name='budget_total_spent'),
        ),
        migrations.AddField(
            model_name='modelproviderconnection',
            name='is_internal',
            field=models.BooleanField(blank=True, default=False, help_text='Whether the model provider connection is internal, not visible to the user', null=True, verbose_name='is_internal'),
        ),
    ]
</file>

<file path="label_studio/ml_model_providers/migrations/0006_modelproviderconnection_google_application_credentials_and_more.py">
# Generated by Django 5.1.4 on 2025-01-03 20:58

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        (
            "ml_model_providers",
            "0005_modelproviderconnection_budget_alert_threshold_and_more",
        ),
    ]

    operations = [
        migrations.AddField(
            model_name="modelproviderconnection",
            name="google_application_credentials",
            field=models.TextField(
                blank=True,
                help_text="The content of GOOGLE_APPLICATION_CREDENTIALS json file",
                null=True,
                verbose_name="google application credentials",
            ),
        ),
        migrations.AddField(
            model_name="modelproviderconnection",
            name="google_location",
            field=models.CharField(
                blank=True,
                help_text="Google project location",
                max_length=255,
                null=True,
                verbose_name="google location",
            ),
        ),
        migrations.AddField(
            model_name="modelproviderconnection",
            name="google_project_id",
            field=models.CharField(
                blank=True,
                help_text="Google project ID",
                max_length=255,
                null=True,
                verbose_name="google project id",
            ),
        ),
        migrations.AlterField(
            model_name="modelproviderconnection",
            name="provider",
            field=models.CharField(
                choices=[
                    ("OpenAI", "OpenAI"),
                    ("AzureOpenAI", "AzureOpenAI"),
                    ("VertexAI", "VertexAI"),
                    ("Custom", "Custom"),
                ],
                default="OpenAI",
                max_length=255,
            ),
        ),
    ]
</file>

<file path="label_studio/ml_model_providers/migrations/0007_alter_modelproviderconnection_provider.py">
# Generated by Django 5.1.4 on 2025-01-10 18:38

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        (
            "ml_model_providers",
            "0006_modelproviderconnection_google_application_credentials_and_more",
        ),
    ]

    operations = [
        migrations.AlterField(
            model_name="modelproviderconnection",
            name="provider",
            field=models.CharField(
                choices=[
                    ("OpenAI", "OpenAI"),
                    ("AzureOpenAI", "AzureOpenAI"),
                    ("VertexAI", "VertexAI"),
                    ("Gemini", "Gemini"),
                    ("Custom", "Custom"),
                ],
                default="OpenAI",
                max_length=255,
            ),
        ),
    ]
</file>

<file path="label_studio/ml_model_providers/migrations/0008_alter_modelproviderconnection_provider.py">
# Generated by Django 5.1.4 on 2025-01-10 20:15

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("ml_model_providers", "0007_alter_modelproviderconnection_provider"),
    ]

    operations = [
        migrations.AlterField(
            model_name="modelproviderconnection",
            name="provider",
            field=models.CharField(
                choices=[
                    ("OpenAI", "OpenAI"),
                    ("AzureOpenAI", "AzureOpenAI"),
                    ("VertexAI", "VertexAI"),
                    ("Gemini", "Gemini"),
                    ("Anthropic", "Anthropic"),
                    ("Custom", "Custom"),
                ],
                default="OpenAI",
                max_length=255,
            ),
        ),
    ]
</file>

<file path="label_studio/ml_model_providers/migrations/0009_alter_modelproviderconnection_provider.py">
# Generated by Django 5.1.5 on 2025-02-07 16:47

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("ml_model_providers", "0008_alter_modelproviderconnection_provider"),
    ]

    operations = [
        migrations.AlterField(
            model_name="modelproviderconnection",
            name="provider",
            field=models.CharField(
                choices=[
                    ("OpenAI", "OpenAI"),
                    ("AzureOpenAI", "AzureOpenAI"),
                    ("AzureAIFoundry", "AzureAIFoundry"),
                    ("VertexAI", "VertexAI"),
                    ("Gemini", "Gemini"),
                    ("Anthropic", "Anthropic"),
                    ("Custom", "Custom"),
                ],
                default="OpenAI",
                max_length=255,
            ),
        ),
    ]
</file>

<file path="label_studio/ml_model_providers/models.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
from typing import List

from django.conf import settings
from django.db import models
from django.utils.translation import gettext_lazy as _
from tasks.models import PredictionMeta


class ModelProviders(models.TextChoices):
    OPENAI = 'OpenAI', _('OpenAI')
    AZURE_OPENAI = 'AzureOpenAI', _('AzureOpenAI')
    AZURE_AI_FOUNDRY = 'AzureAIFoundry', _('AzureAIFoundry')
    VERTEX_AI = 'VertexAI', _('VertexAI')
    GEMINI = 'Gemini', _('Gemini')
    ANTHROPIC = 'Anthropic', _('Anthropic')
    CUSTOM = 'Custom', _('Custom')


class ModelProviderConnectionScopes(models.TextChoices):
    ORG = 'Organization', _('Organization')
    USER = 'User', _('User')
    MODEL = 'Model', _('Model')


class ModelProviderConnection(models.Model):

    provider = models.CharField(max_length=255, choices=ModelProviders.choices, default=ModelProviders.OPENAI)

    api_key = models.TextField(_('api_key'), null=True, blank=True, help_text='Model provider API key')

    auth_token = models.TextField(_('auth_token'), null=True, blank=True, help_text='Model provider Auth token')

    deployment_name = models.CharField(max_length=512, null=True, blank=True, help_text='Azure OpenAI deployment name')

    endpoint = models.CharField(max_length=512, null=True, blank=True, help_text='Azure OpenAI endpoint')

    google_application_credentials = models.TextField(
        _('google application credentials'),
        null=True,
        blank=True,
        help_text='The content of GOOGLE_APPLICATION_CREDENTIALS json file',
    )

    google_project_id = models.CharField(
        _('google project id'), max_length=255, null=True, blank=True, help_text='Google project ID'
    )

    google_location = models.CharField(
        _('google location'), max_length=255, null=True, blank=True, help_text='Google project location'
    )

    cached_available_models = models.CharField(
        max_length=4096, null=True, blank=True, help_text='List of available models from the provider'
    )

    scope = models.CharField(
        max_length=255, choices=ModelProviderConnectionScopes.choices, default=ModelProviderConnectionScopes.ORG
    )

    organization = models.ForeignKey(
        'organizations.Organization', on_delete=models.CASCADE, related_name='model_provider_connections', null=True
    )

    created_by = models.ForeignKey(
        settings.AUTH_USER_MODEL,
        related_name='created_model_provider_connections',
        on_delete=models.SET_NULL,
        null=True,
    )

    # Future work - add foreign key for modelinterface / modelinstance

    created_at = models.DateTimeField(_('created at'), auto_now_add=True)

    updated_at = models.DateTimeField(_('updated at'), auto_now=True)

    is_internal = models.BooleanField(
        _('is_internal'),
        default=False,
        help_text='Whether the model provider connection is internal, not visible to the user',
        null=True,
        blank=True,
    )

    budget_limit = models.FloatField(
        _('budget_limit'),
        null=True,
        blank=True,
        default=None,
        help_text='Budget limit for the model provider connection (null if unlimited)',
    )

    budget_last_reset_date = models.DateTimeField(
        _('budget_last_reset_date'),
        null=True,
        blank=True,
        default=None,
        help_text='Date and time the budget was last reset',
    )

    budget_reset_period = models.CharField(
        _('budget_reset_period'),
        max_length=20,
        choices=[
            ('Monthly', 'Monthly'),
            ('Yearly', 'Yearly'),
        ],
        null=True,
        blank=True,
        default=None,
        help_text='Budget reset period for the model provider connection (null if not reset)',
    )

    budget_total_spent = models.FloatField(
        _('budget_total_spent'),
        null=True,
        blank=True,
        default=None,
        help_text='Tracked total budget spent for the given provider connection within the current budget period',
    )

    budget_alert_threshold = models.FloatField(
        _('budget_alert_threshold'),
        null=True,
        blank=True,
        default=None,
        help_text='Budget alert threshold for the given provider connection',
    )

    # Check if user is Admin or Owner
    # This will need to be updated if we ever use this model in LSO as `is_owner` and
    # `is_administrator` only exist in LSE
    def has_permission(self, user):
        return (
            user.is_administrator or user.is_owner or user.is_manager
        ) and user.active_organization_id == self.organization_id

    def update_budget_total_spent_from_predictions_meta(self, predictions_meta: List[PredictionMeta]):
        total_cost = sum(meta.total_cost or 0 for meta in predictions_meta)
        # opting for the goofy "self.budget_total_spent or 0" to avoid a db migration
        self.budget_total_spent = (self.budget_total_spent or 0) + total_cost
        self.save(update_fields=['budget_total_spent'])
</file>

<file path="label_studio/ml_models/migrations/0001_initial.py">
# Generated by Django 3.2.23 on 2024-02-16 21:03

from django.conf import settings
from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    initial = True

    dependencies = [
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
        ('organizations', '0006_alter_organizationmember_deleted_at'),
    ]

    operations = [
        migrations.CreateModel(
            name='ModelInterface',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('title', models.CharField(help_text='Model name', max_length=500, verbose_name='title')),
                ('description', models.TextField(blank=True, help_text='Model description', null=True, verbose_name='description')),
                ('created_at', models.DateTimeField(auto_now_add=True, verbose_name='created at')),
                ('updated_at', models.DateTimeField(auto_now=True, verbose_name='updated at')),
                ('created_by', models.ForeignKey(null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='created_models', to=settings.AUTH_USER_MODEL)),
                ('organization', models.ForeignKey(null=True, on_delete=django.db.models.deletion.CASCADE, related_name='model_interfaces', to='organizations.organization')),
            ],
        ),
        migrations.CreateModel(
            name='ThirdPartyModelVersion',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('title', models.CharField(help_text='Model name', max_length=500, verbose_name='title')),
                ('prompt', models.TextField(help_text='Prompt to execute', verbose_name='prompt')),
                ('provider', models.CharField(choices=[('OpenAI', 'OpenAI')], default='OpenAI', help_text='The model provider to use e.g. OpenAI', max_length=255)),
                ('provider_model_id', models.CharField(help_text='The model ID to use within the given provider, e.g. gpt-3.5', max_length=255)),
                ('created_at', models.DateTimeField(auto_now_add=True, verbose_name='created at')),
                ('updated_at', models.DateTimeField(auto_now=True, verbose_name='updated at')),
                ('created_by', models.ForeignKey(null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='created_third_party_model_versions', to=settings.AUTH_USER_MODEL)),
                ('organization', models.ForeignKey(null=True, on_delete=django.db.models.deletion.CASCADE, related_name='third_party_model_versions', to='organizations.organization')),
                ('parent_model', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='model_versions', to='ml_models.modelinterface')),
            ],
            options={
                'abstract': False,
            },
        ),
    ]
</file>

<file path="label_studio/ml_models/migrations/0002_modelrun.py">
# Generated by Django 3.2.23 on 2024-02-26 20:19

from django.conf import settings
from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    dependencies = [
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
        ('projects', '0026_auto_20231103_0020'),
        ('organizations', '0006_alter_organizationmember_deleted_at'),
        ('ml_models', '0001_initial'),
    ]

    operations = [
        migrations.CreateModel(
            name='ModelRun',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('project_subset', models.CharField(choices=[('All', 'All'), ('HasGT', 'HasGT')], default='HasGT', max_length=255)),
                ('status', models.CharField(choices=[('Pending', 'Pending'), ('InProgress', 'InProgress'), ('Completed', 'Completed'), ('Failed', 'Failed'), ('Canceled', 'Canceled')], default='Pending', max_length=255)),
                ('created_at', models.DateTimeField(auto_now_add=True, verbose_name='created at')),
                ('triggered_at', models.DateTimeField(verbose_name='triggered at')),
                ('completed_at', models.DateTimeField(default=None, null=True, verbose_name='completed at')),
                ('created_by', models.ForeignKey(null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='model_runs', to=settings.AUTH_USER_MODEL)),
                ('model_version', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='model_runs', to='ml_models.thirdpartymodelversion')),
                ('organization', models.ForeignKey(null=True, on_delete=django.db.models.deletion.CASCADE, related_name='model_runs', to='organizations.organization')),
                ('project', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='model_runs', to='projects.project')),
            ],
        ),
    ]
</file>

<file path="label_studio/ml_models/migrations/0003_auto_20240228_2228.py">
# Generated by Django 3.2.23 on 2024-02-28 22:28

from django.db import migrations, models
import ml_models.models


class Migration(migrations.Migration):

    dependencies = [
        ('projects', '0026_auto_20231103_0020'),
        ('ml_models', '0002_modelrun'),
    ]

    operations = [
        migrations.AddField(
            model_name='modelinterface',
            name='associated_projects',
            field=models.ManyToManyField(blank=True, to='projects.Project'),
        ),
        migrations.AddField(
            model_name='modelinterface',
            name='input_fields',
            field=models.JSONField(default=list, validators=[ml_models.models.validate_string_list]),
        ),
        migrations.AddField(
            model_name='modelinterface',
            name='output_classes',
            field=models.JSONField(default=list, validators=[ml_models.models.validate_string_list]),
        ),
    ]
</file>

<file path="label_studio/ml_models/migrations/0004_modelrun_job_id.py">
# Generated by Django 3.2.23 on 2024-03-12 22:40

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('ml_models', '0003_auto_20240228_2228'),
    ]

    operations = [
        migrations.AddField(
            model_name='modelrun',
            name='job_id',
            field=models.CharField(blank=True, default=None, help_text='Job ID for inference job for a ModelRun e.g. Adala job ID', max_length=255, null=True),
        ),
    ]
</file>

<file path="label_studio/ml_models/migrations/0005_auto_20240319_1738.py">
# Generated by Django 3.2.23 on 2024-03-19 17:38

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('ml_models', '0004_modelrun_job_id'),
    ]

    operations = [
        migrations.AddField(
            model_name='modelrun',
            name='predictions_updated_at',
            field=models.DateTimeField(default=None, null=True, verbose_name='predictions updated at'),
        ),
        migrations.AlterField(
            model_name='modelrun',
            name='triggered_at',
            field=models.DateTimeField(default=None, null=True, verbose_name='triggered at'),
        ),
    ]
</file>

<file path="label_studio/ml_models/migrations/0006_alter_modelrun_project_subset.py">
# Generated by Django 3.2.23 on 2024-06-06 22:25

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('ml_models', '0005_auto_20240319_1738'),
    ]

    operations = [
        migrations.AlterField(
            model_name='modelrun',
            name='project_subset',
            field=models.CharField(choices=[('All', 'All'), ('HasGT', 'HasGT'), ('Sample', 'Sample')], default='HasGT', max_length=255),
        ),
    ]
</file>

<file path="label_studio/ml_models/migrations/0007_auto_20240617_2200.py">
# Generated by Django 3.2.25 on 2024-06-17 22:00

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('ml_models', '0006_alter_modelrun_project_subset'),
    ]

    operations = [
        migrations.AddField(
            model_name='modelrun',
            name='total_correct_predictions',
            field=models.IntegerField(default=0, verbose_name='total correct predictions'),
        ),
        migrations.AddField(
            model_name='modelrun',
            name='total_predictions',
            field=models.IntegerField(default=0, verbose_name='total predictions'),
        ),
    ]
</file>

<file path="label_studio/ml_models/migrations/0008_modelrun_total_tasks.py">
# Generated by Django 3.2.25 on 2024-06-18 22:22

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('ml_models', '0007_auto_20240617_2200'),
    ]

    operations = [
        migrations.AddField(
            model_name='modelrun',
            name='total_tasks',
            field=models.IntegerField(default=0, verbose_name='total tasks'),
        ),
    ]
</file>

<file path="label_studio/ml_models/migrations/0009_alter_thirdpartymodelversion_provider.py">
# Generated by Django 3.2.25 on 2024-07-22 20:54

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('ml_models', '0008_modelrun_total_tasks'),
    ]

    operations = [
        migrations.AlterField(
            model_name='thirdpartymodelversion',
            name='provider',
            field=models.CharField(choices=[('OpenAI', 'OpenAI'), ('AzureOpenAI', 'AzureOpenAI')], default='OpenAI', help_text='The model provider to use e.g. OpenAI', max_length=255),
        ),
    ]
</file>

<file path="label_studio/ml_models/migrations/0010_modelinterface_skill_name.py">
# Generated by Django 4.2.15 on 2024-08-14 16:44

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("ml_models", "0009_alter_thirdpartymodelversion_provider"),
    ]

    operations = [
        migrations.AddField(
            model_name="modelinterface",
            name="skill_name",
            field=models.CharField(
                choices=[
                    ("TextClassification", "TextClassification"),
                    ("NamedEntityRecognition", "NamedEntityRecognition"),
                ],
                max_length=255,
                null=True,
            ),
        ),
        migrations.RunSQL(
            # set existing ModelInterface objects to text classification
            sql=[
                "UPDATE ml_models_modelinterface SET skill_name = 'TextClassification' WHERE skill_name IS NULL;",
            ],
            # statement above is not reversible, but doesn't matter because the column will be dropped, so do nothing
            reverse_sql=[]
        ),
    ]
</file>

<file path="label_studio/ml_models/migrations/0011_thirdpartymodelversion_model_provider_connection.py">
# Generated by Django 3.2.25 on 2024-09-12 21:59

from django.db import migrations, models, transaction
import django.db.models.deletion
import django_migration_linter as linter
from core.redis import start_job_async_or_sync
from ml_models.models import ThirdPartyModelVersion
from ml_model_providers.models import ModelProviderConnection, ModelProviders


def _fill_model_version_model_provider_connection():
    for provider in [ModelProviders.OPENAI, ModelProviders.AZURE_OPENAI]:
        this_provider_model_versions = ThirdPartyModelVersion.objects.filter(provider=provider).values('id', 'organization_id', 'provider_model_id')
        for provider_model_version in this_provider_model_versions:
            connection_ids = ModelProviderConnection.objects.filter(
                organization_id=provider_model_version['organization_id'],
                provider=provider,
                **({'deployment_name': provider_model_version['provider_model_id']} if provider == ModelProviders.AZURE_OPENAI else {}),
            ).values_list('id', flat=True)[:1]
            connection_id = connection_ids[0] if connection_ids else None
            ThirdPartyModelVersion.objects.filter(id=provider_model_version['id']).update(model_provider_connection_id=connection_id)

def forwards(apps, schema_editor):
    start_job_async_or_sync(_fill_model_version_model_provider_connection)


def backwards(apps, schema_editor):
    pass


class Migration(migrations.Migration):
    atomic = False

    dependencies = [
        ('ml_model_providers', '0003_modelproviderconnection_cached_available_models'),
        ('ml_models', '0010_modelinterface_skill_name'),
    ]

    operations = [
        linter.IgnoreMigration(),
        migrations.AddField(
            model_name='thirdpartymodelversion',
            name='model_provider_connection',
            field=models.ForeignKey(null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='model_versions', to='ml_model_providers.modelproviderconnection'),
        ),
        migrations.RunPython(forwards, backwards)
    ]
</file>

<file path="label_studio/ml_models/migrations/0012_alter_thirdpartymodelversion_provider.py">
# Generated by Django 4.2.15 on 2024-09-16 13:36

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('ml_models', '0011_thirdpartymodelversion_model_provider_connection'),
    ]

    operations = [
        migrations.AlterField(
            model_name='thirdpartymodelversion',
            name='provider',
            field=models.CharField(choices=[('OpenAI', 'OpenAI'), ('AzureOpenAI', 'AzureOpenAI'), ('Custom', 'Custom')], default='OpenAI', help_text='The model provider to use e.g. OpenAI', max_length=255),
        ),
    ]
</file>

<file path="label_studio/ml_models/migrations/0013_alter_thirdpartymodelversion_provider.py">
# Generated by Django 5.1.4 on 2025-01-03 20:58

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("ml_models", "0012_alter_thirdpartymodelversion_provider"),
    ]

    operations = [
        migrations.AlterField(
            model_name="thirdpartymodelversion",
            name="provider",
            field=models.CharField(
                choices=[
                    ("OpenAI", "OpenAI"),
                    ("AzureOpenAI", "AzureOpenAI"),
                    ("VertexAI", "VertexAI"),
                    ("Custom", "Custom"),
                ],
                default="OpenAI",
                help_text="The model provider to use e.g. OpenAI",
                max_length=255,
            ),
        ),
    ]
</file>

<file path="label_studio/ml_models/migrations/0014_alter_thirdpartymodelversion_provider.py">
# Generated by Django 5.1.4 on 2025-01-10 18:38

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("ml_models", "0013_alter_thirdpartymodelversion_provider"),
    ]

    operations = [
        migrations.AlterField(
            model_name="thirdpartymodelversion",
            name="provider",
            field=models.CharField(
                choices=[
                    ("OpenAI", "OpenAI"),
                    ("AzureOpenAI", "AzureOpenAI"),
                    ("VertexAI", "VertexAI"),
                    ("Gemini", "Gemini"),
                    ("Custom", "Custom"),
                ],
                default="OpenAI",
                help_text="The model provider to use e.g. OpenAI",
                max_length=255,
            ),
        ),
    ]
</file>

<file path="label_studio/ml_models/migrations/0015_alter_thirdpartymodelversion_provider.py">
# Generated by Django 5.1.4 on 2025-01-10 20:15

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("ml_models", "0014_alter_thirdpartymodelversion_provider"),
    ]

    operations = [
        migrations.AlterField(
            model_name="thirdpartymodelversion",
            name="provider",
            field=models.CharField(
                choices=[
                    ("OpenAI", "OpenAI"),
                    ("AzureOpenAI", "AzureOpenAI"),
                    ("VertexAI", "VertexAI"),
                    ("Gemini", "Gemini"),
                    ("Anthropic", "Anthropic"),
                    ("Custom", "Custom"),
                ],
                default="OpenAI",
                help_text="The model provider to use e.g. OpenAI",
                max_length=255,
            ),
        ),
    ]
</file>

<file path="label_studio/ml_models/migrations/0016_alter_thirdpartymodelversion_provider.py">
# Generated by Django 5.1.5 on 2025-02-07 16:47

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("ml_models", "0015_alter_thirdpartymodelversion_provider"),
    ]

    operations = [
        migrations.AlterField(
            model_name="thirdpartymodelversion",
            name="provider",
            field=models.CharField(
                choices=[
                    ("OpenAI", "OpenAI"),
                    ("AzureOpenAI", "AzureOpenAI"),
                    ("AzureAIFoundry", "AzureAIFoundry"),
                    ("VertexAI", "VertexAI"),
                    ("Gemini", "Gemini"),
                    ("Anthropic", "Anthropic"),
                    ("Custom", "Custom"),
                ],
                default="OpenAI",
                help_text="The model provider to use e.g. OpenAI",
                max_length=255,
            ),
        ),
    ]
</file>

<file path="label_studio/ml_models/models.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license."""

import logging

from django.conf import settings
from django.db import models
from django.utils.translation import gettext_lazy as _
from ml_model_providers.models import ModelProviderConnection, ModelProviders
from projects.models import Project
from rest_framework.exceptions import ValidationError
from tasks.models import Annotation, FailedPrediction, Prediction, PredictionMeta

logger = logging.getLogger(__name__)


# skills are partitions of projects (label config + input columns + output columns) into categories of labeling tasks
class SkillNames(models.TextChoices):
    TEXT_CLASSIFICATION = 'TextClassification', _('TextClassification')
    NAMED_ENTITY_RECOGNITION = 'NamedEntityRecognition', _('NamedEntityRecognition')


def validate_string_list(value):
    if not value:
        raise ValidationError('list should not be empty')
    if not isinstance(value, list):
        raise ValidationError('Value must be a list')
    if not all(isinstance(item, str) for item in value):
        raise ValidationError('All items in the list must be strings')


class ModelInterface(models.Model):
    title = models.CharField(_('title'), max_length=500, null=False, blank=False, help_text='Model name')

    description = models.TextField(_('description'), null=True, blank=True, help_text='Model description')

    created_by = models.ForeignKey(
        settings.AUTH_USER_MODEL, related_name='created_models', on_delete=models.SET_NULL, null=True
    )

    created_at = models.DateTimeField(_('created at'), auto_now_add=True)

    updated_at = models.DateTimeField(_('updated at'), auto_now=True)

    organization = models.ForeignKey(
        'organizations.Organization', on_delete=models.CASCADE, related_name='model_interfaces', null=True
    )

    skill_name = models.CharField(max_length=255, choices=SkillNames.choices, null=True)

    input_fields = models.JSONField(default=list, validators=[validate_string_list])

    output_classes = models.JSONField(default=list, validators=[validate_string_list])

    associated_projects = models.ManyToManyField('projects.Project', blank=True)

    def has_permission(self, user):
        return user.active_organization == self.organization


class ModelVersion(models.Model):
    class Meta:
        abstract = True

    title = models.CharField(_('title'), max_length=500, null=False, blank=False, help_text='Model name')

    parent_model = models.ForeignKey(ModelInterface, related_name='model_versions', on_delete=models.CASCADE)

    prompt = models.TextField(_('prompt'), null=False, blank=False, help_text='Prompt to execute')

    model_provider_connection = models.ForeignKey(
        ModelProviderConnection, related_name='model_versions', on_delete=models.SET_NULL, null=True
    )

    @property
    def full_title(self):
        return f'{self.parent_model.title}__{self.title}'

    def delete(self, *args, **kwargs):
        """
        Deletes Predictions associated with ModelVersion
        """
        model_runs = ModelRun.objects.filter(model_version=self.id)
        for model_run in model_runs:
            model_run.delete_predictions()
        super().delete(*args, **kwargs)


class ThirdPartyModelVersion(ModelVersion):
    provider = models.CharField(
        max_length=255,
        choices=ModelProviders.choices,
        default=ModelProviders.OPENAI,
        help_text='The model provider to use e.g. OpenAI',
    )

    provider_model_id = models.CharField(
        max_length=255,
        blank=False,
        null=False,
        help_text='The model ID to use within the given provider, e.g. gpt-3.5',
    )

    created_by = models.ForeignKey(
        settings.AUTH_USER_MODEL,
        related_name='created_third_party_model_versions',
        on_delete=models.SET_NULL,
        null=True,
    )

    created_at = models.DateTimeField(_('created at'), auto_now_add=True)

    updated_at = models.DateTimeField(_('updated at'), auto_now=True)

    organization = models.ForeignKey(
        'organizations.Organization', on_delete=models.CASCADE, related_name='third_party_model_versions', null=True
    )

    @property
    def project(self):
        # TODO: can it be just a property of the model version?
        if self.parent_model and self.parent_model.associated_projects.exists():
            return self.parent_model.associated_projects.first()
        return None

    def has_permission(self, user):
        return user.active_organization == self.organization


class ModelRun(models.Model):
    class ProjectSubset(models.TextChoices):
        ALL = 'All', _('All')
        HASGT = 'HasGT', _('HasGT')
        SAMPLE = 'Sample', _('Sample')

    class FileType(models.TextChoices):
        INPUT = 'Input', _('Input')
        OUTPUT = 'Output', _('Output')

    class ModelRunStatus(models.TextChoices):
        PENDING = 'Pending', _('Pending')
        INPROGRESS = 'InProgress', _('InProgress')
        COMPLETED = 'Completed', ('Completed')
        FAILED = 'Failed', ('Failed')
        CANCELED = 'Canceled', ('Canceled')

    organization = models.ForeignKey(
        'organizations.Organization', on_delete=models.CASCADE, related_name='model_runs', null=True
    )

    project = models.ForeignKey(Project, on_delete=models.CASCADE, related_name='model_runs')

    model_version = models.ForeignKey(ThirdPartyModelVersion, on_delete=models.CASCADE, related_name='model_runs')

    created_by = models.ForeignKey(
        settings.AUTH_USER_MODEL,
        related_name='model_runs',
        on_delete=models.SET_NULL,
        null=True,
    )

    project_subset = models.CharField(max_length=255, choices=ProjectSubset.choices, default=ProjectSubset.HASGT)

    status = models.CharField(max_length=255, choices=ModelRunStatus.choices, default=ModelRunStatus.PENDING)

    job_id = models.CharField(
        max_length=255,
        null=True,
        blank=True,
        default=None,
        help_text='Job ID for inference job for a ModelRun e.g. Adala job ID',
    )

    total_predictions = models.IntegerField(_('total predictions'), default=0)

    total_correct_predictions = models.IntegerField(_('total correct predictions'), default=0)

    total_tasks = models.IntegerField(_('total tasks'), default=0)

    created_at = models.DateTimeField(_('created at'), auto_now_add=True)

    triggered_at = models.DateTimeField(_('triggered at'), null=True, default=None)

    predictions_updated_at = models.DateTimeField(_('predictions updated at'), null=True, default=None)

    completed_at = models.DateTimeField(_('completed at'), null=True, default=None)

    def has_permission(self, user):
        return user.active_organization == self.organization

    def delete_predictions(self):
        """
        Deletes any predictions that have originated from a ModelRun

        Executing a raw SQL query here for speed. This ignores any foreign key relationships
        so if another model has a Prediction fk and set to on_delete=CASCADE for example,
        it will not take affect. The only relationship like this that currently exists
        is in Annotation.parent_prediction, which we are handling here.
        """
        predictions = Prediction.objects.filter(model_run=self.id)
        prediction_ids = [p.id for p in predictions]
        # to delete all dependencies where predictions are foreign keys.
        Annotation.objects.filter(parent_prediction__in=prediction_ids).update(parent_prediction=None)
        try:
            from stats.models import PredictionStats

            prediction_stats_to_be_deleted = PredictionStats.objects.filter(prediction_to__in=prediction_ids)
            prediction_stats_to_be_deleted.delete()
        except Exception as e:
            logger.info(f'PredictionStats model does not exist , exception:{e}')

        # Delete failed predictions. Currently no other model references this, no fk relationships to remove
        failed_predictions = FailedPrediction.objects.filter(model_run=self.id)
        failed_predictions_ids = [p.id for p in failed_predictions]

        # delete predictions meta
        PredictionMeta.objects.filter(prediction__in=prediction_ids).delete()
        PredictionMeta.objects.filter(failed_prediction__in=failed_predictions_ids).delete()

        # remove predictions from db
        predictions._raw_delete(predictions.db)
        failed_predictions._raw_delete(failed_predictions.db)

    def delete(self, *args, **kwargs):
        """
        Deletes Predictions associated with ModelRun
        """
        self.delete_predictions()
        super().delete(*args, **kwargs)
</file>

<file path="label_studio/organizations/management/commands/destroy_organization.py">
import logging

from django.core.management.base import BaseCommand
from organizations.functions import destroy_organization
from organizations.models import Organization

log = logging.getLogger(__name__)


class Command(BaseCommand):
    help = 'Destroy organization'

    def add_arguments(self, parser):
        parser.add_argument('organization_id', type=int)

    def handle(self, *args, **options):
        org = Organization.objects.filter(pk=options['organization_id']).first()
        if org is None:
            print(f'Organization with id: {options["organization_id"]} not found')
            return
        yes = input(
            f'You are trying to remove organization with id: {org.id} and title: "{org.title}". This is not reversible!! Are you sure? yes/no: '
        )
        if yes == 'yes':
            destroy_organization(org)
</file>

<file path="label_studio/organizations/migrations/__init__.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
</file>

<file path="label_studio/organizations/migrations/0001_initial.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
# Generated by Django 2.1.3 on 2020-04-17 22:29

from django.conf import settings
from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    initial = True

    dependencies = [
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
    ]

    operations = [
        migrations.CreateModel(
            name='Organization',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('title', models.CharField(max_length=1000, verbose_name='organization title')),
                ('created_at', models.DateTimeField(auto_now_add=True, verbose_name='created at')),
                ('updated_at', models.DateTimeField(auto_now=True, verbose_name='updated at')),
                ('created_by', models.OneToOneField(null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='organization', to=settings.AUTH_USER_MODEL, verbose_name='created_by')),
            ],
            options={
                'db_table': 'organization',
            },
        ),
        migrations.CreateModel(
            name='OrganizationMember',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('role', models.CharField(choices=[('Administrator', 'Administrator'), ('Manager', 'Manager'), ('Coordinator', 'Coordinator'), ('Collaborator', 'Collaborator'), ('Non Activated', 'Not Activated'), ('Disabled', 'Disabled')], default='Non Activated', max_length=100)),
                ('created_at', models.DateTimeField(auto_now_add=True, verbose_name='created at')),
                ('updated_at', models.DateTimeField(auto_now=True, verbose_name='updated at')),
                ('organization', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='organizations.Organization')),
                ('user', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='om_through', to=settings.AUTH_USER_MODEL)),
            ],
        ),
        migrations.AddField(
            model_name='organization',
            name='users',
            field=models.ManyToManyField(related_name='organizations', through='organizations.OrganizationMember', to=settings.AUTH_USER_MODEL),
        ),
    ]
</file>

<file path="label_studio/organizations/migrations/0001_squashed_0008_auto_20201005_1552.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
# Generated by Django 3.1.4 on 2021-03-03 07:31

import core.utils.common
from django.conf import settings
from django.db import migrations, models
import django.db.models.deletion


def rename_disabled_to_off0006(apps, schema_editor):
    OrganizationMember = apps.get_model('organizations', 'OrganizationMember')
    OrganizationMember.objects.filter(role="Disabled").update(role="Off")

    migrations.AlterField(
        model_name='organizationmember',
        name='role',
        field=models.CharField(
            choices=[('Administrator', 'Administrator'), ('Manager', 'Manager'), ('Coordinator', 'Coordinator'),
                     ('Collaborator', 'Collaborator'), ('Not Activated', 'Not Activated'), ('Off', 'Off')],
            default='Not Activated', help_text='Organization membership role', max_length=100),
    )


def rename_disabled_to_off0007(apps, schema_editor):
    OrganizationMember = apps.get_model('organizations', 'OrganizationMember')
    OrganizationMember.objects.filter(role="Off").update(role="Deactivated")

    migrations.AlterField(
        model_name='organizationmember',
        name='role',
        field=models.CharField(
            choices=[('Administrator', 'Administrator'), ('Manager', 'Manager'), ('Coordinator', 'Coordinator'),
                     ('Collaborator', 'Collaborator'), ('Not Activated', 'Not Activated'),
                     ('Deactivated', 'Deactivated')],
            default='Not Activated', help_text='Organization membership role', max_length=100),
    )


class Migration(migrations.Migration):

    replaces = [('organizations', '0001_initial'), ('organizations', '0002_organization_token'), ('organizations', '0003_auto_20200418_0202'), ('organizations', '0004_auto_20200501_1751'), ('organizations', '0005_auto_20200811_2313'), ('organizations', '0006_auto_20200923_1423'), ('organizations', '0007_auto_20200923_2200'), ('organizations', '0008_auto_20201005_1552')]

    initial = True

    dependencies = [
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
    ]

    operations = [
        migrations.CreateModel(
            name='Organization',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('title', models.CharField(max_length=1000, verbose_name='organization title')),
                ('created_at', models.DateTimeField(auto_now_add=True, verbose_name='created at')),
                ('updated_at', models.DateTimeField(auto_now=True, verbose_name='updated at')),
                ('created_by', models.OneToOneField(null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='organization', to=settings.AUTH_USER_MODEL, verbose_name='created_by')),
            ],
            options={
                'db_table': 'organization',
            },
        ),
        migrations.CreateModel(
            name='OrganizationMember',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('role', models.CharField(choices=[('Administrator', 'Administrator'), ('Manager', 'Manager'), ('Coordinator', 'Coordinator'), ('Collaborator', 'Collaborator'), ('Not Activated', 'Not Activated'), ('Disabled', 'Disabled')], default='Not Activated', max_length=100)),
                ('created_at', models.DateTimeField(auto_now_add=True, verbose_name='created at')),
                ('updated_at', models.DateTimeField(auto_now=True, verbose_name='updated at')),
                ('organization', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='organizations.organization')),
                ('user', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='om_through', to=settings.AUTH_USER_MODEL)),
            ],
        ),
        migrations.AddField(
            model_name='organization',
            name='users',
            field=models.ManyToManyField(related_name='organizations', through='organizations.OrganizationMember', to=settings.AUTH_USER_MODEL),
        ),
        migrations.AddField(
            model_name='organization',
            name='token',
            field=models.CharField(blank=True, default=core.utils.common.create_hash, max_length=256, null=True, unique=True, verbose_name='token'),
        ),
        migrations.AlterField(
            model_name='organizationmember',
            name='organization',
            field=models.ForeignKey(help_text='Organization ID', on_delete=django.db.models.deletion.CASCADE, to='organizations.organization'),
        ),
        migrations.AlterField(
            model_name='organizationmember',
            name='role',
            field=models.CharField(choices=[('Administrator', 'Administrator'), ('Manager', 'Manager'), ('Coordinator', 'Coordinator'), ('Collaborator', 'Collaborator'), ('Not Activated', 'Not Activated'), ('Disabled', 'Disabled')], default='Not Activated', help_text='Organization membership role', max_length=100),
        ),
        migrations.AlterField(
            model_name='organizationmember',
            name='user',
            field=models.ForeignKey(help_text='User ID', on_delete=django.db.models.deletion.CASCADE, related_name='om_through', to=settings.AUTH_USER_MODEL),
        ),
        migrations.AddField(
            model_name='organization',
            name='default_role',
            field=models.CharField(default='Not Activated', help_text='Default membership role for invited users', max_length=100),
        ),
        migrations.RunPython(
            code=rename_disabled_to_off0006,
        ),
        migrations.RunPython(
            code=rename_disabled_to_off0007,
        ),
        migrations.AlterField(
            model_name='organizationmember',
            name='role',
            field=models.CharField(choices=[('Administrator', 'Administrator'), ('Manager', 'Manager'), ('Coordinator', 'Coordinator'), ('Collaborator', 'Collaborator'), ('Not Activated', 'Not Activated'), ('Deactivated', 'Deactivated')], default='Not Activated', help_text='Organization membership role', max_length=100),
        ),
    ]
</file>

<file path="label_studio/organizations/migrations/0002_auto_20210310_2044.py">
# Generated by Django 3.1.4 on 2021-03-10 20:44

from django.db import migrations


class Migration(migrations.Migration):

    dependencies = [
        ('organizations', '0001_squashed_0008_auto_20201005_1552'),
    ]

    operations = [
        migrations.RemoveField(
            model_name='organization',
            name='default_role',
        ),
        migrations.RemoveField(
            model_name='organizationmember',
            name='role',
        ),
    ]
</file>

<file path="label_studio/organizations/migrations/0003_auto_20211010_1339.py">
# Generated by Django 3.1.13 on 2021-10-10 13:39

from django.db import migrations


class Migration(migrations.Migration):

    dependencies = [
        ('organizations', '0002_auto_20210310_2044'),
    ]

    operations = [
        migrations.AlterModelOptions(
            name='organizationmember',
            options={'ordering': ['pk']},
        ),
    ]
</file>

<file path="label_studio/organizations/migrations/0004_organization_contact_info.py">
# Generated by Django 3.2.20 on 2023-08-15 23:18

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('organizations', '0003_auto_20211010_1339'),
    ]

    operations = [
        migrations.AddField(
            model_name='organization',
            name='contact_info',
            field=models.EmailField(blank=True, max_length=254, null=True, verbose_name='contact info'),
        ),
    ]
</file>

<file path="label_studio/organizations/migrations/0005_organizationmember_deleted_at.py">
# Generated by Django 3.2.20 on 2023-10-23 14:57

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('organizations', '0004_organization_contact_info'),
    ]

    operations = [
        migrations.AddField(
            model_name='organizationmember',
            name='deleted_at',
            field=models.DateTimeField(db_index=True, default=None, help_text='Timestamp indicating when the organization member was marked as deleted.  If NULL, the member is not considered deleted.', null=True, verbose_name='deleted at'),
        ),
    ]
</file>

<file path="label_studio/organizations/migrations/0006_alter_organizationmember_deleted_at.py">
# Generated by Django 3.2.20 on 2023-11-14 23:18

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('organizations', '0005_organizationmember_deleted_at'),
    ]

    operations = [
        migrations.AlterField(
            model_name='organizationmember',
            name='deleted_at',
            field=models.DateTimeField(blank=True, db_index=True, default=None, help_text='Timestamp indicating when the organization member was marked as deleted.  If NULL, the member is not considered deleted.', null=True, verbose_name='deleted at'),
        ),
    ]
</file>

<file path="label_studio/organizations/tests/factories.py">
import factory
from organizations.models import Organization


class OrganizationFactory(factory.django.DjangoModelFactory):
    title = factory.Faker('company')
    created_by = factory.SubFactory('users.tests.factories.UserFactory', active_organization=None)

    class Meta:
        model = Organization

    @classmethod
    def _create(cls, model_class, *args, **kwargs):
        return Organization.create_organization(**kwargs)

    @factory.post_generation
    def created_by_active_organization(self, create, extracted, **kwargs):
        if not create or not self.created_by:
            return
        self.created_by.active_organization = self
        self.created_by.save(update_fields=['active_organization'])
</file>

<file path="label_studio/organizations/__init__.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
</file>

<file path="label_studio/organizations/admin.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""

# Register your models here.
</file>

<file path="label_studio/organizations/api.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import logging

from core.feature_flags import flag_set
from core.mixins import GetParentObjectMixin
from core.utils.common import load_func
from django.conf import settings
from django.urls import reverse
from django.utils.decorators import method_decorator
from drf_yasg import openapi
from drf_yasg.utils import swagger_auto_schema
from organizations.models import Organization, OrganizationMember
from organizations.serializers import (
    OrganizationIdSerializer,
    OrganizationInviteSerializer,
    OrganizationMemberSerializer,
    OrganizationMemberUserSerializer,
    OrganizationSerializer,
    OrganizationsParamsSerializer,
)
from rest_framework import generics, status
from rest_framework.exceptions import NotFound, PermissionDenied
from rest_framework.generics import get_object_or_404
from rest_framework.pagination import PageNumberPagination
from rest_framework.parsers import FormParser, JSONParser, MultiPartParser
from rest_framework.permissions import IsAuthenticated
from rest_framework.response import Response
from rest_framework.settings import api_settings
from rest_framework.views import APIView
from users.models import User

from label_studio.core.permissions import ViewClassPermission, all_permissions
from label_studio.core.utils.params import bool_from_request

logger = logging.getLogger(__name__)

HasObjectPermission = load_func(settings.MEMBER_PERM)


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Organizations'],
        x_fern_sdk_group_name='organizations',
        x_fern_sdk_method_name='list',
        operation_summary='List your organizations',
        operation_description="""
        Return a list of the organizations you've created or that you have access to.
        """,
    ),
)
class OrganizationListAPI(generics.ListCreateAPIView):
    queryset = Organization.objects.all()
    parser_classes = (JSONParser, FormParser, MultiPartParser)
    permission_required = ViewClassPermission(
        GET=all_permissions.organizations_view,
        PUT=all_permissions.organizations_change,
        POST=all_permissions.organizations_create,
        PATCH=all_permissions.organizations_change,
        DELETE=all_permissions.organizations_change,
    )
    serializer_class = OrganizationIdSerializer

    def filter_queryset(self, queryset):
        return queryset.filter(
            organizationmember__in=self.request.user.om_through.filter(deleted_at__isnull=True)
        ).distinct()

    def get(self, request, *args, **kwargs):
        return super(OrganizationListAPI, self).get(request, *args, **kwargs)

    @swagger_auto_schema(auto_schema=None)
    def post(self, request, *args, **kwargs):
        return super(OrganizationListAPI, self).post(request, *args, **kwargs)


class OrganizationMemberPagination(PageNumberPagination):
    page_size = 20
    page_size_query_param = 'page_size'

    def get_page_size(self, request):
        # emulate "unlimited" page_size
        if (
            self.page_size_query_param in request.query_params
            and request.query_params[self.page_size_query_param] == '-1'
        ):
            return 1000000
        return super().get_page_size(request)


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Organizations'],
        x_fern_sdk_group_name=['organizations', 'members'],
        x_fern_sdk_method_name='list',
        x_fern_pagination={
            'offset': '$request.page',
            'results': '$response.results',
        },
        operation_summary='Get organization members list',
        operation_description='Retrieve a list of the organization members and their IDs.',
    ),
)
class OrganizationMemberListAPI(generics.ListAPIView):
    parser_classes = (JSONParser, FormParser, MultiPartParser)
    permission_required = ViewClassPermission(
        GET=all_permissions.organizations_view,
        PUT=all_permissions.organizations_change,
        PATCH=all_permissions.organizations_change,
        DELETE=all_permissions.organizations_change,
    )
    serializer_class = OrganizationMemberUserSerializer
    pagination_class = OrganizationMemberPagination

    def get_serializer_context(self):
        return {
            'contributed_to_projects': bool_from_request(self.request.GET, 'contributed_to_projects', False),
            'request': self.request,
        }

    def get_queryset(self):
        org = generics.get_object_or_404(self.request.user.organizations, pk=self.kwargs[self.lookup_field])
        if flag_set('fix_backend_dev_3134_exclude_deactivated_users', self.request.user):
            serializer = OrganizationsParamsSerializer(data=self.request.GET)
            serializer.is_valid(raise_exception=True)
            active = serializer.validated_data.get('active')

            # return only active users (exclude DISABLED and NOT_ACTIVATED)
            if active:
                return org.active_members.order_by('user__username')

            # organization page to show all members
            return org.members.order_by('user__username')
        else:
            return org.members.order_by('user__username')


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Organizations'],
        x_fern_sdk_group_name=['organizations', 'members'],
        x_fern_sdk_method_name='get',
        operation_summary='Get organization member details',
        operation_description='Get organization member details by user ID.',
        manual_parameters=[
            openapi.Parameter(
                name='user_pk',
                type=openapi.TYPE_INTEGER,
                in_=openapi.IN_PATH,
                description='A unique integer value identifying the user to get organization details for.',
            ),
        ],
        responses={200: OrganizationMemberSerializer()},
    ),
)
@method_decorator(
    name='delete',
    decorator=swagger_auto_schema(
        tags=['Organizations'],
        x_fern_sdk_group_name=['organizations', 'members'],
        x_fern_sdk_method_name='delete',
        operation_summary='Soft delete an organization member',
        operation_description='Soft delete a member from the organization.',
        manual_parameters=[
            openapi.Parameter(
                name='user_pk',
                type=openapi.TYPE_INTEGER,
                in_=openapi.IN_PATH,
                description='A unique integer value identifying the user to be deleted from the organization.',
            ),
        ],
        responses={
            204: 'Member deleted successfully.',
            405: 'User cannot soft delete self.',
            404: 'Member not found',
        },
    ),
)
class OrganizationMemberDetailAPI(GetParentObjectMixin, generics.RetrieveDestroyAPIView):
    permission_required = ViewClassPermission(
        GET=all_permissions.organizations_view,
        DELETE=all_permissions.organizations_change,
    )
    parent_queryset = Organization.objects.all()
    parser_classes = (JSONParser, FormParser, MultiPartParser)
    serializer_class = OrganizationMemberSerializer
    http_method_names = ['delete', 'get']

    @property
    def permission_classes(self):
        if self.request.method == 'DELETE':
            return [IsAuthenticated, HasObjectPermission]
        return api_settings.DEFAULT_PERMISSION_CLASSES

    def get_queryset(self):
        return OrganizationMember.objects.filter(organization=self.parent_object)

    def get_serializer_context(self):
        return {
            **super().get_serializer_context(),
            'organization': self.parent_object,
        }

    def get(self, request, pk, user_pk):
        queryset = self.get_queryset()
        user = get_object_or_404(User, pk=user_pk)
        member = get_object_or_404(queryset, user=user)
        self.check_object_permissions(request, member)
        serializer = self.get_serializer(member)
        return Response(serializer.data)

    def delete(self, request, pk=None, user_pk=None):
        org = self.parent_object
        if org != request.user.active_organization:
            raise PermissionDenied('You can delete members only for your current active organization')

        user = get_object_or_404(User, pk=user_pk)
        member = get_object_or_404(OrganizationMember, user=user, organization=org)
        if member.deleted_at is not None:
            raise NotFound('Member not found')

        if member.user_id == request.user.id:
            return Response({'detail': 'User cannot soft delete self'}, status=status.HTTP_405_METHOD_NOT_ALLOWED)

        member.soft_delete()
        return Response(status=204)  # 204 No Content is a common HTTP status for successful delete requests


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Organizations'],
        x_fern_sdk_group_name='organizations',
        x_fern_sdk_method_name='get',
        operation_summary=' Get organization settings',
        operation_description='Retrieve the settings for a specific organization by ID.',
    ),
)
@method_decorator(
    name='patch',
    decorator=swagger_auto_schema(
        tags=['Organizations'],
        x_fern_sdk_group_name='organizations',
        x_fern_sdk_method_name='update',
        operation_summary='Update organization settings',
        operation_description='Update the settings for a specific organization by ID.',
    ),
)
class OrganizationAPI(generics.RetrieveUpdateAPIView):

    parser_classes = (JSONParser, FormParser, MultiPartParser)
    queryset = Organization.objects.all()
    permission_required = all_permissions.organizations_change
    serializer_class = OrganizationSerializer

    redirect_route = 'organizations-dashboard'
    redirect_kwarg = 'pk'

    def get(self, request, *args, **kwargs):
        return super(OrganizationAPI, self).get(request, *args, **kwargs)

    def patch(self, request, *args, **kwargs):
        return super(OrganizationAPI, self).patch(request, *args, **kwargs)

    @swagger_auto_schema(auto_schema=None)
    def put(self, request, *args, **kwargs):
        return super(OrganizationAPI, self).put(request, *args, **kwargs)


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Invites'],
        x_fern_sdk_group_name='organizations',
        x_fern_sdk_method_name='get_invite',
        operation_summary='Get organization invite link',
        operation_description='Get a link to use to invite a new member to an organization in Label Studio Enterprise.',
        responses={200: OrganizationInviteSerializer()},
    ),
)
class OrganizationInviteAPI(generics.RetrieveAPIView):
    parser_classes = (JSONParser,)
    queryset = Organization.objects.all()
    permission_required = all_permissions.organizations_change

    def get(self, request, *args, **kwargs):
        org = request.user.active_organization
        invite_url = '{}?token={}'.format(reverse('user-signup'), org.token)
        if hasattr(settings, 'FORCE_SCRIPT_NAME') and settings.FORCE_SCRIPT_NAME:
            invite_url = invite_url.replace(settings.FORCE_SCRIPT_NAME, '', 1)
        serializer = OrganizationInviteSerializer(data={'invite_url': invite_url, 'token': org.token})
        serializer.is_valid()
        return Response(serializer.data, status=200)


@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Invites'],
        x_fern_sdk_group_name='organizations',
        x_fern_sdk_method_name='reset_token',
        operation_summary='Reset organization token',
        operation_description='Reset the token used in the invitation link to invite someone to an organization.',
        responses={200: OrganizationInviteSerializer()},
    ),
)
class OrganizationResetTokenAPI(APIView):
    permission_required = all_permissions.organizations_invite
    parser_classes = (JSONParser,)

    def post(self, request, *args, **kwargs):
        org = request.user.active_organization
        org.reset_token()
        logger.debug(f'New token for organization {org.pk} is {org.token}')
        invite_url = '{}?token={}'.format(reverse('user-signup'), org.token)
        serializer = OrganizationInviteSerializer(data={'invite_url': invite_url, 'token': org.token})
        serializer.is_valid()
        return Response(serializer.data, status=201)
</file>

<file path="label_studio/organizations/apps.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
from django.apps import AppConfig


class OrganizationsConfig(AppConfig):
    name = 'organizations'
</file>

<file path="label_studio/organizations/forms.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""

from django.forms import CharField, HiddenInput, ModelForm

from .models import Organization


class OrganizationForm(ModelForm):
    """ """

    org_update = CharField(widget=HiddenInput(), required=False)

    class Meta:
        model = Organization
        fields = ('title',)


class OrganizationSignupForm(ModelForm):
    """ """

    class Meta:
        model = Organization
        fields = ('title',)
</file>

<file path="label_studio/organizations/functions.py">
from core.utils.common import temporary_disconnect_all_signals
from django.db import transaction
from organizations.models import Organization, OrganizationMember
from projects.models import Project


def create_organization(title, created_by, **kwargs):
    from core.feature_flags import flag_set

    JWT_ACCESS_TOKEN_ENABLED = flag_set('fflag__feature_develop__prompts__dia_1829_jwt_token_auth')

    with transaction.atomic():
        org = Organization.objects.create(title=title, created_by=created_by, **kwargs)
        OrganizationMember.objects.create(user=created_by, organization=org)
        if JWT_ACCESS_TOKEN_ENABLED:
            # set auth tokens to new system for new users
            org.jwt.api_tokens_enabled = True
            org.jwt.legacy_api_tokens_enabled = False
            org.jwt.save()
        return org


def destroy_organization(org):
    with temporary_disconnect_all_signals():
        Project.objects.filter(organization=org).delete()
        if hasattr(org, 'saml'):
            org.saml.delete()
        org.delete()
</file>

<file path="label_studio/organizations/middleware.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import logging

from organizations.models import Organization

logger = logging.getLogger(__name__)


class DummyGetSessionMiddleware:
    def __init__(self, get_response):
        self.get_response = get_response

    def __call__(self, request):
        org = Organization.objects.first()
        user = request.user
        if user and user.is_authenticated and user.active_organization is None:
            user.active_organization = org
            user.save(update_fields=['active_organization'])
        if org is not None:
            request.session['organization_pk'] = org.id
        response = self.get_response(request)
        return response
</file>

<file path="label_studio/organizations/mixins.py">
from django.utils.functional import cached_property


class OrganizationMixin:
    @cached_property
    def active_members(self):
        return self.members


class OrganizationMemberMixin:
    def has_permission(self, user):
        if user.active_organization_id == self.organization_id:
            return True
        return False
</file>

<file path="label_studio/organizations/models.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import logging

from core.utils.common import create_hash, load_func
from django.conf import settings
from django.db import models, transaction
from django.db.models import Count, Q
from django.utils import timezone
from django.utils.functional import cached_property
from django.utils.translation import gettext_lazy as _

logger = logging.getLogger(__name__)

OrganizationMemberMixin = load_func(settings.ORGANIZATION_MEMBER_MIXIN)


class OrganizationMember(OrganizationMemberMixin, models.Model):
    """ """

    user = models.ForeignKey(
        settings.AUTH_USER_MODEL, on_delete=models.CASCADE, related_name='om_through', help_text='User ID'
    )
    organization = models.ForeignKey(
        'organizations.Organization', on_delete=models.CASCADE, help_text='Organization ID'
    )

    created_at = models.DateTimeField(_('created at'), auto_now_add=True)
    updated_at = models.DateTimeField(_('updated at'), auto_now=True)

    deleted_at = models.DateTimeField(
        _('deleted at'),
        default=None,
        null=True,
        blank=True,
        db_index=True,
        help_text='Timestamp indicating when the organization member was marked as deleted.  '
        'If NULL, the member is not considered deleted.',
    )

    # objects = OrganizationMemberQuerySet.as_manager()

    @classmethod
    def find_by_user(cls, user_or_user_pk, organization_pk):
        from users.models import User

        user_pk = user_or_user_pk.pk if isinstance(user_or_user_pk, User) else user_or_user_pk
        return OrganizationMember.objects.get(user=user_pk, organization=organization_pk)

    @cached_property
    def is_deleted(self):
        return bool(self.deleted_at)

    @cached_property
    def is_owner(self):
        return self.user.id == self.organization.created_by.id

    class Meta:
        ordering = ['pk']

    def soft_delete(self):
        with transaction.atomic():
            self.deleted_at = timezone.now()
            self.save(update_fields=['deleted_at'])
            self.user.active_organization = self.user.organizations.filter(
                organizationmember__deleted_at__isnull=True
            ).first()
            self.user.save(update_fields=['active_organization'])

        self.user.task_locks.all().delete()


OrganizationMixin = load_func(settings.ORGANIZATION_MIXIN)


class Organization(OrganizationMixin, models.Model):
    """ """

    title = models.CharField(_('organization title'), max_length=1000, null=False)

    token = models.CharField(_('token'), max_length=256, default=create_hash, unique=True, null=True, blank=True)

    users = models.ManyToManyField(settings.AUTH_USER_MODEL, related_name='organizations', through=OrganizationMember)

    created_by = models.OneToOneField(
        settings.AUTH_USER_MODEL,
        on_delete=models.SET_NULL,
        null=True,
        related_name='organization',
        verbose_name=_('created_by'),
    )

    created_at = models.DateTimeField(_('created at'), auto_now_add=True)
    updated_at = models.DateTimeField(_('updated at'), auto_now=True)

    contact_info = models.EmailField(_('contact info'), blank=True, null=True)

    def __str__(self):
        return self.title + ', id=' + str(self.pk)

    @classmethod
    def create_organization(cls, created_by=None, title='Your Organization', **kwargs):
        _create_organization = load_func(settings.CREATE_ORGANIZATION)
        return _create_organization(title=title, created_by=created_by, **kwargs)

    @classmethod
    def find_by_user(cls, user, check_deleted=False):
        memberships = OrganizationMember.objects.filter(user=user).prefetch_related('organization')
        if not memberships.exists():
            raise ValueError(f'No memberships found for user {user}')
        membership = memberships.first()
        if check_deleted:
            return (membership.organization, True) if membership.deleted_at else (membership.organization, False)

        return membership.organization

    @classmethod
    def find_by_invite_url(cls, url):
        token = url.strip('/').split('/')[-1]
        if len(token):
            return Organization.objects.get(token=token)
        else:
            raise KeyError(f"Can't find Organization by welcome URL: {url}")

    def has_user(self, user):
        return self.users.filter(pk=user.pk).exists()

    def has_deleted(self, user):
        return OrganizationMember.objects.filter(user=user, organization=self, deleted_at__isnull=False).exists()

    def has_project_member(self, user):
        return self.projects.filter(members__user=user).exists()

    def has_permission(self, user):
        return OrganizationMember.objects.filter(user=user, organization=self, deleted_at__isnull=True).exists()

    def add_user(self, user):
        if self.users.filter(pk=user.pk).exists():
            logger.debug('User already exists in organization.')
            return

        with transaction.atomic():
            om = OrganizationMember(user=user, organization=self)
            om.save()

            return om

    def remove_user(self, user):
        OrganizationMember.objects.filter(user=user, organization=self).delete()
        if user.active_organization_id == self.id:
            user.active_organization = user.organizations.filter(organizationmember__deleted_at__isnull=True).first()
            user.save(update_fields=['active_organization'])

    def reset_token(self):
        self.token = create_hash()
        self.save(update_fields=['token'])

    def check_max_projects(self):
        """This check raise an exception if the projects limit is hit"""
        pass

    def projects_sorted_by_created_at(self):
        return (
            self.projects.all()
            .order_by('-created_at')
            .annotate(tasks_count=Count('tasks'), labeled_tasks_count=Count('tasks', filter=Q(tasks__is_labeled=True)))
            .prefetch_related('created_by')
        )

    def created_at_prettify(self):
        return self.created_at.strftime('%d %b %Y %H:%M:%S')

    def per_project_invited_users(self):
        from users.models import User

        invited_ids = self.projects.values_list('members__user__pk', flat=True).distinct()
        per_project_invited_users = User.objects.filter(pk__in=invited_ids)
        return per_project_invited_users

    def should_verify_ssl_certs(self) -> bool:
        if hasattr(self, 'billing') and (org_verify := self.billing.verify_ssl_certs()) is not None:
            return org_verify
        return settings.VERIFY_SSL_CERTS

    @cached_property
    def secure_mode(self):
        return False

    @cached_property
    def members(self):
        return OrganizationMember.objects.filter(organization=self)

    class Meta:
        db_table = 'organization'
</file>

<file path="label_studio/organizations/serializers.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
from collections import OrderedDict

import ujson as json
from drf_dynamic_fields import DynamicFieldsMixin
from organizations.models import Organization, OrganizationMember
from rest_framework import serializers
from users.serializers import UserSerializer


class OrganizationIdSerializer(DynamicFieldsMixin, serializers.ModelSerializer):
    class Meta:
        model = Organization
        fields = ['id', 'title', 'contact_info', 'created_at']


class OrganizationSerializer(DynamicFieldsMixin, serializers.ModelSerializer):
    class Meta:
        model = Organization
        fields = '__all__'


class UserSerializerWithProjects(UserSerializer):
    created_projects = serializers.SerializerMethodField(read_only=True)
    contributed_to_projects = serializers.SerializerMethodField(read_only=True)

    def get_created_projects(self, user):
        if not self.context.get('contributed_to_projects', False):
            return None

        current_user = self.context['request'].user
        return user.created_projects.filter(organization=current_user.active_organization).values('id', 'title')

    def get_contributed_to_projects(self, user):
        if not self.context.get('contributed_to_projects', False):
            return None

        current_user = self.context['request'].user
        projects = user.annotations.filter(project__organization=current_user.active_organization).values(
            'project__id', 'project__title'
        )
        contributed_to = [(json.dumps({'id': p['project__id'], 'title': p['project__title']}), 0) for p in projects]
        contributed_to = OrderedDict(contributed_to)  # remove duplicates without ordering losing
        return [json.loads(key) for key in contributed_to]

    class Meta(UserSerializer.Meta):
        fields = UserSerializer.Meta.fields + ('created_projects', 'contributed_to_projects')


class NewUserSerializer(UserSerializer):
    created_projects = serializers.SerializerMethodField(read_only=True)
    contributed_to_projects = serializers.SerializerMethodField(read_only=True)

    def get_created_projects(self, user):
        if not self.context.get('contributed_to_projects', False):
            return None

        current_user = self.context['request'].user
        return user.created_projects.filter(organization=current_user.active_organization).values('id', 'title')

    def get_contributed_to_projects(self, user):
        if not self.context.get('contributed_to_projects', False):
            return None

        current_user = self.context['request'].user
        projects = user.annotations.filter(project__organization=current_user.active_organization).values(
            'project__id', 'project__title'
        )
        contributed_to = [(json.dumps({'id': p['project__id'], 'title': p['project__title']}), 0) for p in projects]
        contributed_to = OrderedDict(contributed_to)  # remove duplicates without ordering losing
        return [json.loads(key) for key in contributed_to]

    class Meta(UserSerializer.Meta):
        all_fields = list(UserSerializer.Meta.fields)
        if 'org_membership' in all_fields:
            del all_fields[all_fields.index('org_membership')]
        fields = all_fields + ['created_projects', 'contributed_to_projects']


class OrganizationMemberUserSerializer(DynamicFieldsMixin, serializers.ModelSerializer):
    """Adds all user properties"""

    user = UserSerializerWithProjects()

    class Meta:
        model = OrganizationMember
        fields = ['id', 'organization', 'user']


class NewOrganizationMemberUserSerializer(DynamicFieldsMixin, serializers.ModelSerializer):
    """Adds all user properties"""

    user = NewUserSerializer()

    class Meta:
        model = OrganizationMember
        fields = ['id', 'organization', 'user']


class OrganizationMemberSerializer(DynamicFieldsMixin, serializers.ModelSerializer):
    annotations_count = serializers.SerializerMethodField(read_only=True)
    contributed_projects_count = serializers.SerializerMethodField(read_only=True)

    def get_annotations_count(self, member):
        org = self.context.get('organization')
        return member.user.annotations.filter(project__organization=org).count()

    def get_contributed_projects_count(self, member):
        org = self.context.get('organization')
        return member.user.annotations.filter(project__organization=org).values('project').distinct().count()

    class Meta:
        model = OrganizationMember
        fields = ['user', 'organization', 'contributed_projects_count', 'annotations_count', 'created_at']


class OrganizationInviteSerializer(serializers.Serializer):
    token = serializers.CharField(required=False)
    invite_url = serializers.CharField(required=False)


class OrganizationsParamsSerializer(serializers.Serializer):
    active = serializers.BooleanField(required=False, default=False)
    contributed_to_projects = serializers.BooleanField(required=False, default=False)
</file>

<file path="label_studio/organizations/urls.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
from django.urls import include, path
from organizations import api, views

app_name = 'organizations'

# TODO: there should be only one patterns list based on API (with api/ prefix removed)
# Page URLs
_urlpatterns = [
    # get organization page
    path('', views.organization_people_list, name='organization-index'),
]

# API URLs
_api_urlpattens = [
    # organization list viewset
    path('', api.OrganizationListAPI.as_view(), name='organization-list'),
    # organization detail viewset
    path('<int:pk>', api.OrganizationAPI.as_view(), name='organization-detail'),
    # organization memberships list viewset
    path('<int:pk>/memberships', api.OrganizationMemberListAPI.as_view(), name='organization-memberships-list'),
    path(
        '<int:pk>/memberships/<int:user_pk>/',
        api.OrganizationMemberDetailAPI.as_view(),
        name='organization-membership-detail',
    ),
]
# TODO: these urlpatterns should be moved in core/urls with include('organizations.urls')
urlpatterns = [
    path('organization/', views.simple_view, name='organization-simple'),
    path('organization/webhooks', views.simple_view, name='organization-simple-webhooks'),
    path('people/', include(_urlpatterns)),
    # TODO: temporary route, remove as needed
    path('models/', views.simple_view, name='models'),
    path('api/organizations/', include((_api_urlpattens, app_name), namespace='api')),
    # invite
    path('api/invite', api.OrganizationInviteAPI.as_view(), name='organization-invite'),
    path('api/invite/reset-token', api.OrganizationResetTokenAPI.as_view(), name='organization-reset-token'),
]
</file>

<file path="label_studio/organizations/views.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
from django.contrib.auth.decorators import login_required
from django.shortcuts import render


@login_required
def organization_people_list(request):
    return render(request, 'organizations/people_list.html')


@login_required
def simple_view(request):
    return render(request, 'organizations/people_list.html')
</file>

<file path="label_studio/projects/functions/__init__.py">
from core.feature_flags import flag_set
from core.utils.db import SQCount
from django.db.models import Count, OuterRef, Q
from tasks.models import Annotation, Prediction, Task


def annotate_task_number(queryset):
    tasks = Task.objects.filter(project=OuterRef('id')).values_list('id')
    return queryset.annotate(task_number=SQCount(tasks))


def annotate_finished_task_number(queryset):
    tasks = Task.objects.filter(project=OuterRef('id'), is_labeled=True).values_list('id')
    return queryset.annotate(finished_task_number=SQCount(tasks))


def annotate_total_predictions_number(queryset):
    predictions = Prediction.objects.filter(project=OuterRef('id')).values('id')
    return queryset.annotate(total_predictions_number=SQCount(predictions))


def annotate_total_annotations_number(queryset):
    subquery = Annotation.objects.filter(Q(project=OuterRef('pk')) & Q(was_cancelled=False)).values('id')
    return queryset.annotate(total_annotations_number=SQCount(subquery))


def annotate_num_tasks_with_annotations(queryset):
    # @todo: check do we really need this counter?
    # this function is very slow because of tasks__id and distinct
    subquery = (
        Annotation.objects.filter(
            Q(project=OuterRef('pk')) & Q(ground_truth=False) & Q(was_cancelled=False) & Q(result__isnull=False)
        )
        .values('task__id')
        .distinct()
    )
    return queryset.annotate(num_tasks_with_annotations=SQCount(subquery))


def annotate_useful_annotation_number(queryset):
    subquery = Annotation.objects.filter(
        Q(project=OuterRef('pk')) & Q(was_cancelled=False) & Q(ground_truth=False) & Q(result__isnull=False)
    ).values('id')
    return queryset.annotate(useful_annotation_number=SQCount(subquery))


def annotate_ground_truth_number(queryset):
    subquery = Annotation.objects.filter(Q(project=OuterRef('pk')) & Q(ground_truth=True)).values('id')
    return queryset.annotate(ground_truth_number=SQCount(subquery))


def annotate_skipped_annotations_number(queryset):
    subquery = Annotation.objects.filter(Q(project=OuterRef('pk')) & Q(was_cancelled=True)).values('id')
    return queryset.annotate(skipped_annotations_number=SQCount(subquery))
</file>

<file path="label_studio/projects/functions/next_task.py">
import logging
from collections import Counter
from typing import List, Tuple, Union

from core.feature_flags import flag_set
from core.utils.common import conditional_atomic, db_is_not_sqlite, load_func
from django.conf import settings
from django.db.models import BooleanField, Case, Count, Exists, F, Max, OuterRef, Q, QuerySet, Value, When
from django.db.models.fields import DecimalField
from projects.functions.stream_history import add_stream_history
from projects.models import Project
from tasks.models import Annotation, Task
from users.models import User

logger = logging.getLogger(__name__)

get_tasks_agreement_queryset = load_func(settings.GET_TASKS_AGREEMENT_QUERYSET)


def get_next_task_logging_level(user: User) -> int:
    level = logging.DEBUG
    if flag_set('fflag_fix_back_dev_4185_next_task_additional_logging_long', user=user):
        level = logging.INFO
    return level


def _get_random_unlocked(task_query: QuerySet[Task], user: User, upper_limit=None) -> Union[Task, None]:
    for task in task_query.order_by('?').only('id')[: settings.RANDOM_NEXT_TASK_SAMPLE_SIZE]:
        try:
            task = Task.objects.select_for_update(skip_locked=True).get(pk=task.id)
            if not task.has_lock(user):
                return task
        except Task.DoesNotExist:
            logger.debug('Task with id {} locked'.format(task.id))


def _get_first_unlocked(tasks_query: QuerySet[Task], user) -> Union[Task, None]:
    # Skip tasks that are locked due to being taken by collaborators
    for task_id in tasks_query.values_list('id', flat=True):
        try:
            task = Task.objects.select_for_update(skip_locked=True).get(pk=task_id)
            if not task.has_lock(user):
                return task

        except Task.DoesNotExist:
            logger.debug('Task with id {} locked'.format(task_id))


def _try_ground_truth(tasks: QuerySet[Task], project: Project, user: User) -> Union[Task, None]:
    """Returns task from ground truth set"""
    ground_truth = Annotation.objects.filter(task=OuterRef('pk'), ground_truth=True)
    not_solved_tasks_with_ground_truths = tasks.annotate(has_ground_truths=Exists(ground_truth)).filter(
        has_ground_truths=True
    )
    if not_solved_tasks_with_ground_truths.exists():
        if project.sampling == project.SEQUENCE:
            return _get_first_unlocked(not_solved_tasks_with_ground_truths, user)
        return _get_random_unlocked(not_solved_tasks_with_ground_truths, user)


def _try_tasks_with_overlap(tasks: QuerySet[Task]) -> Tuple[Union[Task, None], QuerySet[Task]]:
    """Filter out tasks without overlap (doesn't return next task)"""
    tasks_with_overlap = tasks.filter(overlap__gt=1)
    if tasks_with_overlap.exists():
        return None, tasks_with_overlap
    else:
        return None, tasks.filter(overlap=1)


def _try_breadth_first(tasks: QuerySet[Task], user: User) -> Union[Task, None]:
    """Try to find tasks with maximum amount of annotations, since we are trying to label tasks as fast as possible"""

    tasks = tasks.annotate(annotations_count=Count('annotations', filter=~Q(annotations__completed_by=user)))
    max_annotations_count = tasks.aggregate(Max('annotations_count'))['annotations_count__max']
    if max_annotations_count == 0:
        # there is no any labeled tasks found
        return

    # find any task with maximal amount of created annotations
    not_solved_tasks_labeling_started = tasks.annotate(
        reach_max_annotations_count=Case(
            When(annotations_count=max_annotations_count, then=Value(True)),
            default=Value(False),
            output_field=BooleanField(),
        )
    )
    not_solved_tasks_labeling_with_max_annotations = not_solved_tasks_labeling_started.filter(
        reach_max_annotations_count=True
    )
    if not_solved_tasks_labeling_with_max_annotations.exists():
        # try to complete tasks that are already in progress
        return _get_random_unlocked(not_solved_tasks_labeling_with_max_annotations, user)


def _try_uncertainty_sampling(
    tasks: QuerySet[Task],
    project: Project,
    user_solved_tasks_array: List[int],
    user: User,
    prepared_tasks: QuerySet[Task],
) -> Union[Task, None]:
    task_with_current_predictions = tasks.filter(predictions__model_version=project.model_version)
    if task_with_current_predictions.exists():
        logger.debug('Use uncertainty sampling')
        # collect all clusters already solved by user, count number of solved task in them
        user_solved_clusters = (
            prepared_tasks.filter(pk__in=user_solved_tasks_array)
            .annotate(cluster=Max('predictions__cluster'))
            .values_list('cluster', flat=True)
        )
        user_solved_clusters = Counter(user_solved_clusters)
        # order each task by the count of how many tasks solved in it's cluster
        cluster_num_solved_map = [When(predictions__cluster=k, then=v) for k, v in user_solved_clusters.items()]

        # WARNING! this call doesn't work after consequent annotate
        num_tasks_with_current_predictions = task_with_current_predictions.count()
        if cluster_num_solved_map:
            task_with_current_predictions = task_with_current_predictions.annotate(
                cluster_num_solved=Case(*cluster_num_solved_map, default=0, output_field=DecimalField())
            )
            # next task is chosen from least solved cluster and with lowest prediction score
            possible_next_tasks = task_with_current_predictions.order_by('cluster_num_solved', 'predictions__score')
        else:
            possible_next_tasks = task_with_current_predictions.order_by('predictions__score')

        num_annotators = project.annotators().count()
        if num_annotators > 1 and num_tasks_with_current_predictions > 0:
            # try to randomize tasks to avoid concurrent labeling between several annotators
            next_task = _get_random_unlocked(
                possible_next_tasks, user, upper_limit=min(num_annotators + 1, num_tasks_with_current_predictions)
            )
        else:
            next_task = _get_first_unlocked(possible_next_tasks, user)
    else:
        # uncertainty sampling fallback: choose by random sampling
        logger.debug(
            f'Uncertainty sampling fallbacks to random sampling '
            f'(current project.model_version={str(project.model_version)})'
        )
        next_task = _get_random_unlocked(tasks, user)
    return next_task


def get_not_solved_tasks_qs(
    user: User, project: Project, prepared_tasks: QuerySet[Task], assigned_flag: Union[bool, None], queue_info: str
) -> Tuple[QuerySet[Task], List[int], str, bool]:
    user_solved_tasks_array = user.annotations.filter(project=project, task__isnull=False)
    user_solved_tasks_array = user_solved_tasks_array.distinct().values_list('task__pk', flat=True)
    not_solved_tasks = prepared_tasks.exclude(pk__in=user_solved_tasks_array)

    # annotation can't have postponed draft, so skip annotation__project filter
    postponed_drafts = user.drafts.filter(task__project=project, was_postponed=True)
    if postponed_drafts.exists():
        user_postponed_tasks = postponed_drafts.distinct().values_list('task__pk', flat=True)
        not_solved_tasks = not_solved_tasks.exclude(pk__in=user_postponed_tasks)

    prioritized_on_agreement = False
    # if annotator is assigned for tasks, he must solve it regardless of is_labeled=True
    if not assigned_flag:
        # include tasks that have been completed if their agreement is not at threshold if threshold setting is set
        lse_project = getattr(project, 'lse_project', None)
        if (
            lse_project
            and flag_set('fflag_feat_optic_161_project_settings_for_low_agreement_threshold_score_short', user='auto')
            and lse_project.agreement_threshold is not None
            and get_tasks_agreement_queryset
            and user.is_project_annotator(project)
        ):
            not_solved_tasks = (
                get_tasks_agreement_queryset(not_solved_tasks)
                # include tasks that are not labeled or are labeled but fall below the agreement threshold
                .filter(
                    Q(_agreement__lt=lse_project.agreement_threshold, is_labeled=True) | Q(is_labeled=False)
                ).annotate(annotators=Count('annotations__completed_by', distinct=True))
                # skip tasks that have been annotated by the maximum additional number of annotators
                .filter(annotators__lt=F('overlap') + lse_project.max_additional_annotators_assignable)
            )
            prioritized_on_agreement, not_solved_tasks = _prioritize_low_agreement_tasks(not_solved_tasks, lse_project)

        # otherwise, filtering out completed tasks is sufficient
        else:
            not_solved_tasks = not_solved_tasks.filter(is_labeled=False)

    if not flag_set('fflag_fix_back_lsdv_4523_show_overlap_first_order_27022023_short'):
        # show tasks with overlap > 1 first (unless tasks are already prioritized on agreement)
        if project.show_overlap_first and not prioritized_on_agreement:
            # don't output anything - just filter tasks with overlap
            logger.debug(f'User={user} tries overlap first from prepared tasks')
            _, not_solved_tasks = _try_tasks_with_overlap(not_solved_tasks)
            queue_info += (' & ' if queue_info else '') + 'Show overlap first'

    return not_solved_tasks, user_solved_tasks_array, queue_info, prioritized_on_agreement


def _prioritize_low_agreement_tasks(tasks, lse_project):
    # if there are any tasks with agreement below the threshold which are labeled, prioritize them over the rest
    # and return all tasks to be considered for sampling in order by least agreement
    prioritized_low_agreement = tasks.filter(_agreement__lt=lse_project.agreement_threshold, is_labeled=True)

    if prioritized_low_agreement.exists():
        return True, tasks.order_by('-is_labeled', '_agreement')

    return False, tasks


def get_next_task_without_dm_queue(
    user: User,
    project: Project,
    not_solved_tasks: QuerySet,
    assigned_flag: Union[bool, None],
    prioritized_low_agreement: bool,
) -> Tuple[Union[Task, None], bool, str]:
    next_task = None
    use_task_lock = True
    queue_info = ''

    # ordered by data manager
    if assigned_flag:
        logger.debug(f'User={user} try to get task from assigned')
        next_task = not_solved_tasks.first()
        use_task_lock = False
        queue_info += (' & ' if queue_info else '') + 'Manually assigned queue'

    # If current user has already lock one task - return it (without setting the lock again)
    if not next_task:
        next_task = Task.get_locked_by(user, tasks=not_solved_tasks)
        if next_task:
            logger.debug(f'User={user} got already locked for them {next_task}')
            use_task_lock = False
            queue_info += (' & ' if queue_info else '') + 'Task lock'

    if not next_task and prioritized_low_agreement:
        logger.debug(f'User={user} tries low agreement from prepared tasks')
        next_task = _get_first_unlocked(not_solved_tasks, user)
        queue_info += (' & ' if queue_info else '') + 'Low agreement queue'

    if not next_task and project.show_ground_truth_first:
        logger.debug(f'User={user} tries ground truth from prepared tasks')
        next_task = _try_ground_truth(not_solved_tasks, project, user)
        queue_info += (' & ' if queue_info else '') + 'Ground truth queue'

    if not next_task and project.maximum_annotations > 1:
        # if there are any tasks in progress (with maximum number of annotations), randomly sampling from them
        logger.debug(f'User={user} tries depth first from prepared tasks')
        next_task = _try_breadth_first(not_solved_tasks, user)
        if next_task:
            queue_info += (' & ' if queue_info else '') + 'Breadth first queue'

    return next_task, use_task_lock, queue_info


def skipped_queue(next_task, prepared_tasks, project, user, queue_info):
    if not next_task and project.skip_queue == project.SkipQueue.REQUEUE_FOR_ME:
        q = Q(project=project, task__isnull=False, was_cancelled=True, task__is_labeled=False)
        skipped_tasks = user.annotations.filter(q).order_by('updated_at').values_list('task__pk', flat=True)
        if skipped_tasks.exists():
            preserved_order = Case(*[When(pk=pk, then=pos) for pos, pk in enumerate(skipped_tasks)])
            skipped_tasks = prepared_tasks.filter(pk__in=skipped_tasks).order_by(preserved_order)
            next_task = _get_first_unlocked(skipped_tasks, user)
            queue_info = 'Skipped queue'

    return next_task, queue_info


def postponed_queue(next_task, prepared_tasks, project, user, queue_info):
    if not next_task:
        q = Q(task__project=project, task__isnull=False, was_postponed=True, task__is_labeled=False)
        postponed_tasks = user.drafts.filter(q).order_by('updated_at').values_list('task__pk', flat=True)
        if postponed_tasks.exists():
            preserved_order = Case(*[When(pk=pk, then=pos) for pos, pk in enumerate(postponed_tasks)])
            postponed_tasks = prepared_tasks.filter(pk__in=postponed_tasks).order_by(preserved_order)
            next_task = _get_first_unlocked(postponed_tasks, user)
            if next_task is not None:
                next_task.allow_postpone = False
            queue_info = 'Postponed draft queue'

    return next_task, queue_info


def get_task_from_qs_with_sampling(
    not_solved_tasks: QuerySet[Task],
    user_solved_tasks_array: List[int],
    prepared_tasks: QuerySet,
    user: User,
    project: Project,
    queue_info: str,
) -> Tuple[Union[Task, None], str]:
    next_task = None
    if project.sampling == project.SEQUENCE:
        logger.debug(f'User={user} tries sequence sampling from prepared tasks')
        next_task = _get_first_unlocked(not_solved_tasks, user)
        if next_task:
            queue_info += (' & ' if queue_info else '') + 'Sequence queue'

    elif project.sampling == project.UNCERTAINTY:
        logger.debug(f'User={user} tries uncertainty sampling from prepared tasks')
        next_task = _try_uncertainty_sampling(not_solved_tasks, project, user_solved_tasks_array, user, prepared_tasks)
        if next_task:
            queue_info += (' & ' if queue_info else '') + 'Active learning or random queue'

    elif project.sampling == project.UNIFORM:
        logger.debug(f'User={user} tries random sampling from prepared tasks')
        next_task = _get_random_unlocked(not_solved_tasks, user)
        if next_task:
            queue_info += (' & ' if queue_info else '') + 'Uniform random queue'

    return next_task, queue_info


def get_next_task(
    user: User,
    prepared_tasks: QuerySet,
    project: Project,
    dm_queue: Union[bool, None],
    assigned_flag: Union[bool, None] = None,
) -> Tuple[Union[Task, None], str]:
    logger.debug(f'get_next_task called. user: {user}, project: {project}, dm_queue: {dm_queue}')

    with conditional_atomic(predicate=db_is_not_sqlite):
        next_task = None
        use_task_lock = True
        queue_info = ''

        not_solved_tasks, user_solved_tasks_array, queue_info, prioritized_low_agreement = get_not_solved_tasks_qs(
            user, project, prepared_tasks, assigned_flag, queue_info
        )

        if not dm_queue:
            next_task, use_task_lock, queue_info = get_next_task_without_dm_queue(
                user, project, not_solved_tasks, assigned_flag, prioritized_low_agreement
            )

        if flag_set('fflag_fix_back_lsdv_4523_show_overlap_first_order_27022023_short'):
            # show tasks with overlap > 1 first
            if not next_task and project.show_overlap_first:
                # don't output anything - just filter tasks with overlap
                logger.debug(f'User={user} tries overlap first from prepared tasks')
                _, tasks_with_overlap = _try_tasks_with_overlap(not_solved_tasks)
                queue_info += 'Show overlap first'
                next_task, queue_info = get_task_from_qs_with_sampling(
                    tasks_with_overlap, user_solved_tasks_array, prepared_tasks, user, project, queue_info
                )

        if not next_task:
            if dm_queue:
                queue_info += (' & ' if queue_info else '') + 'Data manager queue'
                logger.debug(f'User={user} tries sequence sampling from prepared tasks')
                next_task = not_solved_tasks.first()

            else:
                next_task, queue_info = get_task_from_qs_with_sampling(
                    not_solved_tasks, user_solved_tasks_array, prepared_tasks, user, project, queue_info
                )

        next_task, queue_info = postponed_queue(next_task, prepared_tasks, project, user, queue_info)

        next_task, queue_info = skipped_queue(next_task, prepared_tasks, project, user, queue_info)

        if next_task and use_task_lock:
            # set lock for the task with TTL 3x time more then current average lead time (or 1 hour by default)
            next_task.set_lock(user)

        logger.log(
            get_next_task_logging_level(user),
            f'get_next_task finished. next_task: {next_task}, queue_info: {queue_info}',
        )

        # debug for critical overlap issue
        if next_task and flag_set('fflag_fix_back_dev_4185_next_task_additional_logging_long', user):
            try:
                count = next_task.annotations.filter(was_cancelled=False).count()
                task_overlap_reached = count >= next_task.overlap
                global_overlap_reached = count >= project.maximum_annotations
                locks = next_task.locks.count() > project.maximum_annotations - next_task.annotations.count()
                if next_task.is_labeled or task_overlap_reached or global_overlap_reached or locks:
                    from tasks.serializers import TaskSimpleSerializer

                    local = dict(locals())
                    local.pop('prepared_tasks', None)
                    local.pop('user_solved_tasks_array', None)
                    local.pop('not_solved_tasks', None)

                    task = TaskSimpleSerializer(next_task).data
                    task.pop('data', None)
                    task.pop('predictions', None)
                    for i, a in enumerate(task['annotations']):
                        task['annotations'][i] = dict(task['annotations'][i])
                        task['annotations'][i].pop('result', None)

                    project = next_task.project
                    project_data = {
                        'maximum_annotations': project.maximum_annotations,
                        'skip_queue': project.skip_queue,
                        'sampling': project.sampling,
                        'show_ground_truth_first': project.show_ground_truth_first,
                        'show_overlap_first': project.show_overlap_first,
                        'overlap_cohort_percentage': project.overlap_cohort_percentage,
                        'project_id': project.id,
                        'title': project.title,
                    }
                    logger.info(
                        f'DEBUG INFO: get_next_task is_labeled/overlap: '
                        f'LOCALS ==> {local} :: PROJECT ==> {project_data} :: '
                        f'NEXT_TASK ==> {task}'
                    )
            except Exception as e:
                logger.error(f'get_next_task is_labeled/overlap try/except: {str(e)}')
                pass

        add_stream_history(next_task, user, project)
        return next_task, queue_info
</file>

<file path="label_studio/projects/functions/stream_history.py">
from django.conf import settings
from django.db import transaction
from projects.models import LabelStreamHistory
from tasks.models import Annotation, Task

TASK_ID_KEY = 'taskId'
ANNOTATION_ID_KEY = 'annotationId'


def add_stream_history(next_task, user, project):
    if next_task is not None:
        with transaction.atomic():
            history, created = LabelStreamHistory.objects.get_or_create(user=user, project=project)
            new_history_data = {TASK_ID_KEY: next_task.id, ANNOTATION_ID_KEY: None}
            if created:
                history.data = [new_history_data]
            else:
                task_ids = set([h[TASK_ID_KEY] for h in history.data])
                if next_task.id not in task_ids:
                    history.data.append(new_history_data)
                if len(task_ids) + 1 > settings.LABEL_STREAM_HISTORY_LIMIT:
                    history.data = history.data[-settings.LABEL_STREAM_HISTORY_LIMIT :]
            history.save()


def fill_history_annotation(user, task, annotation):
    history = user.histories.filter(project=task.project).first()
    if history and history.data:
        for item in history.data:
            if item[TASK_ID_KEY] == task.id:
                item[ANNOTATION_ID_KEY] = annotation.id
        history.save()


def get_label_stream_history(user, project):
    result = []

    with transaction.atomic():
        history = user.histories.filter(project=project).first()
        if history is None:
            return result

        data = history.data

        task_ids = set([h[TASK_ID_KEY] for h in history.data])
        annotation_ids = set([h[ANNOTATION_ID_KEY] for h in history.data])
        existing_task_ids = set(Task.objects.filter(pk__in=task_ids).values_list('id', flat=True))
        existing_annotation_ids = set(Annotation.objects.filter(pk__in=annotation_ids).values_list('id', flat=True))

        result = []
        for item in data:
            if item[TASK_ID_KEY] not in existing_task_ids:
                continue
            if item[ANNOTATION_ID_KEY] not in existing_annotation_ids:
                item[ANNOTATION_ID_KEY] = None
            result.append(item)
        history.data = result
        history.save(update_fields=['data'])

    return result
</file>

<file path="label_studio/projects/functions/utils.py">
from logging import getLogger
from typing import TYPE_CHECKING

from tasks.models import AnnotationDraft, Task

logger = getLogger(__name__)


if TYPE_CHECKING:
    from projects.models import Project, ProjectSummary


def make_queryset_from_iterable(tasks_list):
    """
    Make queryset from list/set of int/Tasks
    :param tasks_list: Iterable of Tasks or IDs
    :return: Tasks queryset
    """
    if isinstance(tasks_list, set):
        tasks_list = list(tasks_list)
    # Make query set from list of IDs
    if isinstance(tasks_list, list) and len(tasks_list) > 0:
        # Extract task IDs from Tasks list
        if isinstance(tasks_list[0], Task):
            tasks_list = [task.id for task in tasks_list]
        queryset = Task.objects.filter(id__in=tasks_list)
    else:
        ids = []
        for task in tasks_list:
            if isinstance(task, Task):
                ids.append(task.id)
            elif isinstance(task, int):
                ids.append(task)
            else:
                raise ValueError(f'Unknown object type: {str(task)}')
        queryset = Task.objects.filter(id__in=ids)
    return queryset


def recalculate_created_annotations_and_labels_from_scratch(
    project: 'Project', summary: 'ProjectSummary', organization_id: int
) -> None:
    """Recalculate from scratch:
     task columns
     created_labels
     created_annotations
     created_labels_drafts

    :param project: Project
    :param summary: ProjectSummary
    :param organization_id: Organization.id, it is required for django-rq displaying on admin page
    """
    logger.info(f'Reset cache started for project {project.id} and organization {organization_id}')
    logger.info(f'recalculate_created_annotations_and_labels_from_scratch project_id={project.id}')
    summary.all_data_columns = {}
    summary.common_data_columns = []
    summary.update_data_columns(project.tasks.only('data'))

    summary.created_labels, summary.created_annotations = {}, {}
    summary.update_created_annotations_and_labels(project.annotations.all())

    summary.created_labels_drafts = {}
    drafts = AnnotationDraft.objects.filter(task__project=project)
    summary.update_created_labels_drafts(drafts)

    logger.info(
        f'Reset cache finished for project {project.id} and organization {organization_id}:\n'
        f'created_annotations = {summary.created_annotations}\n'
        f'created_labels = {summary.created_labels}\n'
        f'created_labels_drafts = {summary.created_labels_drafts}'
    )
</file>

<file path="label_studio/projects/migrations/__init__.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
</file>

<file path="label_studio/projects/migrations/0001_squashed_0065_auto_20210223_2014.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
# Generated by Django 3.1.4 on 2021-03-03 07:37

import annoying.fields
import core.utils.common
from django.conf import settings
import django.core.validators
from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    replaces = [('projects', '0001_initial'), ('projects', '0002_auto_20190318_2010'), ('projects', '0003_auto_20190318_2011'), ('projects', '0004_auto_20190318_2012'), ('projects', '0005_auto_20190318_2014'), ('projects', '0006_auto_20190321_1457'), ('projects', '0007_auto_20190326_2059'), ('projects', '0008_auto_20190328_2028'), ('projects', '0009_auto_20190403_1517'), ('projects', '0010_mlbackendscheduledjob'), ('projects', '0011_auto_20190418_2056'), ('projects', '0012_auto_20190423_1445'), ('projects', '0012_auto_20190422_1253'), ('projects', '0013_merge_20190424_0748'), ('projects', '0014_auto_20190424_0748'), ('projects', '0015_auto_20190506_1532'), ('projects', '0016_projecttemplate_ml_backend'), ('projects', '0017_auto_20190507_1126'), ('projects', '0018_auto_20190507_1633'), ('projects', '0019_auto_20190507_2010'), ('projects', '0020_auto_20190508_1850'), ('projects', '0021_auto_20190509_2057'), ('projects', '0022_auto_20190510_1324'), ('projects', '0023_project_show_skip_button'), ('projects', '0024_project_enable_empty_completion'), ('projects', '0025_project_cluster_annotation'), ('projects', '0026_auto_20190624_1254'), ('projects', '0029_auto_20190702_0959'), ('projects', '0030_mlbackendtrainjob'), ('projects', '0031_project_show_completion_history'), ('projects', '0032_auto_20190708_1648'), ('projects', '0033_auto_20190712_0849'), ('projects', '0034_auto_20190726_1928'), ('projects', '0035_auto_20190805_1955'), ('projects', '0036_auto_20190805_2058'), ('projects', '0037_remove_project_active_learning_enabled'), ('projects', '0038_auto_20190921_0830'), ('projects', '0039_auto_20190923_1323'), ('projects', '0040_auto_20190930_0909'), ('projects', '0041_project_use_kappa'), ('projects', '0042_auto_20191114_1430'), ('projects', '0043_auto_20191126_1145'), ('projects', '0044_auto_20191206_1641'), ('projects', '0045_auto_20200302_1221'), ('projects', '0046_project_agreement_method'), ('projects', '0047_auto_20200313_1925'), ('projects', '0048_auto_20200316_1400'), ('projects', '0049_project_control_weights'), ('projects', '0050_auto_20200417_1019'), ('projects', '0051_project_team'), ('projects', '0052_projecttemplate_organization'), ('projects', '0053_auto_20200504_1324'), ('projects', '0054_project_result_count'), ('projects', '0055_storage'), ('projects', '0056_set_project_skip_onboarding'), ('projects', '0057_auto_20201015_1553'), ('projects', '0058_project_organization'), ('projects', '0059_remove_project_team'), ('projects', '0058_remove_projecttemplate_business'), ('projects', '0059_auto_20210122_1542'), ('projects', '0060_merge_20210126_1328'), ('projects', '0061_delete_storage'), ('projects', '0062_auto_20210217_2135'), ('projects', '0063_auto_20210222_1246'), ('projects', '0064_auto_20210223_0726'), ('projects', '0065_auto_20210223_2014')]

    initial = True

    dependencies = [
        ('organizations', '0001_squashed_0008_auto_20201005_1552'),
        # ('tasks', '0038_delete_storagelink'),
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
        # ('organizations', '0004_auto_20200501_1751'),
    ]

    operations = [
        migrations.CreateModel(
            name='MLBackend',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('url', models.TextField(blank=True, default='http://localhost:8999', null=True, verbose_name='url')),
                ('name', models.TextField(blank=True, default='default', null=True, verbose_name='name')),
                ('title', models.TextField(blank=True, default='Default ML backend', null=True, verbose_name='title')),
                ('type', models.CharField(choices=[('IN', 'Internal'), ('EX', 'External')], default='IN', max_length=100, null=True, verbose_name='type')),
                ('description', models.TextField(blank=True, default='', null=True, verbose_name='description')),
                ('created_at', models.DateTimeField(auto_now_add=True, verbose_name='created at')),
                ('updated_at', models.DateTimeField(auto_now=True, verbose_name='updated at')),
                ('created_by', models.ForeignKey(null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='custom_models', to=settings.AUTH_USER_MODEL, verbose_name='custom models')),
            ],
        ),
        migrations.CreateModel(
            name='Project',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('title', models.CharField(max_length=1000, verbose_name='name')),
                ('label_config', models.TextField(blank=True, null=True, verbose_name='label config')),
                ('expert_instruction', models.TextField(blank=True, default='', null=True, verbose_name='expert instruction')),
                ('show_instruction', models.BooleanField(default=False, verbose_name='show instruction')),
                ('skip_onboarding', models.BooleanField(default=False)),
                ('active_learning_enabled', models.BooleanField(default=True, verbose_name='active learning enabled')),
                ('maximum_completions', models.IntegerField(default=1, verbose_name='maximum completion number')),
                ('model_version', models.TextField(blank=True, default='', null=True, verbose_name='model version')),
                ('data_types', models.JSONField(default=dict, null=True, verbose_name='data_types')),
                ('has_finished', models.BooleanField(default=False, verbose_name='has finished')),
                ('is_published', models.BooleanField(default=False, verbose_name='published')),
                ('created_at', models.DateTimeField(auto_now_add=True, verbose_name='created at')),
                ('updated_at', models.DateTimeField(auto_now=True, verbose_name='updated at')),
                ('created_by', models.ForeignKey(null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='created_projects', to=settings.AUTH_USER_MODEL, verbose_name='created by')),
                ('ml_backend', models.ForeignKey(null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='projects', to='projects.mlbackend')),
            ],
            options={
                'db_table': 'project',
            },
        ),
        migrations.CreateModel(
            name='ProjectOnboardingSteps',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('code', models.CharField(choices=[('DU', 'Upload your data'), ('CF', 'Configure settings'), ('PB', 'Publish project'), ('IE', 'Invite annotators')], max_length=2, null=True)),
                ('title', models.CharField(max_length=1000, verbose_name='title')),
                ('description', models.TextField(verbose_name='descrition')),
                ('order', models.IntegerField(default=0)),
                ('created_at', models.DateTimeField(auto_now_add=True, verbose_name='created at')),
                ('updated_at', models.DateTimeField(auto_now=True, verbose_name='updated at')),
            ],
            options={
                'ordering': ['order'],
            },
        ),
        migrations.CreateModel(
            name='ProjectStats',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('data', models.JSONField(default=dict)),
                ('model_version', models.TextField(blank=True, default='', null=True, verbose_name='model version')),
                ('created_at', models.DateTimeField(auto_now_add=True, verbose_name='created at')),
                ('updated_at', models.DateTimeField(auto_now=True, verbose_name='updated at')),
                ('project', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='stats', to='projects.project')),
            ],
        ),
        migrations.CreateModel(
            name='ProjectTemplate',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('title', models.CharField(max_length=1000, verbose_name='title')),
                ('description', models.TextField(verbose_name='descrition')),
                ('cover_image_url', models.CharField(max_length=1000, verbose_name='cover image')),
                ('input_example', models.TextField(verbose_name='input example')),
                ('input_example_json', models.JSONField(default=list, verbose_name='input example json')),
                ('output_example', models.TextField(verbose_name='output example')),
                ('output_example_json', models.JSONField(default=list, verbose_name='output example json')),
                ('label_config', models.TextField(verbose_name='label config')),
                ('expert_instruction', models.TextField(default='', verbose_name='expert instruction')),
                ('tags', models.JSONField(default=list, verbose_name='tags')),
                ('task_data', models.JSONField(default=list, verbose_name='task data')),
                ('is_published', models.BooleanField(default=True, verbose_name='published')),
                ('created_at', models.DateTimeField(auto_now_add=True, verbose_name='created at')),
                ('updated_at', models.DateTimeField(auto_now=True, verbose_name='updated at')),
            ],
        ),
        migrations.CreateModel(
            name='UploadedDataFile',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('file', models.FileField(upload_to='uploads/%Y/%m/%d')),
            ],
        ),
        migrations.CreateModel(
            name='ProjectOnboarding',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('finished', models.BooleanField(default=False)),
                ('created_at', models.DateTimeField(auto_now_add=True, verbose_name='created at')),
                ('updated_at', models.DateTimeField(auto_now=True, verbose_name='updated at')),
                ('project', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='projects.project')),
                ('step', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='po_through', to='projects.projectonboardingsteps')),
            ],
        ),
        migrations.AddField(
            model_name='project',
            name='onboarding_steps',
            field=models.ManyToManyField(related_name='projects', through='projects.ProjectOnboarding', to='projects.ProjectOnboardingSteps'),
        ),
        migrations.AddField(
            model_name='project',
            name='template_used',
            field=models.ForeignKey(null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='projects', to='projects.projecttemplate', verbose_name='projects'),
        ),
        migrations.AddField(
            model_name='projectstats',
            name='num_completions',
            field=models.IntegerField(default=0, verbose_name='num_completions'),
        ),
        migrations.AddField(
            model_name='projectstats',
            name='num_honeypots',
            field=models.IntegerField(default=0, verbose_name='num_honeypots'),
        ),
        migrations.AddField(
            model_name='projectstats',
            name='num_completions_fit_predictions',
            field=models.FloatField(default=0.0, verbose_name='num_completions_fit_predictions'),
        ),
        migrations.AddField(
            model_name='projectstats',
            name='num_honeypots_fit_predictions',
            field=models.FloatField(default=0.0, verbose_name='num_honeypots_fit_predictions'),
        ),
        migrations.RemoveField(
            model_name='project',
            name='ml_backend',
        ),
        migrations.CreateModel(
            name='MLBackendConnection',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('schema', models.JSONField(default=dict)),
                ('created_at', models.DateTimeField(auto_now_add=True, verbose_name='created at')),
                ('ml_backend', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='project_connections', to='projects.mlbackend')),
                ('project', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='ml_backend_connections', to='projects.project')),
            ],
        ),
        migrations.AddField(
            model_name='project',
            name='ml_backend_active_connection',
            field=models.OneToOneField(help_text='The connection ID that identifies current ML backend.', null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='active_project', to='projects.mlbackendconnection'),
        ),
        migrations.AddField(
            model_name='project',
            name='batch_size',
            field=models.IntegerField(default=100, help_text='Batch size for machine learning', verbose_name='batch_size'),
        ),
        migrations.AddField(
            model_name='project',
            name='interval',
            field=models.IntegerField(default=100, help_text='Interval of calculations for MLBackend', verbose_name='interval'),
        ),
        migrations.CreateModel(
            name='MLBackendScheduledJob',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('job_id', models.CharField(max_length=128)),
                ('created_on', models.DateTimeField(auto_now_add=True)),
                ('project', models.OneToOneField(null=True, on_delete=django.db.models.deletion.CASCADE, related_name='scheduled_job', to='projects.project')),
            ],
        ),
        migrations.AlterField(
            model_name='project',
            name='active_learning_enabled',
            field=models.BooleanField(default=False, verbose_name='active learning enabled'),
        ),
        migrations.AlterField(
            model_name='project',
            name='active_learning_enabled',
            field=models.BooleanField(default=False, verbose_name='active learning enabled'),
        ),
        migrations.AlterField(
            model_name='mlbackend',
            name='created_by',
            field=models.ForeignKey(help_text='User ID who creates this model', null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='custom_models', to=settings.AUTH_USER_MODEL, verbose_name='custom models'),
        ),
        migrations.AlterField(
            model_name='mlbackend',
            name='description',
            field=models.TextField(blank=True, default='', help_text='Model description', null=True, verbose_name='description'),
        ),
        migrations.AlterField(
            model_name='mlbackend',
            name='name',
            field=models.TextField(blank=True, default='default', help_text='Model name', null=True, verbose_name='name'),
        ),
        migrations.AlterField(
            model_name='mlbackend',
            name='title',
            field=models.TextField(blank=True, default='Default ML backend', help_text='Model title', null=True, verbose_name='title'),
        ),
        migrations.AlterField(
            model_name='mlbackend',
            name='type',
            field=models.CharField(choices=[('IN', 'Internal'), ('EX', 'External')], default='IN', help_text='Backend type', max_length=100, null=True, verbose_name='type'),
        ),
        migrations.AlterField(
            model_name='mlbackend',
            name='url',
            field=models.TextField(blank=True, default='http://localhost:8999', help_text='Model server URL', null=True, verbose_name='url'),
        ),
        migrations.AlterField(
            model_name='project',
            name='active_learning_enabled',
            field=models.BooleanField(default=False, help_text='Enable machine learning backend and active learning strategy for the task selection', verbose_name='active learning enabled'),
        ),
        migrations.AlterField(
            model_name='project',
            name='expert_instruction',
            field=models.TextField(blank=True, default='', help_text='Expert instruction in HTML format', null=True, verbose_name='expert instruction'),
        ),
        migrations.AlterField(
            model_name='project',
            name='is_published',
            field=models.BooleanField(default=False, help_text='Project publishing flag for experts', verbose_name='published'),
        ),
        migrations.AlterField(
            model_name='project',
            name='label_config',
            field=models.TextField(blank=True, help_text='Label config in XML format. More about it in <a href="https://labelstud.io/guide/setup.html">documentation</a>', null=True, verbose_name='label config'),
        ),
        migrations.AlterField(
            model_name='project',
            name='maximum_completions',
            field=models.IntegerField(default=1, help_text='Maximum overlaps of expert completions for one task. If the completion number per task is equal or greater to this value, the task becomes finished (is_labeled=True)', verbose_name='maximum completion number'),
        ),
        migrations.AlterField(
            model_name='project',
            name='model_version',
            field=models.TextField(blank=True, default='', help_text='Machine learning model version', null=True, verbose_name='model version'),
        ),
        migrations.AlterField(
            model_name='project',
            name='show_instruction',
            field=models.BooleanField(default=False, help_text='Show the instruction to the expert before he starts', verbose_name='show instruction'),
        ),
        migrations.AlterField(
            model_name='project',
            name='skip_onboarding',
            field=models.BooleanField(default=False, help_text='Skip onboarding steps'),
        ),
        migrations.AlterField(
            model_name='project',
            name='title',
            field=models.CharField(help_text='Project name', max_length=1000, verbose_name='name'),
        ),
        migrations.AddField(
            model_name='projecttemplate',
            name='ml_backend',
            field=models.ForeignKey(null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='project_templates', to='projects.mlbackend'),
        ),
        migrations.AlterField(
            model_name='projectonboardingsteps',
            name='code',
            field=models.CharField(choices=[('DU', 'Upload your data'), ('CF', 'Configure settings'), ('PB', 'Publish project'), ('IE', 'Invite collaborators')], max_length=2, null=True),
        ),
        migrations.AlterField(
            model_name='project',
            name='title',
            field=models.CharField(help_text='Project name, length is from 3 to 50 chars', max_length=50, validators=[django.core.validators.MinLengthValidator(3), django.core.validators.MaxLengthValidator(50)], verbose_name='name'),
        ),
        migrations.AddField(
            model_name='project',
            name='show_skip_button',
            field=models.BooleanField(default=True, help_text='Show a skip button in interface and allow collaborators to skip the task', verbose_name='show skip button'),
        ),
        migrations.AddField(
            model_name='project',
            name='enable_empty_completion',
            field=models.BooleanField(default=True, help_text='Allow submit empty completions', verbose_name='enable empty completion'),
        ),
        migrations.AddField(
            model_name='project',
            name='cluster_annotation',
            field=models.BooleanField(default=False, help_text='If enabled, completions for all tasks in one cluster are propagated', verbose_name='cluster annotation'),
        ),
        migrations.AlterField(
            model_name='projectonboardingsteps',
            name='description',
            field=models.TextField(verbose_name='description'),
        ),
        migrations.AlterUniqueTogether(
            name='project',
            unique_together={('title', 'created_by')},
        ),
        migrations.AddField(
            model_name='project',
            name='min_completions_to_start_training',
            field=models.IntegerField(default=10, help_text='Minimum number of completed tasks after which training is started', verbose_name='min_completions_to_start_training'),
        ),
        migrations.AlterField(
            model_name='project',
            name='batch_size',
            field=models.IntegerField(default=1, help_text='How many predictions are sent to ML backend in one request', verbose_name='batch_size'),
        ),
        migrations.RemoveField(
            model_name='project',
            name='interval',
        ),
        migrations.AddField(
            model_name='project',
            name='show_completion_history',
            field=models.BooleanField(default=False, help_text='Show completion history to collaborator', verbose_name='show completion history'),
        ),
        migrations.CreateModel(
            name='MLBackendTrainJob',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('job_id', models.CharField(max_length=128)),
                ('created_at', models.DateTimeField(auto_now_add=True, verbose_name='created at')),
                ('updated_at', models.DateTimeField(auto_now=True, verbose_name='updated at')),
                ('project', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='ml_backend_train_jobs', to='projects.project')),
                ('completed_tasks_count', models.IntegerField(default=0, help_text='Number of completed tasks at the moment when this job is created', verbose_name='completed_tasks_count')),
                ('is_finished', models.BooleanField(default=False, help_text='Whether this job is finished.', verbose_name='is_finished')),
                ('model_version', models.TextField(blank=True, default='', help_text='Model version returned when job is finished', null=True, verbose_name='model version')),
            ],
        ),
        migrations.AlterField(
            model_name='project',
            name='batch_size',
            field=models.IntegerField(default=100, help_text='How many predictions are sent to ML backend in one request', verbose_name='batch_size'),
        ),
        migrations.AddField(
            model_name='project',
            name='show_collab_predictions',
            field=models.BooleanField(default=True, help_text='If set collaborator gets model predictions', verbose_name='show predictions to collaborator'),
        ),
        migrations.AlterField(
            model_name='project',
            name='label_config',
            field=models.TextField(blank=True, help_text='Label config in XML format. See more about it in documentation', null=True, verbose_name='label config'),
        ),
        migrations.RemoveField(
            model_name='project',
            name='active_learning_enabled',
        ),
        migrations.AddField(
            model_name='project',
            name='show_ground_truths_first',
            field=models.BooleanField(default=True, verbose_name='show ground truths first'),
        ),
        migrations.AddField(
            model_name='project',
            name='sampling',
            field=models.CharField(choices=[('Sequential sampling', 'Tasks are ordered by their IDs'), ('Uniform sampling', 'Tasks are chosen randomly'), ('Uncertainty sampling', 'Tasks are chosen according to model uncertainty scores (active learning mode)')], default='Uniform sampling', max_length=100, null=True),
        ),
        migrations.AddField(
            model_name='project',
            name='task_data_login',
            field=models.CharField(blank=True, help_text='Task data credentials: login', max_length=100, null=True, verbose_name='task_data_login'),
        ),
        migrations.AddField(
            model_name='project',
            name='task_data_password',
            field=models.CharField(blank=True, help_text='Task data credentials: password', max_length=100, null=True, verbose_name='task_data_password'),
        ),
        migrations.AddField(
            model_name='project',
            name='use_kappa',
            field=models.BooleanField(default=False, help_text="If categorical variables are used in labeling (e.g. choices), Cohen's Kappa statistic is computed to measure inter-rater reliability instead of basic agreement"),
        ),
        migrations.AddField(
            model_name='project',
            name='metric_name',
            field=models.TextField(blank=True, default='', help_text='Evaluation metric chosen for this project', null=True, verbose_name='metric_name'),
        ),
        migrations.AddField(
            model_name='project',
            name='metric_params',
            field=models.JSONField(blank=True, default=dict, null=True, verbose_name='metric params'),
        ),
        migrations.AlterField(
            model_name='project',
            name='ml_backend_active_connection',
            field=models.OneToOneField(blank=True, help_text='The connection ID that identifies current ML backend.', null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='active_project', to='projects.mlbackendconnection'),
        ),
        migrations.AlterField(
            model_name='project',
            name='template_used',
            field=models.ForeignKey(blank=True, null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='projects', to='projects.projecttemplate', verbose_name='Project templates'),
        ),
        migrations.AlterField(
            model_name='projectonboardingsteps',
            name='code',
            field=models.CharField(choices=[('DU', 'Import your data'), ('CF', 'Configure settings'), ('PB', 'Publish project'), ('IE', 'Invite collaborators')], max_length=2, null=True),
        ),
        migrations.AddField(
            model_name='project',
            name='metric_threshold',
            field=models.FloatField(default=0.5, help_text='Threshold imposed on dist between two completions: if dist>threshold, they are considered as equal', verbose_name='metric_threshold'),
        ),
        migrations.AddField(
            model_name='project',
            name='overlap_cohort_percentage',
            field=models.IntegerField(default=100, verbose_name='overlap_cohort_percentage'),
        ),
        migrations.AddField(
            model_name='project',
            name='show_overlap_first',
            field=models.BooleanField(default=True, verbose_name='show overlap first'),
        ),
        migrations.AddField(
            model_name='project',
            name='agreement_threshold',
            field=models.FloatField(default=0.0, help_text='Minimal agreement score to consider tasks labeled for sending to ML backend (inclusively)', verbose_name='agreement_threshold'),
        ),
        migrations.AddField(
            model_name='project',
            name='agreement_method',
            field=models.CharField(choices=[('Single linkage', 'Threshold based completions grouping using single linkage method'), ('Complete linkage', 'Threshold based completions grouping using complete linkage method'), ('No grouping', 'Compute agreement without grouping (just averaging the distances between labeling results)')], default='Single linkage', max_length=100, null=True),
        ),
        migrations.AddField(
            model_name='projecttemplate',
            name='created_by',
            field=models.ForeignKey(null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='project_templates', to=settings.AUTH_USER_MODEL, verbose_name='created by'),
        ),
        migrations.AddField(
            model_name='projecttemplate',
            name='is_private',
            field=models.BooleanField(default=True, help_text='If template is private, it is accessible only from private team', verbose_name='private'),
        ),
        migrations.AddField(
            model_name='projecttemplate',
            name='project_settings',
            field=models.JSONField(default=dict, help_text='general dict serialized project settings', verbose_name='project settings'),
        ),
        migrations.AlterField(
            model_name='projecttemplate',
            name='cover_image_url',
            field=models.CharField(blank=True, default='', max_length=1000, null=True, verbose_name='cover image'),
        ),
        migrations.AlterField(
            model_name='projecttemplate',
            name='description',
            field=models.TextField(default='', null=True, verbose_name='description'),
        ),
        migrations.AlterField(
            model_name='projecttemplate',
            name='input_example',
            field=models.TextField(blank=True, verbose_name='input example'),
        ),
        migrations.AlterField(
            model_name='projecttemplate',
            name='output_example',
            field=models.TextField(blank=True, verbose_name='output example'),
        ),
        migrations.AddField(
            model_name='project',
            name='control_weights',
            field=models.JSONField(default=dict, help_text='Weights for control tags', null=True, verbose_name='control weights'),
        ),
        migrations.CreateModel(
            name='MLBackendPredictionJob',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('job_id', models.CharField(max_length=128)),
                ('created_at', models.DateTimeField(auto_now_add=True, verbose_name='created at')),
                ('updated_at', models.DateTimeField(auto_now=True, verbose_name='updated at')),
                ('project', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='ml_backend_prediction_jobs', to='projects.project')),
            ],
        ),
        migrations.DeleteModel(
            name='MLBackendScheduledJob',
        ),
        migrations.AddField(
            model_name='projecttemplate',
            name='organization',
            field=models.ForeignKey(null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='project_templates', to='organizations.organization'),
        ),
        migrations.CreateModel(
            name='ProjectMember',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('enabled', models.BooleanField(default=True, help_text='Project member is enabled')),
                ('created_at', models.DateTimeField(auto_now_add=True, verbose_name='created at')),
                ('updated_at', models.DateTimeField(auto_now=True, verbose_name='updated at')),
            ],
        ),
        migrations.AddField(
            model_name='project',
            name='token',
            field=models.CharField(blank=True, default=core.utils.common.create_hash, max_length=256, null=True, verbose_name='token'),
        ),
        migrations.AddField(
            model_name='projectmember',
            name='project',
            field=models.ForeignKey(help_text='Project ID', on_delete=django.db.models.deletion.CASCADE, related_name='members', to='projects.project'),
        ),
        migrations.AddField(
            model_name='projectmember',
            name='user',
            field=models.ForeignKey(help_text='User ID', on_delete=django.db.models.deletion.CASCADE, related_name='project_memberships', to=settings.AUTH_USER_MODEL),
        ),
        migrations.AddField(
            model_name='project',
            name='result_count',
            field=models.IntegerField(default=0, help_text='Total results inside of completions counter', verbose_name='result count'),
        ),
        migrations.CreateModel(
            name='Storage',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('title', models.CharField(help_text='Cloud storage title', max_length=256, null=True, verbose_name='title')),
                ('description', models.TextField(blank=True, help_text='Cloud storage description', null=True, verbose_name='description')),
                ('type', models.CharField(choices=[('s3', 'AWS S3 storage')], default='s3', max_length=32)),
                ('path', models.TextField(blank=True, help_text='Cloud storage path (e.g. bucket name)', null=True, verbose_name='path')),
                ('prefix', models.TextField(blank=True, help_text='Cloud storage prefix (e.g. container path)', null=True, verbose_name='prefix')),
                ('regex', models.TextField(blank=True, help_text='Cloud storage regex for filtering objects', null=True, verbose_name='regex')),
                ('data_key', models.TextField(blank=True, help_text='Data key to connect BLOBs with object tags', null=True, verbose_name='data_key')),
                ('use_blob_urls', models.BooleanField(default=False, help_text='Interpret objects as BLOBs and generate URLs', verbose_name='use BLOB URLs')),
                ('params', models.JSONField(blank=True, default=dict, help_text='Cloud storage specific parameters (e.g. credentials stored in key: value format', null=True, verbose_name='params')),
                ('created_at', models.DateTimeField(auto_now_add=True, help_text='Creation time', verbose_name='created at')),
                ('project', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='storages', to='projects.project')),
            ],
        ),
        migrations.AlterField(
            model_name='project',
            name='task_data_login',
            field=models.CharField(blank=True, help_text='Task data credentials: login', max_length=256, null=True, verbose_name='task_data_login'),
        ),
        migrations.AlterField(
            model_name='project',
            name='task_data_password',
            field=models.CharField(blank=True, help_text='Task data credentials: password', max_length=256, null=True, verbose_name='task_data_password'),
        ),
        migrations.AddField(
            model_name='project',
            name='organization',
            field=models.ForeignKey(null=True, on_delete=django.db.models.deletion.CASCADE, related_name='projects', to='organizations.organization'),
        ),
        migrations.CreateModel(
            name='ProjectSummary',
            fields=[
                ('project', annoying.fields.AutoOneToOneField(on_delete=django.db.models.deletion.CASCADE, primary_key=True, related_name='summary', serialize=False, to='projects.project')),
                ('created_at', models.DateTimeField(auto_now_add=True, help_text='Creation time', verbose_name='created at')),
                ('all_data_columns', models.JSONField(default=dict, help_text='All data columns found in imported tasks', null=True, verbose_name='all data columns')),
                ('common_data_columns', models.JSONField(default=list, help_text='Common data columns found in imported tasks', null=True, verbose_name='common data columns')),
                ('created_annotations', models.JSONField(default=dict, help_text='Unique annotation types identified by tuple (from_name, to_name, type)', null=True, verbose_name='created annotations')),
                ('created_labels', models.JSONField(default=dict, help_text='Unique labels', null=True, verbose_name='created labels')),
            ],
        ),
        migrations.DeleteModel(
            name='UploadedDataFile',
        ),
        migrations.DeleteModel(
            name='Storage',
        ),
        migrations.AddField(
            model_name='project',
            name='is_draft',
            field=models.BooleanField(default=False, help_text='Whether the project is in the middle of creation process', verbose_name='is draft'),
        ),
        migrations.AlterField(
            model_name='project',
            name='label_config',
            field=models.TextField(blank=True, default='<View></View>', help_text='Label config in XML format. See more about it in documentation', null=True, verbose_name='label config'),
        ),
        migrations.AlterField(
            model_name='project',
            name='title',
            field=models.CharField(blank=True, help_text='Project name, length is from 3 to 50 chars', max_length=50, null=True, validators=[django.core.validators.MinLengthValidator(3), django.core.validators.MaxLengthValidator(50)], verbose_name='name'),
        ),
        migrations.RemoveField(
            model_name='project',
            name='batch_size',
        ),
        migrations.RemoveField(
            model_name='project',
            name='cluster_annotation',
        ),
        migrations.RemoveField(
            model_name='project',
            name='has_finished',
        ),
        migrations.RemoveField(
            model_name='project',
            name='onboarding_steps',
        ),
        migrations.RemoveField(
            model_name='project',
            name='skip_onboarding',
        ),
        migrations.AddField(
            model_name='project',
            name='description',
            field=models.TextField(blank=True, default='', help_text='Expert instruction in HTML format', null=True, verbose_name='expert instruction'),
        ),
        migrations.RemoveField(
            model_name='mlbackendconnection',
            name='ml_backend',
        ),
        migrations.RemoveField(
            model_name='mlbackendconnection',
            name='project',
        ),
        migrations.RemoveField(
            model_name='mlbackendpredictionjob',
            name='project',
        ),
        migrations.RemoveField(
            model_name='mlbackendtrainjob',
            name='project',
        ),
        migrations.RemoveField(
            model_name='projectstats',
            name='project',
        ),
        migrations.RemoveField(
            model_name='project',
            name='ml_backend_active_connection',
        ),
        migrations.RemoveField(
            model_name='projecttemplate',
            name='ml_backend',
        ),
        migrations.DeleteModel(
            name='MLBackend',
        ),
        migrations.DeleteModel(
            name='MLBackendConnection',
        ),
        migrations.DeleteModel(
            name='MLBackendPredictionJob',
        ),
        migrations.DeleteModel(
            name='MLBackendTrainJob',
        ),
        migrations.DeleteModel(
            name='ProjectStats',
        ),
        migrations.AlterField(
            model_name='project',
            name='title',
            field=models.CharField(blank=True, default='', help_text='Project name, length is from 3 to 50 chars', max_length=50, null=True, validators=[django.core.validators.MinLengthValidator(3), django.core.validators.MaxLengthValidator(50)], verbose_name='title'),
        ),
    ]
</file>

<file path="label_studio/projects/migrations/0002_auto_20210304_1457.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
# Generated by Django 3.1.4 on 2021-03-04 14:57

from django.db import migrations


class Migration(migrations.Migration):

    dependencies = [
        ('projects', '0001_squashed_0065_auto_20210223_2014'),
    ]

    operations = [
        migrations.AlterUniqueTogether(
            name='project',
            unique_together=set(),
        ),
    ]
</file>

<file path="label_studio/projects/migrations/0003_auto_20210305_1008.py">
# Generated by Django 3.1.4 on 2021-03-05 10:08

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('projects', '0002_auto_20210304_1457'),
    ]

    operations = [
        migrations.RenameField(
            model_name='project',
            old_name='enable_empty_completion',
            new_name='enable_empty_annotation'
        ),
        migrations.AlterField(
            model_name='project',
            name='enable_empty_annotation',
            field=models.BooleanField(default=True, help_text='Allow submit empty annotations', verbose_name='enable empty annotation'),
        ),
        migrations.RenameField(
            model_name='project',
            old_name='maximum_completions',
            new_name='maximum_annotations'
        ),
        migrations.AlterField(
            model_name='project',
            name='maximum_annotations',
            field=models.IntegerField(default=1, help_text='Maximum overlaps of expert annotations for one task. If the annotation number per task is equal or greater to this value, the task becomes finished (is_labeled=True)', verbose_name='maximum annotation number'),
        ),
        migrations.RenameField(
            model_name='project',
            old_name='min_completions_to_start_training',
            new_name='min_annotations_to_start_training'
        ),
        migrations.AlterField(
            model_name='project',
            name='min_annotations_to_start_training',
            field=models.IntegerField(default=10, help_text='Minimum number of completed tasks after which training is started', verbose_name='min_annotations_to_start_training'),
        ),
        migrations.RenameField(
            model_name='project',
            old_name='show_completion_history',
            new_name='show_annotation_history'
        ),
        migrations.AlterField(
            model_name='project',
            name='show_annotation_history',
            field=models.BooleanField(default=False, help_text='Show annotation history to collaborator', verbose_name='show annotation history'),
        ),
        migrations.AlterField(
            model_name='project',
            name='result_count',
            field=models.IntegerField(default=0, help_text='Total results inside of annotations counter', verbose_name='result count'),
        ),
    ]
</file>

<file path="label_studio/projects/migrations/0003_project_color.py">
# Generated by Django 3.1.4 on 2021-03-05 20:35

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('projects', '0002_auto_20210304_1457'),
    ]

    operations = [
        migrations.AddField(
            model_name='project',
            name='color',
            field=models.CharField(blank=True, default='#FFFFFF', max_length=16, null=True, verbose_name='color'),
        ),
    ]
</file>

<file path="label_studio/projects/migrations/0004_auto_20210306_0506.py">
# Generated by Django 3.1.4 on 2021-03-06 05:06

import django.core.validators
from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('projects', '0003_project_color'),
    ]

    operations = [
        migrations.RemoveField(
            model_name='project',
            name='agreement_method',
        ),
        migrations.RemoveField(
            model_name='project',
            name='agreement_threshold',
        ),
        migrations.RemoveField(
            model_name='project',
            name='metric_name',
        ),
        migrations.RemoveField(
            model_name='project',
            name='metric_params',
        ),
        migrations.RemoveField(
            model_name='project',
            name='metric_threshold',
        ),
        migrations.RemoveField(
            model_name='project',
            name='use_kappa',
        ),
        migrations.AlterField(
            model_name='project',
            name='description',
            field=models.TextField(blank=True, default='', help_text='Project description', null=True, verbose_name='description'),
        ),
        migrations.AlterField(
            model_name='project',
            name='enable_empty_completion',
            field=models.BooleanField(default=True, help_text='Allow annotators to submit empty annotations', verbose_name='enable empty annotation'),
        ),
        migrations.AlterField(
            model_name='project',
            name='expert_instruction',
            field=models.TextField(blank=True, default='', help_text='Labeling instructions in HTML format', null=True, verbose_name='expert instruction'),
        ),
        migrations.AlterField(
            model_name='project',
            name='is_draft',
            field=models.BooleanField(default=False, help_text='Whether or not the project is in the middle of being created', verbose_name='is draft'),
        ),
        migrations.AlterField(
            model_name='project',
            name='is_published',
            field=models.BooleanField(default=False, help_text='Whether or not the project is published to annotators', verbose_name='published'),
        ),
        migrations.AlterField(
            model_name='project',
            name='maximum_completions',
            field=models.IntegerField(default=1, help_text='Maximum number of annotations for one task. If the number of annotations per task is equal or greater to this value, the task is completed (is_labeled=True)', verbose_name='maximum completion number'),
        ),
        migrations.AlterField(
            model_name='project',
            name='min_completions_to_start_training',
            field=models.IntegerField(default=10, help_text='Minimum number of completed tasks after which model training is started', verbose_name='min_completions_to_start_training'),
        ),
        migrations.AlterField(
            model_name='project',
            name='result_count',
            field=models.IntegerField(default=0, help_text='Total results inside of annotations counter', verbose_name='result count'),
        ),
        migrations.AlterField(
            model_name='project',
            name='show_collab_predictions',
            field=models.BooleanField(default=True, help_text='If set, the annotator can view model predictions', verbose_name='show predictions to annotator'),
        ),
        migrations.AlterField(
            model_name='project',
            name='show_completion_history',
            field=models.BooleanField(default=False, help_text='Show annotation history to annotator', verbose_name='show annotation history'),
        ),
        migrations.AlterField(
            model_name='project',
            name='show_instruction',
            field=models.BooleanField(default=False, help_text='Show instructions to the annotator before they start', verbose_name='show instruction'),
        ),
        migrations.AlterField(
            model_name='project',
            name='show_skip_button',
            field=models.BooleanField(default=True, help_text='Show a skip button in interface and allow annotators to skip the task', verbose_name='show skip button'),
        ),
        migrations.AlterField(
            model_name='project',
            name='title',
            field=models.CharField(blank=True, default='', help_text='Project name. Must be between 3 to 50 characters long.', max_length=50, null=True, validators=[django.core.validators.MinLengthValidator(3), django.core.validators.MaxLengthValidator(50)], verbose_name='title'),
        ),
        migrations.AlterField(
            model_name='projectsummary',
            name='common_data_columns',
            field=models.JSONField(default=list, help_text='Common data columns found across imported tasks', null=True, verbose_name='common data columns'),
        ),
        migrations.AlterField(
            model_name='projecttemplate',
            name='expert_instruction',
            field=models.TextField(default='', verbose_name='annotator instructions'),
        ),
    ]
</file>

<file path="label_studio/projects/migrations/0005_merge_20210308_1141.py">
# Generated by Django 3.1.4 on 2021-03-08 11:41

from django.db import migrations


class Migration(migrations.Migration):

    dependencies = [
        ('projects', '0004_auto_20210306_0506'),
        ('projects', '0003_auto_20210305_1008'),
    ]

    operations = [
    ]
</file>

<file path="label_studio/projects/migrations/0006_auto_20210308_1559.py">
# Generated by Django 3.1.4 on 2021-03-08 15:59

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('projects', '0005_merge_20210308_1141'),
    ]

    operations = [
        migrations.AlterField(
            model_name='project',
            name='enable_empty_annotation',
            field=models.BooleanField(default=True, help_text='Allow annotators to submit empty annotations', verbose_name='enable empty annotation'),
        ),
        migrations.AlterField(
            model_name='project',
            name='maximum_annotations',
            field=models.IntegerField(default=1, help_text='Maximum number of annotations for one task. If the number of annotations per task is equal or greater to this value, the task is completed (is_labeled=True)', verbose_name='maximum annotation number'),
        ),
        migrations.AlterField(
            model_name='project',
            name='min_annotations_to_start_training',
            field=models.IntegerField(default=10, help_text='Minimum number of completed tasks after which model training is started', verbose_name='min_annotations_to_start_training'),
        ),
        migrations.AlterField(
            model_name='project',
            name='show_annotation_history',
            field=models.BooleanField(default=False, help_text='Show annotation history to annotator', verbose_name='show annotation history'),
        ),
    ]
</file>

<file path="label_studio/projects/migrations/0007_auto_20210309_1304.py">
# Generated by Django 3.1.4 on 2021-03-09 13:04

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('projects', '0006_auto_20210308_1559'),
    ]

    operations = [
        migrations.RenameField(
            model_name='project',
            old_name='show_ground_truths_first',
            new_name='show_ground_truth_first'
        ),
        migrations.AlterField(
            model_name='project',
            name='show_ground_truth_first',
            field=models.BooleanField(default=True, verbose_name='show ground truth first'),
        ),
    ]
</file>

<file path="label_studio/projects/migrations/0008_auto_20210314_1840.py">
# Generated by Django 3.1.4 on 2021-03-14 18:40

import django.core.validators
from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('projects', '0007_auto_20210309_1304'),
    ]

    operations = [
        migrations.AlterField(
            model_name='project',
            name='sampling',
            field=models.CharField(choices=[('Sequential sampling', 'Tasks are ordered by their IDs'), ('Uniform sampling', 'Tasks are chosen randomly'), ('Uncertainty sampling', 'Tasks are chosen according to model uncertainty scores (active learning mode)')], default='Sequential sampling', max_length=100, null=True),
        ),
        migrations.AlterField(
            model_name='project',
            name='title',
            field=models.CharField(blank=True, default='', help_text='Project name. Must be between 3 and 50 characters long.', max_length=50, null=True, validators=[django.core.validators.MinLengthValidator(3), django.core.validators.MaxLengthValidator(50)], verbose_name='title'),
        ),
    ]
</file>

<file path="label_studio/projects/migrations/0009_project_evaluate_predictions_automatically.py">
# Generated by Django 3.1.4 on 2021-04-30 13:52

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('projects', '0008_auto_20210314_1840'),
    ]

    operations = [
        migrations.AddField(
            model_name='project',
            name='evaluate_predictions_automatically',
            field=models.BooleanField(default=False, help_text='Retrieve and display predictions when loading a task',
                                      verbose_name='evaluate predictions automatically'),
        ),
    ]
</file>

<file path="label_studio/projects/migrations/0010_auto_20210505_2037.py">
# Generated by Django 3.1.4 on 2021-05-05 20:37

from django.db import migrations


class Migration(migrations.Migration):

    dependencies = [
        ('projects', '0009_project_evaluate_predictions_automatically'),
    ]

    operations = [
        migrations.RemoveField(
            model_name='project',
            name='template_used',
        ),
        migrations.DeleteModel(
            name='ProjectTemplate',
        ),
    ]
</file>

<file path="label_studio/projects/migrations/0011_auto_20210517_2101.py">
# Generated by Django 3.1.4 on 2021-05-17 21:01

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('projects', '0010_auto_20210505_2037'),
    ]

    operations = [
        migrations.AlterField(
            model_name='project',
            name='sampling',
            field=models.CharField(choices=[('Sequential sampling', 'Tasks are ordered by Data manager ordering'), ('Uniform sampling', 'Tasks are chosen randomly'), ('Uncertainty sampling', 'Tasks are chosen according to model uncertainty scores (active learning mode)')], default='Sequential sampling', max_length=100, null=True),
        ),
    ]
</file>

<file path="label_studio/projects/migrations/0012_auto_20210906_1323.py">
# Generated by Django 3.1.12 on 2021-09-06 13:23

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('projects', '0011_auto_20210517_2101'),
    ]

    operations = [
        migrations.AlterField(
            model_name='project',
            name='show_ground_truth_first',
            field=models.BooleanField(default=False, verbose_name='show ground truth first'),
        ),
        migrations.AlterField(
            model_name='project',
            name='show_overlap_first',
            field=models.BooleanField(default=False, verbose_name='show overlap first'),
        ),
    ]
</file>

<file path="label_studio/projects/migrations/0013_project_reveal_preannotations_interactively.py">
# Generated by Django 3.1.13 on 2021-11-29 11:32

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('projects', '0012_auto_20210906_1323'),
    ]

    operations = [
        migrations.AddField(
            model_name='project',
            name='reveal_preannotations_interactively',
            field=models.BooleanField(default=False, help_text='Reveal pre-annotations interactively', verbose_name='reveal_preannotations_interactively'),
        ),
    ]
</file>

<file path="label_studio/projects/migrations/0013_project_skip_queue.py">
# Generated by Django 3.1.13 on 2021-11-02 11:45

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('projects', '0012_auto_20210906_1323'),
    ]

    operations = [
        migrations.AddField(
            model_name='project',
            name='skip_queue',
            field=models.CharField(choices=[('REQUEUE_FOR_ME', 'Requeue for me'), ('REQUEUE_FOR_OTHERS', 'Requeue for others'), ('IGNORE_SKIPPED', 'Ignore skipped')], default='REQUEUE_FOR_OTHERS', max_length=100, null=True),
        ),
    ]
</file>

<file path="label_studio/projects/migrations/0014_project_parsed_label_config.py">
# Generated by Django 3.1.13 on 2021-12-13 09:14

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('projects', '0013_project_reveal_preannotations_interactively'),
    ]

    operations = [
        migrations.AddField(
            model_name='project',
            name='parsed_label_config',
            field=models.TextField(blank=True, default='', help_text='Parsed label config in JSON format. See more about it in documentation', null=True, verbose_name='parsed label config'),
        ),
    ]
</file>

<file path="label_studio/projects/migrations/0015_merge_20220117_0749.py">
# Generated by Django 3.1.14 on 2022-01-17 07:49

from django.db import migrations


class Migration(migrations.Migration):

    dependencies = [
        ('projects', '0014_project_parsed_label_config'),
        ('projects', '0013_project_skip_queue'),
    ]

    operations = [
    ]
</file>

<file path="label_studio/projects/migrations/0016_auto_20220211_2218.py">
# Generated by Django 3.1.14 on 2022-02-11 22:18

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('projects', '0015_merge_20220117_0749'),
    ]

    operations = [
        migrations.AlterField(
            model_name='project',
            name='min_annotations_to_start_training',
            field=models.IntegerField(default=0, help_text='Minimum number of completed tasks after which model training is started', verbose_name='min_annotations_to_start_training'),
        ),
        migrations.RemoveField(
            model_name='project',
            name='parsed_label_config',
        ),
        migrations.AddField(
            model_name='project',
            name='parsed_label_config',
            field=models.JSONField(blank=True, default=None, help_text='Parsed label config in JSON format. See more about it in documentation', null=True, verbose_name='parsed label config'),
        ),
    ]
</file>

<file path="label_studio/projects/migrations/0017_project_pinned_at.py">
# Generated by Django 3.2.13 on 2022-07-06 12:02

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('projects', '0016_auto_20220211_2218'),
    ]

    operations = [
        migrations.AddField(
            model_name='project',
            name='pinned_at',
            field=models.DateTimeField(default=None, help_text='Pinned date and time', null=True, verbose_name='pinned at'),
        ),
    ]
</file>

<file path="label_studio/projects/migrations/0018_alter_project_control_weights.py">
# Generated by Django 3.2.14 on 2022-11-25 15:03

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('projects', '0017_project_pinned_at'),
    ]

    operations = [
        migrations.AlterField(
            model_name='project',
            name='control_weights',
            field=models.JSONField(default=dict, help_text="Dict of weights for each control tag in metric calculation. Each control tag (e.g. label or choice) will have it's own key in control weight dict with weight for each label and overall weight.For example, if bounding box annotation with control tag named my_bbox should be included with 0.33 weight in agreement calculation, and the first label Car should be twice more important than Airplaine, then you have to need the specify: {'my_bbox': {'type': 'RectangleLabels', 'labels': {'Car': 1.0, 'Airplaine': 0.5}, 'overall': 0.33}", null=True, verbose_name='control weights'),
        ),
    ]
</file>

<file path="label_studio/projects/migrations/0019_labelstreamhistory.py">
# Generated by Django 3.2.16 on 2023-02-02 16:43

from django.conf import settings
from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    dependencies = [
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
        ('projects', '0018_alter_project_control_weights'),
    ]

    operations = [
        migrations.CreateModel(
            name='LabelStreamHistory',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('data', models.JSONField(default=list)),
                ('project', models.ForeignKey(help_text='Project ID', on_delete=django.db.models.deletion.CASCADE, related_name='histories', to='projects.project')),
                ('user', models.ForeignKey(help_text='User ID', on_delete=django.db.models.deletion.CASCADE, related_name='histories', to=settings.AUTH_USER_MODEL)),
            ],
        ),
    ]
</file>

<file path="label_studio/projects/migrations/0019_project_project_pinned__a39ccb_idx.py">
# Generated by Django 3.2.16 on 2023-02-10 12:37

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('projects', '0018_alter_project_control_weights'),
    ]

    operations = [
        migrations.AddIndex(
            model_name='project',
            index=models.Index(fields=['pinned_at', 'created_at'], name='project_pinned__a39ccb_idx'),
        ),
    ]
</file>

<file path="label_studio/projects/migrations/0020_labelstreamhistory_unique_history.py">
# Generated by Django 3.2.16 on 2023-02-03 11:37

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('projects', '0019_labelstreamhistory'),
    ]

    operations = [
        migrations.AddConstraint(
            model_name='labelstreamhistory',
            constraint=models.UniqueConstraint(fields=('user', 'project'), name='unique_history'),
        ),
    ]
</file>

<file path="label_studio/projects/migrations/0021_merge_20230215_1943.py">
# Generated by Django 3.2.16 on 2023-02-15 19:43

from django.db import migrations


class Migration(migrations.Migration):

    dependencies = [
        ('projects', '0019_project_project_pinned__a39ccb_idx'),
        ('projects', '0020_labelstreamhistory_unique_history'),
    ]

    operations = [
    ]
</file>

<file path="label_studio/projects/migrations/0022_projectimport.py">
# Generated by Django 3.2.16 on 2023-04-18 13:11

from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    dependencies = [
        ('projects', '0021_merge_20230215_1943'),
    ]

    operations = [
        migrations.CreateModel(
            name='ProjectImport',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('preannotated_from_fields', models.JSONField(blank=True, null=True)),
                ('commit_to_project', models.BooleanField(default=False)),
                ('return_task_ids', models.BooleanField(default=False)),
                ('status', models.CharField(choices=[('created', 'Created'), ('in_progress', 'In progress'), ('failed', 'Failed'), ('completed', 'Completed')], default='created', max_length=64)),
                ('url', models.CharField(blank=True, max_length=2048, null=True)),
                ('traceback', models.TextField(blank=True, null=True)),
                ('error', models.TextField(blank=True, null=True)),
                ('created_at', models.DateTimeField(auto_now_add=True, help_text='Creation time', null=True, verbose_name='created at')),
                ('updated_at', models.DateTimeField(auto_now_add=True, help_text='Updated time', null=True, verbose_name='updated at')),
                ('finished_at', models.DateTimeField(default=None, help_text='Complete or fail time', null=True, verbose_name='finished at')),
                ('task_count', models.IntegerField(default=0)),
                ('annotation_count', models.IntegerField(default=0)),
                ('prediction_count', models.IntegerField(default=0)),
                ('duration', models.IntegerField(default=0)),
                ('file_upload_ids', models.JSONField(default=list)),
                ('could_be_tasks_list', models.BooleanField(default=False)),
                ('found_formats', models.JSONField(default=list)),
                ('data_columns', models.JSONField(default=list)),
                ('tasks', models.JSONField(blank=True, null=True)),
                ('task_ids', models.JSONField(default=list)),
                ('project', models.ForeignKey(null=True, on_delete=django.db.models.deletion.CASCADE, related_name='imports', to='projects.project')),
            ],
        ),
    ]
</file>

<file path="label_studio/projects/migrations/0022_projectsummary_created_labels_drafts.py">
# Generated by Django 3.2.16 on 2023-04-05 11:06

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('projects', '0021_merge_20230215_1943'),
    ]

    operations = [
        migrations.AddField(
            model_name='projectsummary',
            name='created_labels_drafts',
            field=models.JSONField(default=dict, help_text='Unique drafts labels', null=True, verbose_name='created labels in drafts'),
        ),
    ]
</file>

<file path="label_studio/projects/migrations/0023_merge_20230512_1333.py">
# Generated by Django 3.2.16 on 2023-05-12 13:33

from django.db import migrations


class Migration(migrations.Migration):

    dependencies = [
        ('projects', '0022_projectimport'),
        ('projects', '0022_projectsummary_created_labels_drafts'),
    ]

    operations = [
    ]
</file>

<file path="label_studio/projects/migrations/0023_projectreimport.py">
# Generated by Django 3.2.16 on 2023-05-25 14:40

from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    dependencies = [
        ('projects', '0022_projectimport'),
    ]

    operations = [
        migrations.CreateModel(
            name='ProjectReimport',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('status', models.CharField(choices=[('created', 'Created'), ('in_progress', 'In progress'), ('failed', 'Failed'), ('completed', 'Completed')], default='created', max_length=64)),
                ('error', models.TextField(blank=True, null=True)),
                ('task_count', models.IntegerField(default=0)),
                ('annotation_count', models.IntegerField(default=0)),
                ('prediction_count', models.IntegerField(default=0)),
                ('duration', models.IntegerField(default=0)),
                ('file_upload_ids', models.JSONField(default=list)),
                ('files_as_tasks_list', models.BooleanField(default=False)),
                ('found_formats', models.JSONField(default=list)),
                ('data_columns', models.JSONField(default=list)),
                ('traceback', models.TextField(blank=True, null=True)),
                ('project', models.ForeignKey(null=True, on_delete=django.db.models.deletion.CASCADE, related_name='reimports', to='projects.project')),
            ],
        ),
    ]
</file>

<file path="label_studio/projects/migrations/0024_merge_0023_merge_20230512_1333_0023_projectreimport.py">
# Generated by Django 3.2.16 on 2023-07-04 16:31

from django.db import migrations


class Migration(migrations.Migration):

    dependencies = [
        ('projects', '0023_merge_20230512_1333'),
        ('projects', '0023_projectreimport'),
    ]

    operations = [
    ]
</file>

<file path="label_studio/projects/migrations/0025_project_label_config_hash.py">
# Generated by Django 3.2.20 on 2023-09-21 15:41

from django.db import migrations, models

class Migration(migrations.Migration):

    dependencies = [
        ('projects', '0024_merge_0023_merge_20230512_1333_0023_projectreimport'),

    ]

    operations = [
        migrations.AddField(
            model_name='project',
            name='label_config_hash',
            field=models.BigIntegerField(default=None, null=True),
        ),
    ]
</file>

<file path="label_studio/projects/migrations/0026_auto_20231103_0020.py">
# Generated by Django 3.2.20 on 2023-11-03 00:20

from django.db import migrations
from projects.models import Project
from core.models import AsyncMigrationStatus
from core.redis import start_job_async_or_sync
import logging


logger = logging.getLogger(__name__)


def _fill_label_config_hash(migration_name):
    project_tuples = Project.objects.all().values_list('id', 'parsed_label_config')
    for project_id, parsed_label_config in project_tuples:
        migration = AsyncMigrationStatus.objects.create(
            project_id=project_id,
            name=migration_name,
            status=AsyncMigrationStatus.STATUS_STARTED,
        )

        hashed_label_config = hash(str(parsed_label_config))
        Project.objects.filter(id=project_id).update(label_config_hash=hashed_label_config)

        migration.status = AsyncMigrationStatus.STATUS_FINISHED
        migration.save()


def fill_label_config_hash(migration_name):
    logger.info('Start filling label config hash')
    start_job_async_or_sync(_fill_label_config_hash, migration_name=migration_name)
    logger.info('Finished filling label config hash')


def forward(apps, schema_editor):
    fill_label_config_hash('0026_auto_20231103_0020')


def backwards(apps, schema_editor):
    pass


class Migration(migrations.Migration):

    dependencies = [
        ('projects', '0025_project_label_config_hash'),
    ]

    operations = [
        migrations.RunPython(forward, backwards)
    ]
</file>

<file path="label_studio/projects/migrations/0027_project_custom_task_lock_ttl.py">
# Generated by Django 4.2.16 on 2024-10-29 15:11

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('projects', '0026_auto_20231103_0020'),
    ]

    operations = [
        migrations.AddField(
            model_name='project',
            name='custom_task_lock_ttl',
            field=models.IntegerField(default=None, help_text='Custom task lock TTL in seconds. If not set, the default value is used', null=True, verbose_name='custom_task_lock_ttl'),
        ),
    ]
</file>

<file path="label_studio/projects/migrations/0028_auto_20241107_1031.py">
# Generated by Django 4.2.16 on 2024-11-07 10:31

from django.db import migrations
import logging

from core.models import AsyncMigrationStatus
from core.redis import start_job_async_or_sync
from django.db.models import Count, Min
from projects.models import ProjectMember

logger = logging.getLogger(__name__)
migration_name = '0028_auto_20241107_1031'


def forward_migration(migration_name):

    logger.info(f'Starting async migration {migration_name}')
    migration = AsyncMigrationStatus.objects.create(
        name=migration_name,
        status=AsyncMigrationStatus.STATUS_STARTED,
    )

    try:
        # Get projects with duplicates
        projects_with_duplicates = (
            ProjectMember.objects
            .values('project_id', 'user_id')
            .annotate(entry_count=Count('id'))
            .filter(entry_count__gt=1)
            .values_list('project_id', flat=True)
            .distinct()
        )

        for project_id in projects_with_duplicates:
            # Remove duplicates for each project
            duplicates = (
                ProjectMember.objects
                .filter(project_id=project_id)
                .values('user_id')
                .annotate(count=Count('id'), min_id=Min('id'))
                .filter(count__gt=1)
            )
            total_deleted = 0
            for dup in duplicates:
                user_id = dup['user_id']
                min_id = dup['min_id']
                entries_to_delete = (
                    ProjectMember.objects
                    .filter(user_id=user_id, project_id=project_id)
                    .exclude(id=min_id)
                )
                deleted_count, _ = entries_to_delete.delete()
                total_deleted += deleted_count
            logger.info(f'Deleted {total_deleted} duplicate ProjectMember entries for project ID {project_id}.')

    except Exception as e:
        migration.status = AsyncMigrationStatus.STATUS_FAILED
        migration.save()
        logger.error(f'Async migration {migration_name} failed: {e}')
        raise

    migration.status = AsyncMigrationStatus.STATUS_FINISHED
    migration.save()
    logger.info(f'Async migration {migration_name} complete')


def forwards(apps, schema_editor):
    # Dispatch migration to workers without passing unpicklable objects
    start_job_async_or_sync(forward_migration, migration_name=migration_name)


def backwards(apps, schema_editor):
    pass


class Migration(migrations.Migration):
    atomic = False

    dependencies = [
        ("projects", "0027_project_custom_task_lock_ttl"),
    ]

    operations = [
        migrations.RunPython(forwards, backwards),
    ]
</file>

<file path="label_studio/projects/templatetags/__init__.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
</file>

<file path="label_studio/projects/templatetags/custom_filters.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
from django import template

register = template.Library()


@register.filter(name='seconds_to_pretty_time')
def seconds_to_pretty_time(value, show_seconds=False):
    if value is None:
        return 'N/A'
    if value < 60:
        value = int(value)
        if show_seconds:
            return f'{value} second' + ('s' if value > 1 else '')
        return '< 1 minute'
    if value < 3600:
        m = int(value / 60.0)
        if m == 1:
            return '1 minute'
        else:
            return f'{m} minutes'
    h = int(value / 3600)
    if h == 1:
        return '1 hour'
    else:
        return f'{h} hours'
</file>

<file path="label_studio/projects/tests/factories.py">
import factory
from projects.models import Project, ProjectMember


class ProjectFactory(factory.django.DjangoModelFactory):
    title = factory.Faker('bs')
    description = factory.Faker('paragraph')
    organization = factory.SubFactory('organizations.tests.factories.OrganizationFactory')
    created_by = factory.SelfAttribute('organization.created_by')

    class Meta:
        model = Project

    @factory.post_generation
    def created_by_relationship(self, create, extracted, **kwargs):
        if not create or not self.created_by:
            return
        ProjectMember.objects.create(user=self.created_by, project=self)
</file>

<file path="label_studio/projects/tests/test_api.py">
from django.test import TestCase
from django.urls import reverse
from django.utils.http import urlencode
from rest_framework.test import APIClient
from tasks.models import Task

from .factories import ProjectFactory


class TestProjectCountsListAPI(TestCase):
    @classmethod
    def setUpTestData(cls):
        cls.project_1 = ProjectFactory()
        cls.project_2 = ProjectFactory(organization=cls.project_1.organization)
        Task.objects.create(project=cls.project_1, data={'text': 'Task 1'})
        Task.objects.create(project=cls.project_1, data={'text': 'Task 2'})
        Task.objects.create(project=cls.project_2, data={'text': 'Task 3'})

    def get_url(self, **params):
        return f'{reverse("projects:api:project-counts-list")}?{urlencode(params)}'

    def test_get_counts(self):
        client = APIClient()
        client.force_authenticate(user=self.project_1.created_by)
        response = client.get(self.get_url(include='id,task_number,finished_task_number,total_predictions_number'))
        self.assertEqual(response.status_code, 200)
        self.assertEqual(response.json()['count'], 2)
        self.assertEqual(
            response.json()['results'],
            [
                {
                    'id': self.project_1.id,
                    'task_number': 2,
                    'finished_task_number': 0,
                    'total_predictions_number': 0,
                },
                {
                    'id': self.project_2.id,
                    'task_number': 1,
                    'finished_task_number': 0,
                    'total_predictions_number': 0,
                },
            ],
        )
</file>

<file path="label_studio/projects/tests/test_project_sample_task.py">
import json
from unittest.mock import patch

import projects.api
import pytest
from django.test import TestCase
from django.urls import reverse
from projects.tests.factories import ProjectFactory
from rest_framework.test import APIClient


@pytest.mark.django_db
class TestProjectSampleTask(TestCase):
    @classmethod
    def setUpTestData(cls):
        cls.project = ProjectFactory()

    @property
    def url(self):
        return reverse('projects:api:project-sample-task', kwargs={'pk': self.project.id})

    def test_sample_task_with_happy_path(self):
        """Test that ProjectSampleTask.post successfully creates a complete sample task with annotations and predictions"""
        client = APIClient()
        client.force_authenticate(user=self.project.created_by)
        user_id = self.project.created_by.id
        label_config = """
        <View>
          <Text name='text' value='$text'/>
          <Choices name='sentiment' toName='text'>
            <Choice value='Positive'/>
            <Choice value='Negative'/>
            <Choice value='Neutral'/>
          </Choices>
        </View>
        """
        sample_prediction = {
            'model_version': 'sample model version',
            'result': [
                {
                    'id': 'abc123',
                    'from_name': 'sentiment',
                    'to_name': 'text',
                    'type': 'choices',
                    'value': {'choices': ['Positive']},
                }
            ],
            'score': 0.95,
        }
        sample_annotation = {
            'was_cancelled': False,
            'ground_truth': False,
            'result': [
                {
                    'id': 'def456',
                    'from_name': 'sentiment',
                    'to_name': 'text',
                    'type': 'choices',
                    'value': {'choices': ['Positive']},
                }
            ],
            'completed_by': -1,
        }
        sample_task = {
            'id': 1,
            'data': {'text': 'This is a sample task for labeling.'},
            'predictions': [sample_prediction],
            'annotations': [sample_annotation],
        }

        with patch.object(
            projects.api.LabelInterface,
            'generate_complete_sample_task',
            return_value=sample_task,
        ):
            response = client.post(
                self.url,
                data=json.dumps({'label_config': label_config, 'include_annotation_and_prediction': True}),
                content_type='application/json',
            )

            assert response.status_code == 200
            response_data = response.json()
            assert 'sample_task' in response_data
            sample_task_with_annotator_id_set = sample_task.copy()
            sample_task_with_annotator_id_set['annotations'][0]['completed_by'] = user_id
            assert response_data['sample_task'] == sample_task_with_annotator_id_set

    def test_sample_task_fallback_when_generate_task_fails(self):
        """Test fallback to project.get_sample_task when LabelInterface.generate_complete_sample_task fails"""
        client = APIClient()
        client.force_authenticate(user=self.project.created_by)
        label_config = """
        <View>
          <Text name='text' value='$text'/>
          <Choices name='sentiment' toName='text'>
            <Choice value='Positive'/>
            <Choice value='Negative'/>
            <Choice value='Neutral'/>
          </Choices>
        </View>
        """
        fallback_data = {'id': 999, 'data': {'text': 'Fallback task'}}

        with (
            patch.object(
                projects.api.LabelInterface,
                'generate_complete_sample_task',
                side_effect=ValueError('Failed to generate sample task'),
            ),
            patch('projects.api.Project.get_sample_task', return_value=fallback_data),
        ):

            response = client.post(
                self.url,
                data=json.dumps({'label_config': label_config, 'include_annotation_and_prediction': True}),
                content_type='application/json',
            )

            assert response.status_code == 200
            response_data = response.json()
            assert 'sample_task' in response_data
            assert response_data['sample_task'] == fallback_data

    def test_sample_task_fallback_when_prediction_generation_fails(self):
        """Test fallback to project.get_sample_task when LabelInterface.generate_sample_prediction raises an exception"""
        client = APIClient()
        client.force_authenticate(user=self.project.created_by)
        label_config = """
        <View>
          <Text name='text' value='$text'/>
          <Choices name='sentiment' toName='text'>
            <Choice value='Positive'/>
            <Choice value='Negative'/>
            <Choice value='Neutral'/>
          </Choices>
        </View>
        """
        fallback_data = {'id': 999, 'data': {'text': 'Fallback task'}}

        with (
            patch.object(
                projects.api.LabelInterface,
                'generate_sample_prediction',
                return_value=None,
            ),
            patch('projects.api.Project.get_sample_task', return_value=fallback_data),
        ):
            response = client.post(
                self.url,
                data=json.dumps({'label_config': label_config, 'include_annotation_and_prediction': True}),
                content_type='application/json',
            )

            assert response.status_code == 200
            response_data = response.json()
            assert 'sample_task' in response_data
            assert response_data['sample_task'] == fallback_data

    def test_sample_task_with_include_annotation_and_prediction_false(self):
        """Test that setting include_annotation_and_prediction=False bypasses LabelInterface.generate_complete_sample_task"""
        client = APIClient()
        client.force_authenticate(user=self.project.created_by)
        label_config = """
        <View>
          <Text name='text' value='$text'/>
          <Choices name='sentiment' toName='text'>
            <Choice value='Positive'/>
            <Choice value='Negative'/>
            <Choice value='Neutral'/>
          </Choices>
        </View>
        """

        with patch('projects.api.Project.get_sample_task', return_value=None) as mock_get_sample_task, patch.object(
            projects.api.LabelInterface, 'generate_complete_sample_task', return_value=None
        ) as mock_generate_complete:  # Shouldn't be called

            client.post(
                self.url,
                data=json.dumps({'label_config': label_config, 'include_annotation_and_prediction': False}),
                content_type='application/json',
            )

            mock_get_sample_task.assert_called_once()
            mock_generate_complete.assert_not_called()

    def test_sample_task_default_behavior(self):
        """Test that omitting include_annotation_and_prediction defaults to False and uses simple sample task"""
        client = APIClient()
        client.force_authenticate(user=self.project.created_by)
        label_config = """
        <View>
          <Text name='text' value='$text'/>
          <Choices name='sentiment' toName='text'>
            <Choice value='Positive'/>
            <Choice value='Negative'/>
            <Choice value='Neutral'/>
          </Choices>
        </View>
        """

        with patch('projects.api.Project.get_sample_task', return_value=None) as mock_get_sample_task, patch.object(
            projects.api.LabelInterface, 'generate_complete_sample_task', return_value=None
        ) as mock_generate_complete:  # Shouldn't be called

            client.post(
                self.url,
                data=json.dumps({'label_config': label_config}),
                content_type='application/json',
            )

            mock_get_sample_task.assert_called_once()
            mock_generate_complete.assert_not_called()
</file>

<file path="label_studio/projects/__init__.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
</file>

<file path="label_studio/projects/api.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import logging
import os
import pathlib

import drf_yasg.openapi as openapi
from core.filters import ListFilter
from core.label_config import config_essential_data_has_changed
from core.mixins import GetParentObjectMixin
from core.permissions import ViewClassPermission, all_permissions
from core.redis import start_job_async_or_sync
from core.utils.common import paginator, paginator_help, temporary_disconnect_all_signals
from core.utils.exceptions import LabelStudioDatabaseException, ProjectExistException
from core.utils.io import find_dir, find_file, read_yaml
from data_manager.functions import filters_ordering_selected_items_exist, get_prepared_queryset
from django.conf import settings
from django.db import IntegrityError
from django.db.models import F
from django.http import Http404
from django.utils.decorators import method_decorator
from django_filters import CharFilter, FilterSet
from django_filters.rest_framework import DjangoFilterBackend
from drf_yasg.utils import swagger_auto_schema
from label_studio_sdk.label_interface.interface import LabelInterface
from ml.serializers import MLBackendSerializer
from projects.functions.next_task import get_next_task
from projects.functions.stream_history import get_label_stream_history
from projects.functions.utils import recalculate_created_annotations_and_labels_from_scratch
from projects.models import Project, ProjectImport, ProjectManager, ProjectReimport, ProjectSummary
from projects.serializers import (
    GetFieldsSerializer,
    ProjectCountsSerializer,
    ProjectImportSerializer,
    ProjectLabelConfigSerializer,
    ProjectModelVersionExtendedSerializer,
    ProjectReimportSerializer,
    ProjectSerializer,
    ProjectSummarySerializer,
)
from rest_framework import filters, generics, status
from rest_framework.exceptions import NotFound
from rest_framework.exceptions import ValidationError as RestValidationError
from rest_framework.pagination import PageNumberPagination
from rest_framework.parsers import FormParser, JSONParser, MultiPartParser
from rest_framework.permissions import AllowAny
from rest_framework.response import Response
from rest_framework.settings import api_settings
from rest_framework.views import exception_handler
from tasks.models import Task
from tasks.serializers import (
    NextTaskSerializer,
    TaskSerializer,
    TaskSimpleSerializer,
    TaskWithAnnotationsAndPredictionsAndDraftsSerializer,
)
from webhooks.models import WebhookAction
from webhooks.utils import api_webhook, api_webhook_for_delete, emit_webhooks_for_instance

from label_studio.core.utils.common import load_func

logger = logging.getLogger(__name__)

ProjectImportPermission = load_func(settings.PROJECT_IMPORT_PERMISSION)

_result_schema = openapi.Schema(
    title='Labeling result',
    description='Labeling result (choices, labels, bounding boxes, etc.)',
    type=openapi.TYPE_OBJECT,
    properties={
        'from_name': openapi.Schema(
            title='from_name',
            description='The name of the labeling tag from the project config',
            type=openapi.TYPE_STRING,
        ),
        'to_name': openapi.Schema(
            title='to_name',
            description='The name of the labeling tag from the project config',
            type=openapi.TYPE_STRING,
        ),
        'value': openapi.Schema(
            title='value',
            description='Labeling result value. Format depends on chosen ML backend',
            type=openapi.TYPE_OBJECT,
        ),
    },
    example={'from_name': 'image_class', 'to_name': 'image', 'value': {'labels': ['Cat']}},
)

_task_data_schema = openapi.Schema(
    title='Task data',
    description='Task data',
    type=openapi.TYPE_OBJECT,
    example={'id': 1, 'my_image_url': '/static/samples/kittens.jpg'},
)

_project_schema = openapi.Schema(
    title='Project',
    description='Project',
    type=openapi.TYPE_OBJECT,
    properties={
        'title': openapi.Schema(
            title='title',
            description='Project title',
            type=openapi.TYPE_STRING,
            example='My project',
        ),
        'description': openapi.Schema(
            title='description',
            description='Project description',
            type=openapi.TYPE_STRING,
            example='My first project',
        ),
        'label_config': openapi.Schema(
            title='label_config',
            description='Label config in XML format',
            type=openapi.TYPE_STRING,
            example='<View>[...]</View>',
        ),
        'expert_instruction': openapi.Schema(
            title='expert_instruction',
            description='Labeling instructions to show to the user',
            type=openapi.TYPE_STRING,
            example='Label all cats',
        ),
        'show_instruction': openapi.Schema(
            title='show_instruction',
            description='Show labeling instructions',
            type=openapi.TYPE_BOOLEAN,
        ),
        'show_skip_button': openapi.Schema(
            title='show_skip_button',
            description='Show skip button',
            type=openapi.TYPE_BOOLEAN,
        ),
        'enable_empty_annotation': openapi.Schema(
            title='enable_empty_annotation',
            description='Allow empty annotations',
            type=openapi.TYPE_BOOLEAN,
        ),
        'show_annotation_history': openapi.Schema(
            title='show_annotation_history',
            description='Show annotation history',
            type=openapi.TYPE_BOOLEAN,
        ),
        'reveal_preannotations_interactively': openapi.Schema(
            title='reveal_preannotations_interactively',
            description='Reveal preannotations interactively. If set to True, predictions will be shown to the user only after selecting the area of interest',
            type=openapi.TYPE_BOOLEAN,
        ),
        'show_collab_predictions': openapi.Schema(
            title='show_collab_predictions',
            description='Show predictions to annotators',
            type=openapi.TYPE_BOOLEAN,
        ),
        'maximum_annotations': openapi.Schema(
            title='maximum_annotations',
            description='Maximum annotations per task',
            type=openapi.TYPE_INTEGER,
        ),
        'color': openapi.Schema(
            title='color',
            description='Project color in HEX format',
            type=openapi.TYPE_STRING,
            default='#FFFFFF',
        ),
        'control_weights': openapi.Schema(
            title='control_weights',
            description='Dict of weights for each control tag in metric calculation. Each control tag (e.g. label or choice) will '
            'have its own key in control weight dict with weight for each label and overall weight. '
            'For example, if a bounding box annotation with a control tag named my_bbox should be included with 0.33 weight in agreement calculation, '
            'and the first label Car should be twice as important as Airplane, then you need to specify: '
            "{'my_bbox': {'type': 'RectangleLabels', 'labels': {'Car': 1.0, 'Airplane': 0.5}, 'overall': 0.33}",
            type=openapi.TYPE_OBJECT,
            example={
                'my_bbox': {'type': 'RectangleLabels', 'labels': {'Car': 1.0, 'Airplaine': 0.5}, 'overall': 0.33}
            },
        ),
    },
)


class ProjectListPagination(PageNumberPagination):
    page_size = 30
    page_size_query_param = 'page_size'


class ProjectFilterSet(FilterSet):
    ids = ListFilter(field_name='id', lookup_expr='in')
    title = CharFilter(field_name='title', lookup_expr='icontains')


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Projects'],
        x_fern_sdk_group_name='projects',
        x_fern_sdk_method_name='list',
        x_fern_audiences=['public'],
        x_fern_pagination={
            'offset': '$request.page',
            'results': '$response.results',
        },
        operation_summary='List your projects',
        operation_description="""
    Return a list of the projects that you've created.

    To perform most tasks with the Label Studio API, you must specify the project ID, sometimes referred to as the `pk`.
    To retrieve a list of your Label Studio projects, update the following command to match your own environment.
    Replace the domain name, port, and authorization token, then run the following from the command line:
    ```bash
    curl -X GET {}/api/projects/ -H 'Authorization: Token abc123'
    ```
    """.format(
            settings.HOSTNAME or 'https://localhost:8080'
        ),
    ),
)
@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Projects'],
        operation_summary='Create new project',
        x_fern_sdk_group_name='projects',
        x_fern_sdk_method_name='create',
        x_fern_audiences=['public'],
        operation_description="""
    Create a project and set up the labeling interface in Label Studio using the API.

    ```bash
    curl -H Content-Type:application/json -H 'Authorization: Token abc123' -X POST '{}/api/projects' \
    --data '{{"title": "My project", "label_config": "<View></View>"}}'
    ```
    """.format(
            settings.HOSTNAME or 'https://localhost:8080'
        ),
        request_body=_project_schema,
    ),
)
class ProjectListAPI(generics.ListCreateAPIView):
    parser_classes = (JSONParser, FormParser, MultiPartParser)
    serializer_class = ProjectSerializer
    filter_backends = [filters.OrderingFilter, DjangoFilterBackend]
    filterset_class = ProjectFilterSet
    permission_required = ViewClassPermission(
        GET=all_permissions.projects_view,
        POST=all_permissions.projects_create,
    )
    pagination_class = ProjectListPagination

    def get_queryset(self):
        serializer = GetFieldsSerializer(data=self.request.query_params)
        serializer.is_valid(raise_exception=True)
        fields = serializer.validated_data.get('include')
        filter = serializer.validated_data.get('filter')
        projects = Project.objects.filter(organization=self.request.user.active_organization).order_by(
            F('pinned_at').desc(nulls_last=True), '-created_at'
        )
        if filter in ['pinned_only', 'exclude_pinned']:
            projects = projects.filter(pinned_at__isnull=filter == 'exclude_pinned')
        return ProjectManager.with_counts_annotate(projects, fields=fields).prefetch_related('members', 'created_by')

    def get_serializer_context(self):
        context = super(ProjectListAPI, self).get_serializer_context()
        context['created_by'] = self.request.user
        return context

    def perform_create(self, ser):
        try:
            ser.save(organization=self.request.user.active_organization)
        except IntegrityError as e:
            if str(e) == 'UNIQUE constraint failed: project.title, project.created_by_id':
                raise ProjectExistException(
                    'Project with the same name already exists: {}'.format(ser.validated_data.get('title', ''))
                )
            raise LabelStudioDatabaseException('Database error during project creation. Try again.')

    def get(self, request, *args, **kwargs):
        return super(ProjectListAPI, self).get(request, *args, **kwargs)

    @api_webhook(WebhookAction.PROJECT_CREATED)
    def post(self, request, *args, **kwargs):
        return super(ProjectListAPI, self).post(request, *args, **kwargs)


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Projects'],
        x_fern_sdk_group_name='projects',
        x_fern_sdk_method_name='counts',
        x_fern_audiences=['public'],
        x_fern_pagination={
            'offset': '$request.page',
            'results': '$response.results',
        },
        operation_summary="List project's counts",
        operation_description='Returns a list of projects with their counts. For example, task_number which is the total task number in project',
    ),
)
class ProjectCountsListAPI(generics.ListAPIView):
    serializer_class = ProjectCountsSerializer
    filterset_class = ProjectFilterSet
    permission_required = ViewClassPermission(
        GET=all_permissions.projects_view,
    )
    pagination_class = ProjectListPagination

    def get_queryset(self):
        serializer = GetFieldsSerializer(data=self.request.query_params)
        serializer.is_valid(raise_exception=True)
        fields = serializer.validated_data.get('include')
        return Project.objects.with_counts(fields=fields).filter(organization=self.request.user.active_organization)


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Projects'],
        x_fern_sdk_group_name='projects',
        x_fern_sdk_method_name='get',
        x_fern_audiences=['public'],
        operation_summary='Get project by ID',
        operation_description='Retrieve information about a project by project ID.',
        responses={
            '200': openapi.Response(
                description='Project information',
                schema=ProjectSerializer,
                examples={
                    'application/json': {
                        'id': 1,
                        'title': 'My project',
                        'description': 'My first project',
                        'label_config': '<View>[...]</View>',
                        'expert_instruction': 'Label all cats',
                        'show_instruction': True,
                        'show_skip_button': True,
                        'enable_empty_annotation': True,
                        'show_annotation_history': True,
                        'organization': 1,
                        'color': '#FF0000',
                        'maximum_annotations': 1,
                        'is_published': True,
                        'model_version': '1.0.0',
                        'is_draft': False,
                        'created_by': {
                            'id': 1,
                            'first_name': 'Jo',
                            'last_name': 'Doe',
                            'email': 'manager@humansignal.com',
                        },
                        'created_at': '2023-08-24T14:15:22Z',
                        'min_annotations_to_start_training': 0,
                        'start_training_on_annotation_update': True,
                        'show_collab_predictions': True,
                        'num_tasks_with_annotations': 10,
                        'task_number': 100,
                        'useful_annotation_number': 10,
                        'ground_truth_number': 5,
                        'skipped_annotations_number': 0,
                        'total_annotations_number': 10,
                        'total_predictions_number': 0,
                        'sampling': 'Sequential sampling',
                        'show_ground_truth_first': True,
                        'show_overlap_first': True,
                        'overlap_cohort_percentage': 100,
                        'task_data_login': 'user',
                        'task_data_password': 'secret',
                        'control_weights': {},
                        'parsed_label_config': '{"tag": {...}}',
                        'evaluate_predictions_automatically': False,
                        'config_has_control_tags': True,
                        'skip_queue': 'REQUEUE_FOR_ME',
                        'reveal_preannotations_interactively': True,
                        'pinned_at': '2023-08-24T14:15:22Z',
                        'finished_task_number': 10,
                        'queue_total': 10,
                        'queue_done': 100,
                    }
                },
            )
        },
    ),
)
@method_decorator(
    name='delete',
    decorator=swagger_auto_schema(
        tags=['Projects'],
        x_fern_sdk_group_name='projects',
        x_fern_sdk_method_name='delete',
        x_fern_audiences=['public'],
        operation_summary='Delete project',
        operation_description='Delete a project by specified project ID.',
    ),
)
@method_decorator(
    name='patch',
    decorator=swagger_auto_schema(
        tags=['Projects'],
        x_fern_sdk_group_name='projects',
        x_fern_sdk_method_name='update',
        x_fern_audiences=['public'],
        operation_summary='Update project',
        operation_description='Update the project settings for a specific project.',
        request_body=_project_schema,
    ),
)
class ProjectAPI(generics.RetrieveUpdateDestroyAPIView):
    parser_classes = (JSONParser, FormParser, MultiPartParser)
    queryset = Project.objects.with_counts()
    permission_required = ViewClassPermission(
        GET=all_permissions.projects_view,
        DELETE=all_permissions.projects_delete,
        PATCH=all_permissions.projects_change,
        PUT=all_permissions.projects_change,
        POST=all_permissions.projects_create,
    )
    serializer_class = ProjectSerializer

    redirect_route = 'projects:project-detail'
    redirect_kwarg = 'pk'

    def get_queryset(self):
        serializer = GetFieldsSerializer(data=self.request.query_params)
        serializer.is_valid(raise_exception=True)
        fields = serializer.validated_data.get('include')
        return Project.objects.with_counts(fields=fields).filter(organization=self.request.user.active_organization)

    def get(self, request, *args, **kwargs):
        return super(ProjectAPI, self).get(request, *args, **kwargs)

    @api_webhook_for_delete(WebhookAction.PROJECT_DELETED)
    def delete(self, request, *args, **kwargs):
        return super(ProjectAPI, self).delete(request, *args, **kwargs)

    @api_webhook(WebhookAction.PROJECT_UPDATED)
    def patch(self, request, *args, **kwargs):
        project = self.get_object()
        label_config = self.request.data.get('label_config')

        # config changes can break view, so we need to reset them
        if label_config:
            try:
                _has_changes = config_essential_data_has_changed(label_config, project.label_config)
            except KeyError:
                pass

        return super(ProjectAPI, self).patch(request, *args, **kwargs)

    def perform_destroy(self, instance):
        # we don't need to relaculate counters if we delete whole project
        with temporary_disconnect_all_signals():
            instance.delete()

    @swagger_auto_schema(auto_schema=None)
    @api_webhook(WebhookAction.PROJECT_UPDATED)
    def put(self, request, *args, **kwargs):
        return super(ProjectAPI, self).put(request, *args, **kwargs)


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Projects'],
        operation_summary='Get next task to label',
        x_fern_sdk_group_name='projects',
        x_fern_sdk_method_name='next_task',
        x_fern_audiences=['public'],
        operation_description="""
    Get the next task for labeling. If you enable Machine Learning in
    your project, the response might include a "predictions"
    field. It contains a machine learning prediction result for
    this task.
    """,
        responses={200: TaskWithAnnotationsAndPredictionsAndDraftsSerializer()},
    ),
)  # leaving this method decorator info in case we put it back in swagger API docs
class ProjectNextTaskAPI(generics.RetrieveAPIView):
    permission_required = all_permissions.tasks_view
    serializer_class = TaskWithAnnotationsAndPredictionsAndDraftsSerializer  # using it for swagger API docs
    queryset = Project.objects.all()
    swagger_schema = None  # this endpoint doesn't need to be in swagger API docs

    def get(self, request, *args, **kwargs):
        project = self.get_object()
        dm_queue = filters_ordering_selected_items_exist(request.data)
        prepared_tasks = get_prepared_queryset(request, project)

        next_task, queue_info = get_next_task(request.user, prepared_tasks, project, dm_queue)

        if next_task is None:
            raise NotFound(
                f'There are still some tasks to complete for the user={request.user}, '
                f'but they seem to be locked by another user.'
            )

        # serialize task
        context = {'request': request, 'project': project, 'resolve_uri': True, 'annotations': False}
        serializer = NextTaskSerializer(next_task, context=context)
        response = serializer.data

        response['queue'] = queue_info
        return Response(response)


class LabelStreamHistoryAPI(generics.RetrieveAPIView):
    permission_required = all_permissions.tasks_view
    queryset = Project.objects.all()
    swagger_schema = None  # this endpoint doesn't need to be in swagger API docs

    def get(self, request, *args, **kwargs):
        project = self.get_object()

        history = get_label_stream_history(request.user, project)

        return Response(history)


@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Projects'],
        x_fern_audiences=['internal'],
        operation_summary='Validate label config',
        operation_description='Validate an arbitrary labeling configuration.',
        responses={204: 'Validation success'},
        request_body=ProjectLabelConfigSerializer,
    ),
)
class LabelConfigValidateAPI(generics.CreateAPIView):
    parser_classes = (JSONParser, FormParser, MultiPartParser)
    permission_classes = (AllowAny,)
    serializer_class = ProjectLabelConfigSerializer

    def post(self, request, *args, **kwargs):
        return super(LabelConfigValidateAPI, self).post(request, *args, **kwargs)

    def create(self, request, *args, **kwargs):
        serializer = self.get_serializer(data=request.data)
        try:
            serializer.is_valid(raise_exception=True)
        except RestValidationError as exc:
            context = self.get_exception_handler_context()
            response = exception_handler(exc, context)
            response = self.finalize_response(request, response)
            return response

        return Response(status=status.HTTP_204_NO_CONTENT)


@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Projects'],
        operation_id='api_projects_validate_label_config',
        operation_summary='Validate project label config',
        x_fern_sdk_group_name='projects',
        x_fern_sdk_method_name='validate_config',
        x_fern_audiences=['public'],
        operation_description="""
        Determine whether the label configuration for a specific project is valid.
        """,
        manual_parameters=[
            openapi.Parameter(
                name='id',
                type=openapi.TYPE_INTEGER,
                in_=openapi.IN_PATH,
                description='A unique integer value identifying this project.',
            ),
        ],
        request_body=ProjectLabelConfigSerializer,
    ),
)
class ProjectLabelConfigValidateAPI(generics.RetrieveAPIView):
    """Validate label config"""

    parser_classes = (JSONParser, FormParser, MultiPartParser)
    serializer_class = ProjectLabelConfigSerializer
    permission_required = all_permissions.projects_change
    queryset = Project.objects.all()

    def post(self, request, *args, **kwargs):
        project = self.get_object()
        label_config = self.request.data.get('label_config')
        if not label_config:
            raise RestValidationError('Label config is not set or is empty')

        # check new config includes meaningful changes
        has_changed = config_essential_data_has_changed(label_config, project.label_config)
        project.validate_config(label_config, strict=True)
        return Response({'config_essential_data_has_changed': has_changed}, status=status.HTTP_200_OK)

    @swagger_auto_schema(auto_schema=None)
    def get(self, request, *args, **kwargs):
        return super(ProjectLabelConfigValidateAPI, self).get(request, *args, **kwargs)


class ProjectSummaryAPI(generics.RetrieveAPIView):
    parser_classes = (JSONParser,)
    serializer_class = ProjectSummarySerializer
    permission_required = all_permissions.projects_view
    queryset = ProjectSummary.objects.all()

    @swagger_auto_schema(auto_schema=None)
    def get(self, *args, **kwargs):
        return super(ProjectSummaryAPI, self).get(*args, **kwargs)


class ProjectSummaryResetAPI(GetParentObjectMixin, generics.CreateAPIView):
    """This API is useful when we need to reset project.summary.created_labels and created_labels_drafts
    and recalculate them from scratch. It's hard to correctly follow all changes in annotation region
    labels and these fields aren't calculated properly after some time. Label config changes are not allowed
    when these changes touch any labels from these created_labels* dictionaries.
    """

    parser_classes = (JSONParser,)
    parent_queryset = Project.objects.all()
    permission_required = ViewClassPermission(
        POST=all_permissions.projects_change,
    )

    @swagger_auto_schema(auto_schema=None)
    def post(self, *args, **kwargs):
        project = self.parent_object
        summary = project.summary
        start_job_async_or_sync(
            recalculate_created_annotations_and_labels_from_scratch,
            project,
            summary,
            organization_id=self.request.user.active_organization.id,
        )
        return Response(status=status.HTTP_200_OK)


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Projects'],
        x_fern_sdk_group_name='tasks',
        x_fern_sdk_method_name='create_many_status',
        x_fern_audiences=['public'],
        operation_summary='Get project import info',
        operation_description='Return data related to async project import operation',
        manual_parameters=[
            openapi.Parameter(
                name='id',
                type=openapi.TYPE_INTEGER,
                in_=openapi.IN_PATH,
                description='A unique integer value identifying this project import.',
            ),
        ],
    ),
)
class ProjectImportAPI(generics.RetrieveAPIView):
    permission_required = all_permissions.projects_change
    permission_classes = api_settings.DEFAULT_PERMISSION_CLASSES + [ProjectImportPermission]
    parser_classes = (JSONParser,)
    serializer_class = ProjectImportSerializer
    queryset = ProjectImport.objects.all()
    lookup_url_kwarg = 'import_pk'


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Projects'],
        x_fern_audiences=['internal'],
        operation_summary='Get project reimport info',
        operation_description='Return data related to async project reimport operation',
        manual_parameters=[
            openapi.Parameter(
                name='id',
                type=openapi.TYPE_INTEGER,
                in_=openapi.IN_PATH,
                description='A unique integer value identifying this project reimport.',
            ),
        ],
    ),
)
class ProjectReimportAPI(generics.RetrieveAPIView):
    permission_required = all_permissions.projects_change
    permission_classes = api_settings.DEFAULT_PERMISSION_CLASSES + [ProjectImportPermission]
    parser_classes = (JSONParser,)
    serializer_class = ProjectReimportSerializer
    queryset = ProjectReimport.objects.all()
    lookup_url_kwarg = 'reimport_pk'


@method_decorator(
    name='delete',
    decorator=swagger_auto_schema(
        tags=['Projects'],
        x_fern_sdk_group_name='projects',
        x_fern_sdk_method_name='delete_all_tasks',
        x_fern_audiences=['public'],
        operation_summary='Delete all tasks',
        operation_description='Delete all tasks from a specific project.',
        manual_parameters=[
            openapi.Parameter(
                name='id',
                type=openapi.TYPE_INTEGER,
                in_=openapi.IN_PATH,
                description='A unique integer value identifying this project.',
            ),
        ],
        responses={204: 'Tasks deleted'},
    ),
)
@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Projects'],
        x_fern_audiences=['internal'],  # TODO: deprecate this endpoint in favor of tasks:tasks-list
        operation_summary='List project tasks',
        operation_description="""
            Retrieve a paginated list of tasks for a specific project. For example, use the following cURL command:
            ```bash
            curl -X GET {}/api/projects/{{id}}/tasks/?page=1&page_size=10 -H 'Authorization: Token abc123'
            ```
        """.format(
            settings.HOSTNAME or 'https://localhost:8080'
        ),
        manual_parameters=[
            openapi.Parameter(
                name='id',
                type=openapi.TYPE_INTEGER,
                in_=openapi.IN_PATH,
                description='A unique integer value identifying this project.',
            ),
        ]
        + paginator_help('tasks', 'Projects')['manual_parameters'],
    ),
)
class ProjectTaskListAPI(GetParentObjectMixin, generics.ListCreateAPIView, generics.DestroyAPIView):
    parser_classes = (JSONParser, FormParser)
    queryset = Task.objects.all()
    parent_queryset = Project.objects.all()
    permission_required = ViewClassPermission(
        GET=all_permissions.tasks_view,
        POST=all_permissions.tasks_change,
        DELETE=all_permissions.tasks_delete,
    )
    serializer_class = TaskSerializer
    redirect_route = 'projects:project-settings'
    redirect_kwarg = 'pk'

    def get_serializer_class(self):
        if self.request.method == 'GET':
            return TaskSimpleSerializer
        else:
            return TaskSerializer

    def filter_queryset(self, queryset):
        project = generics.get_object_or_404(Project.objects.for_user(self.request.user), pk=self.kwargs.get('pk', 0))
        # ordering is deprecated here
        tasks = Task.objects.filter(project=project).order_by('-updated_at')
        page = paginator(tasks, self.request)
        if page:
            return page
        else:
            raise Http404

    def delete(self, request, *args, **kwargs):
        project = generics.get_object_or_404(Project.objects.for_user(self.request.user), pk=self.kwargs['pk'])
        task_ids = list(Task.objects.filter(project=project).values('id'))
        Task.delete_tasks_without_signals(Task.objects.filter(project=project))
        logger.info(f'calling reset project_id={project.id} ProjectTaskListAPI.delete()')
        project.summary.reset()
        emit_webhooks_for_instance(request.user.active_organization, None, WebhookAction.TASKS_DELETED, task_ids)
        return Response(status=204)

    def get(self, *args, **kwargs):
        return super(ProjectTaskListAPI, self).get(*args, **kwargs)

    @swagger_auto_schema(auto_schema=None)
    def post(self, *args, **kwargs):
        return super(ProjectTaskListAPI, self).post(*args, **kwargs)

    def get_serializer_context(self):
        context = super(ProjectTaskListAPI, self).get_serializer_context()
        context['project'] = self.parent_object
        return context

    def perform_create(self, serializer):
        project = self.parent_object
        instance = serializer.save(project=project)
        emit_webhooks_for_instance(
            self.request.user.active_organization, project, WebhookAction.TASKS_CREATED, [instance]
        )
        return instance


def read_templates_and_groups():
    annotation_templates_dir = find_dir('annotation_templates')
    configs = []
    for config_file in pathlib.Path(annotation_templates_dir).glob('**/*.yml'):
        config = read_yaml(config_file)
        if settings.VERSION_EDITION == 'Community':
            if settings.VERSION_EDITION.lower() != config.get('type', 'community'):
                continue
        if config.get('image', '').startswith('/static') and settings.HOSTNAME:
            # if hostname set manually, create full image urls
            config['image'] = settings.HOSTNAME + config['image']
        configs.append(config)
    template_groups_file = find_file(os.path.join('annotation_templates', 'groups.txt'))
    with open(template_groups_file, encoding='utf-8') as f:
        groups = f.read().splitlines()
    logger.debug(f'{len(configs)} templates found.')
    return {'templates': configs, 'groups': groups}


class TemplateListAPI(generics.ListAPIView):
    parser_classes = (JSONParser, FormParser, MultiPartParser)
    permission_required = all_permissions.projects_view
    swagger_schema = None
    # load this once in memory for performance
    templates_and_groups = read_templates_and_groups()

    def list(self, request, *args, **kwargs):
        return Response(self.templates_and_groups)


class ProjectSampleTask(generics.RetrieveAPIView):
    parser_classes = (JSONParser,)
    queryset = Project.objects.all()
    permission_required = all_permissions.projects_view
    serializer_class = ProjectSerializer
    swagger_schema = None

    def post(self, request, *args, **kwargs):
        label_config = self.request.data.get('label_config')
        include_annotation_and_prediction = self.request.data.get('include_annotation_and_prediction', False)

        if not label_config:
            raise RestValidationError('Label config is not set or is empty')

        project = self.get_object()

        if include_annotation_and_prediction:
            try:
                label_interface = LabelInterface(label_config)
                complete_task = label_interface.generate_complete_sample_task(raise_on_failure=True)
                # set the annotation's user id to the current user instead of -1
                user_id = request.user.id
                for annotation in complete_task['annotations']:
                    annotation['completed_by'] = user_id
                return Response({'sample_task': complete_task}, status=200)
            except Exception as e:
                logger.error(
                    f'Error generating enhanced sample task, falling back to original method: {str(e)}. Label config: {label_config}'
                )
                # Fallback to project.get_sample_task if LabelInterface.generate_complete_sample_task failed
                return Response({'sample_task': project.get_sample_task(label_config)}, status=200)
        else:
            # Use the simple sample task generation method
            return Response({'sample_task': project.get_sample_task(label_config)}, status=200)


class ProjectModelVersions(generics.RetrieveAPIView):
    parser_classes = (JSONParser,)
    swagger_schema = None
    permission_required = all_permissions.projects_view
    queryset = Project.objects.all()

    def get(self, request, *args, **kwargs):
        # TODO make sure "extended" is the right word and is
        # consistent with other APIs we've got
        extended = self.request.query_params.get('extended', False)
        include_live_models = self.request.query_params.get('include_live_models', False)
        project = self.get_object()
        data = project.get_model_versions(with_counters=True, extended=extended)

        if extended:
            serializer_models = None
            serializer = ProjectModelVersionExtendedSerializer(data, many=True)

            if include_live_models:
                ml_models = project.get_ml_backends()
                serializer_models = MLBackendSerializer(ml_models, many=True)

            # serializer.is_valid(raise_exception=True)
            return Response({'static': serializer.data, 'live': serializer_models and serializer_models.data})
        else:
            return Response(data=data)

    def delete(self, request, *args, **kwargs):
        project = self.get_object()
        model_version = request.data.get('model_version', None)

        if not model_version:
            raise RestValidationError('model_version param is required')

        count = project.delete_predictions(model_version=model_version)

        return Response(data=count)
</file>

<file path="label_studio/projects/mixins.py">
from typing import TYPE_CHECKING, Mapping, Optional

from core.redis import start_job_async_or_sync
from django.db.models import QuerySet
from django.utils.functional import cached_property

if TYPE_CHECKING:
    from users.models import User


class ProjectMixin:
    def rearrange_overlap_cohort(self):
        """
        Async start rearrange overlap depending on annotation count in tasks
        """
        start_job_async_or_sync(self._rearrange_overlap_cohort)

    def update_tasks_counters_and_is_labeled(self, tasks_queryset, from_scratch=True):
        """
        Async start updating tasks counters and than is_labeled
        :param tasks_queryset: Tasks to update queryset
        :param from_scratch: Skip calculated tasks
        """
        # get only id from queryset to decrease data size in job
        if not (isinstance(tasks_queryset, set) or isinstance(tasks_queryset, list)):
            tasks_queryset = set(tasks_queryset.values_list('id', flat=True))
        start_job_async_or_sync(
            self._update_tasks_counters_and_is_labeled, list(tasks_queryset), from_scratch=from_scratch
        )

    def update_tasks_counters_and_task_states(
        self,
        tasks_queryset,
        maximum_annotations_changed,
        overlap_cohort_percentage_changed,
        tasks_number_changed,
        from_scratch=True,
        recalculate_stats_counts: Optional[Mapping[str, int]] = None,
    ):
        """
        Async start updating tasks counters and than rearrange
        :param tasks_queryset: Tasks to update queryset
        :param maximum_annotations_changed: If maximum_annotations param changed
        :param overlap_cohort_percentage_changed: If cohort_percentage param changed
        :param tasks_number_changed: If tasks number changed in project
        :param from_scratch: Skip calculated tasks
        """
        # get only id from queryset to decrease data size in job
        if not (isinstance(tasks_queryset, set) or isinstance(tasks_queryset, list)):
            tasks_queryset = set(tasks_queryset.values_list('id', flat=True))
        start_job_async_or_sync(
            self._update_tasks_counters_and_task_states,
            tasks_queryset,
            maximum_annotations_changed,
            overlap_cohort_percentage_changed,
            tasks_number_changed,
            from_scratch=from_scratch,
            recalculate_stats_counts=recalculate_stats_counts,
        )

    def update_tasks_states(
        self, maximum_annotations_changed, overlap_cohort_percentage_changed, tasks_number_changed
    ):
        """
        Async start updating tasks states after settings change
        :param maximum_annotations_changed: If maximum_annotations param changed
        :param overlap_cohort_percentage_changed: If cohort_percentage param changed
        :param tasks_number_changed: If tasks number changed in project
        """
        start_job_async_or_sync(
            self._update_tasks_states,
            maximum_annotations_changed,
            overlap_cohort_percentage_changed,
            tasks_number_changed,
        )

    def has_permission(self, user):
        """
        Dummy stub for has_permission
        """
        user.project = self  # link for activity log
        return True

    def _can_use_overlap(self):
        """
        Returns if we can use overlap for is_labeled calculation
        :return:
        """
        return True

    @cached_property
    def all_members(self) -> QuerySet['User']:
        """
        Returns all users of project
        :return: QuerySet[User]
        """
        from users.models import User

        return User.objects.filter(id__in=self.organization.members.values_list('user__id'))
</file>

<file path="label_studio/projects/models.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import json
import logging
from typing import Any, Mapping, Optional

from annoying.fields import AutoOneToOneField
from core.label_config import (
    check_control_in_config_by_regex,
    check_toname_in_config_by_regex,
    config_line_stipped,
    extract_data_types,
    get_all_control_tag_tuples,
    get_all_labels,
    get_all_object_tag_names,
    get_all_types,
    get_annotation_tuple,
    get_original_fromname_by_regex,
    get_sample_task,
    validate_label_config,
)
from core.utils.common import (
    create_hash,
    get_attr_or_item,
    load_func,
    merge_labels_counters,
)
from core.utils.db import fast_first
from django.conf import settings
from django.core.validators import MaxLengthValidator, MinLengthValidator
from django.db import models, transaction
from django.db.models import Avg, BooleanField, Case, Count, JSONField, Max, Q, Sum, Value, When
from django.utils.translation import gettext_lazy as _
from label_studio_sdk._extensions.label_studio_tools.core.label_config import parse_config
from labels_manager.models import Label
from projects.functions import (
    annotate_finished_task_number,
    annotate_ground_truth_number,
    annotate_num_tasks_with_annotations,
    annotate_skipped_annotations_number,
    annotate_task_number,
    annotate_total_annotations_number,
    annotate_total_predictions_number,
    annotate_useful_annotation_number,
)
from projects.functions.utils import make_queryset_from_iterable
from projects.signals import ProjectSignals
from rest_framework.exceptions import ValidationError
from tasks.models import (
    Annotation,
    AnnotationDraft,
    Prediction,
    Q_task_finished_annotations,
    Task,
    bulk_update_stats_project_tasks,
)

logger = logging.getLogger(__name__)


class ProjectManager(models.Manager):
    COUNTER_FIELDS = [
        'task_number',
        'finished_task_number',
        'total_predictions_number',
        'total_annotations_number',
        'num_tasks_with_annotations',
        'useful_annotation_number',
        'ground_truth_number',
        'skipped_annotations_number',
    ]

    ANNOTATED_FIELDS = {
        'task_number': annotate_task_number,
        'finished_task_number': annotate_finished_task_number,
        'total_predictions_number': annotate_total_predictions_number,
        'total_annotations_number': annotate_total_annotations_number,
        'num_tasks_with_annotations': annotate_num_tasks_with_annotations,
        'useful_annotation_number': annotate_useful_annotation_number,
        'ground_truth_number': annotate_ground_truth_number,
        'skipped_annotations_number': annotate_skipped_annotations_number,
    }

    def for_user(self, user):
        return self.filter(organization=user.active_organization)

    def with_counts(self, fields=None):
        return self.with_counts_annotate(self, fields=fields)

    @staticmethod
    def with_counts_annotate(queryset, fields=None):
        available_fields = ProjectManager.ANNOTATED_FIELDS
        if fields is None:
            to_annotate = available_fields
        else:
            to_annotate = {field: available_fields[field] for field in fields if field in available_fields}

        for _, annotate_func in to_annotate.items():  # noqa: F402
            queryset = annotate_func(queryset)

        return queryset


ProjectMixin = load_func(settings.PROJECT_MIXIN)


# LSE recalculate all stats
recalculate_all_stats = load_func(settings.RECALCULATE_ALL_STATS)


class Project(ProjectMixin, models.Model):
    class SkipQueue(models.TextChoices):
        # requeue to the end of the same annotators queue => annotator gets this task at the end of the queue
        REQUEUE_FOR_ME = 'REQUEUE_FOR_ME', 'Requeue for me'
        # requeue skipped tasks back to the common queue, excluding skipping annotator [current default] => another annotator gets this task
        REQUEUE_FOR_OTHERS = 'REQUEUE_FOR_OTHERS', 'Requeue for others'
        # ignore skipped tasks => skip is a valid annotation, task is completed (finished=True)
        IGNORE_SKIPPED = 'IGNORE_SKIPPED', 'Ignore skipped'

    objects = ProjectManager()
    __original_label_config = None

    title = models.CharField(
        _('title'),
        null=True,
        blank=True,
        default='',
        max_length=settings.PROJECT_TITLE_MAX_LEN,
        help_text=f'Project name. Must be between {settings.PROJECT_TITLE_MIN_LEN} and {settings.PROJECT_TITLE_MAX_LEN} characters long.',
        validators=[
            MinLengthValidator(settings.PROJECT_TITLE_MIN_LEN),
            MaxLengthValidator(settings.PROJECT_TITLE_MAX_LEN),
        ],
    )
    description = models.TextField(
        _('description'), blank=True, null=True, default='', help_text='Project description'
    )

    organization = models.ForeignKey(
        'organizations.Organization', on_delete=models.CASCADE, related_name='projects', null=True
    )
    label_config = models.TextField(
        _('label config'),
        blank=True,
        null=True,
        default='<View></View>',
        help_text='Label config in XML format. See more about it in documentation',
    )
    parsed_label_config = models.JSONField(
        _('parsed label config'),
        blank=True,
        null=True,
        default=None,
        help_text='Parsed label config in JSON format. See more about it in documentation',
    )
    label_config_hash = models.BigIntegerField(null=True, default=None)
    expert_instruction = models.TextField(
        _('expert instruction'), blank=True, null=True, default='', help_text='Labeling instructions in HTML format'
    )
    show_instruction = models.BooleanField(
        _('show instruction'), default=False, help_text='Show instructions to the annotator before they start'
    )

    show_skip_button = models.BooleanField(
        _('show skip button'),
        default=True,
        help_text='Show a skip button in interface and allow annotators to skip the task',
    )
    enable_empty_annotation = models.BooleanField(
        _('enable empty annotation'), default=True, help_text='Allow annotators to submit empty annotations'
    )

    reveal_preannotations_interactively = models.BooleanField(
        _('reveal_preannotations_interactively'), default=False, help_text='Reveal pre-annotations interactively'
    )
    show_annotation_history = models.BooleanField(
        _('show annotation history'), default=False, help_text='Show annotation history to annotator'
    )
    show_collab_predictions = models.BooleanField(
        _('show predictions to annotator'), default=True, help_text='If set, the annotator can view model predictions'
    )

    # evaluate is the wrong word here. correct should be retrieve_predictions_automatically
    # deprecated
    evaluate_predictions_automatically = models.BooleanField(
        _('evaluate predictions automatically'),
        default=False,
        help_text='Retrieve and display predictions when loading a task',
    )
    token = models.CharField(_('token'), max_length=256, default=create_hash, null=True, blank=True)
    result_count = models.IntegerField(
        _('result count'), default=0, help_text='Total results inside of annotations counter'
    )
    color = models.CharField(_('color'), max_length=16, default='#FFFFFF', null=True, blank=True)

    created_by = models.ForeignKey(
        settings.AUTH_USER_MODEL,
        related_name='created_projects',
        on_delete=models.SET_NULL,
        null=True,
        verbose_name=_('created by'),
    )
    maximum_annotations = models.IntegerField(
        _('maximum annotation number'),
        default=1,
        help_text='Maximum number of annotations for one task. '
        'If the number of annotations per task is equal or greater '
        'to this value, the task is completed (is_labeled=True)',
    )
    min_annotations_to_start_training = models.IntegerField(
        _('min_annotations_to_start_training'),
        default=0,
        help_text='Minimum number of completed tasks after which model training is started',
    )

    control_weights = JSONField(
        _('control weights'),
        null=True,
        default=dict,
        help_text='Dict of weights for each control tag in metric calculation. Each control tag (e.g. label or choice) will '
        "have it's own key in control weight dict with weight for each label and overall weight."
        'For example, if bounding box annotation with control tag named my_bbox should be included with 0.33 weight in agreement calculation, '
        'and the first label Car should be twice more important than Airplaine, then you have to need the specify: '
        "{'my_bbox': {'type': 'RectangleLabels', 'labels': {'Car': 1.0, 'Airplaine': 0.5}, 'overall': 0.33}",
    )

    # Welcome reader! You might be wondering how `model_version` is
    # set and used; let's explain. `model_version` can either be set
    # to the prediction `model_version` associated with the
    # `tasks.Prediction` model, or to the ML backend title. Yes,
    # understandably, this can be confusing. However, this appears to
    # be the best approach we currently have for improving the
    # experience while maintaining backward compatibility.
    model_version = models.TextField(
        _('model version'), blank=True, null=True, default='', help_text='Machine learning model version'
    )

    data_types = JSONField(_('data_types'), default=dict, null=True)

    is_draft = models.BooleanField(
        _('is draft'), default=False, help_text='Whether or not the project is in the middle of being created'
    )
    is_published = models.BooleanField(
        _('published'), default=False, help_text='Whether or not the project is published to annotators'
    )
    created_at = models.DateTimeField(_('created at'), auto_now_add=True)
    updated_at = models.DateTimeField(_('updated at'), auto_now=True)

    SEQUENCE = 'Sequential sampling'
    UNIFORM = 'Uniform sampling'
    UNCERTAINTY = 'Uncertainty sampling'

    SAMPLING_CHOICES = (
        (SEQUENCE, 'Tasks are ordered by Data manager ordering'),
        (UNIFORM, 'Tasks are chosen randomly'),
        (UNCERTAINTY, 'Tasks are chosen according to model uncertainty scores (active learning mode)'),
    )

    sampling = models.CharField(max_length=100, choices=SAMPLING_CHOICES, null=True, default=SEQUENCE)
    skip_queue = models.CharField(
        max_length=100, choices=SkipQueue.choices, null=True, default=SkipQueue.REQUEUE_FOR_OTHERS
    )
    show_ground_truth_first = models.BooleanField(_('show ground truth first'), default=False)
    show_overlap_first = models.BooleanField(_('show overlap first'), default=False)
    overlap_cohort_percentage = models.IntegerField(_('overlap_cohort_percentage'), default=100)

    task_data_login = models.CharField(
        _('task_data_login'), max_length=256, blank=True, null=True, help_text='Task data credentials: login'
    )
    task_data_password = models.CharField(
        _('task_data_password'), max_length=256, blank=True, null=True, help_text='Task data credentials: password'
    )

    pinned_at = models.DateTimeField(_('pinned at'), null=True, default=None, help_text='Pinned date and time')

    custom_task_lock_ttl = models.IntegerField(
        _('custom_task_lock_ttl'),
        null=True,
        default=None,
        help_text='Custom task lock TTL in seconds. If not set, the default value is used',
    )

    def __init__(self, *args, **kwargs):
        super(Project, self).__init__(*args, **kwargs)
        self.__original_label_config = self.label_config
        self.__maximum_annotations = self.maximum_annotations
        self.__overlap_cohort_percentage = self.overlap_cohort_percentage
        self.__skip_queue = self.skip_queue

        # TODO: once bugfix with incorrect data types in List
        # logging.warning('! Please, remove code below after patching of all projects (extract_data_types)')
        if self.label_config is not None:
            data_types = extract_data_types(self.label_config)
            if self.data_types != data_types:
                self.data_types = data_types

    @property
    def num_tasks(self):
        return self.tasks.count()

    @property
    def ml_backend(self):
        return fast_first(self.ml_backends.all())

    @property
    def should_retrieve_predictions(self):
        """Returns true if the model was set to be used"""
        if self.show_collab_predictions:
            ml = self.ml_backend
            if ml:
                return ml.title == self.model_version

        return False

    @property
    def num_annotations(self):
        return Annotation.objects.filter(project=self).count()

    @property
    def num_drafts(self):
        return AnnotationDraft.objects.filter(task__project=self).count()

    @property
    def has_predictions(self):
        return self.get_current_predictions().exists()

    @property
    def has_any_predictions(self):
        return Prediction.objects.filter(Q(project=self.id)).exists()

    @property
    def business(self):
        return self.created_by.business

    @property
    def is_private(self):
        return None

    @property
    def secure_mode(self):
        return False

    @property
    def one_object_in_label_config(self):
        return len(self.data_types) <= 1

    @property
    def get_labeled_count(self):
        return self.tasks.filter(is_labeled=True).count()

    @property
    def get_collected_count(self):
        return self.tasks.count()

    @property
    def get_total_possible_count(self):
        """
            Tasks has overlap - how many tc should be accepted
            possible count = sum [ t.overlap for t in tasks]

        :return: N int total amount of Annotations that should be submitted
        """
        if self.tasks.count() == 0:
            return 0
        return self.tasks.aggregate(Sum('overlap'))['overlap__sum']

    @property
    def get_available_for_labeling(self):
        return self.get_collected_count - self.get_labeled_count

    @property
    def need_annotators(self):
        return self.maximum_annotations - self.num_annotators

    @classmethod
    def find_by_invite_url(cls, url):
        token = url.strip('/').split('/')[-1]
        if len(token):
            return Project.objects.get(token=token)
        else:
            raise KeyError(f"Can't find Project by invite URL: {url}")

    def reset_token(self):
        self.token = create_hash()
        self.save(update_fields=['token'])

    def add_collaborator(self, user):
        created = False
        with transaction.atomic():
            try:
                ProjectMember.objects.get(user=user, project=self)
            except ProjectMember.DoesNotExist:
                ProjectMember.objects.create(user=user, project=self)
                created = True
            else:
                logger.debug(f'Project membership {self} for user {user} already exists')
        return created

    def has_collaborator(self, user):
        return ProjectMember.objects.filter(user=user, project=self).exists()

    def has_collaborator_enabled(self, user):
        membership = ProjectMember.objects.filter(user=user, project=self)
        return membership.exists() and membership.first().enabled

    def _update_tasks_states(
        self, maximum_annotations_changed, overlap_cohort_percentage_changed, tasks_number_changed
    ):
        """
        Update tasks states after settings change
        :param maximum_annotations_changed: If maximum_annotations param changed
        :param overlap_cohort_percentage_changed: If cohort_percentage param changed
        :param tasks_number_changed: If tasks number changed in project
        """
        logger.info(
            f'Starting _update_tasks_states with params: Project {str(self)} maximum_annotations '
            f'{self.maximum_annotations} and percentage {self.overlap_cohort_percentage}'
        )
        # if only maximum annotations parameter is tweaked
        if maximum_annotations_changed and (not overlap_cohort_percentage_changed or self.maximum_annotations == 1):
            tasks_with_overlap = self.tasks.filter(overlap__gt=1)
            if tasks_with_overlap.exists():
                # if there is a part with overlapped tasks, affect only them
                tasks_with_overlap.update(overlap=self.maximum_annotations)
            elif self.overlap_cohort_percentage < 100:
                self._rearrange_overlap_cohort()
            else:
                # otherwise affect all tasks
                self.tasks.update(overlap=self.maximum_annotations)
                tasks_with_overlap = self.tasks.all()
            # update is_labeled after change
            bulk_update_stats_project_tasks(tasks_with_overlap, project=self)

        # if cohort slider is tweaked
        elif overlap_cohort_percentage_changed and self.maximum_annotations > 1:
            self._rearrange_overlap_cohort()

        # if adding/deleting tasks and cohort settings are applied
        elif tasks_number_changed and self.overlap_cohort_percentage < 100 and self.maximum_annotations > 1:
            self._rearrange_overlap_cohort()

    def _rearrange_overlap_cohort(self):
        """
        Rearrange overlap depending on annotation count in tasks
        """
        all_project_tasks = Task.objects.filter(project=self)
        max_annotations = self.maximum_annotations
        must_tasks = int(self.tasks.count() * self.overlap_cohort_percentage / 100 + 0.5)
        logger.info(
            f'Starting _rearrange_overlap_cohort with params: Project {str(self)} maximum_annotations '
            f'{max_annotations} and percentage {self.overlap_cohort_percentage}'
        )
        tasks_with_max_annotations = all_project_tasks.annotate(
            anno=Count('annotations', filter=Q_task_finished_annotations & Q(annotations__ground_truth=False))
        ).filter(anno__gte=max_annotations)

        tasks_with_min_annotations = all_project_tasks.exclude(id__in=tasks_with_max_annotations)
        # check how many tasks left to finish
        left_must_tasks = max(must_tasks - tasks_with_max_annotations.count(), 0)
        logger.info(f'Required tasks {must_tasks} and left required tasks {left_must_tasks}')
        if left_must_tasks > 0:
            # if there are unfinished tasks update tasks with count(annotations) >= overlap
            ids = list(tasks_with_max_annotations.values_list('id', flat=True))
            all_project_tasks.filter(id__in=ids).update(overlap=max_annotations, is_labeled=True)
            # order other tasks by count(annotations)
            tasks_with_min_annotations = (
                tasks_with_min_annotations.annotate(anno=Count('annotations')).order_by('-anno').distinct()
            )
            # assign overlap depending on annotation count
            # assign max_annotations and update is_labeled
            ids = list(tasks_with_min_annotations[:left_must_tasks].values_list('id', flat=True))
            all_project_tasks.filter(id__in=ids).update(overlap=max_annotations)
            # assign 1 to left
            ids = list(tasks_with_min_annotations[left_must_tasks:].values_list('id', flat=True))
            min_tasks_to_update = all_project_tasks.filter(id__in=ids)
            min_tasks_to_update.update(overlap=1)
        else:
            ids = list(tasks_with_max_annotations.values_list('id', flat=True))
            all_project_tasks.filter(id__in=ids).update(overlap=max_annotations)
            ids = list(tasks_with_min_annotations.values_list('id', flat=True))
            all_project_tasks.filter(id__in=ids).update(overlap=1)
        # update is labeled after tasks rearrange overlap
        bulk_update_stats_project_tasks(all_project_tasks, project=self)

    def remove_tasks_by_file_uploads(self, file_upload_ids):
        self.tasks.filter(file_upload_id__in=file_upload_ids).delete()

    def advance_onboarding(self):
        """Move project to next onboarding step"""
        po_qs = self.steps_left.order_by('step__order')
        count = po_qs.count()

        if count:
            po = po_qs.first()
            po.finished = True
            po.save()

            return count != 1

    def created_at_prettify(self):
        return self.created_at.strftime('%d %b %Y %H:%M:%S')

    def onboarding_step_finished(self, step):
        """Mark specific step as finished"""
        pos = ProjectOnboardingSteps.objects.get(code=step)
        po = ProjectOnboarding.objects.get(project=self, step=pos)
        po.finished = True
        po.save()

        return po

    def data_types_json(self):
        return json.dumps(self.data_types)

    def available_data_keys(self):
        return sorted(list(self.data_types.keys()))

    @classmethod
    def validate_label_config(cls, config_string):
        validate_label_config(config_string)

    def validate_config(self, config_string, strict=False):
        self.validate_label_config(config_string)
        if not hasattr(self, 'summary'):
            return

        with transaction.atomic():
            # Lock summary for update to avoid race conditions
            summary = ProjectSummary.objects.select_for_update().get(project=self)

            if self.num_tasks == 0:
                logger.debug(f'Project {self} has no tasks: nothing to validate here. Ensure project summary is empty')
                logger.info(f'calling reset project_id={self.id} validate_config() num_tasks={self.num_tasks}')
                summary.reset()
                return

            # validate data columns consistency
            fields_from_config = get_all_object_tag_names(config_string)
            if not fields_from_config:
                logger.debug('Data fields not found in labeling config')
                return

            # TODO: DEV-2939 Add validation for fields addition in label config
            """fields_from_config = {field.split('[')[0] for field in fields_from_config}  # Repeater tag support
            fields_from_data = set(self.summary.common_data_columns)
            fields_from_data.discard(settings.DATA_UNDEFINED_NAME)
            if fields_from_data and not fields_from_config.issubset(fields_from_data):
                different_fields = list(fields_from_config.difference(fields_from_data))
                raise ValidationError(
                    f'These fields are not present in the data: {",".join(different_fields)}'
                )"""

            if self.num_annotations == 0 and self.num_drafts == 0:
                logger.debug(
                    f'Project {self} has no annotations and drafts: nothing to validate here. '
                    f'Ensure annotations-related project summary is empty'
                )
                logger.info(
                    f'calling reset project_id={self.id} validate_config() num_annotations={self.num_annotations} num_drafts={self.num_drafts}'
                )
                summary.reset(tasks_data_based=False)
                return

        # validate annotations consistency
        annotations_from_config = set(get_all_control_tag_tuples(config_string))
        if not annotations_from_config:
            logger.debug('Annotation schema is not found in config')
            return
        annotations_from_data = set(self.summary.created_annotations)
        if annotations_from_data and not annotations_from_data.issubset(annotations_from_config):
            different_annotations = list(annotations_from_data.difference(annotations_from_config))
            diff_str = []
            for ann_tuple in different_annotations:
                from_name, to_name, t = ann_tuple.split('|')
                if t.lower() == 'textarea':  # avoid textarea to_name check (see DEV-1598)
                    continue
                if (
                    not check_control_in_config_by_regex(config_string, from_name)
                    or not check_toname_in_config_by_regex(config_string, to_name)
                    or t not in get_all_types(config_string)
                ):
                    diff_str.append(
                        f'{self.summary.created_annotations[ann_tuple]} '
                        f'with from_name={from_name}, to_name={to_name}, type={t}'
                    )
            if len(diff_str) > 0:
                diff_str = '\n'.join(diff_str)
                raise ValidationError(
                    f'Created annotations are incompatible with provided labeling schema, we found:\n{diff_str}'
                )

        # validate labels consistency
        labels_from_config, dynamic_label_from_config = get_all_labels(config_string)
        created_labels = merge_labels_counters(self.summary.created_labels, self.summary.created_labels_drafts)

        def display_count(count: int, type: str) -> Optional[str]:
            """Helper for displaying pluralized sources of validation errors,
            eg "1 draft" or "3 annotations"
            """
            if not count:
                return None
            return f'{count} {type}{"s" if count > 1 else ""}'

        for control_tag_from_data, labels_from_data in created_labels.items():
            # Check if labels created in annotations, and their control tag has been removed
            if (
                labels_from_data
                and (
                    (control_tag_from_data not in labels_from_config)
                    and (control_tag_from_data not in dynamic_label_from_config)
                )
                and not check_control_in_config_by_regex(config_string, control_tag_from_data)
            ):
                raise ValidationError(
                    f'There are {sum(labels_from_data.values(), 0)} annotation(s) created with tag '
                    f'"{control_tag_from_data}", you can\'t remove it'
                )
            labels_from_config_by_tag = set(
                labels_from_config[get_original_fromname_by_regex(config_string, control_tag_from_data)]
            )
            parsed_config = parse_config(config_string)
            tag_types = [tag_info['type'] for _, tag_info in parsed_config.items()]
            # DEV-1990 Workaround for Video labels as there are no labels in VideoRectangle tag
            if 'VideoRectangle' in tag_types:
                for key in labels_from_config:
                    labels_from_config_by_tag |= set(labels_from_config[key])
            if 'Taxonomy' in tag_types:
                custom_tags = Label.objects.filter(links__project=self).values_list('value', flat=True)
                flat_custom_tags = set([item for sublist in custom_tags for item in sublist])
                labels_from_config_by_tag |= flat_custom_tags
            # check if labels from is subset if config labels
            if not set(labels_from_data).issubset(set(labels_from_config_by_tag)):
                different_labels = list(set(labels_from_data).difference(labels_from_config_by_tag))
                diff_str = ''
                for label in different_labels:
                    annotation_label_count = self.summary.created_labels.get(control_tag_from_data, {}).get(label, 0)
                    draft_label_count = self.summary.created_labels_drafts.get(control_tag_from_data, {}).get(label, 0)
                    annotation_display_count = display_count(annotation_label_count, 'annotation')
                    draft_display_count = display_count(draft_label_count, 'draft')

                    display = [disp for disp in [annotation_display_count, draft_display_count] if disp]
                    if display:
                        diff_str += f'{label} ({", ".join(display)})\n'

                if (strict is True) and (
                    (control_tag_from_data not in dynamic_label_from_config)
                    and (
                        not check_control_in_config_by_regex(
                            config_string, control_tag_from_data, filter=dynamic_label_from_config.keys()
                        )
                    )
                ):
                    # raise error if labels not dynamic and not in regex rules
                    raise ValidationError(
                        f'These labels still exist in annotations or drafts:\n{diff_str}'
                        f'Please add labels to tag with name="{str(control_tag_from_data)}".'
                    )
                else:
                    logger.info(f'project_id={self.id} inconsistent labels in config and annotations: {diff_str}')

    def _label_config_has_changed(self):
        return self.label_config != self.__original_label_config

    @property
    def label_config_is_not_default(self):
        return self.label_config != Project._meta.get_field('label_config').default

    def should_none_model_version(self, model_version):
        """
        Returns True if the model version provided matches the object's model version,
        or no model version is set for the object but model version exists in ML backend.
        """
        return self.model_version == model_version or self.ml_backend_in_model_version

    def delete_predictions(self, model_version=None):
        """
        Deletes the predictions based on the provided model version.
        If no model version is provided, it deletes all the predictions for this project.

        :param model_version: Identifier of the model version (default is None)
        :type model_version: str, optional
        :return: Dictionary with count of deleted predictions
        :rtype: dict
        """
        params = {'project': self}

        if model_version:
            params.update({'model_version': model_version})

        predictions = Prediction.objects.filter(**params)

        with transaction.atomic():
            # If we are deleting specific model_version then we need
            # to remove that from the project
            if self.should_none_model_version(model_version):
                self.model_version = None
                self.save(update_fields=['model_version'])

            _, deleted_map = predictions.delete()

        count = deleted_map.get('tasks.Prediction', 0)
        return {'deleted_predictions': count}

    def get_updated_weights(self):
        outputs = self.get_parsed_config()
        control_weights = {}
        exclude_control_types = ('Filter',)

        def get_label(label):
            label_value = self.control_weights.get(control_name, {}).get('labels', {}).get(label)
            return label_value if label_value is not None else 1.0

        def get_overall(name):
            weights = self.control_weights.get(name, None)
            if not weights:
                return 1.0
            else:
                weight = weights.get('overall', None)
                return weight if weight is not None else 1.0

        for control_name in outputs:
            control_type = outputs[control_name]['type']
            if control_type in exclude_control_types:
                continue

            control_weights[control_name] = {
                'overall': get_overall(control_name),
                'type': control_type,
                'labels': {label: get_label(label) for label in outputs[control_name].get('labels', [])},
            }
        return control_weights

    def save(self, *args, update_fields=None, recalc=True, **kwargs):
        exists = True if self.pk else False
        project_with_config_just_created = not exists and self.label_config

        label_config_has_changed = self._label_config_has_changed()
        logger.debug(
            f'Label config has changed: {label_config_has_changed}, original: {self.__original_label_config}, new: {self.label_config}'
        )

        if label_config_has_changed or project_with_config_just_created:
            self.data_types = extract_data_types(self.label_config)
            self.parsed_label_config = parse_config(self.label_config)
            self.label_config_hash = hash(str(self.parsed_label_config))
            if update_fields is not None:
                update_fields = {'data_types', 'parsed_label_config', 'label_config_hash'}.union(update_fields)

        if self.label_config and (self._label_config_has_changed() or not exists or not self.control_weights):
            self.control_weights = self.get_updated_weights()
            if update_fields is not None:
                update_fields = {'control_weights'}.union(update_fields)

        super(Project, self).save(*args, update_fields=update_fields, **kwargs)

        if label_config_has_changed:
            # save the new label config for future comparison
            self.__original_label_config = self.label_config
            # if tasks are already imported, emit signal that project is configured and ready for labeling
            if self.num_tasks > 0:
                logger.debug(f'Sending post_label_config_and_import_tasks signal for project {self.id}')
                ProjectSignals.post_label_config_and_import_tasks.send(sender=Project, project=self)
            else:
                logger.debug(
                    f'No tasks imported for project {self.id}, skipping post_label_config_and_import_tasks signal'
                )

        if not exists:
            steps = ProjectOnboardingSteps.objects.all()
            objs = [ProjectOnboarding(project=self, step=step) for step in steps]
            ProjectOnboarding.objects.bulk_create(objs)

        # argument for recalculate project task stats
        if recalc:
            self.update_tasks_states(
                maximum_annotations_changed=self.__maximum_annotations != self.maximum_annotations,
                overlap_cohort_percentage_changed=self.__overlap_cohort_percentage != self.overlap_cohort_percentage,
                tasks_number_changed=False,
            )
            self.__maximum_annotations = self.maximum_annotations
            self.__overlap_cohort_percentage = self.overlap_cohort_percentage

        if self.__skip_queue != self.skip_queue:
            bulk_update_stats_project_tasks(
                self.tasks.filter(Q(annotations__isnull=False) & Q(annotations__ground_truth=False))
            )

        if hasattr(self, 'summary'):
            with transaction.atomic():
                # Lock summary for update to avoid race conditions
                summary = ProjectSummary.objects.select_for_update().get(project=self)
                # Ensure project.summary is consistent with current tasks / annotations
                if self.num_tasks == 0:
                    logger.info(f'calling reset project_id={self.id} Project.save() num_tasks={self.num_tasks}')
                    summary.reset()
                elif self.num_annotations == 0 and self.num_drafts == 0:
                    logger.info(
                        f'calling reset project_id={self.id} Project.save() num_annotations={self.num_annotations} num_drafts={self.num_drafts}'
                    )
                    summary.reset(tasks_data_based=False)

    def get_member_ids(self):
        if hasattr(self, 'team_link'):
            # project has defined team scope
            # TODO: avoid checking team but rather add all project members when creating a project
            return self.team_link.team.members.values_list('user', flat=True)
        else:
            from users.models import User

            # TODO: may want to return all users from organization
            return User.objects.none()

    def has_team_user(self, user):
        return hasattr(self, 'team_link') and self.team_link.team.has_user(user)

    def annotators(self):
        """Annotators connected to this project including team members"""
        from users.models import User

        member_ids = self.get_member_ids()
        team_members = User.objects.filter(id__in=member_ids).order_by('email')

        # add members from invited projects
        project_member_ids = self.members.values_list('user__id', flat=True)
        project_members = User.objects.filter(id__in=project_member_ids)

        annotators = team_members | project_members

        # set annotator.team_member=True if annotator is not an invited user
        annotators = annotators.annotate(
            team_member=Case(
                When(id__in=project_member_ids, then=Value(False)),
                default=Value(True),
                output_field=BooleanField(),
            )
        )
        return annotators

    def annotators_with_annotations(self, min_count=500):
        """Annotators with annotation number > min_number

        :param min_count: minimal annotation number to leave an annotators
        :return: filtered annotators
        """
        annotators = self.annotators()
        q = Q(annotations__project=self) & Q_task_finished_annotations & Q(annotations__ground_truth=False)
        annotators = annotators.annotate(annotation_count=Count('annotations', filter=q, distinct=True))
        return annotators.filter(annotation_count__gte=min_count)

    def labeled_tasks(self):
        return self.tasks.filter(is_labeled=True)

    def has_annotations(self):
        from tasks.models import Annotation  # prevent cycling imports

        return Annotation.objects.filter(Q(project=self) & Q(ground_truth=False)).count() > 0

    # [TODO] this should be a template tag or something like this
    @property
    def label_config_line(self):
        c = self.label_config
        return config_line_stipped(c)

    def get_sample_task(self, label_config=None):
        config = label_config or self.label_config
        task, _, _ = get_sample_task(config)
        return task

    def eta(self):
        """
            Show eta for project to be finished
            eta = avg task annotations finish time * remain annotations

            task has overlap = amount of task annotations to consider as finished (is_labeled)
            remain annotations = sum ( task annotations to be done to fulfill each unfinished task overlap)

        :return: time in seconds
        """
        # finished tasks * overlap
        finished_tasks = Task.objects.filter(project=self.id, is_labeled=True)
        # one could make more than need to overlap
        min_n_finished_annotations = sum([ft.overlap for ft in finished_tasks])

        annotations_unfinished_tasks = Annotation.objects.filter(
            project=self.id, task__is_labeled=False, ground_truth=False, result__isnull=False
        ).count()

        # get minimum remain annotations
        total_annotations_needed = self.get_total_possible_count
        annotations_remain = total_annotations_needed - min_n_finished_annotations - annotations_unfinished_tasks

        # get average time of all finished TC
        finished_annotations = Annotation.objects.filter(
            Q(project=self.id) & Q(ground_truth=False), result__isnull=False
        ).values('lead_time')
        avg_lead_time = finished_annotations.aggregate(avg_lead_time=Avg('lead_time'))['avg_lead_time']

        if avg_lead_time is None:
            return None
        return avg_lead_time * annotations_remain

    def finished(self):
        return not self.tasks.filter(is_labeled=False).exists()

    def annotations_lead_time(self):
        annotations = Annotation.objects.filter(Q(project=self.id) & Q(ground_truth=False))
        return annotations.aggregate(avg_lead_time=Avg('lead_time'))['avg_lead_time']

    @staticmethod
    def django_settings():
        return settings

    @staticmethod
    def max_tasks_file_size():
        return settings.TASKS_MAX_FILE_SIZE

    def get_parsed_config(self):
        if self.parsed_label_config is None:
            try:
                self.parsed_label_config = parse_config(self.label_config)
                self.save(update_fields=['parsed_label_config'])
            except Exception as e:
                logger.error(f'Error parsing label config for project {self.id}: {e}', exc_info=True)
                return {}

        return self.parsed_label_config

    def get_counters(self):
        """Method to get extra counters data from Manager method with_counts()"""
        result = {}
        for field in ProjectManager.COUNTER_FIELDS:
            value = getattr(self, field, None)
            if value is not None:
                result[field] = value
        return result

    def get_model_versions(self, with_counters=False, extended=False):
        """
        Get model_versions from project predictions.
        :param with_counters: Boolean, if True, counts predictions for each version. Default is False.
        :param extended: Boolean, if True, returns additional information. Default is False.
        :return: Dict or list containing model versions and their count predictions.
        """
        predictions = Prediction.objects.filter(project=self)

        if extended:
            model_versions = list(
                predictions.values('model_version').annotate(count=Count('model_version'), latest=Max('created_at'))
            )

            # remove the load from the DB side and sort in here
            model_versions.sort(key=lambda x: x['latest'], reverse=True)

            return model_versions
        else:
            # TODO this needs to be removed at some point
            model_versions = predictions.values('model_version').annotate(count=Count('model_version'))
            output = {r['model_version']: r['count'] for r in model_versions}

            # Ensure that self.model_version exists in output
            if self.model_version and self.model_version not in output:
                output[self.model_version] = 0

            # Return as per requirement
            return output if with_counters else list(output.keys())

    def get_ml_backends(self, *args, **kwargs):
        from ml.models import MLBackend

        return MLBackend.objects.filter(project=self, **kwargs)

    def has_ml_backend(self, *args, **kwargs):
        return self.get_ml_backends(**kwargs).exists()

    @property
    def ml_backend_in_model_version(self):
        """
        Returns True if the ml_backend title matches this model version.
        If this model version is not set, Returns False
        """
        return bool(self.model_version and self.has_ml_backend(title=self.model_version))

    def update_ml_backends_state(self):
        """
        Updates the state of all ml_backends associated with this instance.

        :return: List of updated MLBackend instances.
        """
        ml_backends = self.get_ml_backends()
        for mlb in ml_backends:
            mlb.update_state()

        return ml_backends

    def get_active_ml_backends(self):
        from ml.models import MLBackendState

        return self.get_ml_backends(state=MLBackendState.CONNECTED)

    def get_all_storage_objects(self, type_='import'):
        from io_storages.models import get_storage_classes

        if hasattr(self, '_storage_objects'):
            return self._storage_objects

        storage_objects = []
        for storage_class in get_storage_classes(type_):
            storage_objects += list(storage_class.objects.filter(project=self))

        self._storage_objects = storage_objects
        return storage_objects

    def resolve_storage_uri(self, url: str) -> Optional[Mapping[str, Any]]:
        from io_storages.functions import get_storage_by_url

        storage_objects = self.get_all_storage_objects()
        storage = get_storage_by_url(url, storage_objects)

        if storage:
            return {
                'url': storage.generate_http_url(url),
                'presign_ttl': storage.presign_ttl,
            }

    def _update_tasks_counters_and_is_labeled(self, task_ids, from_scratch=True):
        """
        Update tasks counters and is_labeled in batches of size settings.BATCH_SIZE.
        :param task_ids: List of task ids to be updated
        :param from_scratch: Skip calculated tasks
        :return: Count of updated tasks
        """
        from tasks.functions import update_tasks_counters

        num_tasks_updated = 0
        page_idx = 0

        while task_ids_slice := task_ids[page_idx * settings.BATCH_SIZE : (page_idx + 1) * settings.BATCH_SIZE]:
            with transaction.atomic():
                # If counters are updated, is_labeled must be updated as well. Hence, if either fails, we
                # will roll back.
                queryset = make_queryset_from_iterable(task_ids_slice)
                num_tasks_updated += update_tasks_counters(queryset, from_scratch)
                bulk_update_stats_project_tasks(queryset, self)
            page_idx += 1
        return num_tasks_updated

    def _update_tasks_counters_and_task_states(
        self,
        queryset,
        maximum_annotations_changed,
        overlap_cohort_percentage_changed,
        tasks_number_changed,
        from_scratch=True,
        recalculate_stats_counts: Optional[Mapping[str, int]] = None,
    ):
        """
        Update tasks counters and update tasks states (rearrange and\or is_labeled)
        :param queryset: Tasks to update queryset
        :param from_scratch: Skip calculated tasks
        :return: Count of updated tasks
        """
        from tasks.functions import update_tasks_counters

        queryset = make_queryset_from_iterable(queryset)
        objs = update_tasks_counters(queryset, from_scratch)
        self._update_tasks_states(maximum_annotations_changed, overlap_cohort_percentage_changed, tasks_number_changed)

        if recalculate_all_stats and recalculate_stats_counts:
            recalculate_all_stats(self.id, **recalculate_stats_counts)

        return objs

    def __str__(self):
        return f'{self.title} (id={self.id})' or _('Business number %d') % self.pk

    class Meta:
        db_table = 'project'
        indexes = [
            models.Index(fields=['pinned_at', 'created_at']),
        ]


class ProjectOnboardingSteps(models.Model):
    """ """

    DATA_UPLOAD = 'DU'
    CONF_SETTINGS = 'CF'
    PUBLISH = 'PB'
    INVITE_EXPERTS = 'IE'

    STEPS_CHOICES = (
        (DATA_UPLOAD, 'Import your data'),
        (CONF_SETTINGS, 'Configure settings'),
        (PUBLISH, 'Publish project'),
        (INVITE_EXPERTS, 'Invite collaborators'),
    )

    code = models.CharField(max_length=2, choices=STEPS_CHOICES, null=True)

    title = models.CharField(_('title'), max_length=1000, null=False)
    description = models.TextField(_('description'), null=False)
    order = models.IntegerField(default=0)

    created_at = models.DateTimeField(_('created at'), auto_now_add=True)
    updated_at = models.DateTimeField(_('updated at'), auto_now=True)

    class Meta:
        ordering = ['order']


class ProjectOnboarding(models.Model):
    """ """

    step = models.ForeignKey(ProjectOnboardingSteps, on_delete=models.CASCADE, related_name='po_through')
    project = models.ForeignKey(Project, on_delete=models.CASCADE)

    finished = models.BooleanField(default=False)

    created_at = models.DateTimeField(_('created at'), auto_now_add=True)
    updated_at = models.DateTimeField(_('updated at'), auto_now=True)

    def save(self, *args, **kwargs):
        super(ProjectOnboarding, self).save(*args, **kwargs)
        if ProjectOnboarding.objects.filter(project=self.project, finished=True).count() == 4:
            self.project.skip_onboarding = True
            self.project.save(recalc=False)


class LabelStreamHistory(models.Model):

    user = models.ForeignKey(
        settings.AUTH_USER_MODEL, on_delete=models.CASCADE, related_name='histories', help_text='User ID'
    )
    project = models.ForeignKey(Project, on_delete=models.CASCADE, related_name='histories', help_text='Project ID')
    data = models.JSONField(default=list)

    class Meta:
        constraints = [models.UniqueConstraint(fields=['user', 'project'], name='unique_history')]


class ProjectMember(models.Model):

    user = models.ForeignKey(
        settings.AUTH_USER_MODEL, on_delete=models.CASCADE, related_name='project_memberships', help_text='User ID'
    )
    project = models.ForeignKey(Project, on_delete=models.CASCADE, related_name='members', help_text='Project ID')
    enabled = models.BooleanField(default=True, help_text='Project member is enabled')
    created_at = models.DateTimeField(_('created at'), auto_now_add=True)
    updated_at = models.DateTimeField(_('updated at'), auto_now=True)


class ProjectSummary(models.Model):

    project = AutoOneToOneField(Project, primary_key=True, on_delete=models.CASCADE, related_name='summary')
    created_at = models.DateTimeField(_('created at'), auto_now_add=True, help_text='Creation time')

    # { col1: task_count_with_col1, col2: task_count_with_col2 }
    all_data_columns = JSONField(
        _('all data columns'), null=True, default=dict, help_text='All data columns found in imported tasks'
    )
    # [col1, col2]
    common_data_columns = JSONField(
        _('common data columns'), null=True, default=list, help_text='Common data columns found across imported tasks'
    )
    # { (from_name, to_name, type): annotation_count }
    created_annotations = JSONField(
        _('created annotations'),
        null=True,
        default=dict,
        help_text='Unique annotation types identified by tuple (from_name, to_name, type)',
    )
    # { from_name: {label1: task_count_with_label1, label2: task_count_with_label2} }
    created_labels = JSONField(_('created labels'), null=True, default=dict, help_text='Unique labels')
    created_labels_drafts = JSONField(
        _('created labels in drafts'), null=True, default=dict, help_text='Unique drafts labels'
    )

    def has_permission(self, user):
        user.project = self.project  # link for activity log
        return self.project.has_permission(user)

    def reset(self, tasks_data_based=True):
        import traceback

        logger.info(
            f'reset summary project_id={self.project_id} {tasks_data_based=} {self.all_data_columns=} {traceback.format_stack(limit=4)=}'
        )
        if tasks_data_based:
            self.all_data_columns = {}
            self.common_data_columns = []
        self.created_annotations = {}
        self.created_labels = {}
        self.created_labels_drafts = {}
        self.save()

    def update_data_columns(self, tasks):
        common_data_columns = set()
        all_data_columns = dict(self.all_data_columns)
        for task in tasks:
            try:
                task_data = get_attr_or_item(task, 'data')
            except KeyError:
                task_data = task
            task_data_keys = task_data.keys()
            for column in task_data_keys:
                all_data_columns[column] = all_data_columns.get(column, 0) + 1
            if not common_data_columns:
                common_data_columns = set(task_data_keys)
            else:
                common_data_columns &= set(task_data_keys)

        self.all_data_columns = all_data_columns
        if not self.common_data_columns:
            self.common_data_columns = list(sorted(common_data_columns))
        else:
            self.common_data_columns = list(sorted(set(self.common_data_columns) & common_data_columns))
        logger.info(f'update summary.all_data_columns project_id={self.project_id} {self.all_data_columns=}')
        logger.info(f'update summary.common_data_columns project_id={self.project_id} {self.common_data_columns=}')
        self.save(update_fields=['all_data_columns', 'common_data_columns'])

    def remove_data_columns(self, tasks):
        all_data_columns = dict(self.all_data_columns)
        keys_to_remove = []

        for task in tasks:
            task_data = get_attr_or_item(task, 'data')
            for key in task_data.keys():
                if key in all_data_columns:
                    all_data_columns[key] -= 1
                    if all_data_columns[key] == 0:
                        keys_to_remove.append(key)
                        all_data_columns.pop(key)
        self.all_data_columns = all_data_columns

        if keys_to_remove:
            common_data_columns = list(self.common_data_columns)
            for key in keys_to_remove:
                if key in common_data_columns:
                    common_data_columns.remove(key)
            self.common_data_columns = common_data_columns
        logger.info(f'remove summary.all_data_columns project_id={self.project_id} {self.all_data_columns=}')
        logger.info(f'remove summary.common_data_columns project_id={self.project_id} {self.common_data_columns=}')
        self.save(
            update_fields=[
                'all_data_columns',
                'common_data_columns',
            ]
        )

    def _get_annotation_key(self, result):
        result_type = result.get('type', None)
        if result_type in ('relation', 'pairwise', None):
            return None
        if 'from_name' not in result or 'to_name' not in result:
            logger.error(
                'Unexpected annotation.result format: "from_name" or "to_name" not found',
                extra={'sentry_skip': True},
            )
            return None
        result_from_name = result['from_name']
        key = get_annotation_tuple(result_from_name, result['to_name'], result_type or '')
        return key

    def _get_labels(self, result):
        result_type = result.get('type')
        # DEV-1990 Workaround for Video labels as there are no labels in VideoRectangle tag
        if result_type in ['videorectangle']:
            result_type = 'labels'
        result_value = result['value'].get(result_type)
        if not result_value or not isinstance(result_value, list) or result_type == 'text':
            # Non-list values are not labels. TextArea list values (texts) are not labels too.
            return []
        # Labels are stored in list
        labels = []
        for label in result_value:
            if result_type == 'taxonomy' and isinstance(label, list):
                for label_ in label:
                    labels.append(str(label_))
            else:
                labels.append(str(label))
        return labels

    def update_created_annotations_and_labels(self, annotations):
        created_annotations = dict(self.created_annotations)
        labels = dict(self.created_labels)
        for annotation in annotations:
            results = get_attr_or_item(annotation, 'result') or []
            if not isinstance(results, list):
                continue

            for result in results:
                # aggregate annotation types
                key = self._get_annotation_key(result)
                if not key:
                    continue
                created_annotations[key] = created_annotations.get(key, 0) + 1
                from_name = result['from_name']

                # aggregate labels
                if from_name not in self.created_labels:
                    labels[from_name] = dict()

                for label in self._get_labels(result):
                    labels[from_name][label] = labels[from_name].get(label, 0) + 1

        logger.debug(f'summary.created_annotations = {created_annotations}')
        logger.debug(f'summary.created_labels = {labels}')
        self.created_annotations = created_annotations
        self.created_labels = labels
        self.save(update_fields=['created_annotations', 'created_labels'])

    def remove_created_annotations_and_labels(self, annotations):
        # we are going to remove all annotations, so we'll reset the corresponding fields on the summary
        remove_all_annotations = self.project.annotations.count() == len(annotations)
        created_annotations, created_labels = (
            ({}, {}) if remove_all_annotations else (dict(self.created_annotations), dict(self.created_labels))
        )

        if not remove_all_annotations:
            for annotation in annotations:
                results = get_attr_or_item(annotation, 'result') or []
                if not isinstance(results, list):
                    continue

                for result in results:
                    # reduce annotation counters
                    key = self._get_annotation_key(result)
                    if key in created_annotations:
                        created_annotations[key] -= 1
                        if created_annotations[key] == 0:
                            created_annotations.pop(key)

                    # reduce labels counters
                    from_name = result.get('from_name', None)
                    if from_name not in created_labels:
                        continue
                    for label in self._get_labels(result):
                        label = str(label)
                        if label in created_labels[from_name]:
                            created_labels[from_name][label] -= 1
                            if created_labels[from_name][label] == 0:
                                created_labels[from_name].pop(label)
                    if not created_labels[from_name]:
                        created_labels.pop(from_name)

        logger.debug(f'summary.created_annotations = {created_annotations}')
        logger.debug(f'summary.created_labels = {created_labels}')
        self.created_annotations = created_annotations
        self.created_labels = created_labels
        self.save(update_fields=['created_annotations', 'created_labels'])

    def update_created_labels_drafts(self, drafts):
        labels = dict(self.created_labels_drafts)
        for draft in drafts:
            results = get_attr_or_item(draft, 'result') or []
            if not isinstance(results, list):
                continue

            for result in results:
                if 'from_name' not in result:
                    continue
                from_name = result['from_name']

                # aggregate labels
                if from_name not in self.created_labels_drafts:
                    labels[from_name] = dict()

                for label in self._get_labels(result):
                    labels[from_name][label] = labels[from_name].get(label, 0) + 1

        logger.debug(f'update summary.created_labels_drafts = {labels}')
        self.created_labels_drafts = labels
        self.save(update_fields=['created_labels_drafts'])

    def remove_created_drafts_and_labels(self, drafts):
        # we are going to remove all drafts, so we'll reset the corresponding field on the summary
        remove_all_drafts = AnnotationDraft.objects.filter(task__project=self.project).count() == len(drafts)
        labels = {} if remove_all_drafts else dict(self.created_labels_drafts)

        if not remove_all_drafts:
            for draft in drafts:
                results = get_attr_or_item(draft, 'result') or []
                if not isinstance(results, list):
                    continue

                for result in results:
                    # reduce labels counters
                    from_name = result.get('from_name', None)
                    if from_name not in labels:
                        continue
                    for label in self._get_labels(result):
                        label = str(label)
                        if label in labels[from_name]:
                            labels[from_name][label] -= 1
                            if labels[from_name][label] == 0:
                                labels[from_name].pop(label)
                    if not labels[from_name]:
                        labels.pop(from_name)
        logger.debug(f'summary.created_labels_drafts = {labels}')
        self.created_labels_drafts = labels
        self.save(update_fields=['created_labels_drafts'])


class ProjectImport(models.Model):
    class Status(models.TextChoices):
        CREATED = 'created', _('Created')
        IN_PROGRESS = 'in_progress', _('In progress')
        FAILED = 'failed', _('Failed')
        COMPLETED = 'completed', _('Completed')

    project = models.ForeignKey('projects.Project', null=True, related_name='imports', on_delete=models.CASCADE)
    preannotated_from_fields = models.JSONField(null=True, blank=True)
    commit_to_project = models.BooleanField(default=False)
    return_task_ids = models.BooleanField(default=False)
    status = models.CharField(max_length=64, choices=Status.choices, default=Status.CREATED)
    url = models.CharField(max_length=2048, null=True, blank=True)
    traceback = models.TextField(null=True, blank=True)
    error = models.TextField(null=True, blank=True)
    created_at = models.DateTimeField(_('created at'), null=True, auto_now_add=True, help_text='Creation time')
    updated_at = models.DateTimeField(_('updated at'), null=True, auto_now_add=True, help_text='Updated time')
    finished_at = models.DateTimeField(_('finished at'), help_text='Complete or fail time', null=True, default=None)
    task_count = models.IntegerField(default=0)
    annotation_count = models.IntegerField(default=0)
    prediction_count = models.IntegerField(default=0)
    duration = models.IntegerField(default=0)
    file_upload_ids = models.JSONField(default=list)
    could_be_tasks_list = models.BooleanField(default=False)
    found_formats = models.JSONField(default=list)
    data_columns = models.JSONField(default=list)
    tasks = models.JSONField(blank=True, null=True)
    task_ids = models.JSONField(default=list)

    def has_permission(self, user):
        return self.project.has_permission(user)


class ProjectReimport(models.Model):
    class Status(models.TextChoices):
        CREATED = 'created', _('Created')
        IN_PROGRESS = 'in_progress', _('In progress')
        FAILED = 'failed', _('Failed')
        COMPLETED = 'completed', _('Completed')

    project = models.ForeignKey('projects.Project', null=True, related_name='reimports', on_delete=models.CASCADE)
    status = models.CharField(max_length=64, choices=Status.choices, default=Status.CREATED)
    error = models.TextField(null=True, blank=True)
    task_count = models.IntegerField(default=0)
    annotation_count = models.IntegerField(default=0)
    prediction_count = models.IntegerField(default=0)
    duration = models.IntegerField(default=0)
    file_upload_ids = models.JSONField(default=list)
    files_as_tasks_list = models.BooleanField(default=False)
    found_formats = models.JSONField(default=list)
    data_columns = models.JSONField(default=list)
    traceback = models.TextField(null=True, blank=True)

    def has_permission(self, user):
        return self.project.has_permission(user)
</file>

<file path="label_studio/projects/permissions.py">
from rest_framework.permissions import BasePermission


class ProjectImportPermission(BasePermission):
    """
    Checks if the user has access to the project import API
    Default case is always true
    """

    def has_permission(self, request, view):
        return True
</file>

<file path="label_studio/projects/serializers.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import bleach
from constants import SAFE_HTML_ATTRIBUTES, SAFE_HTML_TAGS
from django.db.models import Q
from label_studio_sdk.label_interface import LabelInterface
from label_studio_sdk.label_interface.control_tags import (
    BrushLabelsTag,
    BrushTag,
    ChoicesTag,
    DateTimeTag,
    EllipseLabelsTag,
    EllipseTag,
    HyperTextLabelsTag,
    KeyPointLabelsTag,
    KeyPointTag,
    LabelsTag,
    NumberTag,
    ParagraphLabelsTag,
    PolygonLabelsTag,
    PolygonTag,
    RatingTag,
    RectangleLabelsTag,
    RectangleTag,
    TaxonomyTag,
    TextAreaTag,
    TimeSeriesLabelsTag,
    VideoRectangleTag,
)
from projects.models import Project, ProjectImport, ProjectOnboarding, ProjectReimport, ProjectSummary
from rest_flex_fields import FlexFieldsModelSerializer
from rest_framework import serializers
from rest_framework.serializers import SerializerMethodField
from tasks.models import Task
from users.serializers import UserSimpleSerializer


class CreatedByFromContext:
    requires_context = True

    def __call__(self, serializer_field):
        return serializer_field.context.get('created_by')


class ProjectSerializer(FlexFieldsModelSerializer):
    """Serializer get numbers from project queryset annotation,
    make sure, that you use correct one(Project.objects.with_counts())
    """

    task_number = serializers.IntegerField(default=None, read_only=True, help_text='Total task number in project')
    total_annotations_number = serializers.IntegerField(
        default=None,
        read_only=True,
        help_text='Total annotations number in project including '
        'skipped_annotations_number and ground_truth_number.',
    )
    total_predictions_number = serializers.IntegerField(
        default=None,
        read_only=True,
        help_text='Total predictions number in project including '
        'skipped_annotations_number, ground_truth_number, and '
        'useful_annotation_number.',
    )
    useful_annotation_number = serializers.IntegerField(
        default=None,
        read_only=True,
        help_text='Useful annotation number in project not including '
        'skipped_annotations_number and ground_truth_number. '
        'Total annotations = annotation_number + '
        'skipped_annotations_number + ground_truth_number',
    )
    ground_truth_number = serializers.IntegerField(
        default=None, read_only=True, help_text='Honeypot annotation number in project'
    )
    skipped_annotations_number = serializers.IntegerField(
        default=None, read_only=True, help_text='Skipped by collaborators annotation number in project'
    )
    num_tasks_with_annotations = serializers.IntegerField(
        default=None, read_only=True, help_text='Tasks with annotations count'
    )

    created_by = UserSimpleSerializer(default=CreatedByFromContext(), help_text='Project owner')

    parsed_label_config = serializers.JSONField(
        default=None, read_only=True, help_text='JSON-formatted labeling configuration'
    )
    start_training_on_annotation_update = SerializerMethodField(
        default=None, read_only=False, help_text='Start model training after any annotations are submitted or updated'
    )
    config_has_control_tags = SerializerMethodField(
        default=None, read_only=True, help_text='Flag to detect is project ready for labeling'
    )
    config_suitable_for_bulk_annotation = serializers.SerializerMethodField(
        default=None, read_only=True, help_text='Flag to detect is project ready for bulk annotation'
    )
    finished_task_number = serializers.IntegerField(default=None, read_only=True, help_text='Finished tasks')

    queue_total = serializers.SerializerMethodField()
    queue_done = serializers.SerializerMethodField()

    @property
    def user_id(self):
        try:
            return self.context['request'].user.id
        except KeyError:
            return next(iter(self.context['user_cache']))

    @staticmethod
    def get_config_has_control_tags(project):
        return len(project.get_parsed_config()) > 0

    @staticmethod
    def get_config_suitable_for_bulk_annotation(project):
        li = LabelInterface(project.label_config)

        # List of tags that should not be present
        disallowed_tags = [
            LabelsTag,
            BrushTag,
            BrushLabelsTag,
            EllipseTag,
            EllipseLabelsTag,
            KeyPointTag,
            KeyPointLabelsTag,
            PolygonTag,
            PolygonLabelsTag,
            RectangleTag,
            RectangleLabelsTag,
            HyperTextLabelsTag,
            ParagraphLabelsTag,
            TimeSeriesLabelsTag,
            VideoRectangleTag,
        ]

        # Return False if any disallowed tag is present
        for tag_class in disallowed_tags:
            if li.find_tags_by_class(tag_class):
                return False

        # Check perRegion/perItem for expanded list of tags, plus value="no" for Choices/Taxonomy
        allowed_tags_for_checks = [ChoicesTag, TaxonomyTag, DateTimeTag, NumberTag, RatingTag, TextAreaTag]
        for tag_class in allowed_tags_for_checks:
            tags = li.find_tags_by_class(tag_class)
            for tag in tags:
                per_region = tag.attr.get('perRegion', 'false').lower() == 'true'
                per_item = tag.attr.get('perItem', 'false').lower() == 'true'
                if per_region or per_item:
                    return False
                # For ChoicesTag and TaxonomyTag, the value attribute must not be set at all
                if tag_class in [ChoicesTag, TaxonomyTag]:
                    if 'value' in tag.attr:
                        return False

        # For TaxonomyTag, check labeling and apiUrl
        taxonomy_tags = li.find_tags_by_class(TaxonomyTag)
        for tag in taxonomy_tags:
            labeling = tag.attr.get('labeling', 'false').lower() == 'true'
            if labeling:
                return False
            api_url = tag.attr.get('apiUrl', None)
            if api_url is not None:
                return False

        # If all checks pass, return True
        return True

    @staticmethod
    def get_parsed_label_config(project):
        return project.get_parsed_config()

    def get_start_training_on_annotation_update(self, instance):
        # FIXME: remake this logic with start_training_on_annotation_update
        return True if instance.min_annotations_to_start_training else False

    def to_internal_value(self, data):
        # FIXME: remake this logic with start_training_on_annotation_update
        initial_data = data
        data = super().to_internal_value(data)

        if 'start_training_on_annotation_update' in initial_data:
            data['min_annotations_to_start_training'] = int(initial_data['start_training_on_annotation_update'])

        if 'expert_instruction' in initial_data:
            data['expert_instruction'] = bleach.clean(
                initial_data['expert_instruction'], tags=SAFE_HTML_TAGS, attributes=SAFE_HTML_ATTRIBUTES
            )

        return data

    class Meta:
        model = Project
        extra_kwargs = {
            'memberships': {'required': False},
            'title': {'required': False},
            'created_by': {'required': False},
        }
        fields = [
            'id',
            'title',
            'description',
            'label_config',
            'expert_instruction',
            'show_instruction',
            'show_skip_button',
            'enable_empty_annotation',
            'show_annotation_history',
            'organization',
            'color',
            'maximum_annotations',
            'is_published',
            'model_version',
            'is_draft',
            'created_by',
            'created_at',
            'min_annotations_to_start_training',
            'start_training_on_annotation_update',
            'show_collab_predictions',
            'num_tasks_with_annotations',
            'task_number',
            'useful_annotation_number',
            'ground_truth_number',
            'skipped_annotations_number',
            'total_annotations_number',
            'total_predictions_number',
            'sampling',
            'show_ground_truth_first',
            'show_overlap_first',
            'overlap_cohort_percentage',
            'task_data_login',
            'task_data_password',
            'control_weights',
            'parsed_label_config',
            'evaluate_predictions_automatically',
            'config_has_control_tags',
            'skip_queue',
            'reveal_preannotations_interactively',
            'pinned_at',
            'finished_task_number',
            'queue_total',
            'queue_done',
            'config_suitable_for_bulk_annotation',
        ]

    def validate_label_config(self, value):
        if self.instance is None:
            # No project created yet
            Project.validate_label_config(value)
        else:
            # Existing project is updated
            self.instance.validate_config(value)
        return value

    def validate_model_version(self, value):
        """Custom model_version validation"""
        p = self.instance

        # Only run the validation if model_version is about to change
        # and it contains a string
        if p is not None and p.model_version != value and value != '':
            # that model_version should either match live ml backend
            # or match version in predictions

            if p.ml_backends.filter(title=value).union(p.predictions.filter(project=p, model_version=value)).exists():
                return value
            else:
                raise serializers.ValidationError(
                    "Model version doesn't exist either as live model or as static predictions."
                )

        return value

    def update(self, instance, validated_data):
        if validated_data.get('show_collab_predictions') is False:
            instance.model_version = ''

        return super().update(instance, validated_data)

    def get_queue_total(self, project):
        remain = project.tasks.filter(
            Q(is_labeled=False) & ~Q(annotations__completed_by_id=self.user_id)
            | Q(annotations__completed_by_id=self.user_id)
        ).distinct()
        return remain.count()

    def get_queue_done(self, project):
        tasks_filter = {
            'project': project,
            'annotations__completed_by_id': self.user_id,
        }

        if project.skip_queue == project.SkipQueue.REQUEUE_FOR_ME:
            tasks_filter['annotations__was_cancelled'] = False

        already_done_tasks = Task.objects.filter(**tasks_filter)
        result = already_done_tasks.distinct().count()

        return result


class ProjectCountsSerializer(ProjectSerializer):
    class Meta:
        model = Project
        fields = [
            'id',
            'task_number',
            'finished_task_number',
            'total_predictions_number',
            'total_annotations_number',
            'num_tasks_with_annotations',
            'useful_annotation_number',
            'ground_truth_number',
            'skipped_annotations_number',
        ]


class ProjectOnboardingSerializer(serializers.ModelSerializer):
    class Meta:
        model = ProjectOnboarding
        fields = '__all__'


class ProjectLabelConfigSerializer(serializers.Serializer):
    label_config = serializers.CharField(help_text=Project.label_config.field.help_text)

    def validate_label_config(self, config):
        Project.validate_label_config(config)
        return config


class ProjectSummarySerializer(serializers.ModelSerializer):
    class Meta:
        model = ProjectSummary
        fields = '__all__'


class ProjectImportSerializer(serializers.ModelSerializer):
    class Meta:
        model = ProjectImport
        fields = '__all__'


class ProjectReimportSerializer(serializers.ModelSerializer):
    class Meta:
        model = ProjectReimport
        fields = '__all__'


class ProjectModelVersionExtendedSerializer(serializers.Serializer):
    model_version = serializers.CharField()
    count = serializers.IntegerField()
    latest = serializers.DateTimeField()


class GetFieldsSerializer(serializers.Serializer):
    include = serializers.CharField(required=False)
    filter = serializers.CharField(required=False, default='all')

    def validate_include(self, value):
        if value is not None:
            value = value.split(',')
        return value

    def validate_filter(self, value):
        if value in ['all', 'pinned_only', 'exclude_pinned']:
            return value
</file>

<file path="label_studio/projects/signals.py">
from django.dispatch import Signal


class ProjectSignals:
    """
    Signals for project: implements observer pattern for custom signals.
    Example:

    # publisher
    ProjectSignals.my_signal.send(sender=self, project=project)

    # observer
    @receiver(ProjectSignals.my_signal)
    def my_observer(sender, **kwargs):
        ...
    """

    post_label_config_and_import_tasks = Signal()
</file>

<file path="label_studio/projects/urls.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
from django.urls import include, path

from . import api, views

app_name = 'projects'

# reverse for projects:name
_urlpatterns = [
    path('', views.project_list, name='project-index'),
    path('<int:pk>/settings/', views.project_settings, name='project-settings', kwargs={'sub_path': ''}),
    path('<int:pk>/settings/<sub_path>', views.project_settings, name='project-settings-anything'),
    path('upload-example/', views.upload_example_using_config, name='project-upload-example-using-config'),
]

# reverse for projects:api:name
_api_urlpatterns = [
    # CRUD
    path('', api.ProjectListAPI.as_view(), name='project-list'),
    path('<int:pk>/', api.ProjectAPI.as_view(), name='project-detail'),
    path('counts/', api.ProjectCountsListAPI.as_view(), name='project-counts-list'),
    # Get next task
    path('<int:pk>/next/', api.ProjectNextTaskAPI.as_view(), name='project-next'),
    # Label stream history
    path('<int:pk>/label-stream-history/', api.LabelStreamHistoryAPI.as_view(), name='label-stream-history'),
    # Validate label config in general
    path('validate/', api.LabelConfigValidateAPI.as_view(), name='label-config-validate'),
    # Validate label config for project
    path('<int:pk>/validate/', api.ProjectLabelConfigValidateAPI.as_view(), name='project-label-config-validate'),
    # Project summary
    path('<int:pk>/summary/', api.ProjectSummaryAPI.as_view(), name='project-summary'),
    # Project summary
    path(
        '<int:pk>/summary/reset/',
        api.ProjectSummaryResetAPI.as_view(),
        name='project-summary-reset',
    ),
    # Project import
    path('<int:pk>/imports/<int:import_pk>/', api.ProjectImportAPI.as_view(), name='project-imports'),
    # Project reimport
    path('<int:pk>/reimports/<int:reimport_pk>/', api.ProjectReimportAPI.as_view(), name='project-reimports'),
    # Tasks list for the project: get and destroy
    path('<int:pk>/tasks/', api.ProjectTaskListAPI.as_view(), name='project-tasks-list'),
    # Generate sample task for this project
    path('<int:pk>/sample-task/', api.ProjectSampleTask.as_view(), name='project-sample-task'),
    # List available model versions
    path('<int:pk>/model-versions/', api.ProjectModelVersions.as_view(), name='project-model-versions'),
]

_api_urlpatterns_templates = [
    path('', api.TemplateListAPI.as_view(), name='template-list'),
]


urlpatterns = [
    path('projects/', include(_urlpatterns)),
    path('api/projects/', include((_api_urlpatterns, app_name), namespace='api')),
    path('api/templates/', include((_api_urlpatterns_templates, app_name), namespace='api-templates')),
]
</file>

<file path="label_studio/projects/views.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import json
import logging

import lxml.etree
from core.label_config import get_sample_task
from core.utils.common import get_organization_from_request
from django.contrib.auth.decorators import login_required
from django.http import HttpResponse
from django.shortcuts import render
from django.views.decorators.http import require_http_methods
from organizations.models import Organization
from projects.models import Project
from rest_framework import generics, status
from rest_framework.exceptions import ValidationError

logger = logging.getLogger(__name__)


@login_required
def project_list(request):
    return render(request, 'projects/list.html')


@login_required
def project_settings(request, pk, sub_path):
    return render(request, 'projects/settings.html')


def playground_replacements(request, task_data):
    if request.GET.get('playground', '0') == '1':
        for key in task_data:
            if '/samples/time-series.csv' in task_data[key]:
                task_data[key] = 'https://app.heartex.ai' + task_data[key]
    return task_data


@require_http_methods(['POST'])
def upload_example_using_config(request):
    """Generate upload data example by config only"""
    config = request.POST.get('label_config', '')

    org_pk = get_organization_from_request(request)
    secure_mode = False
    if org_pk is not None:
        org = generics.get_object_or_404(Organization, pk=org_pk)
        secure_mode = org.secure_mode

    try:
        Project.validate_label_config(config)
        task_data, _, _ = get_sample_task(config, secure_mode)
        task_data = playground_replacements(request, task_data)
    except (ValueError, ValidationError, lxml.etree.Error):
        response = HttpResponse('error while example generating', status=status.HTTP_400_BAD_REQUEST)
    else:
        response = HttpResponse(json.dumps(task_data))
    return response
</file>

<file path="label_studio/tasks/management/commands/annotations_fill_updated_by.py">
import importlib

from django.core.management.base import BaseCommand


class Command(BaseCommand):
    help = 'Fill updated_by field for Annotations'

    def add_arguments(self, parser):
        pass

    def handle(self, *args, **options):
        migration = importlib.import_module('tasks.migrations.0033_annotation_updated_by_fill')
        migration._fill_annotations_updated_by()
</file>

<file path="label_studio/tasks/management/commands/calculate_stats_all_orgs.py">
from django.core.management.base import BaseCommand


class Command(BaseCommand):
    help = 'Recalculate project stats (total_annotations, etc) for all organizations'

    def add_arguments(self, parser):
        parser.add_argument(
            '--from-scratch',
            dest='from_scratch',
            action='store_true',
            default=False,
            help='Start recalculation from scratch',
        )
        parser.add_argument(
            '--redis',
            dest='redis',
            action='store_true',
            default=False,
            help='Use rq workers with redis (async background processing)',
        )

    def handle(self, *args, **options):
        from tasks.functions import calculate_stats_all_orgs

        calculate_stats_all_orgs(from_scratch=options['from_scratch'], redis=options['redis'])
</file>

<file path="label_studio/tasks/management/commands/calculate_stats.py">
import logging

from core.redis import start_job_async_or_sync
from django.core.management.base import BaseCommand
from projects.models import Project
from tasks.functions import update_tasks_counters

logger = logging.getLogger(__name__)


class Command(BaseCommand):
    help = 'Recalculate organization project stats (total_annotations, etc)'

    def add_arguments(self, parser):
        parser.add_argument('organization', type=int, help='organization id')

    def handle(self, *args, **options):
        logger.debug(f"Start recalculating for Organization {options['organization']}.")
        projects = Project.objects.filter(organization_id=options['organization'])

        for project in projects:
            logger.debug(f'Start processing project {project.id}.')
            start_job_async_or_sync(update_tasks_counters, project.tasks.all())
            logger.debug(f'End processing project {project.id}.')

        logger.debug(f"Organization {options['organization']} stats were recalculated.")
</file>

<file path="label_studio/tasks/migrations/__init__.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
</file>

<file path="label_studio/tasks/migrations/0001_squashed_0041_taskcompletionhistory_was_cancelled.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
# Generated by Django 3.1.4 on 2021-03-03 10:37

from django.conf import settings
import django.contrib.postgres.fields.jsonb
from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    replaces = [('tasks', '0001_initial'), ('tasks', '0002_task_meta'), ('tasks', '0003_auto_20190328_1641'), ('tasks', '0003_auto_20190327_1906'), ('tasks', '0004_merge_20190329_1219'), ('tasks', '0005_auto_20190506_1532'), ('tasks', '0006_task_exposed'), ('tasks', '0007_auto_20190621_1106'), ('tasks', '0008_auto_20190621_1426'), ('tasks', '0009_remove_taskcompletion_was_generated'), ('tasks', '0010_auto_20190813_2215'), ('tasks', '0011_taskcompletion_updates_history'), ('tasks', '0012_auto_20190921_0830'), ('tasks', '0013_auto_20190925_1602'), ('tasks', '0014_auto_20191002_1002'), ('tasks', '0015_taskcompletion_lead_time'), ('tasks', '0016_completionpairwisestats'), ('tasks', '0017_task_taken_at'), ('tasks', '0018_auto_20200202_2017'), ('tasks', '0019_task_overlap'), ('tasks', '0020_review'), ('tasks', '0021_auto_20200417_1019'), ('tasks', '0022_taskcompletion_result_count'), ('tasks', '0023_storagelink'), ('tasks', '0024_auto_20200705_1436'), ('tasks', '0025_taskcompletionhistory'), ('tasks', '0026_remove_taskcompletion_updates_history'), ('tasks', '0027_taskcompletion_draft'), ('tasks', '0028_auto_20200814_1514'), ('tasks', '0029_auto_20200904_2035'), ('tasks', '0030_auto_20200904_2043'), ('tasks', '0031_auto_20200910_1402'), ('tasks', '0032_auto_20200921_1921'), ('tasks', '0033_auto_20201005_1552'), ('tasks', '0034_auto_20201203_1113'), ('tasks', '0035_remove_experts'), ('tasks', '0036_auto_20210121_1524'), ('tasks', '0036_auto_20210119_1144'), ('tasks', '0037_merge_20210126_1328'), ('tasks', '0038_delete_storagelink'), ('tasks', '0039_task_file_upload'), ('tasks', '0035_tasklock'), ('tasks', '0039_merge_20210222_1244'), ('tasks', '0040_merge_20210225_1126'), ('tasks', '0041_taskcompletionhistory_was_cancelled')]

    initial = True

    dependencies = [
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
        ('data_import', '0001_initial'),
    ]

    operations = [
        migrations.CreateModel(
            name='Task',
            fields=[
                ('id', models.AutoField(auto_created=True, db_index=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('data', models.JSONField(help_text='User uploaded data for the task. Data is formatted corresponding to the project label config. Examples of data for your project can be found at Upload Dialog on Data Manager page', verbose_name='data')),
                ('accuracy', models.FloatField(default=None, help_text='Completion agreement among experts', null=True, verbose_name='accuracy')),
                ('created_at', models.DateTimeField(auto_now_add=True, help_text='Task creation time', verbose_name='created at')),
                ('updated_at', models.DateTimeField(auto_now=True, help_text='Task last update time', verbose_name='updated at')),
                ('is_labeled', models.BooleanField(default=False, help_text='True if the completion number for this task is greater or equal to the project maximum_completions', verbose_name='is_labeled')),
                ('project', models.ForeignKey(help_text='Project id for this task', null=True, on_delete=django.db.models.deletion.CASCADE, related_name='tasks', to='projects.project')),
                ('meta', models.JSONField(default=dict, help_text='Meta is a user uploaded data and it can be useful as MLBackend input (embeddings, advanced vectors, other info). It will be passed to ML while training/predict steps', null=True, verbose_name='meta')),
            ],
            options={
                'db_table': 'task',
                'ordering': ['-accuracy', '-updated_at'],
            },
        ),
        migrations.CreateModel(
            name='Prediction',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('result', models.JSONField(default=dict, null=True, verbose_name='result')),
                ('score', models.FloatField(default=0.0, verbose_name='score')),
                ('model_version', models.TextField(blank=True, default='', null=True, verbose_name='model version')),
                ('created_at', models.DateTimeField(auto_now_add=True, verbose_name='created at')),
                ('updated_at', models.DateTimeField(auto_now=True, verbose_name='updated at')),
                ('task', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='predictions', to='tasks.task')),
                ('cluster', models.IntegerField(default=None, help_text='Cluster for the current prediction', null=True, verbose_name='cluster')),
            ],
            options={
                'db_table': 'prediction',
            },
        ),
        migrations.CreateModel(
            name='TaskCompletion',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('state', models.JSONField(default=dict, help_text='Editor state (system data)', null=True, verbose_name='state')),
                ('result', models.JSONField(default=list, help_text='The main value of expert work - labeling result in JSON format', null=True, verbose_name='result')),
                ('was_cancelled', models.BooleanField(default=False, help_text='Expert skipped the task', verbose_name='was cancelled')),
                ('honeypot', models.BooleanField(default=False, help_text='This completion is Ground Truth (honeypot)', verbose_name='honeypot')),
                ('created_at', models.DateTimeField(auto_now_add=True, help_text='Creation time', verbose_name='created at')),
                ('updated_at', models.DateTimeField(auto_now=True, help_text='Last update time', verbose_name='updated at')),
                ('completed_by', models.ForeignKey(help_text='User who made this completion', null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='completions', to=settings.AUTH_USER_MODEL)),
                ('task', models.ForeignKey(help_text='Corresponding task for this completion', null=True, on_delete=django.db.models.deletion.CASCADE, related_name='completions', to='tasks.task', db_index=settings.DJANGO_DB != settings.DJANGO_DB_MYSQL)),
                ('prediction', models.JSONField(default=dict, help_text='Prediction viewed at the time of completion', null=True, verbose_name='prediction')),
                ('prediction_equal_score', models.FloatField(default=0.0, help_text='Comparison result with prediction viewed at the time of completion', verbose_name='prediction_equal_score')),
                ('updates_history', models.JSONField(default=list, help_text='Updates history by experts', null=True, verbose_name='update history')),
            ],
            options={
                'db_table': 'task_completion',
            },
        ),
        migrations.AddField(
            model_name='prediction',
            name='ground_truth_match',
            field=models.FloatField(blank=True, default=None, help_text='Ground truth matching score if related task has ground truth', null=True, verbose_name='ground_truth_match'),
        ),
        migrations.AddField(
            model_name='prediction',
            name='neighbors',
            field=models.JSONField(blank=True, default=list, help_text='Array task IDs of the closest neighbors', null=True, verbose_name='neighbors'),
        ),
        migrations.AlterField(
            model_name='prediction',
            name='result',
            field=models.JSONField(default=dict, help_text='Prediction result', null=True, verbose_name='result'),
        ),
        migrations.AlterField(
            model_name='prediction',
            name='score',
            field=models.FloatField(default=0.0, help_text='Prediction score', verbose_name='score'),
        ),
        migrations.AddField(
            model_name='taskcompletion',
            name='lead_time',
            field=models.FloatField(default=None, help_text='how much time spent to solve the completion', null=True, verbose_name='lead time'),
        ),
        migrations.CreateModel(
            name='CompletionPairwiseStats',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('matching_score', models.FloatField(default=0.0, help_text='Matching score', verbose_name='matching score')),
                ('first', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='first_pairwise_stats', to='tasks.taskcompletion')),
                ('second', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='second_pairwise_stats', to='tasks.taskcompletion')),
            ],
        ),
        migrations.AddField(
            model_name='task',
            name='taken_at',
            field=models.DateTimeField(blank=True, help_text='Time when task has been taken in work', null=True, verbose_name='taken_at'),
        ),
        migrations.AddField(
            model_name='prediction',
            name='mislabeling',
            field=models.FloatField(default=0.0, help_text='Related task mislabeling score', verbose_name='mislabeling'),
        ),
        migrations.AlterField(
            model_name='task',
            name='data',
            field=models.JSONField(help_text='User imported (uploaded) data for the task. Data is formatted corresponding to the project label config. Examples of data for your project can be found at Import Dialog on Data Manager page', verbose_name='data'),
        ),
        migrations.AlterField(
            model_name='task',
            name='meta',
            field=models.JSONField(default=dict, help_text='Meta is a user imported (uploaded) data and it can be useful as MLBackend input(embeddings, advanced vectors, other info). It will be passed to ML while training/predict steps', null=True, verbose_name='meta'),
        ),
        migrations.AlterField(
            model_name='taskcompletion',
            name='completed_by',
            field=models.ForeignKey(help_text='Expert ID who made this completion', null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='completions', to=settings.AUTH_USER_MODEL),
        ),
        migrations.AddField(
            model_name='task',
            name='overlap',
            field=models.IntegerField(default=1, help_text='Number of distinct annotators which process current task', verbose_name='overlap'),
        ),
        migrations.CreateModel(
            name='Review',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('created_at', models.DateTimeField(auto_now_add=True, help_text='Creation time', verbose_name='created at')),
                ('result', models.FloatField(default=0.0, help_text='Review result from -1.0 to 1.0', verbose_name='result')),
                ('completed_by', models.ForeignKey(help_text='Expert ID who made this completion', null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='reviews', to=settings.AUTH_USER_MODEL)),
                ('completion', models.ForeignKey(help_text='Completion review by users', null=True, on_delete=django.db.models.deletion.CASCADE, related_name='reviews', to='tasks.taskcompletion')),
            ],
        ),
        migrations.AddField(
            model_name='taskcompletion',
            name='result_count',
            field=models.IntegerField(default=0, help_text='Results inside of completion counter', verbose_name='result count'),
        ),
        migrations.AlterField(
            model_name='prediction',
            name='score',
            field=models.FloatField(default=0.0, help_text='Prediction score', null=True, verbose_name='score'),
        ),
        migrations.RemoveField(
            model_name='taskcompletion',
            name='updates_history',
        ),
        migrations.AddField(
            model_name='taskcompletion',
            name='draft',
            field=models.JSONField(default=None, help_text='Autosave during labeling', null=True, verbose_name='draft'),
        ),
        migrations.AddIndex(
            model_name='task',
            index=models.Index(fields=['project', 'is_labeled'], name='task_project_6acf5f_idx'),
        ),
        migrations.AddIndex(
            model_name='taskcompletion',
            index=models.Index(fields=['task', 'honeypot'], name='task_comple_task_id_07c6ca_idx'),
        ),
        migrations.AlterField(
            model_name='task',
            name='overlap',
            field=models.IntegerField(db_index=True, default=1, help_text='Number of distinct annotators which process current task', verbose_name='overlap'),
        ),
        migrations.AlterField(
            model_name='taskcompletion',
            name='result',
            field=models.JSONField(default=None, help_text='The main value of expert work - labeling result in JSON format', null=True, verbose_name='result'),
        ),
        migrations.AddIndex(
            model_name='task',
            index=models.Index(fields=['id', 'overlap'], name='task_id_7a9aca_idx'),
        ),
        migrations.AlterField(
            model_name='prediction',
            name='score',
            field=models.FloatField(default=None, help_text='Prediction score', null=True, verbose_name='score'),
        ),
        migrations.CreateModel(
            name='TaskCompletionDraft',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('result', models.JSONField(help_text='Draft result in JSON format', verbose_name='result')),
                ('lead_time', models.FloatField(help_text='how much time spent to solve the completion', verbose_name='lead time')),
                ('created_at', models.DateTimeField(auto_now_add=True, help_text='Creation time', verbose_name='created at')),
                ('updated_at', models.DateTimeField(auto_now=True, help_text='Last update time', verbose_name='updated at')),
            ],
        ),
        migrations.RemoveField(
            model_name='taskcompletion',
            name='draft',
        ),
        migrations.DeleteModel(
            name='CompletionPairwiseStats',
        ),
        migrations.AddField(
            model_name='taskcompletiondraft',
            name='completion',
            field=models.ForeignKey(blank=True, help_text='Corresponding completion for this draft', null=True, on_delete=django.db.models.deletion.CASCADE, related_name='drafts', to='tasks.taskcompletion'),
        ),
        migrations.AddField(
            model_name='taskcompletiondraft',
            name='task',
            field=models.ForeignKey(blank=True, help_text='Corresponding task for this draft', null=True, on_delete=django.db.models.deletion.CASCADE, related_name='drafts', to='tasks.task'),
        ),
        migrations.AddField(
            model_name='taskcompletiondraft',
            name='user',
            field=models.ForeignKey(help_text='User who made this draft', on_delete=django.db.models.deletion.CASCADE, related_name='drafts', to=settings.AUTH_USER_MODEL),
        ),
        migrations.RenameField(
            model_name='review',
            old_name='completed_by',
            new_name='completed_by_old',
        ),
        migrations.RenameField(
            model_name='taskcompletion',
            old_name='completed_by',
            new_name='completed_by_old',
        ),
        migrations.AddField(
            model_name='review',
            name='completed_by',
            field=models.ForeignKey(help_text='User ID who made this review', null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='reviews', to=settings.AUTH_USER_MODEL),
        ),
        migrations.AddField(
            model_name='taskcompletion',
            name='completed_by',
            field=models.ForeignKey(help_text='User ID who made this completion', null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='completions', to=settings.AUTH_USER_MODEL),
        ),
        migrations.RemoveField(
            model_name='review',
            name='completed_by_old',
        ),
        migrations.RemoveField(
            model_name='taskcompletion',
            name='completed_by_old',
        ),
        migrations.AlterField(
            model_name='task',
            name='accuracy',
            field=models.FloatField(default=None, help_text='Completion agreement among annotators', null=True, verbose_name='accuracy'),
        ),
        migrations.AlterField(
            model_name='taskcompletion',
            name='lead_time',
            field=models.FloatField(default=None, help_text='How much time spent to solve the completion', null=True, verbose_name='lead time'),
        ),
        migrations.AlterField(
            model_name='taskcompletion',
            name='result',
            field=django.contrib.postgres.fields.jsonb.JSONField(default=None, help_text='The main value of annotator work - labeling result in JSON format', null=True, verbose_name='result'),
        ),
        migrations.AlterField(
            model_name='taskcompletion',
            name='was_cancelled',
            field=models.BooleanField(default=False, help_text='User skipped the task', verbose_name='was cancelled'),
        ),
        migrations.AlterField(
            model_name='prediction',
            name='neighbors',
            field=models.JSONField(blank=True, help_text='Array task IDs of the closest neighbors', null=True, verbose_name='neighbors'),
        ),
        migrations.AlterField(
            model_name='taskcompletion',
            name='result',
            field=models.JSONField(default=None, help_text='The main value of annotator work - labeling result in JSON format', null=True, verbose_name='result'),
        ),
        migrations.AlterField(
            model_name='prediction',
            name='neighbors',
            field=models.JSONField(blank=True, help_text='Array task IDs of the closest neighbors', null=True, verbose_name='neighbors'),
        ),
        migrations.AlterField(
            model_name='taskcompletion',
            name='result',
            field=models.JSONField(default=None, help_text='The main value of annotator work - labeling result in JSON format', null=True, verbose_name='result'),
        ),
        migrations.AddField(
            model_name='task',
            name='file_upload',
            field=models.ForeignKey(blank=True, null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='tasks', to='data_import.fileupload'),
        ),
        migrations.CreateModel(
            name='TaskLock',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('expire_at', models.DateTimeField(verbose_name='expire_at')),
                ('task', models.ForeignKey(help_text='Locked task', on_delete=django.db.models.deletion.CASCADE, related_name='locks', to='tasks.task')),
                ('user', models.ForeignKey(help_text='User who made this task lock', on_delete=django.db.models.deletion.CASCADE, related_name='task_locks', to=settings.AUTH_USER_MODEL)),
            ],
        ),
    ]
</file>

<file path="label_studio/tasks/migrations/0002_auto_20210304_1423.py">
# Generated by Django 3.1.4 on 2021-03-04 14:23
import django

from django.conf import settings
from django.db import migrations, models


class Migration(migrations.Migration):
    atomic = False

    dependencies = [
        ('io_storages', '0001_squashed_0002_auto_20210302_1827'),
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
        ('tasks', '0001_squashed_0041_taskcompletionhistory_was_cancelled'),
    ]

    operations = [
        migrations.RenameModel(
            old_name='TaskCompletion',
            new_name='Annotation',
        ),
        migrations.RenameModel(
            old_name='TaskCompletionDraft',
            new_name='AnnotationDraft',
        ),
        migrations.RenameField(
            model_name='review',
            old_name='completion',
            new_name='annotation',
        ),
        migrations.RenameField(
            model_name='annotationdraft',
            old_name='completion',
            new_name='annotation',
        ),
        migrations.AlterField(
            model_name='annotationdraft',
            name='annotation',
            field=models.ForeignKey(blank=True, help_text='Corresponding annotation for this draft', null=True, on_delete=django.db.models.deletion.CASCADE, related_name='drafts', to='tasks.annotation'),
        ),
        migrations.AlterField(
            model_name='task',
            name='file_upload',
            field=models.ForeignKey(blank=True, help_text='Uploaded file used as data source for this task', null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='tasks', to='data_import.fileupload'),
        ),
        migrations.AlterField(
            model_name='task',
            name='is_labeled',
            field=models.BooleanField(default=False, help_text='True if the annotation number for this task is greater or equal to the project maximum_annotations', verbose_name='is_labeled'),
        ),
        migrations.AlterField(
            model_name='annotation',
            name='completed_by',
            field=models.ForeignKey(help_text='User ID who made this annotation', null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='annotations', to=settings.AUTH_USER_MODEL),
        ),
        migrations.AlterField(
            model_name='annotation',
            name='honeypot',
            field=models.BooleanField(default=False, help_text='This annotation is Ground Truth (honeypot)', verbose_name='honeypot'),
        ),
        migrations.AlterField(
            model_name='annotation',
            name='lead_time',
            field=models.FloatField(default=None, help_text='How much time spent to solve the annotation', null=True, verbose_name='lead time'),
        ),
        migrations.AlterField(
            model_name='annotation',
            name='prediction',
            field=models.JSONField(default=dict, help_text='Prediction viewed at the time of annotation', null=True, verbose_name='prediction'),
        ),
        migrations.AlterField(
            model_name='annotation',
            name='prediction_equal_score',
            field=models.FloatField(default=0.0, help_text='Comparison result with prediction viewed at the time of annotation', verbose_name='prediction_equal_score'),
        ),
        migrations.AlterField(
            model_name='annotation',
            name='result_count',
            field=models.IntegerField(default=0, help_text='Results inside of annotation counter', verbose_name='result count'),
        ),
        migrations.AlterField(
            model_name='annotation',
            name='task',
            field=models.ForeignKey(help_text='Corresponding task for this annotation', null=True, on_delete=django.db.models.deletion.CASCADE, related_name='annotations', to='tasks.task'),
        ),
        migrations.AlterField(
            model_name='annotationdraft',
            name='lead_time',
            field=models.FloatField(help_text='how much time spent to solve the annotation', verbose_name='lead time'),
        ),
    ]
</file>

<file path="label_studio/tasks/migrations/0002_auto_20210305_2035.py">
# Generated by Django 3.1.4 on 2021-03-05 20:35

from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    dependencies = [
        ('data_import', '0001_initial'),
        ('tasks', '0001_squashed_0041_taskcompletionhistory_was_cancelled'),
    ]

    operations = [
        migrations.AlterField(
            model_name='task',
            name='file_upload',
            field=models.ForeignKey(blank=True, help_text='Uploaded file used as data source for this task', null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='tasks', to='data_import.fileupload'),
        ),
    ]
</file>

<file path="label_studio/tasks/migrations/0003_merge_20210308_1141.py">
# Generated by Django 3.1.4 on 2021-03-08 11:41

from django.db import migrations


class Migration(migrations.Migration):

    dependencies = [
        ('tasks', '0002_auto_20210304_1423'),
        ('tasks', '0002_auto_20210305_2035'),
    ]

    operations = [
    ]
</file>

<file path="label_studio/tasks/migrations/0004_auto_20210308_1559.py">
# Generated by Django 3.1.4 on 2021-03-08 15:59

from django.conf import settings
from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    dependencies = [
        ('projects', '0006_auto_20210308_1559'),
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
        ('tasks', '0003_merge_20210308_1141'),
    ]

    operations = [
        migrations.AlterModelOptions(
            name='task',
            options={'ordering': ['-updated_at']},
        ),
        migrations.RemoveField(
            model_name='annotation',
            name='prediction_equal_score',
        ),
        migrations.RemoveField(
            model_name='prediction',
            name='ground_truth_match',
        ),
        migrations.RemoveField(
            model_name='task',
            name='accuracy',
        ),
        migrations.RemoveField(
            model_name='task',
            name='taken_at',
        ),
        migrations.AlterField(
            model_name='annotation',
            name='completed_by',
            field=models.ForeignKey(help_text='User ID of the person who created this annotation', null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='annotations', to=settings.AUTH_USER_MODEL),
        ),
        migrations.AlterField(
            model_name='annotation',
            name='honeypot',
            field=models.BooleanField(default=False, help_text='This annotation is a Ground Truth (honeypot)', verbose_name='honeypot'),
        ),
        migrations.AlterField(
            model_name='annotation',
            name='lead_time',
            field=models.FloatField(default=None, help_text='How much time it took to annotate the task', null=True, verbose_name='lead time'),
        ),
        migrations.AlterField(
            model_name='annotation',
            name='updated_at',
            field=models.DateTimeField(auto_now=True, help_text='Last updated time', verbose_name='updated at'),
        ),
        migrations.AlterField(
            model_name='annotationdraft',
            name='lead_time',
            field=models.FloatField(help_text='How much time it took to annotate the task', verbose_name='lead time'),
        ),
        migrations.AlterField(
            model_name='annotationdraft',
            name='user',
            field=models.ForeignKey(help_text='User who created this draft', on_delete=django.db.models.deletion.CASCADE, related_name='drafts', to=settings.AUTH_USER_MODEL),
        ),
        migrations.AlterField(
            model_name='prediction',
            name='neighbors',
            field=models.JSONField(blank=True, help_text='Array of task IDs of the closest neighbors', null=True, verbose_name='neighbors'),
        ),
        migrations.AlterField(
            model_name='task',
            name='created_at',
            field=models.DateTimeField(auto_now_add=True, help_text='Time a task was created', verbose_name='created at'),
        ),
        migrations.AlterField(
            model_name='task',
            name='data',
            field=models.JSONField(help_text='User imported (uploaded) data for the task. Data is formatted according to the project label config. You can find examples of data for your project on the Import page in the Label Studio Data Manager UI.', verbose_name='data'),
        ),
        migrations.AlterField(
            model_name='task',
            name='is_labeled',
            field=models.BooleanField(default=False, help_text='True if the annotation number for this task is greater than or equal to the number of maximum_completions for the project', verbose_name='is_labeled'),
        ),
        migrations.AlterField(
            model_name='task',
            name='meta',
            field=models.JSONField(default=dict, help_text='Meta is user imported (uploaded) data and can be useful as input for an ML Backend, for embeddings, advanced vectors, and other info. It is passed to ML during training/predicting steps.', null=True, verbose_name='meta'),
        ),
        migrations.AlterField(
            model_name='task',
            name='overlap',
            field=models.IntegerField(db_index=True, default=1, help_text='Number of distinct annotators that processed the current task', verbose_name='overlap'),
        ),
        migrations.AlterField(
            model_name='task',
            name='project',
            field=models.ForeignKey(help_text='Project ID for this task', null=True, on_delete=django.db.models.deletion.CASCADE, related_name='tasks', to='projects.project'),
        ),
        migrations.AlterField(
            model_name='task',
            name='updated_at',
            field=models.DateTimeField(auto_now=True, help_text='Last time a task was updated', verbose_name='updated at'),
        ),
        migrations.DeleteModel(
            name='Review',
        ),
    ]
</file>

<file path="label_studio/tasks/migrations/0005_auto_20210309_1239.py">
# Generated by Django 3.1.4 on 2021-03-09 12:39

from django.conf import settings
from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    dependencies = [
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
        ('tasks', '0004_auto_20210308_1559'),
    ]

    operations = [
        migrations.RemoveIndex(
            model_name='annotation',
            name='task_comple_task_id_07c6ca_idx',
        ),
        migrations.RenameField(
            model_name='annotation',
            old_name='honeypot',
            new_name='ground_truth',
        ),
        migrations.AlterField(
            model_name='annotation',
            name='ground_truth',
            field=models.BooleanField(default=False, help_text='This annotation is a Ground Truth (ground_truth)', verbose_name='ground_truth'),
        ),
        migrations.AlterField(
            model_name='task',
            name='data',
            field=models.JSONField(help_text='User imported or uploaded data for a task. Data is formatted according to the project label config. You can find examples of data for your project on the Import page in the Label Studio Data Manager UI.', verbose_name='data'),
        ),
        migrations.AlterField(
            model_name='task',
            name='is_labeled',
            field=models.BooleanField(default=False, help_text='True if the number of annotations for this task is greater than or equal to the number of maximum_completions for the project', verbose_name='is_labeled'),
        ),
        migrations.AlterField(
            model_name='task',
            name='meta',
            field=models.JSONField(default=dict, help_text='Meta is user imported (uploaded) data and can be useful as input for an ML Backend for embeddings, advanced vectors, and other info. It is passed to ML during training/predicting steps.', null=True, verbose_name='meta'),
        ),
        migrations.AlterField(
            model_name='tasklock',
            name='user',
            field=models.ForeignKey(help_text='User who locked this task', on_delete=django.db.models.deletion.CASCADE, related_name='task_locks', to=settings.AUTH_USER_MODEL),
        ),
        migrations.AddIndex(
            model_name='annotation',
            index=models.Index(fields=['task', 'ground_truth'], name='task_comple_task_id_e82920_idx'),
        ),
    ]
</file>

<file path="label_studio/tasks/migrations/0006_remove_annotation_state.py">
# Generated by Django 3.1.4 on 2021-04-12 23:19

from django.db import migrations


class Migration(migrations.Migration):

    dependencies = [
        ('tasks', '0005_auto_20210309_1239'),
    ]

    operations = [
        migrations.RemoveField(
            model_name='annotation',
            name='state',
        ),
    ]
</file>

<file path="label_studio/tasks/migrations/0007_auto_20210618_1653.py">
# Generated by Django 3.1.12 on 2021-06-18 16:53

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('tasks', '0006_remove_annotation_state'),
    ]

    operations = [
        migrations.AlterField(
            model_name='annotation',
            name='ground_truth',
            field=models.BooleanField(db_index=True, default=False, help_text='This annotation is a Ground Truth (ground_truth)', verbose_name='ground_truth'),
        ),
        migrations.AlterField(
            model_name='annotation',
            name='was_cancelled',
            field=models.BooleanField(db_index=True, default=False, help_text='User skipped the task', verbose_name='was cancelled'),
        ),
    ]
</file>

<file path="label_studio/tasks/migrations/0008_auto_20210903_1332.py">
# Generated by Django 3.1.13 on 2021-09-03 13:32

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('tasks', '0007_auto_20210618_1653'),
    ]

    operations = [
        migrations.AlterField(
            model_name='task',
            name='is_labeled',
            field=models.BooleanField(db_index=True, default=False, help_text='True if the number of annotations for this task is greater than or equal to the number of maximum_completions for the project', verbose_name='is_labeled'),
        ),
    ]
</file>

<file path="label_studio/tasks/migrations/0009_auto_20210913_0739.py">
# Generated by Django 3.1.13 on 2021-09-13 07:39
import logging

from django.db import migrations
from core.utils.common import trigram_migration_operations

logger = logging.getLogger(__name__)


def forwards(apps, schema_editor):
    if not schema_editor.connection.vendor.startswith('postgres'):
        logger.info('Database vendor: {}'.format(schema_editor.connection.vendor))
        logger.info('Skipping migration without attempting to CREATE INDEX')
        return

    schema_editor.execute('create index concurrently tasks_annotations_result_idx on task_completion using gin (upper(cast(result as text)) gin_trgm_ops);')


def backwards(apps, schema_editor):
    if not schema_editor.connection.vendor.startswith('postgres'):
        logger.info('Database vendor: {}'.format(schema_editor.connection.vendor))
        logger.info('Skipping migration without attempting to DROP INDEX')
        return

    schema_editor.execute('drop index tasks_annotations_result_idx;')


class Migration(migrations.Migration):
    atomic = False

    dependencies = [('tasks', '0008_auto_20210903_1332')]

    operations = trigram_migration_operations(migrations.RunPython(forwards, backwards))
</file>

<file path="label_studio/tasks/migrations/0009_auto_20210914_0020.py">
# Generated by Django 3.1.13 on 2021-09-14 00:20

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('tasks', '0008_auto_20210903_1332'),
    ]

    operations = [
        migrations.AddIndex(
            model_name='annotation',
            index=models.Index(fields=['was_cancelled'], name='task_comple_was_can_f87d4e_idx'),
        ),
        migrations.AddIndex(
            model_name='annotation',
            index=models.Index(fields=['ground_truth'], name='task_comple_ground__088a1b_idx'),
        ),
        migrations.AddIndex(
            model_name='annotation',
            index=models.Index(fields=['created_at'], name='task_comple_created_f55e6f_idx'),
        ),
    ]
</file>

<file path="label_studio/tasks/migrations/0010_auto_20210914_0032.py">
# Generated by Django 3.1.13 on 2021-09-14 00:32

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('tasks', '0009_auto_20210914_0020'),
    ]

    operations = [
        migrations.AddIndex(
            model_name='task',
            index=models.Index(fields=['overlap'], name='task_overlap_6a196e_idx'),
        ),
        migrations.AddIndex(
            model_name='task',
            index=models.Index(fields=['is_labeled'], name='task_is_labe_215a18_idx'),
        ),
    ]
</file>

<file path="label_studio/tasks/migrations/0011_merge_20210914_1036.py">
# Generated by Django 3.1.13 on 2021-09-14 10:36

from django.db import migrations


class Migration(migrations.Migration):

    dependencies = [
        ('tasks', '0009_auto_20210913_0739'),
        ('tasks', '0010_auto_20210914_0032'),
    ]

    operations = [
    ]
</file>

<file path="label_studio/tasks/migrations/0012_auto_20211010_1339.py">
# Generated by Django 3.1.13 on 2021-10-10 13:39

from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    dependencies = [
        ('tasks', '0011_merge_20210914_1036'),
    ]

    operations = [
        migrations.AddField(
            model_name='annotation',
            name='parent_prediction',
            field=models.ForeignKey(help_text='Points to the prediction from which the annotation was created', null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='child_annotations', to='tasks.prediction'),
        ),
        migrations.AddField(
            model_name='annotation',
            name='parent_annotation',
            field=models.ForeignKey(help_text='Points to the parent annotation from which this annotation was created',
                                    null=True, on_delete=django.db.models.deletion.SET_NULL,
                                    related_name='child_annotations', to='tasks.annotation'),
        ),
        migrations.AlterField(
            model_name='annotation',
            name='parent_prediction',
            field=models.ForeignKey(help_text='Points to the prediction from which this annotation was created',
                                    null=True, on_delete=django.db.models.deletion.SET_NULL,
                                    related_name='child_annotations', to='tasks.prediction'),
        ),
    ]
</file>

<file path="label_studio/tasks/migrations/0013_task_updated_by.py">
# Generated by Django 3.1.14 on 2022-03-11 20:18

from django.conf import settings
from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    dependencies = [
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
        ('tasks', '0012_auto_20211010_1339'),
    ]

    operations = [
        migrations.AddField(
            model_name='task',
            name='updated_by',
            field=models.ForeignKey(help_text='Last annotator or reviewer who updated this task', null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='updated_tasks', to=settings.AUTH_USER_MODEL, verbose_name='updated by'),
        ),
    ]
</file>

<file path="label_studio/tasks/migrations/0014_task_inner_id.py">
# Generated by Django 3.1.14 on 2022-04-12 09:17

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('tasks', '0013_task_updated_by'),
    ]

    operations = [
        migrations.AddField(
            model_name='task',
            name='inner_id',
            field=models.BigIntegerField(db_index=True, null=True, default=0, help_text='Internal task ID in the project, starts with 1', verbose_name='inner id'),
        ),
    ]
</file>

<file path="label_studio/tasks/migrations/0015_task_fill_inner_id.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import logging

from django.db import migrations
from django.db.models import F, Window
from django.db.models.functions import RowNumber
from django.db.utils import NotSupportedError
from core.bulk_update_utils import bulk_update
from core.feature_flags import flag_set
from django.contrib.auth.models import AnonymousUser

logger = logging.getLogger(__name__)


def remove(apps, schema_editor):
    # if not flag_set('ff_back_2070_inner_id_12052022_short', user=AnonymousUser()):
    return  # we don't want to apply this migration to all projects

    """ Project = apps.get_model('projects', 'Project')
    projects = Project.objects.all()
    for project in projects:
        logger.info(f'Evaluate inner id for {project}')
        project_tasks = project.tasks.all()
        if project_tasks.exists():
            obj = []
            try:
                results = project_tasks.annotate(row_number=Window(
                    expression=RowNumber(),
                    partition_by=['project'],
                    order_by=F('id').asc()))
                for task in results:
                    task.inner_id = task.row_number
                    obj.append(task)
                bulk_update(obj, batch_size=1000)

            # not window is not supported
            except NotSupportedError:
                first_id = project_tasks.order_by('id').first().id
                project_tasks.update(inner_id=F('id') - first_id)"""


def backwards(apps, schema_editor):
    pass


class Migration(migrations.Migration):
    dependencies = [
        ('tasks', '0014_task_inner_id'),
    ]

    operations = [
        migrations.RunPython(remove, backwards),
    ]
</file>

<file path="label_studio/tasks/migrations/0016_auto_20220414_1408.py">
# Generated by Django 3.1.14 on 2022-04-14 14:08

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('tasks', '0015_task_fill_inner_id'),
    ]

    operations = [
        migrations.AddIndex(
            model_name='annotation',
            index=models.Index(fields=['task', 'completed_by'], name='task_comple_task_id_d49cd7_idx'),
        ),
        migrations.AddIndex(
            model_name='task',
            index=models.Index(fields=['id', 'project'], name='task_id_aef988_idx'),
        ),
    ]
</file>

<file path="label_studio/tasks/migrations/0017_auto_20220330_1310.py">
# Generated by Django 3.1.14 on 2022-03-30 10:10

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('tasks', '0016_auto_20220414_1408'),
    ]

    operations = [
        migrations.AddField(
            model_name='task',
            name='total_annotations',
            field=models.IntegerField(db_index=True, default=0, help_text='Number of total annotations for the current task', verbose_name='total_annotations'),
        ),
        migrations.AddField(
            model_name='task',
            name='cancelled_annotations',
            field=models.IntegerField(db_index=True, default=0, help_text='Number of total cancelled annotations for the current task', verbose_name='cancelled_annotations'),
        ),
        migrations.AddField(
            model_name='task',
            name='total_predictions',
            field=models.IntegerField(db_index=True, default=0, help_text='Number of total predictions for the current task', verbose_name='total_predictions'),
        ),
    ]
</file>

<file path="label_studio/tasks/migrations/0017_new_index_anno_result.py">
# Generated by Django 3.1.13 on 2021-09-13 07:39
import logging

from django.db import migrations
from core.utils.common import trigram_migration_operations

logger = logging.getLogger(__name__)


def forwards(apps, schema_editor):
    if not schema_editor.connection.vendor.startswith('postgres'):
        logger.info('Database vendor: {}'.format(schema_editor.connection.vendor))
        logger.info('Skipping migration without attempting to CREATE INDEX')
        return

    schema_editor.execute(
        'create index concurrently tasks_annotations_result_idx2 '
        'on task_completion using gin (cast(result as text) gin_trgm_ops);'
    )


def backwards(apps, schema_editor):
    if not schema_editor.connection.vendor.startswith('postgres'):
        logger.info('Database vendor: {}'.format(schema_editor.connection.vendor))
        logger.info('Skipping migration without attempting to DROP INDEX')
        return

    schema_editor.execute('drop index tasks_annotations_result_idx2;')
    

class Migration(migrations.Migration):
    atomic = False

    dependencies = [('tasks', '0016_auto_20220414_1408')]

    operations = trigram_migration_operations(migrations.RunPython(forwards, backwards))
</file>

<file path="label_studio/tasks/migrations/0018_manual_migrate_counters.py">
import sys
import logging

from django.db import migrations

logger = logging.getLogger(__name__)


def forwards(apps, schema_editor):
    # This migration was moved to 0024_manual_migrate_counters_again.py

    """
    from tasks.functions import calculate_stats_all_orgs
    from django.conf import settings

    if settings.VERSION_EDITION == 'Community':
        run_command = 'label-studio calculate_stats_all_orgs'
    else:
        run_command = 'cd /label-studio-enterprise/label_studio_enterprise && ' \
                      'python3 manage.py calculate_stats_all_orgs'

    if '--skip-long-migrations' in sys.argv:
        logger.error(
            f"You used --skip-long-migrations, so you should run the migration manually as a separate process "
            f"to recalculate task counters, please use Django command `{run_command}`"
        )
        return

    logger.debug('=> Starting calculate_stats_all_orgs for task counters')
    calculate_stats_all_orgs(from_scratch=False, redis=True)
    """
    return


def backwards(apps, schema_editor):
    pass


class Migration(migrations.Migration):
    atomic = False

    dependencies = [('tasks', '0017_auto_20220330_1310'), ]

    operations = [
        migrations.RunPython(forwards, backwards),
    ]
</file>

<file path="label_studio/tasks/migrations/0019_merge_20220512_2038.py">
# Generated by Django 3.1.14 on 2022-05-12 20:38

from django.db import migrations


class Migration(migrations.Migration):

    dependencies = [
        ('tasks', '0017_new_index_anno_result'),
        ('tasks', '0018_manual_migrate_counters'),
    ]

    operations = [
    ]
</file>

<file path="label_studio/tasks/migrations/0020_auto_20220515_2332.py">
# Generated by Django 3.1.14 on 2022-05-15 23:32

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('tasks', '0019_merge_20220512_2038'),
    ]

    operations = [
        migrations.AlterField(
            model_name='task',
            name='total_annotations',
            field=models.IntegerField(db_index=True, default=0, help_text='Number of total annotations for the current task except cancelled annotations', verbose_name='total_annotations'),
        ),
    ]
</file>

<file path="label_studio/tasks/migrations/0020_auto_20220516_0545.py">
# Generated by Django 3.1.14 on 2022-05-16 05:45

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('tasks', '0019_merge_20220512_2038'),
    ]

    operations = [
        migrations.AddIndex(
            model_name='annotation',
            index=models.Index(fields=['id', 'task'], name='task_comple_id_653858_idx'),
        ),
        migrations.AddIndex(
            model_name='annotation',
            index=models.Index(fields=['task', 'was_cancelled'], name='task_comple_task_id_8072c3_idx'),
        ),
    ]
</file>

<file path="label_studio/tasks/migrations/0021_auto_20220515_2358.py">
# Generated by Django 3.1.14 on 2022-05-15 23:58

from django.conf import settings
from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    dependencies = [
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
        ('tasks', '0020_auto_20220515_2332'),
    ]

    operations = [
        migrations.AddField(
            model_name='annotation',
            name='last_action',
            field=models.CharField(choices=[('prediction', 'Created from prediction'), ('imported', 'Imported'), ('submitted', 'Submitted'), ('updated', 'Updated'), ('skipped', 'Skipped'), ('accepted', 'Accepted'), ('rejected', 'Rejected'), ('fixed_and_accepted', 'Fixed and accepted')], default=None, help_text='Action which was performed in the last annotation history item', max_length=128, null=True, verbose_name='last action'),
        ),
        migrations.AddField(
            model_name='annotation',
            name='last_created_by',
            field=models.ForeignKey(default=None, help_text='User who created the last annotation history item', null=True, on_delete=django.db.models.deletion.SET_NULL, to=settings.AUTH_USER_MODEL, verbose_name='last created by'),
        ),
        migrations.AddIndex(
            model_name='annotation',
            index=models.Index(fields=['last_action'], name='task_comple_last_ac_777e69_idx'),
        ),
        migrations.AddIndex(
            model_name='annotation',
            index=models.Index(fields=['last_created_by'], name='task_comple_last_cr_dedad7_idx'),
        ),
    ]
</file>

<file path="label_studio/tasks/migrations/0022_merge_20220517_1128.py">
# Generated by Django 3.1.14 on 2022-05-17 11:28

from django.db import migrations


class Migration(migrations.Migration):

    dependencies = [
        ('tasks', '0020_auto_20220516_0545'),
        ('tasks', '0021_auto_20220515_2358'),
    ]

    operations = [
    ]
</file>

<file path="label_studio/tasks/migrations/0023_auto_20220620_1007.py">
# Generated by Django 3.1.14 on 2022-06-20 10:07

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('tasks', '0022_merge_20220517_1128'),
    ]

    operations = [
        migrations.AlterField(
            model_name='task',
            name='inner_id',
            field=models.BigIntegerField(default=0, help_text='Internal task ID in the project, starts with 1', null=True, verbose_name='inner id'),
        ),
        migrations.AddIndex(
            model_name='task',
            index=models.Index(fields=['project', 'inner_id'], name='task_project_499b59_idx'),
        ),
    ]
</file>

<file path="label_studio/tasks/migrations/0024_manual_migrate_counters_again.py">
import sys
import logging

from django.db import migrations

logger = logging.getLogger(__name__)


def forwards(apps, schema_editor):
    from tasks.functions import calculate_stats_all_orgs
    from django.conf import settings

    if settings.VERSION_EDITION == 'Community':
        run_command = 'label-studio calculate_stats_all_orgs'
    else:
        run_command = 'cd /label-studio-enterprise/label_studio_enterprise && ' \
                      'python3 manage.py calculate_stats_all_orgs'

    if '--skip-long-migrations' in sys.argv:
        logger.error(
            f"You used --skip-long-migrations, so you should run the migration manually as a separate process "
            f"to recalculate task counters, please use Django command `{run_command}`"
        )
        return

    logger.debug('=> Starting calculate_stats_all_orgs for task counters again')
    calculate_stats_all_orgs(from_scratch=False, redis=True)


def backwards(apps, schema_editor):
    pass


class Migration(migrations.Migration):
    atomic = False

    dependencies = [
        ('tasks', '0023_auto_20220620_1007'),
        ('core', '0001_initial'),
        ('projects', '0017_project_pinned_at'),
    ]

    operations = [
        migrations.RunPython(forwards, backwards),
    ]
</file>

<file path="label_studio/tasks/migrations/0025_auto_20220721_0110.py">
# Generated by Django 3.2.13 on 2022-07-21 01:10

from django.conf import settings
from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
        ('tasks', '0024_manual_migrate_counters_again'),
    ]

    operations = [
        migrations.AddField(
            model_name='task',
            name='comment_authors',
            field=models.ManyToManyField(default=None, help_text='Users who wrote comments', null=True, related_name='tasks_with_comments', to=settings.AUTH_USER_MODEL),
        ),
        migrations.AddField(
            model_name='task',
            name='comment_count',
            field=models.IntegerField(db_index=True, default=0, help_text='Number of comments in the task including all annotations', verbose_name='comment count'),
        ),
        migrations.AddField(
            model_name='task',
            name='last_comment_updated_at',
            field=models.DateTimeField(db_index=True, default=None, help_text='When the last comment was updated', null=True, verbose_name='last comment updated at'),
        ),
        migrations.AddField(
            model_name='task',
            name='unresolved_comment_count',
            field=models.IntegerField(db_index=True, default=0, help_text='Number of unresolved comments in the task including all annotations', verbose_name='unresolved comment count'),
        ),
    ]
</file>

<file path="label_studio/tasks/migrations/0026_auto_20220725_1705.py">
# Generated by Django 3.1.14 on 2022-07-25 17:05

from django.conf import settings
from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
        ('tasks', '0025_auto_20220721_0110'),
    ]

    operations = [
        migrations.AlterField(
            model_name='annotation',
            name='last_action',
            field=models.CharField(choices=[('prediction', 'Created from prediction'), ('propagated_annotation', 'Created from another annotation'), ('imported', 'Imported'), ('submitted', 'Submitted'), ('updated', 'Updated'), ('skipped', 'Skipped'), ('accepted', 'Accepted'), ('rejected', 'Rejected'), ('fixed_and_accepted', 'Fixed and accepted'), ('deleted_review', 'Deleted review')], default=None, help_text='Action which was performed in the last annotation history item', max_length=128, null=True, verbose_name='last action'),
        ),
        migrations.AlterField(
            model_name='task',
            name='comment_authors',
            field=models.ManyToManyField(blank=True, default=None, help_text='Users who wrote comments', related_name='tasks_with_comments', to=settings.AUTH_USER_MODEL),
        ),
    ]
</file>

<file path="label_studio/tasks/migrations/0027_auto_20220801_1728.py">
# Generated by Django 3.2.14 on 2022-08-01 17:28

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('tasks', '0026_auto_20220725_1705'),
    ]

    operations = [
        migrations.AddField(
            model_name='annotationdraft',
            name='was_postponed',
            field=models.BooleanField(db_index=True, default=False, help_text='User postponed this draft (clicked a forward button) in the label stream', verbose_name='was postponed'),
        ),
    ]
</file>

<file path="label_studio/tasks/migrations/0028_auto_20220802_2220.py">
# Generated by Django 3.1.14 on 2022-08-02 22:20

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('tasks', '0027_auto_20220801_1728'),
    ]

    operations = [
        migrations.AlterField(
            model_name='annotationdraft',
            name='lead_time',
            field=models.FloatField(blank=True, help_text='How much time it took to annotate the task', null=True, verbose_name='lead time'),
        ),
    ]
</file>

<file path="label_studio/tasks/migrations/0029_annotation_project.py">
# Generated by Django 3.2.14 on 2022-11-01 17:49

from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    dependencies = [
        ('tasks', '0028_auto_20220802_2220'),
    ]

    operations = [
        migrations.AddField(
            model_name='annotation',
            name='project',
            field=models.ForeignKey(help_text='Project ID for this annotation', null=True, on_delete=django.db.models.deletion.CASCADE, related_name='annotations', to='projects.project'),
        ),
    ]
</file>

<file path="label_studio/tasks/migrations/0030_auto_20221102_1118.py">
# Generated by Django 3.2.14 on 2022-11-02 11:18

from django.db import migrations
from tasks.functions import fill_annotations_project


def forward(apps, schema_editor):
    fill_annotations_project()


def backwards(apps, schema_editor):
    pass


class Migration(migrations.Migration):

    dependencies = [
        ('tasks', '0029_annotation_project'),
    ]

    operations = [
        migrations.RunPython(forward, backwards),
    ]
</file>

<file path="label_studio/tasks/migrations/0031_alter_task_options.py">
# Generated by Django 3.2.14 on 2022-11-25 15:03

from django.db import migrations


class Migration(migrations.Migration):

    dependencies = [
        ('tasks', '0030_auto_20221102_1118'),
    ]

    operations = [
        migrations.AlterModelOptions(
            name='task',
            options={},
        ),
    ]
</file>

<file path="label_studio/tasks/migrations/0032_annotation_updated_by.py">
# Generated by Django 3.2.16 on 2022-11-18 23:38

from django.conf import settings
from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    dependencies = [
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
        ('tasks', '0031_alter_task_options'),
    ]

    operations = [
        migrations.AddField(
            model_name='annotation',
            name='updated_by',
            field=models.ForeignKey(help_text='Last user who updated this annotation', null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='updated_annotations', to=settings.AUTH_USER_MODEL, verbose_name='updated by'),
        ),
    ]
</file>

<file path="label_studio/tasks/migrations/0033_annotation_updated_by_fill.py">
# Generated by Django 3.2.16 on 2022-11-18 23:38

from core.models import AsyncMigrationStatus
from core.redis import start_job_async_or_sync
from django.conf import settings
from django.db import migrations, models
from django.db.models import F

from projects.models import Project
from tasks.models import Annotation

import logging


def _fill_annotations_updated_by():
    project_ids = Project.objects.values_list('id', flat=True)
    for project_id in project_ids:
        migration = AsyncMigrationStatus.objects.filter(project_id=project_id, name='0033_annotation_updated_by_fill').first()
        if migration and migration.status == AsyncMigrationStatus.STATUS_FINISHED:
            # Migration for this project already done
            continue

        migration = AsyncMigrationStatus.objects.create(
                project_id=project_id,
                name='0033_annotation_updated_by_fill',
                status=AsyncMigrationStatus.STATUS_STARTED,
        )

        Annotation.objects.filter(project_id=project_id).update(updated_by=F('completed_by'))
        migration.status = AsyncMigrationStatus.STATUS_FINISHED
        migration.save()


def forward(apps, _):
    annotations = Annotation.objects.all()

    if settings.VERSION_EDITION == 'Community':
        if annotations.count() > 100000:
            command = 'label-studio annotations_fill_updated_by'
            logger = logging.getLogger(__name__)
            logger.error(
                "There are over 100,000 annotations in this label studio instance, please run this "
                f"migration manually using {command}"
            )
            return

    start_job_async_or_sync(_fill_annotations_updated_by)


def backward(apps, _):
    pass


class Migration(migrations.Migration):

    dependencies = [
        ('tasks', '0032_annotation_updated_by')
    ]

    operations = [
        migrations.RunPython(forward, backward)
    ]
</file>

<file path="label_studio/tasks/migrations/0034_annotation_unique_id.py">
# Generated by Django 3.2.16 on 2022-12-22 16:26

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('tasks', '0033_annotation_updated_by_fill'),
    ]

    operations = [
        migrations.AddField(
            model_name='annotation',
            name='unique_id',
            field=models.UUIDField(blank=True, editable=False, null=True, unique=True),
        ),
    ]
</file>

<file path="label_studio/tasks/migrations/0034_auto_20221221_1101.py">
# Generated by Django 3.2.16 on 2022-12-21 11:01

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('tasks', '0033_annotation_updated_by_fill'),
    ]

    operations = [
        migrations.RemoveIndex(
            model_name='annotation',
            name='task_comple_last_cr_dedad7_idx',
        ),
        migrations.RemoveIndex(
            model_name='task',
            name='task_is_labe_215a18_idx',
        ),
        migrations.AlterField(
            model_name='annotation',
            name='ground_truth',
            field=models.BooleanField(default=False, help_text='This annotation is a Ground Truth (ground_truth)', verbose_name='ground_truth'),
        ),
        migrations.AlterField(
            model_name='annotation',
            name='was_cancelled',
            field=models.BooleanField(default=False, help_text='User skipped the task', verbose_name='was cancelled'),
        ),
        migrations.AlterField(
            model_name='task',
            name='is_labeled',
            field=models.BooleanField(default=False, help_text='True if the number of annotations for this task is greater than or equal to the number of maximum_completions for the project', verbose_name='is_labeled'),
        ),
    ]
</file>

<file path="label_studio/tasks/migrations/0035_auto_20221221_1116.py">
# Generated by Django 3.2.16 on 2022-12-21 11:16
import logging

from django.db import migrations
from core.redis import start_job_async_or_sync
from core.utils.common import btree_gin_migration_operations

logger = logging.getLogger(__name__)


def async_index_creation():
    from django.db import connection
    with connection.schema_editor(atomic=False) as schema_editor:
        schema_editor.execute('create index concurrently if not exists tasks_annotations_result_proj_gin '
            'on task_completion using gin (project_id, cast(result as text) gin_trgm_ops);'
        )

def forwards(apps, schema_editor):
    if not schema_editor.connection.vendor.startswith('postgres'):
        logger.info('Database vendor: {}'.format(schema_editor.connection.vendor))
        logger.info('Skipping dropping tasks_annotations_result_idx index')
        return

    schema_editor.execute('drop index if exists tasks_annotations_result_idx;')
    schema_editor.execute('drop index if exists tasks_annotations_result_idx2;')
    start_job_async_or_sync(async_index_creation)


def backwards(apps, schema_editor):
    pass


class Migration(migrations.Migration):
    atomic = False

    dependencies = [('tasks', '0034_auto_20221221_1101')]

    operations = btree_gin_migration_operations(migrations.RunPython(forwards, backwards))
</file>

<file path="label_studio/tasks/migrations/0035_tasklock_unique_id.py">
# Generated by Django 3.2.16 on 2022-12-23 11:01

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('tasks', '0034_annotation_unique_id'),
    ]

    operations = [
        migrations.AddField(
            model_name='tasklock',
            name='unique_id',
            field=models.UUIDField(blank=True, editable=False, null=True, unique=True),
        ),
    ]
</file>

<file path="label_studio/tasks/migrations/0036_auto_20221223_1102.py">
# Generated by Django 3.2.16 on 2022-12-23 11:02

from django.db import migrations, models
import uuid


class Migration(migrations.Migration):

    dependencies = [
        ('tasks', '0035_tasklock_unique_id'),
    ]

    operations = [
        migrations.AlterField(
            model_name='annotation',
            name='unique_id',
            field=models.UUIDField(blank=True, default=uuid.uuid4, editable=False, null=True, unique=True),
        ),
        migrations.AlterField(
            model_name='tasklock',
            name='unique_id',
            field=models.UUIDField(blank=True, default=uuid.uuid4, editable=False, null=True, unique=True),
        ),
    ]
</file>

<file path="label_studio/tasks/migrations/0037_merge_0035_auto_20221221_1116_0036_auto_20221223_1102.py">
# Generated by Django 3.2.16 on 2023-01-18 16:45

from django.db import migrations


class Migration(migrations.Migration):

    dependencies = [
        ('tasks', '0035_auto_20221221_1116'),
        ('tasks', '0036_auto_20221223_1102'),
    ]

    operations = [
    ]
</file>

<file path="label_studio/tasks/migrations/0038_auto_20230209_1412.py">
# Generated by Django 3.2.16 on 2023-02-09 14:12

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('tasks', '0037_merge_0035_auto_20221221_1116_0036_auto_20221223_1102'),
    ]

    operations = [
        migrations.AddIndex(
            model_name='annotation',
            index=models.Index(fields=['project', 'ground_truth'], name='task_comple_project_2cbbfc_idx'),
        ),
        migrations.AddIndex(
            model_name='annotation',
            index=models.Index(fields=['project', 'was_cancelled'], name='task_comple_project_5c60f3_idx'),
        ),
    ]
</file>

<file path="label_studio/tasks/migrations/0039_annotation_draft_created_at.py">
# Generated by Django 3.2.16 on 2023-05-26 21:10

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('tasks', '0038_auto_20230209_1412'),
    ]

    operations = [
        migrations.AddField(
            model_name='annotation',
            name='draft_created_at',
            field=models.DateTimeField(default=None, help_text='Draft creation time', null=True, verbose_name='draft created at'),
        ),
    ]
</file>

<file path="label_studio/tasks/migrations/0040_auto_20230628_1101.py">
# Generated by Django 3.2.19 on 2023-06-28 11:01

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('tasks', '0039_annotation_draft_created_at'),
    ]

    operations = [
        migrations.AddField(
            model_name='annotation',
            name='import_id',
            field=models.BigIntegerField(blank=True, db_index=True, default=None, help_text="Original annotation ID that was at the import step or NULL if this annotation wasn't imported", null=True),
        ),
        migrations.AddField(
            model_name='annotationdraft',
            name='import_id',
            field=models.BigIntegerField(blank=True, db_index=True, default=None, help_text="Original draft ID that was at the import step or NULL if this draft wasn't imported", null=True),
        ),
    ]
</file>

<file path="label_studio/tasks/migrations/0041_prediction_project.py">
# Generated by Django 3.2.19 on 2023-08-10 22:43
from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    dependencies = [
        ('projects', '0024_merge_0023_merge_20230512_1333_0023_projectreimport'),
        ('tasks', '0040_auto_20230628_1101'),
    ]

    operations = [
        migrations.AddField(
            model_name='prediction',
            name='project',
            field=models.ForeignKey(null=True, on_delete=django.db.models.deletion.CASCADE, related_name='predictions', to='projects.project'),
        ),

    ]
</file>

<file path="label_studio/tasks/migrations/0042_auto_20230810_2304.py">
# Generated by Django 3.2.19 on 2023-08-10 23:04

from django.db import migrations
from tasks.functions import fill_predictions_project

def forward(apps, schema_editor):
    fill_predictions_project('0042_auto_20230810_2304')


def backwards(apps, schema_editor):
    pass


class Migration(migrations.Migration):
    atomic = False
    dependencies = [
        ('tasks', '0041_prediction_project'),
    ]

    operations = [
        migrations.RunPython(forward, backwards),
    ]
</file>

<file path="label_studio/tasks/migrations/0043_auto_20230825.py">
# Generated by Django 3.2.19 on 2023-08-10 23:04

from django.db import migrations
from tasks.functions import fill_predictions_project

def forward(apps, schema_editor):
    fill_predictions_project('0043_auto_20230825')


def backwards(apps, schema_editor):
    pass


class Migration(migrations.Migration):
    atomic = False
    dependencies = [
        ('tasks', '0042_auto_20230810_2304'),
    ]

    operations = [
        migrations.RunPython(forward, backwards),
    ]
</file>

<file path="label_studio/tasks/migrations/0044_auto_20230907_0155.py">
# Generated by Django 3.2.20 on 2023-09-07 01:55

from django.db import migrations, models
from django.conf import settings

IS_SQLITE = settings.DJANGO_DB == settings.DJANGO_DB_SQLITE

if IS_SQLITE:
    from django.db.migrations import AddIndex
else:
    from django.contrib.postgres.operations import AddIndexConcurrently as AddIndex


class Migration(migrations.Migration):
    atomic = IS_SQLITE

    dependencies = [
        ('tasks', '0043_auto_20230825'),
    ]

    operations = [
        AddIndex(
            model_name='annotation',
            index=models.Index(fields=['project', 'id'], name='task_comple_project_c7e507_idx'),
        ),
        AddIndex(
            model_name='task',
            index=models.Index(fields=['project', 'id'], name='task_project_7b1c80_idx'),
        ),
    ]
</file>

<file path="label_studio/tasks/migrations/0045_auto_20231124_1238.py">
# Generated by Django 3.2.20 on 2023-11-24 12:38

import logging
from django.db import migrations
from core.redis import start_job_async_or_sync

logger = logging.getLogger(__name__)

def async_index_creation():
    from django.db import connection
    create_index_sql_1 = (
        'CREATE INDEX CONCURRENTLY IF NOT EXISTS task_comple_project_0bc0be_idx '
        'ON task_completion (project_id, completed_by_id);'
    )

    create_index_sql_2 = (
        'CREATE INDEX CONCURRENTLY IF NOT EXISTS task_comple_task_id_a6bdec_idx '
        'ON task_completion (task_id, id);'
    )

    with connection.schema_editor(atomic=False) as schema_editor:
        schema_editor.execute(create_index_sql_1)
        schema_editor.execute(create_index_sql_2)
        logger.info('Indexes created concurrently on annotation model')

def forwards(apps, schema_editor):
    database_vendor = schema_editor.connection.vendor
    if database_vendor != 'postgresql':
        logger.info(f'Database vendor: {database_vendor}')
        logger.info('Skipping async index creation for non-PostgreSQL databases')
        return

    # Schedule the index creation job asynchronously using RQ worker
    start_job_async_or_sync(async_index_creation)

def backwards(apps, schema_editor):
    pass

class Migration(migrations.Migration):
    atomic = False

    dependencies = [
        ('tasks', '0044_auto_20230907_0155'),
    ]

    operations = [
        migrations.RunPython(forwards, backwards)
    ]
</file>

<file path="label_studio/tasks/migrations/0046_auto_20240314_1957.py">
# Generated by Django 3.2.24 on 2024-03-14 19:57

from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    dependencies = [
        ('ml', '0007_auto_20240314_1957'),
        ('tasks', '0045_auto_20231124_1238'),
    ]

    operations = [
        migrations.AddField(
            model_name='prediction',
            name='model',
            field=models.ForeignKey(help_text='An ML Backend instance that created the prediction.', null=True, on_delete=django.db.models.deletion.CASCADE, related_name='predictions', to='ml.mlbackend'),
        ),
        migrations.AlterField(
            model_name='prediction',
            name='model_version',
            field=models.TextField(blank=True, default='', help_text='A string value that for model version that produced the prediction. Used in both live models and when uploading offline predictions.', null=True, verbose_name='model version'),
        ),
    ]
</file>

<file path="label_studio/tasks/migrations/0046_prediction_model_run.py">
# Generated by Django 3.2.23 on 2024-03-13 18:13

from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    dependencies = [
        ('ml_models', '0004_modelrun_job_id'),
        ('tasks', '0045_auto_20231124_1238'),
    ]

    operations = [
        migrations.AddField(
            model_name='prediction',
            name='model_run',
            field=models.ForeignKey(help_text='A run of a ModelVersion that created the prediction.', null=True, on_delete=django.db.models.deletion.CASCADE, related_name='predictions', to='ml_models.modelrun'),
        ),
    ]
</file>

<file path="label_studio/tasks/migrations/0047_merge_20240318_2210.py">
# Generated by Django 3.2.24 on 2024-03-18 22:10

from django.db import migrations


class Migration(migrations.Migration):

    dependencies = [
        ('tasks', '0046_auto_20240314_1957'),
        ('tasks', '0046_prediction_model_run'),
    ]

    operations = [
    ]
</file>

<file path="label_studio/tasks/migrations/0048_failedprediction.py">
# Generated by Django 3.2.25 on 2024-08-14 20:14

from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    dependencies = [
        ('ml', '0007_auto_20240314_1957'),
        ('ml_models', '0009_alter_thirdpartymodelversion_provider'),
        ('projects', '0026_auto_20231103_0020'),
        ('tasks', '0047_merge_20240318_2210'),
    ]

    operations = [
        migrations.CreateModel(
            name='FailedPrediction',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('message', models.TextField(blank=True, default=None, help_text='The message explaining why generating this prediction failed', null=True, verbose_name='message')),
                ('error_type', models.CharField(default=None, help_text='The type of error that caused prediction to fail', max_length=512, null=True, verbose_name='error_type')),
                ('model_version', models.TextField(blank=True, default=None, help_text='A string value that for model version that produced the failed prediction. Used in both live models and when uploading offline predictions.', null=True, verbose_name='model version')),
                ('created_at', models.DateTimeField(auto_now_add=True, verbose_name='created at')),
                ('ml_backend_model', models.ForeignKey(help_text='An ML Backend instance that created the failed prediction.', null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='failed_predictions', to='ml.mlbackend')),
                ('model_run', models.ForeignKey(help_text='A run of a ModelVersion that created the failed prediction.', null=True, on_delete=django.db.models.deletion.CASCADE, related_name='failed_predictions', to='ml_models.modelrun')),
                ('project', models.ForeignKey(null=True, on_delete=django.db.models.deletion.CASCADE, related_name='failed_predictions', to='projects.project')),
                ('task', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='failed_predictions', to='tasks.task')),
            ],
        ),
    ]
</file>

<file path="label_studio/tasks/migrations/0049_auto_20240905_1602.py">
# Generated by Django 3.2.25 on 2024-09-05 16:02

from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    dependencies = [
        ('tasks', '0048_failedprediction'),
    ]

    operations = [
        migrations.CreateModel(
            name='PredictionMeta',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('inference_time', models.FloatField(blank=True, help_text='Time taken for inference in seconds', null=True, verbose_name='inference time')),
                ('prompt_tokens_count', models.IntegerField(blank=True, help_text='Number of tokens in the prompt', null=True, verbose_name='prompt tokens count')),
                ('completion_tokens_count', models.IntegerField(blank=True, help_text='Number of tokens in the completion', null=True, verbose_name='completion tokens count')),
                ('total_tokens_count', models.IntegerField(blank=True, help_text='Total number of tokens', null=True, verbose_name='total tokens count')),
                ('prompt_cost', models.FloatField(blank=True, help_text='Cost of the prompt', null=True, verbose_name='prompt cost')),
                ('completion_cost', models.FloatField(blank=True, help_text='Cost of the completion', null=True, verbose_name='completion cost')),
                ('total_cost', models.FloatField(blank=True, help_text='Total cost', null=True, verbose_name='total cost')),
                ('extra', models.JSONField(blank=True, help_text='Additional metadata in JSON format', null=True, verbose_name='extra')),
                ('failed_prediction', models.OneToOneField(blank=True, help_text='Reference to the associated failed prediction', null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='meta', to='tasks.failedprediction')),
                ('prediction', models.OneToOneField(blank=True, help_text='Reference to the associated prediction', null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='meta', to='tasks.prediction')),
            ],
            options={
                'verbose_name': 'Prediction Meta',
                'verbose_name_plural': 'Prediction Metas',
                'db_table': 'prediction_meta',
            },
        ),
        migrations.AddConstraint(
            model_name='predictionmeta',
            constraint=models.CheckConstraint(check=models.Q(models.Q(('prediction__isnull', False), ('failed_prediction__isnull', True)), models.Q(('prediction__isnull', True), ('failed_prediction__isnull', False)), _connector='OR'), name='prediction_or_failed_prediction_not_null'),
        ),
    ]
</file>

<file path="label_studio/tasks/migrations/0050_alter_predictionmeta_failed_prediction_and_more.py">
# Generated by Django 4.2.15 on 2024-09-24 20:15

from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    dependencies = [
        ('tasks', '0049_auto_20240905_1602'),
    ]

    operations = [
        migrations.AlterField(
            model_name='predictionmeta',
            name='failed_prediction',
            field=models.OneToOneField(blank=True, help_text='Reference to the associated failed prediction', null=True, on_delete=django.db.models.deletion.CASCADE, related_name='meta', to='tasks.failedprediction'),
        ),
        migrations.AlterField(
            model_name='predictionmeta',
            name='prediction',
            field=models.OneToOneField(blank=True, help_text='Reference to the associated prediction', null=True, on_delete=django.db.models.deletion.CASCADE, related_name='meta', to='tasks.prediction'),
        ),
    ]
</file>

<file path="label_studio/tasks/migrations/0051_tasklock_created_at.py">
# Generated by Django 4.2.16 on 2024-11-04 10:37

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('tasks', '0050_alter_predictionmeta_failed_prediction_and_more'),
    ]

    operations = [
        migrations.AddField(
            model_name='tasklock',
            name='created_at',
            field=models.DateTimeField(blank=True, default=None, help_text='Creation time', null=True, verbose_name='created_at'),
        ),
    ]
</file>

<file path="label_studio/tasks/migrations/0052_auto_20241030_1757.py">
# Generated by Django 4.2.16 on 2024-10-30 17:57

from django.db import migrations, models
from django.db import connection
from django.conf import settings

from core.models import AsyncMigrationStatus
from core.redis import start_job_async_or_sync

import logging
logger = logging.getLogger(__name__)
migration_name = '0052_auto_20241030_1757'

if connection.vendor == 'sqlite':
    sql_update_created_at = """
    UPDATE tasks_tasklock
    SET created_at = datetime(expire_at, %s);
    """
    sql_params = (f'-{settings.TASK_LOCK_TTL} seconds',)
else:
    sql_update_created_at = """
    UPDATE tasks_tasklock
    SET created_at = expire_at - INTERVAL %s;
    """
    sql_params = ('%s seconds' % settings.TASK_LOCK_TTL,)

def forward_migration(migration_name):
    migration = AsyncMigrationStatus.objects.create(
        name=migration_name,
        status=AsyncMigrationStatus.STATUS_STARTED,
    )
    logger.info(f'Start async migration {migration_name}')

    with connection.cursor() as cursor:
        cursor.execute(sql_update_created_at, sql_params)

    migration.status = AsyncMigrationStatus.STATUS_FINISHED
    migration.save()
    logger.info(f'Async migration {migration_name} complete')

def forwards(apps, schema_editor):
    # Dispatch migrations to rqworkers
    start_job_async_or_sync(forward_migration, migration_name=migration_name)

def backwards(apps, schema_editor):
    pass

class Migration(migrations.Migration):
    atomic = False

    dependencies = [
        ('tasks', '0051_tasklock_created_at'),
    ]

    operations = [
        migrations.AlterField(
            model_name='tasklock',
            name='created_at',
            field=models.DateTimeField(auto_now_add=True, help_text='Creation time', null=True, verbose_name='created at'),
        ),
        migrations.RunPython(forwards, backwards),
    ]
</file>

<file path="label_studio/tasks/migrations/0053_annotation_bulk_created.py">
# Generated by Django 5.1.4 on 2025-01-14 21:39

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("tasks", "0052_auto_20241030_1757"),
    ]

    operations = [
        migrations.AddField(
            model_name="annotation",
            name="bulk_created",
            field=models.BooleanField(
                db_default=False,
                default=False,
                help_text="Annotation was created in bulk mode",
                null=True,
                verbose_name="bulk created",
            ),
        ),
    ]
</file>

<file path="label_studio/tasks/__init__.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
</file>

<file path="label_studio/tasks/api.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import logging

import drf_yasg.openapi as openapi
from core.mixins import GetParentObjectMixin
from core.permissions import ViewClassPermission, all_permissions
from core.utils.common import DjangoFilterDescriptionInspector
from core.utils.params import bool_from_request
from data_manager.api import TaskListAPI as DMTaskListAPI
from data_manager.functions import evaluate_predictions
from data_manager.models import PrepareParams
from data_manager.serializers import DataManagerTaskSerializer
from django.db import transaction
from django.db.models import Q
from django.utils import timezone
from django.utils.decorators import method_decorator
from django_filters.rest_framework import DjangoFilterBackend
from drf_yasg.utils import no_body, swagger_auto_schema
from projects.functions.stream_history import fill_history_annotation
from projects.models import Project
from rest_framework import generics, viewsets
from rest_framework.exceptions import PermissionDenied
from rest_framework.parsers import FormParser, JSONParser, MultiPartParser
from rest_framework.response import Response
from tasks.models import Annotation, AnnotationDraft, Prediction, Task
from tasks.openapi_schema import (
    annotation_request_schema,
    annotation_response_example,
    dm_task_response_example,
    prediction_request_schema,
    prediction_response_example,
    task_request_schema,
    task_response_example,
)
from tasks.serializers import (
    AnnotationDraftSerializer,
    AnnotationSerializer,
    PredictionSerializer,
    TaskSerializer,
    TaskSimpleSerializer,
)
from webhooks.models import WebhookAction
from webhooks.utils import (
    api_webhook,
    api_webhook_for_delete,
    emit_webhooks_for_instance,
)

logger = logging.getLogger(__name__)


# TODO: fix after switch to api/tasks from api/dm/tasks
@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Tasks'],
        x_fern_sdk_group_name='tasks',
        x_fern_sdk_method_name='create',
        x_fern_audiences=['public'],
        operation_summary='Create task',
        operation_description='Create a new labeling task in Label Studio.',
        request_body=task_request_schema,
        responses={
            '201': openapi.Response(
                description='Created task', schema=TaskSerializer, examples={'application/json': task_response_example}
            )
        },
    ),
)
@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Tasks'],
        x_fern_sdk_group_name='tasks',
        x_fern_sdk_method_name='list',
        x_fern_pagination={
            'offset': '$request.page',
            'results': '$response.tasks',
        },
        x_fern_audiences=['public'],
        operation_summary='Get tasks list',
        operation_description="""
    Retrieve a list of tasks with pagination for a specific view or project, by using filters and ordering.
    """,
        manual_parameters=[
            openapi.Parameter(name='view', type=openapi.TYPE_INTEGER, in_=openapi.IN_QUERY, description='View ID'),
            openapi.Parameter(
                name='project', type=openapi.TYPE_INTEGER, in_=openapi.IN_QUERY, description='Project ID'
            ),
            openapi.Parameter(
                name='resolve_uri',
                type=openapi.TYPE_BOOLEAN,
                in_=openapi.IN_QUERY,
                description='Resolve task data URIs using Cloud Storage',
            ),
            openapi.Parameter(
                name='fields',
                type=openapi.TYPE_STRING,
                enum=['all', 'task_only'],
                default='task_only',
                in_=openapi.IN_QUERY,
                description='Set to "all" if you want to include annotations and predictions in the response',
            ),
            openapi.Parameter(
                name='review',
                type=openapi.TYPE_BOOLEAN,
                in_=openapi.IN_QUERY,
                description='Get tasks for review',
            ),
            openapi.Parameter(
                name='include',
                type=openapi.TYPE_STRING,
                in_=openapi.IN_QUERY,
                description='Specify which fields to include in the response',
            ),
            openapi.Parameter(
                name='query',
                type=openapi.TYPE_STRING,
                in_=openapi.IN_QUERY,
                description='Additional query to filter tasks. It must be JSON encoded string of dict containing '
                'one of the following parameters: `{"filters": ..., "selectedItems": ..., "ordering": ...}`. Check '
                '[Data Manager > Create View > see `data` field](#tag/Data-Manager/operation/api_dm_views_create) '
                'for more details about filters, selectedItems and ordering.\n\n'
                '* **filters**: dict with `"conjunction"` string (`"or"` or `"and"`) and list of filters in `"items"` array. '
                'Each filter is a dictionary with keys: `"filter"`, `"operator"`, `"type"`, `"value"`. '
                '[Read more about available filters](https://labelstud.io/sdk/data_manager.html)<br/>'
                '                   Example: `{"conjunction": "or", "items": [{"filter": "filter:tasks:completed_at", "operator": "greater", "type": "Datetime", "value": "2021-01-01T00:00:00.000Z"}]}`\n'
                '* **selectedItems**: dictionary with keys: `"all"`, `"included"`, `"excluded"`. If "all" is `false`, `"included"` must be used. If "all" is `true`, `"excluded"` must be used.<br/>'
                '                   Examples: `{"all": false, "included": [1, 2, 3]}` or `{"all": true, "excluded": [4, 5]}`\n'
                '* **ordering**: list of fields to order by. Currently, ordering is supported by only one parameter. <br/>\n'
                '                   Example: `["completed_at"]`',
            ),
        ],
        responses={
            '200': openapi.Response(
                description='Tasks list',
                schema=openapi.Schema(
                    type=openapi.TYPE_OBJECT,
                    properties={
                        'tasks': openapi.Schema(
                            description='List of tasks',
                            type=openapi.TYPE_ARRAY,
                            items=openapi.Schema(
                                description='Task object',
                                type=openapi.TYPE_OBJECT,
                                # TODO: provide schema for DataManagerTaskSerializer
                                # Right now the schema is defined in override.yml to ensure each item in paginated response is Task object derived from "#/components/schemas/Task"
                                # We need to figure out more elegant way to define schema for DataManagerTaskSerializer to keep it in sync with Task object
                            ),
                        ),
                        'total': openapi.Schema(description='Total number of tasks', type=openapi.TYPE_INTEGER),
                        'total_annotations': openapi.Schema(
                            description='Total number of annotations', type=openapi.TYPE_INTEGER
                        ),
                        'total_predictions': openapi.Schema(
                            description='Total number of predictions', type=openapi.TYPE_INTEGER
                        ),
                    },
                ),
            )
        },
    ),
)
class TaskListAPI(DMTaskListAPI):
    serializer_class = TaskSerializer
    permission_required = ViewClassPermission(
        GET=all_permissions.tasks_view,
        POST=all_permissions.tasks_create,
    )
    filter_backends = [DjangoFilterBackend]
    filterset_fields = ['project']

    def filter_queryset(self, queryset):
        queryset = super().filter_queryset(queryset)
        return queryset.filter(project__organization=self.request.user.active_organization)

    def get_serializer_context(self):
        context = super().get_serializer_context()
        project_id = self.request.data.get('project')
        if project_id:
            context['project'] = generics.get_object_or_404(Project, pk=project_id)
        return context

    def perform_create(self, serializer):
        project_id = self.request.data.get('project')
        project = generics.get_object_or_404(Project, pk=project_id)
        instance = serializer.save(project=project)
        emit_webhooks_for_instance(
            self.request.user.active_organization, project, WebhookAction.TASKS_CREATED, [instance]
        )


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Tasks'],
        x_fern_sdk_group_name='tasks',
        x_fern_sdk_method_name='get',
        x_fern_audiences=['public'],
        operation_summary='Get task',
        operation_description="""
        Get task data, metadata, annotations and other attributes for a specific labeling task by task ID.
        """,
        manual_parameters=[
            openapi.Parameter(name='id', type=openapi.TYPE_STRING, in_=openapi.IN_PATH, description='Task ID'),
        ],
        request_body=no_body,
        responses={
            '200': openapi.Response(
                description='Task',
                schema=DataManagerTaskSerializer,
                examples={'application/json': dm_task_response_example},
            )
        },
    ),
)
@method_decorator(
    name='patch',
    decorator=swagger_auto_schema(
        tags=['Tasks'],
        x_fern_sdk_group_name='tasks',
        x_fern_sdk_method_name='update',
        x_fern_audiences=['public'],
        operation_summary='Update task',
        operation_description='Update the attributes of an existing labeling task.',
        manual_parameters=[
            openapi.Parameter(name='id', type=openapi.TYPE_STRING, in_=openapi.IN_PATH, description='Task ID'),
        ],
        request_body=task_request_schema,
        responses={
            '200': openapi.Response(
                description='Updated task', schema=TaskSerializer, examples={'application/json': task_response_example}
            )
        },
    ),
)
@method_decorator(
    name='delete',
    decorator=swagger_auto_schema(
        tags=['Tasks'],
        x_fern_sdk_group_name='tasks',
        x_fern_sdk_method_name='delete',
        x_fern_audiences=['public'],
        operation_summary='Delete task',
        operation_description='Delete a task in Label Studio. This action cannot be undone!',
        manual_parameters=[
            openapi.Parameter(name='id', type=openapi.TYPE_STRING, in_=openapi.IN_PATH, description='Task ID'),
        ],
        request_body=no_body,
    ),
)
class TaskAPI(generics.RetrieveUpdateDestroyAPIView):
    parser_classes = (JSONParser, FormParser, MultiPartParser)
    permission_required = ViewClassPermission(
        GET=all_permissions.tasks_view,
        PUT=all_permissions.tasks_change,
        PATCH=all_permissions.tasks_change,
        DELETE=all_permissions.tasks_delete,
    )

    def initial(self, request, *args, **kwargs):
        self.task = self.get_object()
        return super().initial(request, *args, **kwargs)

    @staticmethod
    def prefetch(queryset):
        return queryset.prefetch_related(
            'annotations',
            'predictions',
            'annotations__completed_by',
            'project',
            'io_storages_azureblobimportstoragelink',
            'io_storages_gcsimportstoragelink',
            'io_storages_localfilesimportstoragelink',
            'io_storages_redisimportstoragelink',
            'io_storages_s3importstoragelink',
            'file_upload',
            'project__ml_backends',
        )

    def get_retrieve_serializer_context(self, request):
        fields = ['drafts', 'predictions', 'annotations']

        return {
            'resolve_uri': True,
            'predictions': 'predictions' in fields,
            'annotations': 'annotations' in fields,
            'drafts': 'drafts' in fields,
            'request': request,
        }

    def get(self, request, pk):
        context = self.get_retrieve_serializer_context(request)
        context['project'] = project = self.task.project

        # get prediction
        if (
            project.evaluate_predictions_automatically or project.show_collab_predictions
        ) and not self.task.predictions.exists():
            evaluate_predictions([self.task])
            self.task.refresh_from_db()

        serializer = self.get_serializer_class()(
            self.task, many=False, context=context, expand=['annotations.completed_by']
        )
        data = serializer.data
        return Response(data)

    def get_queryset(self):
        task_id = self.request.parser_context['kwargs'].get('pk')
        task = generics.get_object_or_404(Task, pk=task_id)
        review = bool_from_request(self.request.GET, 'review', False)
        selected = {'all': False, 'included': [self.kwargs.get('pk')]}
        if review:
            kwargs = {'fields_for_evaluation': ['annotators', 'reviewed']}
        else:
            kwargs = {'all_fields': True}
        project = self.request.query_params.get('project') or self.request.data.get('project')
        if not project:
            project = task.project.id
        return self.prefetch(
            Task.prepared.get_queryset(
                prepare_params=PrepareParams(project=project, selectedItems=selected, request=self.request), **kwargs
            )
        )

    def get_serializer_class(self):
        # GET => task + annotations + predictions + drafts
        if self.request.method == 'GET':
            return DataManagerTaskSerializer

        # POST, PATCH, PUT
        else:
            return TaskSimpleSerializer

    def patch(self, request, *args, **kwargs):
        return super(TaskAPI, self).patch(request, *args, **kwargs)

    @api_webhook_for_delete(WebhookAction.TASKS_DELETED)
    def delete(self, request, *args, **kwargs):
        return super(TaskAPI, self).delete(request, *args, **kwargs)

    @swagger_auto_schema(auto_schema=None)
    def put(self, request, *args, **kwargs):
        return super(TaskAPI, self).put(request, *args, **kwargs)


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Annotations'],
        operation_summary='Get annotation by its ID',
        operation_description='Retrieve a specific annotation for a task using the annotation result ID.',
        x_fern_sdk_group_name='annotations',
        x_fern_sdk_method_name='get',
        x_fern_audiences=['public'],
        request_body=no_body,
        responses={
            '200': openapi.Response(
                description='Retrieved annotation',
                schema=AnnotationSerializer,
                examples={'application/json': annotation_response_example},
            )
        },
    ),
)
@method_decorator(
    name='patch',
    decorator=swagger_auto_schema(
        tags=['Annotations'],
        x_fern_sdk_group_name='annotations',
        x_fern_sdk_method_name='update',
        x_fern_audiences=['public'],
        operation_summary='Update annotation',
        operation_description='Update existing attributes on an annotation.',
        request_body=annotation_request_schema,
        responses={
            '200': openapi.Response(
                description='Updated annotation',
                schema=AnnotationSerializer,
                examples={'application/json': annotation_response_example},
            )
        },
    ),
)
@method_decorator(
    name='delete',
    decorator=swagger_auto_schema(
        tags=['Annotations'],
        x_fern_sdk_group_name='annotations',
        x_fern_sdk_method_name='delete',
        x_fern_audiences=['public'],
        operation_summary='Delete annotation',
        operation_description="Delete an annotation. This action can't be undone!",
        request_body=no_body,
    ),
)
class AnnotationAPI(generics.RetrieveUpdateDestroyAPIView):
    parser_classes = (JSONParser, FormParser, MultiPartParser)
    permission_required = ViewClassPermission(
        GET=all_permissions.annotations_view,
        PUT=all_permissions.annotations_change,
        PATCH=all_permissions.annotations_change,
        DELETE=all_permissions.annotations_delete,
    )

    serializer_class = AnnotationSerializer
    queryset = Annotation.objects.all()

    def perform_destroy(self, annotation):
        annotation.delete()

    def update(self, request, *args, **kwargs):
        # save user history with annotator_id, time & annotation result
        annotation = self.get_object()
        # use updated instead of save to avoid duplicated signals
        Annotation.objects.filter(id=annotation.id).update(updated_by=request.user)

        task = annotation.task
        if self.request.data.get('ground_truth'):
            task.ensure_unique_groundtruth(annotation_id=annotation.id)
        task.update_is_labeled()
        task.save()  # refresh task metrics

        result = super(AnnotationAPI, self).update(request, *args, **kwargs)

        task.update_is_labeled()
        task.save(update_fields=['updated_at'])  # refresh task metrics
        return result

    def get(self, request, *args, **kwargs):
        return super(AnnotationAPI, self).get(request, *args, **kwargs)

    @api_webhook(WebhookAction.ANNOTATION_UPDATED)
    @swagger_auto_schema(auto_schema=None)
    def put(self, request, *args, **kwargs):
        return super(AnnotationAPI, self).put(request, *args, **kwargs)

    @api_webhook(WebhookAction.ANNOTATION_UPDATED)
    def patch(self, request, *args, **kwargs):
        return super(AnnotationAPI, self).patch(request, *args, **kwargs)

    @api_webhook_for_delete(WebhookAction.ANNOTATIONS_DELETED)
    def delete(self, request, *args, **kwargs):
        return super(AnnotationAPI, self).delete(request, *args, **kwargs)


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Annotations'],
        x_fern_sdk_group_name='annotations',
        x_fern_sdk_method_name='list',
        x_fern_audiences=['public'],
        operation_summary='Get all task annotations',
        operation_description='List all annotations for a task.',
        manual_parameters=[
            openapi.Parameter(name='id', type=openapi.TYPE_INTEGER, in_=openapi.IN_PATH, description='Task ID'),
        ],
        request_body=no_body,
        responses={
            '200': openapi.Response(
                description='Annotation',
                schema=AnnotationSerializer(many=True),
                examples={'application/json': [annotation_response_example]},
            )
        },
    ),
)
@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Annotations'],
        x_fern_sdk_group_name='annotations',
        x_fern_sdk_method_name='create',
        x_fern_audiences=['public'],
        operation_summary='Create annotation',
        operation_description="""
        Add annotations to a task like an annotator does. The content of the result field depends on your 
        labeling configuration. For example, send the following data as part of your POST 
        request to send an empty annotation with the ID of the user who completed the task:
        
        ```json
        {
        "result": {},
        "was_cancelled": true,
        "ground_truth": true,
        "lead_time": 0,
        "task": 0
        "completed_by": 123
        } 
        ```
        """,
        manual_parameters=[
            openapi.Parameter(name='id', type=openapi.TYPE_INTEGER, in_=openapi.IN_PATH, description='Task ID'),
        ],
        request_body=annotation_request_schema,
        responses={
            '201': openapi.Response(
                description='Created annotation',
                schema=AnnotationSerializer,
                examples={'application/json': annotation_response_example},
            )
        },
    ),
)
class AnnotationsListAPI(GetParentObjectMixin, generics.ListCreateAPIView):
    parser_classes = (JSONParser, FormParser, MultiPartParser)
    permission_required = ViewClassPermission(
        GET=all_permissions.annotations_view,
        POST=all_permissions.annotations_create,
    )
    parent_queryset = Task.objects.all()

    serializer_class = AnnotationSerializer

    def get(self, request, *args, **kwargs):
        return super(AnnotationsListAPI, self).get(request, *args, **kwargs)

    @api_webhook(WebhookAction.ANNOTATION_CREATED)
    def post(self, request, *args, **kwargs):
        return super(AnnotationsListAPI, self).post(request, *args, **kwargs)

    def get_queryset(self):
        task = generics.get_object_or_404(Task.objects.for_user(self.request.user), pk=self.kwargs.get('pk', 0))
        return Annotation.objects.filter(Q(task=task) & Q(was_cancelled=False)).order_by('pk')

    def delete_draft(self, draft_id, annotation_id):
        try:
            draft = AnnotationDraft.objects.get(id=draft_id)
            # We call delete on the individual draft object because
            # AnnotationDraft#delete has special behavior (updating created_labels_drafts).
            # This special behavior won't be triggered if we call delete on the queryset.
            # Only for drafts with empty annotation_id, other ones deleted by signal
            draft.delete()
        except AnnotationDraft.DoesNotExist:
            pass

    def perform_create(self, ser):
        task = self.parent_object
        # annotator has write access only to annotations and it can't be checked it after serializer.save()
        user = self.request.user

        # updates history
        result = ser.validated_data.get('result')
        extra_args = {'task_id': self.kwargs['pk'], 'project_id': task.project_id}

        # save stats about how well annotator annotations coincide with current prediction
        # only for finished task annotations
        if result is not None:
            prediction = Prediction.objects.filter(task=task, model_version=task.project.model_version)
            if prediction.exists():
                prediction = prediction.first()
                prediction_ser = PredictionSerializer(prediction).data
            else:
                logger.debug(f'User={self.request.user}: there are no predictions for task={task}')
                prediction_ser = {}
            # serialize annotation
            extra_args.update({'prediction': prediction_ser, 'updated_by': user})

        if 'was_cancelled' in self.request.GET:
            extra_args['was_cancelled'] = bool_from_request(self.request.GET, 'was_cancelled', False)

        if 'completed_by' not in ser.validated_data:
            extra_args['completed_by'] = self.request.user

        draft_id = self.request.data.get('draft_id')
        draft = AnnotationDraft.objects.filter(id=draft_id).first()
        if draft:
            # draft permission check
            if draft.task_id != task.id or not draft.has_permission(user) or draft.user_id != user.id:
                raise PermissionDenied(f'You have no permission to draft id:{draft_id}')

        if draft is not None:
            # if the annotation will be created from draft - get created_at from draft to keep continuity of history
            extra_args['draft_created_at'] = draft.created_at

        # create annotation
        logger.debug(f'User={self.request.user}: save annotation')
        annotation = ser.save(**extra_args)

        logger.debug(f'Save activity for user={self.request.user}')
        self.request.user.activity_at = timezone.now()
        self.request.user.save()

        # Release task if it has been taken at work (it should be taken by the same user, or it makes sentry error
        logger.debug(f'User={user} releases task={task}')
        task.release_lock(user)

        # if annotation created from draft - remove this draft
        if draft_id is not None:
            logger.debug(f'Remove draft {draft_id} after creating annotation {annotation.id}')
            self.delete_draft(draft_id, annotation.id)

        if self.request.data.get('ground_truth'):
            annotation.task.ensure_unique_groundtruth(annotation_id=annotation.id)

        fill_history_annotation(user, task, annotation)

        return annotation


class AnnotationDraftListAPI(generics.ListCreateAPIView):
    parser_classes = (JSONParser, MultiPartParser, FormParser)
    serializer_class = AnnotationDraftSerializer
    permission_required = ViewClassPermission(
        GET=all_permissions.annotations_view,
        POST=all_permissions.annotations_create,
    )
    queryset = AnnotationDraft.objects.all()
    swagger_schema = None

    def filter_queryset(self, queryset):
        task_id = self.kwargs['pk']
        return queryset.filter(task_id=task_id)

    def perform_create(self, serializer):
        task_id = self.kwargs['pk']
        annotation_id = self.kwargs.get('annotation_id')
        user = self.request.user
        logger.debug(f'User {user} is going to create draft for task={task_id}, annotation={annotation_id}')
        serializer.save(task_id=self.kwargs['pk'], annotation_id=annotation_id, user=self.request.user)


class AnnotationDraftAPI(generics.RetrieveUpdateDestroyAPIView):
    parser_classes = (JSONParser, MultiPartParser, FormParser)
    serializer_class = AnnotationDraftSerializer
    queryset = AnnotationDraft.objects.all()
    permission_required = ViewClassPermission(
        GET=all_permissions.annotations_view,
        PUT=all_permissions.annotations_change,
        PATCH=all_permissions.annotations_change,
        DELETE=all_permissions.annotations_delete,
    )
    swagger_schema = None


@method_decorator(
    name='list',
    decorator=swagger_auto_schema(
        tags=['Predictions'],
        x_fern_sdk_group_name='predictions',
        x_fern_sdk_method_name='list',
        x_fern_audiences=['public'],
        operation_summary='List predictions',
        filter_inspectors=[DjangoFilterDescriptionInspector],
        operation_description='List all predictions and their IDs.',
        manual_parameters=[
            openapi.Parameter(
                name='task',
                type=openapi.TYPE_INTEGER,
                in_=openapi.IN_QUERY,
                description='Filter predictions by task ID',
            ),
            openapi.Parameter(
                name='project',
                type=openapi.TYPE_INTEGER,
                in_=openapi.IN_QUERY,
                description='Filter predictions by project ID',
            ),
        ],
        request_body=no_body,
        responses={
            '200': openapi.Response(
                description='Predictions list',
                schema=PredictionSerializer(many=True),
                examples={'application/json': [prediction_response_example]},
            )
        },
    ),
)
@method_decorator(
    name='create',
    decorator=swagger_auto_schema(
        tags=['Predictions'],
        x_fern_sdk_group_name='predictions',
        x_fern_sdk_method_name='create',
        x_fern_audiences=['public'],
        operation_summary='Create prediction',
        operation_description='Create a prediction for a specific task.',
        request_body=prediction_request_schema,
        responses={
            '201': openapi.Response(
                description='Created prediction',
                schema=PredictionSerializer,
                examples={'application/json': prediction_response_example},
            )
        },
    ),
)
@method_decorator(
    name='retrieve',
    decorator=swagger_auto_schema(
        tags=['Predictions'],
        x_fern_sdk_group_name='predictions',
        x_fern_sdk_method_name='get',
        x_fern_audiences=['public'],
        operation_summary='Get prediction details',
        operation_description='Get details about a specific prediction by its ID.',
        manual_parameters=[
            openapi.Parameter(name='id', type=openapi.TYPE_INTEGER, in_=openapi.IN_PATH, description='Prediction ID'),
        ],
        request_body=no_body,
        responses={
            '200': openapi.Response(
                description='Prediction details',
                schema=PredictionSerializer,
                examples={'application/json': prediction_response_example},
            )
        },
    ),
)
@method_decorator(
    name='update',
    decorator=swagger_auto_schema(
        tags=['Predictions'],
        operation_summary='Put prediction',
        x_fern_audiences=['internal'],
        operation_description='Overwrite prediction data by prediction ID.',
        manual_parameters=[
            openapi.Parameter(name='id', type=openapi.TYPE_INTEGER, in_=openapi.IN_PATH, description='Prediction ID'),
        ],
        request_body=prediction_request_schema,
        responses={
            '200': openapi.Response(
                description='Updated prediction',
                schema=PredictionSerializer,
                examples={'application/json': prediction_response_example},
            )
        },
    ),
)
@method_decorator(
    name='partial_update',
    decorator=swagger_auto_schema(
        tags=['Predictions'],
        x_fern_sdk_group_name='predictions',
        x_fern_sdk_method_name='update',
        x_fern_audiences=['public'],
        operation_summary='Update prediction',
        operation_description='Update prediction data by prediction ID.',
        manual_parameters=[
            openapi.Parameter(name='id', type=openapi.TYPE_INTEGER, in_=openapi.IN_PATH, description='Prediction ID'),
        ],
        request_body=prediction_request_schema,
        responses={
            '200': openapi.Response(
                description='Updated prediction',
                schema=PredictionSerializer,
                examples={'application/json': prediction_response_example},
            )
        },
    ),
)
@method_decorator(
    name='destroy',
    decorator=swagger_auto_schema(
        tags=['Predictions'],
        x_fern_sdk_group_name='predictions',
        x_fern_sdk_method_name='delete',
        x_fern_audiences=['public'],
        operation_summary='Delete prediction',
        operation_description='Delete a prediction by prediction ID.',
        manual_parameters=[
            openapi.Parameter(name='id', type=openapi.TYPE_INTEGER, in_=openapi.IN_PATH, description='Prediction ID'),
        ],
        request_body=no_body,
    ),
)
class PredictionAPI(viewsets.ModelViewSet):
    serializer_class = PredictionSerializer
    permission_required = all_permissions.predictions_any
    filter_backends = [DjangoFilterBackend]
    filterset_fields = ['task', 'task__project', 'project']

    def get_queryset(self):
        return Prediction.objects.filter(project__organization=self.request.user.active_organization)


@method_decorator(name='get', decorator=swagger_auto_schema(auto_schema=None))
@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Annotations'],
        x_fern_audiences=['internal'],
        operation_summary='Convert annotation to draft',
        operation_description='Convert annotation to draft',
    ),
)
class AnnotationConvertAPI(generics.RetrieveAPIView):
    permission_required = ViewClassPermission(POST=all_permissions.annotations_change)
    queryset = Annotation.objects.all()

    def process_intermediate_state(self, annotation, draft):
        pass

    def post(self, request, *args, **kwargs):
        annotation = self.get_object()
        organization = annotation.project.organization
        project = annotation.project

        pk = annotation.pk

        with transaction.atomic():
            draft = AnnotationDraft.objects.create(
                result=annotation.result,
                lead_time=annotation.lead_time,
                task=annotation.task,
                annotation=None,
                user=request.user,
            )

            self.process_intermediate_state(annotation, draft)

            annotation.delete()

        emit_webhooks_for_instance(organization, project, WebhookAction.ANNOTATIONS_DELETED, [pk])
        data = AnnotationDraftSerializer(instance=draft).data
        return Response(status=201, data=data)
</file>

<file path="label_studio/tasks/choices.py">
from django.db import models
from django.utils.translation import gettext_lazy as _


class ActionType(models.TextChoices):
    CREATED_FROM_PREDICTION = 'prediction', _('Created from prediction')
    PROPAGATED_ANNOTATION = 'propagated_annotation', _('Created from another annotation')
    IMPORTED = 'imported', _('Imported')
    SUBMITTED = 'submitted', _('Submitted')
    UPDATED = 'updated', _('Updated')
    SKIPPED = 'skipped', _('Skipped')
    ACCEPTED = 'accepted', _('Accepted')
    REJECTED = 'rejected', _('Rejected')
    FIXED_AND_ACCEPTED = 'fixed_and_accepted', _('Fixed and accepted')
    DELETED_REVIEW = 'deleted_review', _('Deleted review')
</file>

<file path="label_studio/tasks/exceptions.py">
from core.utils.exceptions import LabelStudioAPIException
from rest_framework import status


class AnnotationDuplicateError(LabelStudioAPIException):
    status_code = status.HTTP_409_CONFLICT
    default_detail = 'Annotation with this unique id already exists'
</file>

<file path="label_studio/tasks/functions.py">
import json
import logging
import os
import shutil
import sys

from core.bulk_update_utils import bulk_update
from core.models import AsyncMigrationStatus
from core.redis import start_job_async_or_sync
from core.utils.common import batch
from data_export.mixins import ExportMixin
from data_export.models import DataExport
from data_export.serializers import ExportDataSerializer
from data_manager.managers import TaskQuerySet
from django.conf import settings
from django.db import transaction
from django.db.models import Count, Q
from organizations.models import Organization
from projects.models import Project
from tasks.models import Annotation, Prediction, Task

logger = logging.getLogger(__name__)


def calculate_stats_all_orgs(from_scratch, redis, migration_name='0018_manual_migrate_counters'):
    logger = logging.getLogger(__name__)
    # Don't load full Organization objects bc some columns (contact_info, verify_ssl_certs)
    # aren't created until after a migration calls this code
    organization_ids = Organization.objects.order_by('-id').values_list('id', flat=True)

    for org_id in organization_ids:
        logger.debug(f'Start recalculating stats for Organization {org_id}')

        # start async calculation job on redis
        start_job_async_or_sync(
            redis_job_for_calculation,
            org_id,
            from_scratch,
            redis=redis,
            queue_name='critical',
            job_timeout=3600 * 24,  # 24 hours for one organization
            migration_name=migration_name,
        )

        logger.debug(f'Organization {org_id} stats were recalculated')

    logger.debug('All organizations were recalculated')


def redis_job_for_calculation(org_id, from_scratch, migration_name='0018_manual_migrate_counters'):
    """
    Recalculate counters for projects list
    :param org_id: ID of organization to recalculate
    :param from_scratch: Start calculation from scratch or skip calculated tasks
    """
    logger = logging.getLogger()
    logger.setLevel(logging.DEBUG)

    handler = logging.StreamHandler(sys.stdout)
    handler.setLevel(logging.DEBUG)
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)

    project_dicts = (
        Project.objects.filter(organization_id=org_id)
        .order_by('-updated_at')
        .values(
            'id',
            'updated_at',
            'title',
        )
    )
    for project_dict in project_dicts:
        migration = AsyncMigrationStatus.objects.create(
            project_id=project_dict['id'],
            name=migration_name,
            status=AsyncMigrationStatus.STATUS_STARTED,
        )
        project_tasks = Task.objects.filter(project_id=project_dict['id'])
        logger.debug(
            f'Start processing stats project <{project_dict["title"]}> ({project_dict["id"]}) '
            f'with task count {project_tasks.count()} and updated_at {project_dict["updated_at"]}'
        )

        task_count = update_tasks_counters(project_tasks, from_scratch=from_scratch)

        migration.status = AsyncMigrationStatus.STATUS_FINISHED
        migration.meta = {'tasks_processed': task_count, 'total_project_tasks': project_tasks.count()}
        migration.save()
        logger.debug(
            f'End processing counters for project <{project_dict["title"]}> ({project_dict["id"]}), '
            f'processed {str(task_count)} tasks'
        )


def export_project(project_id, export_format, path, serializer_context=None):
    logger = logging.getLogger(__name__)

    project = Project.objects.get(id=project_id)

    export_format = export_format.upper()
    supported_formats = [s['name'] for s in DataExport.get_export_formats(project)]
    assert export_format in supported_formats, f'Export format is not supported, please use {supported_formats}'

    task_ids = (
        Task.objects.filter(project=project).select_related('project').prefetch_related('annotations', 'predictions')
    )

    logger.debug(f'Start exporting project <{project.title}> ({project.id}) with task count {task_ids.count()}.')

    # serializer context
    if isinstance(serializer_context, str):
        serializer_context = json.loads(serializer_context)
    serializer_options = ExportMixin._get_export_serializer_option(serializer_context)

    # export cycle
    tasks = []
    for _task_ids in batch(task_ids, 1000):
        tasks += ExportDataSerializer(_task_ids, many=True, **serializer_options).data

    # convert to output format
    export_file, _, filename = DataExport.generate_export_file(
        project, tasks, export_format, settings.CONVERTER_DOWNLOAD_RESOURCES, {}
    )

    # write to file
    filepath = os.path.join(path, filename) if os.path.isdir(path) else path
    with open(filepath, 'wb') as file:
        shutil.copyfileobj(export_file, file)
    export_file.close()

    logger.debug(f'End exporting project <{project.title}> ({project.id}) in {export_format} format.')

    return filepath


def _fill_annotations_project(project_id):
    Annotation.objects.filter(task__project_id=project_id).update(project_id=project_id)


def fill_annotations_project():
    logger.info('Start filling project field for Annotation model')

    project_ids = Project.objects.all().values_list('id', flat=True)
    for project_id in project_ids:
        start_job_async_or_sync(_fill_annotations_project, project_id)

    logger.info('Finished filling project field for Annotation model')


def _fill_predictions_project(migration_name='0043_auto_20230825'):
    project_ids = Project.objects.all().values_list('id', flat=True)
    for project_id in project_ids:
        migration = AsyncMigrationStatus.objects.create(
            project_id=project_id,
            name=migration_name,
            status=AsyncMigrationStatus.STATUS_STARTED,
        )

        updated_count = Prediction.objects.filter(task__project_id=project_id).update(project_id=project_id)

        migration.status = AsyncMigrationStatus.STATUS_FINISHED
        migration.meta = {
            'predictions_processed': updated_count,
            'total_project_predictions': Prediction.objects.filter(project_id=project_id).count(),
        }
        migration.save()


def fill_predictions_project(migration_name):
    logger.info('Start filling project field for Prediction model')
    start_job_async_or_sync(_fill_predictions_project, migration_name=migration_name)
    logger.info('Finished filling project field for Prediction model')


def update_tasks_counters(queryset, from_scratch=True):
    """
    Update tasks counters for the passed queryset of Tasks
    :param queryset: Tasks to update queryset
    :param from_scratch: Skip calculated tasks
    :return: Count of updated tasks
    """
    objs = []

    total_annotations = Count('annotations', distinct=True, filter=Q(annotations__was_cancelled=False))
    cancelled_annotations = Count('annotations', distinct=True, filter=Q(annotations__was_cancelled=True))
    total_predictions = Count('predictions', distinct=True)
    # construct QuerySet in case of list of Tasks
    if isinstance(queryset, list) and len(queryset) > 0 and isinstance(queryset[0], Task):
        queryset = Task.objects.filter(id__in=[task.id for task in queryset])
    # construct QuerySet in case annotated queryset
    if isinstance(queryset, TaskQuerySet) and queryset.exists() and isinstance(queryset[0], int):
        queryset = Task.objects.filter(id__in=queryset)

    if not from_scratch:
        queryset = queryset.exclude(
            Q(total_annotations__gt=0) | Q(cancelled_annotations__gt=0) | Q(total_predictions__gt=0)
        )

    # filter our tasks with 0 annotations and 0 predictions and update them with 0
    queryset.filter(annotations__isnull=True, predictions__isnull=True).update(
        total_annotations=0, cancelled_annotations=0, total_predictions=0
    )

    # filter our tasks with 0 annotations and 0 predictions
    queryset = queryset.filter(Q(annotations__isnull=False) | Q(predictions__isnull=False))
    queryset = queryset.annotate(
        new_total_annotations=total_annotations,
        new_cancelled_annotations=cancelled_annotations,
        new_total_predictions=total_predictions,
    )

    for task in queryset.only('id', 'total_annotations', 'cancelled_annotations', 'total_predictions'):
        task.total_annotations = task.new_total_annotations
        task.cancelled_annotations = task.new_cancelled_annotations
        task.total_predictions = task.new_total_predictions
        objs.append(task)
    with transaction.atomic():
        bulk_update(
            objs,
            update_fields=['total_annotations', 'cancelled_annotations', 'total_predictions'],
            batch_size=settings.BATCH_SIZE,
        )
    return len(objs)
</file>

<file path="label_studio/tasks/mixins.py">
class TaskMixin:
    def has_permission(self, user: 'User') -> bool:  # noqa: F821
        """Called by Task#has_permission"""
        return True

    def _get_is_labeled_value(self) -> bool:
        n = self.completed_annotations.count()
        return n >= self.overlap

    def update_is_labeled(self, *args, **kwargs) -> None:
        self.is_labeled = self._get_is_labeled_value()

    @classmethod
    def post_process_bulk_update_stats(cls, tasks) -> None:
        pass

    def before_delete_actions(self):
        """
        Actions to execute before task deletion
        """
        pass

    @staticmethod
    def after_bulk_delete_actions(tasks_ids):
        """
        Actions to execute after bulk task deletion
        """
        pass

    def get_rejected_query(self):
        pass


class AnnotationMixin:
    def has_permission(self, user: 'User') -> bool:  # noqa: F821
        """Called by Annotation#has_permission"""
        return True
</file>

<file path="label_studio/tasks/models.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import base64
import datetime
import logging
import numbers
import os
import random
import traceback
import uuid
from typing import Any, Mapping, Optional, Union, cast
from urllib.parse import urljoin

import ujson as json
from core.bulk_update_utils import bulk_update
from core.current_request import get_current_request
from core.feature_flags import flag_set
from core.label_config import SINGLE_VALUED_TAGS
from core.redis import start_job_async_or_sync
from core.utils.common import (
    find_first_one_to_one_related_field_by_prefix,
    load_func,
    string_is_url,
    temporary_disconnect_list_signal,
)
from core.utils.db import fast_first
from core.utils.params import get_env
from data_import.models import FileUpload
from data_manager.managers import PreparedTaskManager, TaskManager
from django.conf import settings
from django.db import OperationalError, models, transaction
from django.db.models import CheckConstraint, F, JSONField, Q
from django.db.models.lookups import GreaterThanOrEqual
from django.db.models.signals import post_delete, post_save, pre_delete, pre_save
from django.dispatch import Signal, receiver
from django.urls import reverse
from django.utils.timesince import timesince
from django.utils.timezone import now
from django.utils.translation import gettext_lazy as _
from label_studio_sdk.label_interface.objects import PredictionValue
from rest_framework.exceptions import ValidationError
from tasks.choices import ActionType

logger = logging.getLogger(__name__)

TaskMixin = load_func(settings.TASK_MIXIN)


class Task(TaskMixin, models.Model):
    """Business tasks from project"""

    id = models.AutoField(
        auto_created=True,
        primary_key=True,
        serialize=False,
        verbose_name='ID',
        db_index=True,
    )
    data = JSONField(
        'data',
        null=False,
        help_text='User imported or uploaded data for a task. Data is formatted according to '
        'the project label config. You can find examples of data for your project '
        'on the Import page in the Label Studio Data Manager UI.',
    )

    meta = JSONField(
        'meta',
        null=True,
        default=dict,
        help_text='Meta is user imported (uploaded) data and can be useful as input for an ML '
        'Backend for embeddings, advanced vectors, and other info. It is passed to '
        'ML during training/predicting steps.',
    )
    project = models.ForeignKey(
        'projects.Project',
        related_name='tasks',
        on_delete=models.CASCADE,
        null=True,
        help_text='Project ID for this task',
    )
    created_at = models.DateTimeField(_('created at'), auto_now_add=True, help_text='Time a task was created')
    updated_at = models.DateTimeField(_('updated at'), auto_now=True, help_text='Last time a task was updated')
    updated_by = models.ForeignKey(
        settings.AUTH_USER_MODEL,
        related_name='updated_tasks',
        on_delete=models.SET_NULL,
        null=True,
        verbose_name=_('updated by'),
        help_text='Last annotator or reviewer who updated this task',
    )
    is_labeled = models.BooleanField(
        _('is_labeled'),
        default=False,
        help_text='True if the number of annotations for this task is greater than or equal '
        'to the number of maximum_completions for the project',
    )
    overlap = models.IntegerField(
        _('overlap'),
        default=1,
        db_index=True,
        help_text='Number of distinct annotators that processed the current task',
    )
    file_upload = models.ForeignKey(
        'data_import.FileUpload',
        on_delete=models.SET_NULL,
        null=True,
        blank=True,
        related_name='tasks',
        help_text='Uploaded file used as data source for this task',
    )
    inner_id = models.BigIntegerField(
        _('inner id'),
        default=0,
        null=True,
        help_text='Internal task ID in the project, starts with 1',
    )
    updates = ['is_labeled']
    total_annotations = models.IntegerField(
        _('total_annotations'),
        default=0,
        db_index=True,
        help_text='Number of total annotations for the current task except cancelled annotations',
    )
    cancelled_annotations = models.IntegerField(
        _('cancelled_annotations'),
        default=0,
        db_index=True,
        help_text='Number of total cancelled annotations for the current task',
    )
    total_predictions = models.IntegerField(
        _('total_predictions'),
        default=0,
        db_index=True,
        help_text='Number of total predictions for the current task',
    )

    comment_count = models.IntegerField(
        _('comment count'),
        default=0,
        db_index=True,
        help_text='Number of comments in the task including all annotations',
    )
    unresolved_comment_count = models.IntegerField(
        _('unresolved comment count'),
        default=0,
        db_index=True,
        help_text='Number of unresolved comments in the task including all annotations',
    )
    comment_authors = models.ManyToManyField(
        settings.AUTH_USER_MODEL,
        blank=True,
        default=None,
        related_name='tasks_with_comments',
        help_text='Users who wrote comments',
    )
    last_comment_updated_at = models.DateTimeField(
        _('last comment updated at'),
        default=None,
        null=True,
        db_index=True,
        help_text='When the last comment was updated',
    )

    objects = TaskManager()  # task manager by default
    prepared = PreparedTaskManager()  # task manager with filters, ordering, etc for data_manager app

    class Meta:
        db_table = 'task'
        indexes = [
            models.Index(fields=['project', 'is_labeled']),
            models.Index(fields=['project', 'inner_id']),
            models.Index(fields=['id', 'project']),
            models.Index(fields=['id', 'overlap']),
            models.Index(fields=['overlap']),
            models.Index(fields=['project', 'id']),
        ]

    @property
    def file_upload_name(self):
        return os.path.basename(self.file_upload.file.name)

    @classmethod
    def get_random(cls, project):
        """Get random task from a project, this should not be used lightly as its expensive method to run"""

        ids = cls.objects.filter(project=project).values_list('id', flat=True)
        if len(ids) == 0:
            return None

        random_id = random.choice(ids)

        return cls.objects.get(id=random_id)

    @classmethod
    def get_locked_by(cls, user, project=None, tasks=None):
        """Retrieve the task locked by specified user. Returns None if the specified user didn't lock anything."""
        lock = None
        if project is not None:
            lock = fast_first(TaskLock.objects.filter(user=user, expire_at__gt=now(), task__project=project))
        elif tasks is not None:
            locked_task = fast_first(tasks.filter(locks__user=user, locks__expire_at__gt=now()))
            if locked_task:
                return locked_task
        else:
            raise Exception('Neither project or tasks passed to get_locked_by')

        if lock:
            return lock.task

    def get_predictions_for_prelabeling(self):
        """This is called to return either new predictions from the
        model or grab static predictions if they were set, depending
        on the projects configuration.

        """
        from data_manager.functions import evaluate_predictions

        project = self.project
        predictions = self.predictions

        # TODO if we use live_model on project then we will need to check for it here
        if project.show_collab_predictions and project.model_version is not None:
            if project.ml_backend_in_model_version:
                new_predictions = evaluate_predictions([self])
                # TODO this is not as clean as I'd want it to
                # be. Effectively retrieve_predictions will work only for
                # tasks where there is no predictions matching current
                # model version. In case it will return a model_version
                # and we can grab predictions explicitly
                if isinstance(new_predictions, str):
                    model_version = new_predictions
                    return predictions.filter(model_version=model_version)
                else:
                    return new_predictions
            else:
                return predictions.filter(model_version=project.model_version)
        else:
            return []

    def get_lock_exclude_query(self, user):
        """
        Get query for excluding annotations from the lock check
        """
        SkipQueue = self.project.SkipQueue

        if self.project.skip_queue == SkipQueue.IGNORE_SKIPPED:
            # IGNORE_SKIPPED: my skipped tasks don't go anywhere
            # alien's and my skipped annotations are counted as regular annotations
            q = Q()
        else:
            if self.project.skip_queue == SkipQueue.REQUEUE_FOR_ME:
                # REQUEUE_FOR_ME means: only my skipped tasks go back to me,
                # alien's skipped annotations are counted as regular annotations
                q = Q(was_cancelled=True) & Q(completed_by=user)
            elif self.project.skip_queue == SkipQueue.REQUEUE_FOR_OTHERS:
                # REQUEUE_FOR_OTHERS: my skipped tasks go to others
                # alien's skipped annotations are not counted at all
                q = Q(was_cancelled=True) & ~Q(completed_by=user)
            else:
                raise Exception(f'Invalid SkipQueue value: {self.project.skip_queue}')

            # for LSE we also need to exclude rejected queue
            rejected_q = self.get_rejected_query()

            if rejected_q:
                q &= rejected_q

        return q | Q(ground_truth=True)

    def has_lock(self, user=None):
        """
        Check whether current task has been locked by some user

        Also has workaround for fixing not consistent is_labeled flag state
        """
        from projects.functions.next_task import get_next_task_logging_level

        q = self.get_lock_exclude_query(user)

        num_locks = self.num_locks_user(user=user)
        num_annotations = self.annotations.exclude(q).count()
        num = num_locks + num_annotations

        if num > self.overlap_with_agreement_threshold(num, num_locks):
            logger.error(
                f'Num takes={num} > overlap={self.overlap} for task={self.id}, '
                f"skipped mode {self.project.skip_queue} - it's a bug",
                extra=dict(
                    lock_ttl=self.locks.values_list('user', 'expire_at'),
                    num_locks=num_locks,
                    num_annotations=num_annotations,
                ),
            )
            # TODO: remove this workaround after fixing the bug with inconsistent is_labeled flag
            if self.is_labeled is False:
                self.update_is_labeled()
                if self.is_labeled is True:
                    self.save(update_fields=['is_labeled'])

        result = bool(num >= self.overlap_with_agreement_threshold(num, num_locks))
        logger.log(
            get_next_task_logging_level(user),
            f'Task {self} locked: {result}; num_locks: {num_locks} num_annotations: {num_annotations} '
            f'skipped mode: {self.project.skip_queue}',
        )
        return result

    @property
    def num_locks(self):
        return self.locks.filter(expire_at__gt=now()).count()

    def overlap_with_agreement_threshold(self, num, num_locks):
        # Limit to one extra annotator at a time when the task is under the threshold and meets the overlap criteria,
        # regardless of the max_additional_annotators_assignable setting. This ensures recalculating agreement after
        # each annotation and prevents concurrent annotations from dropping the agreement below the threshold.
        if (
            hasattr(self.project, 'lse_project')
            and self.project.lse_project
            and self.project.lse_project.agreement_threshold is not None
        ):
            try:
                from stats.models import get_task_agreement
            except (ModuleNotFoundError, ImportError):
                return

            agreement = get_task_agreement(self)
            if agreement is not None and agreement < self.project.lse_project.agreement_threshold:
                return (
                    min(self.overlap + self.project.lse_project.max_additional_annotators_assignable, num + 1)
                    if num_locks == 0
                    else num
                )
        return self.overlap

    def num_locks_user(self, user):
        return self.locks.filter(expire_at__gt=now()).exclude(user=user).count()

    def get_storage_filename(self):
        for link_name in settings.IO_STORAGES_IMPORT_LINK_NAMES:
            if hasattr(self, link_name):
                return getattr(self, link_name).key

    def has_permission(self, user: 'User') -> bool:  # noqa: F821
        mixin_has_permission = cast(bool, super().has_permission(user))

        user.project = self.project  # link for activity log
        return mixin_has_permission and self.project.has_permission(user)

    def clear_expired_locks(self):
        self.locks.filter(expire_at__lt=now()).delete()

    def set_lock(self, user):
        """Lock current task by specified user. Lock lifetime is set by `expire_in_secs`"""
        from projects.functions.next_task import get_next_task_logging_level

        num_locks = self.num_locks
        if num_locks < self.overlap:
            lock_ttl = settings.TASK_LOCK_TTL
            if (
                flag_set('fflag_feat_all_leap_1534_custom_task_lock_timeout_short', user=user)
                and self.project.custom_task_lock_ttl
            ):
                lock_ttl = self.project.custom_task_lock_ttl
            expire_at = now() + datetime.timedelta(seconds=lock_ttl)
            try:
                task_lock = TaskLock.objects.get(task=self, user=user)
            except TaskLock.DoesNotExist:
                TaskLock.objects.create(task=self, user=user, expire_at=expire_at)
            else:
                task_lock.expire_at = expire_at
                task_lock.save()
            logger.log(
                get_next_task_logging_level(user),
                f'User={user} acquires a lock for the task={self} ttl: {lock_ttl}',
            )
        else:
            logger.error(
                f'Current number of locks for task {self.id} is {num_locks}, but overlap={self.overlap}: '
                f"that's a bug because this task should not be taken in a label stream (task should be locked)"
            )
        self.clear_expired_locks()

    def release_lock(self, user=None):
        """Release lock for the task.
        If user specified, it checks whether lock is released by the user who previously has locked that task
        """

        if user is not None:
            self.locks.filter(user=user).delete()
        else:
            self.locks.all().delete()
        self.clear_expired_locks()

    def get_storage_link(self):
        # TODO: how to get neatly any storage class here?
        return find_first_one_to_one_related_field_by_prefix(self, '.*io_storages_')

    @staticmethod
    def is_upload_file(filename):
        if not isinstance(filename, str):
            return False
        return filename.startswith(settings.UPLOAD_DIR + '/')

    @staticmethod
    def prepare_filename(filename):
        if isinstance(filename, str):
            return filename.replace(settings.MEDIA_URL, '')
        return filename

    def resolve_storage_uri(self, url) -> Optional[Mapping[str, Any]]:
        from io_storages.functions import get_storage_by_url

        storage = self.storage
        project = self.project

        if not storage:
            storage_objects = project.get_all_storage_objects(type_='import')
            storage = get_storage_by_url(url, storage_objects)

        if storage:
            return {
                'url': storage.generate_http_url(url),
                'presign_ttl': storage.presign_ttl,
            }

    def resolve_uri(self, task_data, project):
        from io_storages.functions import get_storage_by_url

        if project.task_data_login and project.task_data_password:
            protected_data = {}
            for key, value in task_data.items():
                if isinstance(value, str) and string_is_url(value):
                    path = (
                        reverse('projects-file-proxy', kwargs={'pk': project.pk})
                        + '?url='
                        + base64.urlsafe_b64encode(value.encode()).decode()
                    )
                    value = urljoin(settings.HOSTNAME, path)
                protected_data[key] = value
            return protected_data
        else:
            storage_objects = project.get_all_storage_objects(type_='import')

            # try resolve URLs via storage associated with that task
            for field in task_data:
                # file saved in django file storage
                prepared_filename = self.prepare_filename(task_data[field])
                if settings.CLOUD_FILE_STORAGE_ENABLED and self.is_upload_file(prepared_filename):
                    # permission check: resolve uploaded files to the project only
                    file_upload = fast_first(FileUpload.objects.filter(project=project, file=prepared_filename))
                    if file_upload is not None:
                        task_data[field] = file_upload.url
                    # it's very rare case, e.g. user tried to reimport exported file from another project
                    # or user wrote his django storage path manually
                    else:
                        task_data[field] = task_data[field] + '?not_uploaded_project_file'
                    continue

                # project storage
                # TODO: to resolve nested lists and dicts we should improve get_storage_by_url(),
                # TODO: problem with current approach: it can be used only the first storage that get_storage_by_url
                # TODO: returns. However, maybe the second storage will resolve uris properly.
                # TODO: resolve_uri() already supports them
                storage = self.storage or get_storage_by_url(task_data[field], storage_objects)
                if storage:
                    try:
                        proxy_task = None
                        if flag_set(
                            'fflag_fix_all_lsdv_4711_cors_errors_accessing_task_data_short',
                            user='auto',
                        ):
                            proxy_task = self

                        resolved_uri = storage.resolve_uri(task_data[field], proxy_task)
                    except Exception as exc:
                        logger.debug(exc, exc_info=True)
                        resolved_uri = None
                    if resolved_uri:
                        task_data[field] = resolved_uri
            return task_data

    @property
    def storage(self):
        # maybe task has storage link
        storage_link = self.get_storage_link()
        if storage_link:
            return storage_link.storage

        # or try global storage settings (only s3 for now)
        elif get_env('USE_DEFAULT_S3_STORAGE', default=False, is_bool=True):
            # TODO: this is used to access global environment storage settings.
            # We may use more than one and non-default S3 storage (like GCS, Azure)
            from io_storages.s3.models import S3ImportStorage

            return S3ImportStorage()

    @property
    def completed_annotations(self):
        """Annotations that we take into account when set completed status to the task"""
        if self.project.skip_queue == self.project.SkipQueue.IGNORE_SKIPPED:
            return self.annotations
        else:
            return self.annotations.filter(Q_finished_annotations)

    def increase_project_summary_counters(self):
        if hasattr(self.project, 'summary'):
            summary = self.project.summary
            summary.update_data_columns([self])

    def decrease_project_summary_counters(self):
        if hasattr(self.project, 'summary'):
            summary = self.project.summary
            summary.remove_data_columns([self])

    def ensure_unique_groundtruth(self, annotation_id):
        self.annotations.exclude(id=annotation_id).update(ground_truth=False)

    def save(self, *args, update_fields=None, **kwargs):
        if self.inner_id == 0:
            task = Task.objects.filter(project=self.project).order_by('-inner_id').first()
            max_inner_id = 1
            if task:
                max_inner_id = task.inner_id

            # max_inner_id might be None in the old projects
            self.inner_id = None if max_inner_id is None else (max_inner_id + 1)
            if update_fields is not None:
                update_fields = {'inner_id'}.union(update_fields)

        super().save(*args, update_fields=update_fields, **kwargs)

    @staticmethod
    def delete_tasks_without_signals(queryset):
        """
        Delete Tasks queryset with switched off signals
        :param queryset: Tasks queryset
        """
        signals = [
            (post_delete, update_all_task_states_after_deleting_task, Task),
            (pre_delete, remove_data_columns, Task),
        ]
        with temporary_disconnect_list_signal(signals):
            queryset.delete()

    @staticmethod
    def delete_tasks_without_signals_from_task_ids(task_ids):
        queryset = Task.objects.filter(id__in=task_ids)
        Task.delete_tasks_without_signals(queryset)

    def delete(self, *args, **kwargs):
        self.before_delete_actions()
        result = super().delete(*args, **kwargs)
        # set updated_at field of task to now()
        return result


pre_bulk_create = Signal()   # providing args 'objs' and 'batch_size'
post_bulk_create = Signal()   # providing args 'objs' and 'batch_size'


class AnnotationManager(models.Manager):
    def for_user(self, user):
        return self.filter(project__organization=user.active_organization)

    def bulk_create(self, objs, batch_size=None):
        pre_bulk_create.send(sender=self.model, objs=objs, batch_size=batch_size)
        res = super(AnnotationManager, self).bulk_create(objs, batch_size)
        post_bulk_create.send(sender=self.model, objs=objs, batch_size=batch_size)
        return res


GET_UNIQUE_IDS = """
with tt as (
    select jsonb_array_elements(tch.result) as item from task_completion_history tch
    where task=%(t_id)s and task_annotation=%(tc_id)s
) select count( distinct tt.item -> 'id') from tt"""

AnnotationMixin = load_func(settings.ANNOTATION_MIXIN)


class Annotation(AnnotationMixin, models.Model):
    """Annotations & Labeling results"""

    objects = AnnotationManager()

    result = JSONField(
        'result',
        null=True,
        default=None,
        help_text='The main value of annotator work - ' 'labeling result in JSON format',
    )

    task = models.ForeignKey(
        'tasks.Task',
        on_delete=models.CASCADE,
        related_name='annotations',
        null=True,
        help_text='Corresponding task for this annotation',
    )
    # duplicate relation to project for performance reasons
    project = models.ForeignKey(
        'projects.Project',
        related_name='annotations',
        on_delete=models.CASCADE,
        null=True,
        help_text='Project ID for this annotation',
    )
    completed_by = models.ForeignKey(
        settings.AUTH_USER_MODEL,
        related_name='annotations',
        on_delete=models.SET_NULL,
        null=True,
        help_text='User ID of the person who created this annotation',
    )
    updated_by = models.ForeignKey(
        settings.AUTH_USER_MODEL,
        related_name='updated_annotations',
        on_delete=models.SET_NULL,
        null=True,
        verbose_name=_('updated by'),
        help_text='Last user who updated this annotation',
    )
    was_cancelled = models.BooleanField(_('was cancelled'), default=False, help_text='User skipped the task')
    ground_truth = models.BooleanField(
        _('ground_truth'),
        default=False,
        help_text='This annotation is a Ground Truth (ground_truth)',
    )
    created_at = models.DateTimeField(_('created at'), auto_now_add=True, help_text='Creation time')
    updated_at = models.DateTimeField(_('updated at'), auto_now=True, help_text='Last updated time')
    draft_created_at = models.DateTimeField(
        _('draft created at'), null=True, default=None, help_text='Draft creation time'
    )
    lead_time = models.FloatField(
        _('lead time'),
        null=True,
        default=None,
        help_text='How much time it took to annotate the task',
    )
    prediction = JSONField(
        _('prediction'),
        null=True,
        default=dict,
        help_text='Prediction viewed at the time of annotation',
    )
    result_count = models.IntegerField(_('result count'), default=0, help_text='Results inside of annotation counter')

    parent_prediction = models.ForeignKey(
        'tasks.Prediction',
        on_delete=models.SET_NULL,
        related_name='child_annotations',
        null=True,
        help_text='Points to the prediction from which this annotation was created',
    )
    parent_annotation = models.ForeignKey(
        'tasks.Annotation',
        on_delete=models.SET_NULL,
        related_name='child_annotations',
        null=True,
        help_text='Points to the parent annotation from which this annotation was created',
    )
    unique_id = models.UUIDField(default=uuid.uuid4, null=True, blank=True, unique=True, editable=False)
    import_id = models.BigIntegerField(
        default=None,
        null=True,
        blank=True,
        db_index=True,
        help_text="Original annotation ID that was at the import step or NULL if this annotation wasn't imported",
    )
    last_action = models.CharField(
        _('last action'),
        max_length=128,
        choices=ActionType.choices,
        help_text='Action which was performed in the last annotation history item',
        default=None,
        null=True,
    )
    last_created_by = models.ForeignKey(
        settings.AUTH_USER_MODEL,
        on_delete=models.SET_NULL,
        verbose_name=_('last created by'),
        help_text='User who created the last annotation history item',
        default=None,
        null=True,
    )
    bulk_created = models.BooleanField(
        _('bulk created'),
        default=False,
        db_default=False,
        null=True,
        help_text='Annotation was created in bulk mode',
    )

    class Meta:
        db_table = 'task_completion'
        indexes = [
            models.Index(fields=['created_at']),
            models.Index(fields=['ground_truth']),
            models.Index(fields=['id', 'task']),
            models.Index(fields=['last_action']),
            models.Index(fields=['project', 'ground_truth']),
            models.Index(fields=['project', 'id']),
            models.Index(fields=['project', 'was_cancelled']),
            models.Index(fields=['task', 'completed_by']),
            models.Index(fields=['task', 'ground_truth']),
            models.Index(fields=['task', 'was_cancelled']),
            models.Index(fields=['was_cancelled']),
        ]

    def created_ago(self):
        """Humanize date"""
        return timesince(self.created_at)

    def entities_num(self):
        res = self.result
        if isinstance(res, str):
            res = json.loads(res)
        if res is None:
            res = []

        return len(res)

    def has_permission(self, user: 'User') -> bool:  # noqa: F821
        mixin_has_permission = cast(bool, super().has_permission(user))

        user.project = self.project  # link for activity log
        return mixin_has_permission and self.project.has_permission(user)

    def increase_project_summary_counters(self):
        if hasattr(self.project, 'summary'):
            logger.debug(f'Increase project.summary counters from {self}')
            summary = self.project.summary
            summary.update_created_annotations_and_labels([self])

    def decrease_project_summary_counters(self):
        if hasattr(self.project, 'summary'):
            logger.debug(f'Decrease project.summary counters from {self}')
            summary = self.project.summary
            summary.remove_created_annotations_and_labels([self])

    def update_task(self):
        update_fields = ['updated_at']

        # updated_by
        request = get_current_request()
        if request:
            self.task.updated_by = request.user
            update_fields.append('updated_by')

        self.task.save(update_fields=update_fields)

    def save(self, *args, update_fields=None, **kwargs):
        request = get_current_request()
        if request:
            self.updated_by = request.user
            if update_fields is not None:
                update_fields = {'updated_by'}.union(update_fields)

        unique_list = {result.get('id') for result in (self.result or [])}

        self.result_count = len(unique_list)
        if update_fields is not None:
            update_fields = {'result_count'}.union(update_fields)
        result = super().save(*args, update_fields=update_fields, **kwargs)

        self.update_task()
        return result

    def delete(self, *args, **kwargs):
        result = super().delete(*args, **kwargs)
        self.update_task()
        self.on_delete_update_counters()
        return result

    def on_delete_update_counters(self):
        task = self.task
        logger.debug(f'Start updating counters for task {task.id}.')
        if self.was_cancelled:
            cancelled = task.annotations.all().filter(was_cancelled=True).count()
            Task.objects.filter(id=task.id).update(cancelled_annotations=cancelled)
            logger.debug(f'On delete updated cancelled_annotations for task {task.id}')
        else:
            total = task.annotations.all().filter(was_cancelled=False).count()
            Task.objects.filter(id=task.id).update(total_annotations=total)
            logger.debug(f'On delete updated total_annotations for task {task.id}')

        logger.debug(f'Update task stats for task={task}')
        task.update_is_labeled()
        Task.objects.filter(id=task.id).update(is_labeled=task.is_labeled)

        # remove annotation counters in project summary followed by deleting an annotation
        logger.debug('Remove annotation counters in project summary followed by deleting an annotation')
        self.decrease_project_summary_counters()


class TaskLock(models.Model):
    task = models.ForeignKey(
        'tasks.Task',
        on_delete=models.CASCADE,
        related_name='locks',
        help_text='Locked task',
    )
    expire_at = models.DateTimeField(_('expire_at'))
    unique_id = models.UUIDField(default=uuid.uuid4, null=True, blank=True, unique=True, editable=False)
    user = models.ForeignKey(
        settings.AUTH_USER_MODEL,
        related_name='task_locks',
        on_delete=models.CASCADE,
        help_text='User who locked this task',
    )
    created_at = models.DateTimeField(_('created at'), auto_now_add=True, help_text='Creation time', null=True)


class AnnotationDraft(models.Model):
    result = JSONField(_('result'), help_text='Draft result in JSON format')
    lead_time = models.FloatField(
        _('lead time'),
        blank=True,
        null=True,
        help_text='How much time it took to annotate the task',
    )
    task = models.ForeignKey(
        'tasks.Task',
        on_delete=models.CASCADE,
        related_name='drafts',
        blank=True,
        null=True,
        help_text='Corresponding task for this draft',
    )
    annotation = models.ForeignKey(
        'tasks.Annotation',
        on_delete=models.CASCADE,
        related_name='drafts',
        blank=True,
        null=True,
        help_text='Corresponding annotation for this draft',
    )
    user = models.ForeignKey(
        settings.AUTH_USER_MODEL,
        related_name='drafts',
        on_delete=models.CASCADE,
        help_text='User who created this draft',
    )
    was_postponed = models.BooleanField(
        _('was postponed'),
        default=False,
        help_text='User postponed this draft (clicked a forward button) in the label stream',
        db_index=True,
    )
    import_id = models.BigIntegerField(
        default=None,
        null=True,
        blank=True,
        db_index=True,
        help_text="Original draft ID that was at the import step or NULL if this draft wasn't imported",
    )

    created_at = models.DateTimeField(_('created at'), auto_now_add=True, help_text='Creation time')
    updated_at = models.DateTimeField(_('updated at'), auto_now=True, help_text='Last update time')

    def created_ago(self):
        """Humanize date"""
        return timesince(self.created_at)

    def has_permission(self, user):
        user.project = self.task.project  # link for activity log
        return self.task.project.has_permission(user)

    def save(self, *args, **kwargs):
        with transaction.atomic():
            super().save(*args, **kwargs)
            project = self.task.project
            if hasattr(project, 'summary'):
                project.summary.update_created_labels_drafts([self])

    def delete(self, *args, **kwargs):
        with transaction.atomic():
            project = self.task.project
            if hasattr(project, 'summary'):
                project.summary.remove_created_drafts_and_labels([self])
            super().delete(*args, **kwargs)


class Prediction(models.Model):
    """ML backend / Prompts predictions"""

    result = JSONField('result', null=True, default=dict, help_text='Prediction result')
    score = models.FloatField(_('score'), default=None, help_text='Prediction score', null=True)
    model_version = models.TextField(
        _('model version'),
        default='',
        blank=True,
        null=True,
        help_text='A string value that for model version that produced the prediction. Used in both live models and when uploading offline predictions.',
    )

    model = models.ForeignKey(
        'ml.MLBackend',
        on_delete=models.CASCADE,
        related_name='predictions',
        null=True,
        help_text='An ML Backend instance that created the prediction.',
    )
    model_run = models.ForeignKey(
        'ml_models.ModelRun',
        on_delete=models.CASCADE,
        related_name='predictions',
        null=True,
        help_text='A run of a ModelVersion that created the prediction.',
    )
    cluster = models.IntegerField(
        _('cluster'),
        default=None,
        help_text='Cluster for the current prediction',
        null=True,
    )
    neighbors = JSONField(
        'neighbors',
        null=True,
        blank=True,
        help_text='Array of task IDs of the closest neighbors',
    )
    mislabeling = models.FloatField(_('mislabeling'), default=0.0, help_text='Related task mislabeling score')

    task = models.ForeignKey('tasks.Task', on_delete=models.CASCADE, related_name='predictions')
    project = models.ForeignKey('projects.Project', on_delete=models.CASCADE, related_name='predictions', null=True)
    created_at = models.DateTimeField(_('created at'), auto_now_add=True)
    updated_at = models.DateTimeField(_('updated at'), auto_now=True)

    def created_ago(self):
        """Humanize date"""
        return timesince(self.created_at)

    def has_permission(self, user):
        user.project = self.project  # link for activity log
        return self.project.has_permission(user)

    @classmethod
    def prepare_prediction_result(cls, result, project):
        """
        This function does the following logic of transforming "result" object:
        result is list -> use raw result as is
        result is dict -> put result under single "value" section
        result is string -> find first occurrence of single-valued tag (Choices, TextArea, etc.) and put string under corresponding single field (e.g. "choices": ["my_label"])  # noqa
        """
        if isinstance(result, list):
            # full representation of result
            for item in result:
                if not isinstance(item, dict):
                    raise ValidationError('Each item in prediction result should be dict')
            # TODO: check consistency with project.label_config
            return result

        elif isinstance(result, dict):
            # "value" from result
            # TODO: validate value fields according to project.label_config
            for tag, tag_info in project.get_parsed_config().items():
                tag_type = tag_info['type'].lower()
                if tag_type in result:
                    return [
                        {
                            'from_name': tag,
                            'to_name': ','.join(tag_info['to_name']),
                            'type': tag_type,
                            'value': result,
                        }
                    ]

        elif isinstance(result, (str, numbers.Integral)):
            # If result is of integral type, it could be a representation of data from single-valued control tags (e.g. Choices, Rating, etc.)
            for tag, tag_info in project.get_parsed_config().items():
                tag_type = tag_info['type'].lower()
                if tag_type in SINGLE_VALUED_TAGS and isinstance(result, SINGLE_VALUED_TAGS[tag_type]):
                    return [
                        {
                            'from_name': tag,
                            'to_name': ','.join(tag_info['to_name']),
                            'type': tag_type,
                            'value': {tag_type: [result]},
                        }
                    ]
        else:
            raise ValidationError(f'Incorrect format {type(result)} for prediction result {result}')

    def update_task(self):
        update_fields = ['updated_at']

        # updated_by
        request = get_current_request()
        if request:
            self.task.updated_by = request.user
            update_fields.append('updated_by')

        self.task.save(update_fields=update_fields)

    def save(self, *args, update_fields=None, **kwargs):
        if self.project_id is None and self.task_id:
            logger.warning('project_id is not set for prediction, project_id being set in save method')
            self.project_id = Task.objects.only('project_id').get(pk=self.task_id).project_id
            if update_fields is not None:
                update_fields = {'project_id'}.union(update_fields)

        # "result" data can come in different forms - normalize them to JSON
        self.result = self.prepare_prediction_result(self.result, self.project)

        if update_fields is not None:
            update_fields = {'result'}.union(update_fields)
        # set updated_at field of task to now()
        self.update_task()
        return super(Prediction, self).save(*args, update_fields=update_fields, **kwargs)

    def delete(self, *args, **kwargs):
        result = super().delete(*args, **kwargs)
        # set updated_at field of task to now()
        self.update_task()
        return result

    @classmethod
    def create_no_commit(
        cls, project, label_interface, task_id, data, model_version, model_run
    ) -> Optional['Prediction']:
        """
        Creates a Prediction object from the given result data, without committing it to the database.
        or returns None if it fails.

        Args:
            project: The Project object to associate with the Prediction object.
            label_interface: The LabelInterface object to use to create regions from the result data.
            data: The data to create the Prediction object with.
                    Must contain keys that match control tags in labeling configuration
                    Example:
                        ```
                        {
                            'my_choices': 'positive',
                            'my_textarea': 'generated document summary'
                        }
                        ```
            task_id: The ID of the Task object to associate with the Prediction object.
            model_version: The model version that produced the prediction.
            model_run: The model run that created the prediction.
        """
        try:

            # given the data receive, create annotation regions in LS format
            # e.g. {"sentiment": "positive"} -> {"value": {"choices": ["positive"]}, "from_name": "", "to_name": "", ..}
            pred = PredictionValue(result=label_interface.create_regions(data)).model_dump()

            prediction = Prediction(
                project=project,
                task_id=int(task_id),
                model_version=model_version,
                model_run=model_run,
                score=1.0,  # Setting to 1.0 for now as we don't get back a score
                result=pred['result'],
            )
            return prediction
        except Exception as exc:
            # TODO: handle exceptions better
            logger.error(
                f'Error creating prediction for task {task_id} with {data}: {exc}. '
                f'Traceback: {traceback.format_exc()}'
            )

    class Meta:
        db_table = 'prediction'


class FailedPrediction(models.Model):
    """
    Class for storing failed prediction(s) for a task
    """

    message = models.TextField(
        _('message'),
        default=None,
        blank=True,
        null=True,
        help_text='The message explaining why generating this prediction failed',
    )
    error_type = models.CharField(
        _('error_type'),
        max_length=512,
        default=None,
        null=True,
        help_text='The type of error that caused prediction to fail',
    )
    ml_backend_model = models.ForeignKey(
        'ml.MLBackend',
        on_delete=models.SET_NULL,
        related_name='failed_predictions',
        null=True,
        help_text='An ML Backend instance that created the failed prediction.',
    )
    model_version = models.TextField(
        _('model version'),
        default=None,
        blank=True,
        null=True,
        help_text='A string value that for model version that produced the failed prediction. Used in both live models and when uploading offline predictions.',
    )
    model_run = models.ForeignKey(
        'ml_models.ModelRun',
        on_delete=models.CASCADE,
        related_name='failed_predictions',
        null=True,
        help_text='A run of a ModelVersion that created the failed prediction.',
    )
    project = models.ForeignKey(
        'projects.Project', on_delete=models.CASCADE, related_name='failed_predictions', null=True
    )
    task = models.ForeignKey('tasks.Task', on_delete=models.CASCADE, related_name='failed_predictions')
    created_at = models.DateTimeField(_('created at'), auto_now_add=True)


class PredictionMeta(models.Model):
    prediction = models.OneToOneField(
        'Prediction',
        on_delete=models.CASCADE,
        null=True,
        blank=True,
        related_name='meta',
        help_text=_('Reference to the associated prediction'),
    )
    failed_prediction = models.OneToOneField(
        'FailedPrediction',
        on_delete=models.CASCADE,
        null=True,
        blank=True,
        related_name='meta',
        help_text=_('Reference to the associated failed prediction'),
    )
    inference_time = models.FloatField(
        _('inference time'), null=True, blank=True, help_text=_('Time taken for inference in seconds')
    )
    prompt_tokens_count = models.IntegerField(
        _('prompt tokens count'), null=True, blank=True, help_text=_('Number of tokens in the prompt')
    )
    completion_tokens_count = models.IntegerField(
        _('completion tokens count'), null=True, blank=True, help_text=_('Number of tokens in the completion')
    )
    total_tokens_count = models.IntegerField(
        _('total tokens count'), null=True, blank=True, help_text=_('Total number of tokens')
    )
    prompt_cost = models.FloatField(_('prompt cost'), null=True, blank=True, help_text=_('Cost of the prompt'))
    completion_cost = models.FloatField(
        _('completion cost'), null=True, blank=True, help_text=_('Cost of the completion')
    )
    total_cost = models.FloatField(_('total cost'), null=True, blank=True, help_text=_('Total cost'))
    extra = models.JSONField(_('extra'), null=True, blank=True, help_text=_('Additional metadata in JSON format'))

    @classmethod
    def create_no_commit(cls, data, prediction: Union[Prediction, FailedPrediction]) -> Optional['PredictionMeta']:
        """
        Creates a PredictionMeta object from the given result data, without committing it to the database.
        or returns None if it fails.

        Args:
            data: The data to create the PredictionMeta object with.
                  Example:
                    {
                        'total_cost_usd': 0.1,
                        'prompt_cost_usd': 0.05,
                        'completion_cost_usd': 0.05,
                        'prompt_tokens': 10,
                        'completion_tokens': 10,
                        'inference_time': 0.1
                    }
            prediction: The Prediction or FailedPrediction object to associate with the PredictionMeta object.

        Returns:
            The PredictionMeta object created from the given data, or None if it fails.
        """
        try:
            prediction_meta = PredictionMeta(
                total_cost=data.get('total_cost_usd'),
                prompt_cost=data.get('prompt_cost_usd'),
                completion_cost=data.get('completion_cost_usd'),
                prompt_tokens_count=data.get('prompt_tokens'),
                completion_tokens_count=data.get('completion_tokens'),
                total_tokens_count=data.get('prompt_tokens', 0) + data.get('completion_tokens', 0),
                inference_time=data.get('inference_time'),
            )
            if isinstance(prediction, Prediction):
                prediction_meta.prediction = prediction
            else:
                prediction_meta.failed_prediction = prediction
            logger.debug(f'Created prediction meta: {prediction_meta}')
            return prediction_meta
        except Exception as exc:
            logger.error(f'Error creating prediction meta with {data}: {exc}. Traceback: {traceback.format_exc()}')

    class Meta:
        db_table = 'prediction_meta'
        verbose_name = _('Prediction Meta')
        verbose_name_plural = _('Prediction Metas')
        constraints = [
            CheckConstraint(
                # either prediction or failed_prediction should be not null
                check=(
                    (Q(prediction__isnull=False) & Q(failed_prediction__isnull=True))
                    | (Q(prediction__isnull=True) & Q(failed_prediction__isnull=False))
                ),
                name='prediction_or_failed_prediction_not_null',
            )
        ]


@receiver(post_delete, sender=Task)
def update_all_task_states_after_deleting_task(sender, instance, **kwargs):
    """after deleting_task
    use update_tasks_states for all project
    but call only tasks_number_changed section
    """
    try:
        instance.project.update_tasks_states(
            maximum_annotations_changed=False,
            overlap_cohort_percentage_changed=False,
            tasks_number_changed=True,
        )
    except Exception as exc:
        logger.error('Error in update_all_task_states_after_deleting_task: ' + str(exc))


# =========== PROJECT SUMMARY UPDATES ===========


@receiver(pre_delete, sender=Task)
def remove_data_columns(sender, instance, **kwargs):
    """Reduce data column counters after removing task"""
    instance.decrease_project_summary_counters()


def _task_data_is_not_updated(update_fields):
    if update_fields and list(update_fields) == ['is_labeled']:
        return True


@receiver(pre_save, sender=Task)
def delete_project_summary_data_columns_before_updating_task(sender, instance, update_fields, **kwargs):
    """Before updating task fields - ensure previous info removed from project.summary"""
    if _task_data_is_not_updated(update_fields):
        # we don't need to update counters when other than task.data fields are updated
        return
    try:
        old_task = sender.objects.get(id=instance.id)
    except Task.DoesNotExist:
        # task just created - do nothing
        return
    old_task.decrease_project_summary_counters()


@receiver(post_save, sender=Task)
def update_project_summary_data_columns(sender, instance, created, update_fields, **kwargs):
    """Update task counters in project summary in case when new task has been created"""
    if _task_data_is_not_updated(update_fields):
        # we don't need to update counters when other than task.data fields are updated
        return
    instance.increase_project_summary_counters()


@receiver(pre_save, sender=Annotation)
def delete_project_summary_annotations_before_updating_annotation(sender, instance, **kwargs):
    """Before updating annotation fields - ensure previous info removed from project.summary"""
    try:
        old_annotation = sender.objects.get(id=instance.id)
    except Annotation.DoesNotExist:
        # annotation just created - do nothing
        return
    old_annotation.decrease_project_summary_counters()

    # update task counters if annotation changes it's was_cancelled status
    task = instance.task
    if old_annotation.was_cancelled != instance.was_cancelled:
        if instance.was_cancelled:
            task.cancelled_annotations = task.cancelled_annotations + 1
            task.total_annotations = task.total_annotations - 1
        else:
            task.cancelled_annotations = task.cancelled_annotations - 1
            task.total_annotations = task.total_annotations + 1
        task.update_is_labeled()

        Task.objects.filter(id=instance.task.id).update(
            is_labeled=task.is_labeled,
            total_annotations=task.total_annotations,
            cancelled_annotations=task.cancelled_annotations,
        )


@receiver(post_save, sender=Annotation)
def update_project_summary_annotations_and_is_labeled(sender, instance, created, **kwargs):
    """Update annotation counters in project summary"""
    instance.increase_project_summary_counters()

    # If annotation is changed, update task.is_labeled state
    logger.debug(f'Update task stats for task={instance.task}')
    if instance.was_cancelled:
        instance.task.cancelled_annotations = instance.task.annotations.all().filter(was_cancelled=True).count()
    else:
        instance.task.total_annotations = instance.task.annotations.all().filter(was_cancelled=False).count()
    instance.task.update_is_labeled()
    instance.task.save(update_fields=['is_labeled', 'total_annotations', 'cancelled_annotations'])
    logger.debug(f'Updated total_annotations and cancelled_annotations for {instance.task.id}.')


@receiver(pre_delete, sender=Prediction)
def remove_predictions_from_project(sender, instance, **kwargs):
    """Remove predictions counters"""
    instance.task.total_predictions = instance.task.predictions.all().count() - 1
    instance.task.save(update_fields=['total_predictions'])
    logger.debug(f'Updated total_predictions for {instance.task.id}.')


@receiver(post_save, sender=Prediction)
def save_predictions_to_project(sender, instance, **kwargs):
    """Add predictions counters"""
    instance.task.total_predictions = instance.task.predictions.all().count()
    instance.task.save(update_fields=['total_predictions'])
    logger.debug(f'Updated total_predictions for {instance.task.id}.')


# =========== END OF PROJECT SUMMARY UPDATES ===========


@receiver(post_save, sender=Annotation)
def delete_draft(sender, instance, **kwargs):
    task = instance.task
    query_args = {'task': task, 'annotation': instance}
    drafts = AnnotationDraft.objects.filter(**query_args)
    num_drafts = 0
    for draft in drafts:
        # Delete each draft individually because deleting
        # the whole queryset won't update `created_labels_drafts`
        try:
            draft.delete()
            num_drafts += 1
        except AnnotationDraft.DoesNotExist:
            continue
    logger.debug(f'{num_drafts} drafts removed from task {task} after saving annotation {instance}')


@receiver(post_save, sender=Annotation)
def update_ml_backend(sender, instance, **kwargs):
    if instance.ground_truth:
        return

    project = instance.project

    if hasattr(project, 'ml_backends') and project.min_annotations_to_start_training:
        annotation_count = Annotation.objects.filter(project=project).count()

        # start training every N annotation
        if annotation_count % project.min_annotations_to_start_training == 0:
            for ml_backend in project.ml_backends.all():
                ml_backend.train()


def update_task_stats(task, stats=('is_labeled',), save=True):
    """Update single task statistics:
        accuracy
        is_labeled
    :param task: Task to update
    :param stats: to update separate stats
    :param save: to skip saving in some cases
    :return:
    """
    logger.debug(f'Update stats {stats} for task {task}')
    if 'is_labeled' in stats:
        task.update_is_labeled(save=save)
    if save:
        task.save()


def bulk_update_stats_project_tasks(tasks, project=None):
    """bulk Task update accuracy
       ex: after change settings
       apply several update queries size of batch
       on updated Task objects
       in single transaction as execute sql
    :param tasks:
    :return:
    """
    # recalc accuracy
    if not tasks:
        # break if tasks is empty
        return
    # get project if it's not in params
    if project is None:
        project = tasks[0].project

    with transaction.atomic():
        use_overlap = project._can_use_overlap()
        maximum_annotations = project.maximum_annotations
        # update filters if we can use overlap
        if use_overlap:
            # following definition of `completed_annotations` above, count cancelled annotations
            # as completed if project is in IGNORE_SKIPPED mode
            completed_annotations_f_expr = F('total_annotations')
            if project.skip_queue == project.SkipQueue.IGNORE_SKIPPED:
                completed_annotations_f_expr += F('cancelled_annotations')
            finished_q = Q(GreaterThanOrEqual(completed_annotations_f_expr, maximum_annotations)) | Q(
                GreaterThanOrEqual(completed_annotations_f_expr, 1), overlap=1
            )
            finished_tasks = tasks.filter(finished_q)
            finished_tasks_ids = finished_tasks.values_list('id', flat=True)
            tasks.update(is_labeled=Q(id__in=finished_tasks_ids))

        else:
            # update objects without saving if we can't use overlap
            for task in tasks:
                update_task_stats(task, save=False)
            try:
                # start update query batches
                bulk_update(tasks, update_fields=['is_labeled'], batch_size=settings.BATCH_SIZE)
            except OperationalError:
                logger.error('Operational error while updating tasks: {exc}', exc_info=True)
                # try to update query batches one more time
                start_job_async_or_sync(
                    bulk_update,
                    tasks,
                    in_seconds=settings.BATCH_JOB_RETRY_TIMEOUT,
                    update_fields=['is_labeled'],
                    batch_size=settings.BATCH_SIZE,
                )


Q_finished_annotations = Q(was_cancelled=False) & Q(result__isnull=False)
Q_task_finished_annotations = Q(annotations__was_cancelled=False) & Q(annotations__result__isnull=False)
</file>

<file path="label_studio/tasks/openapi_schema.py">
import drf_yasg.openapi as openapi

result_example = [
    {
        'original_width': 1920,
        'original_height': 1080,
        'image_rotation': 0,
        'from_name': 'bboxes',
        'to_name': 'image',
        'type': 'rectanglelabels',
        'value': {
            'x': 20,
            'y': 30,
            'width': 50,
            'height': 60,
            'rotation': 0,
            'values': {'rectanglelabels': ['Person']},
        },
    }
]

task_response_example = {
    'id': 1,
    'data': {'image': 'https://example.com/image.jpg', 'text': 'Hello, AI!'},
    'project': 1,
    'created_at': '2024-06-18T23:45:46.048490Z',
    'updated_at': '2024-06-18T23:45:46.048538Z',
    'is_labeled': False,
    'overlap': 1,
    'inner_id': 1,
    'total_annotations': 0,
    'cancelled_annotations': 0,
    'total_predictions': 0,
    'comment_count': 0,
    'unresolved_comment_count': 0,
    'last_comment_updated_at': '2024-01-15T09:30:00Z',
    'updated_by': [{'user_id': 1}],
    'file_upload': '42d46c4c-my-pic.jpeg',
    'comment_authors': [1],
}

dm_task_response_example = {
    'id': 13,
    'predictions': [],
    'annotations': [],
    'drafts': [],
    'annotators': [],
    'inner_id': 2,
    'cancelled_annotations': 0,
    'total_annotations': 0,
    'total_predictions': 0,
    'completed_at': None,
    'annotations_results': '',
    'predictions_results': '',
    'predictions_score': None,
    'file_upload': '6b25fc23-some_3.mp4',
    'storage_filename': None,
    'annotations_ids': '',
    'predictions_model_versions': '',
    'avg_lead_time': None,
    'draft_exists': False,
    'updated_by': [],
    'data': {'image': '/data/upload/1/6b25fc23-some_3.mp4'},
    'meta': {},
    'created_at': '2024-06-18T23:45:46.048490Z',
    'updated_at': '2024-06-18T23:45:46.048538Z',
    'is_labeled': False,
    'overlap': 1,
    'comment_count': 0,
    'unresolved_comment_count': 0,
    'last_comment_updated_at': None,
    'project': 1,
    'comment_authors': [],
}

annotation_response_example = {
    'id': 1,
    'result': result_example,
    'task': 1,
    'project': 1,
    'completed_by': 1,
    'updated_by': 1,
    'was_cancelled': False,
    'ground_truth': False,
    'lead_time': 10,
}

prediction_response_example = {'id': 1, 'task': 1, 'result': result_example, 'score': 0.95, 'model_version': 'yolo-v8'}

task_request_schema = openapi.Schema(
    type=openapi.TYPE_OBJECT,
    properties={
        'data': openapi.Schema(
            title='Task data',
            description='Task data dictionary with arbitrary keys and values',
            type=openapi.TYPE_OBJECT,
            example={'image': 'https://example.com/image.jpg', 'text': 'Hello, world!'},
        ),
        'project': openapi.Schema(
            type=openapi.TYPE_INTEGER,
            description='Project ID',
        ),
    },
    example={
        'data': {'image': 'https://example.com/image.jpg', 'text': 'Hello, world!'},
        'project': 1,
    },
)

annotation_request_schema = openapi.Schema(
    type=openapi.TYPE_OBJECT,
    properties={
        'result': openapi.Schema(
            type=openapi.TYPE_ARRAY,
            items=openapi.Schema(
                type=openapi.TYPE_OBJECT,
            ),
            description='Labeling result in JSON format. Read more about the format in [the Label Studio documentation.](https://labelstud.io/guide/task_format)',
            example=result_example,
        ),
        'task': openapi.Schema(type=openapi.TYPE_INTEGER, description='Corresponding task for this annotation'),
        'project': openapi.Schema(type=openapi.TYPE_INTEGER, description='Project ID for this annotation'),
        'completed_by': openapi.Schema(
            type=openapi.TYPE_INTEGER, description='User ID of the person who created this annotation'
        ),
        'updated_by': openapi.Schema(type=openapi.TYPE_INTEGER, description='Last user who updated this annotation'),
        'was_cancelled': openapi.Schema(type=openapi.TYPE_BOOLEAN, description='User skipped the task'),
        'ground_truth': openapi.Schema(type=openapi.TYPE_BOOLEAN, description='This annotation is a Ground Truth'),
        'lead_time': openapi.Schema(
            type=openapi.TYPE_NUMBER,
            description='How much time it took to annotate the task (in seconds)',
            example=100.5,
        ),
    },
    required=[],
    example={
        'result': result_example,
        'was_cancelled': False,
        'ground_truth': True,
    },
)

prediction_request_schema = openapi.Schema(
    type=openapi.TYPE_OBJECT,
    properties={
        'task': openapi.Schema(
            type=openapi.TYPE_INTEGER,
            description='Task ID for which the prediction is created',
        ),
        'result': openapi.Schema(
            type=openapi.TYPE_ARRAY,
            items=openapi.Schema(
                type=openapi.TYPE_OBJECT,
            ),
            description='Prediction result in JSON format. Read more about the format in [the Label Studio documentation.](https://labelstud.io/guide/predictions)',
            example=result_example,
        ),
        'score': openapi.Schema(
            type=openapi.TYPE_NUMBER,
            description='Prediction score. Can be used in Data Manager to sort task by model confidence. '
            'Task with the lowest score will be shown first.',
            example=0.95,
        ),
        'model_version': openapi.Schema(
            type=openapi.TYPE_STRING,
            description='Model version - tag for predictions that can be used to filter tasks in Data Manager, as well as '
            'select specific model version for showing preannotations in the labeling interface',
            example='yolo-v8',
        ),
    },
    example={'result': result_example, 'score': 0.95, 'model_version': 'yolo-v8'},
)
</file>

<file path="label_studio/tasks/serializers.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import logging

import ujson as json
from core.feature_flags import flag_set
from core.label_config import replace_task_data_undefined_with_config_field
from core.utils.common import load_func, retry_database_locked
from core.utils.db import fast_first
from django.conf import settings
from django.db import IntegrityError, transaction
from drf_yasg import openapi
from projects.models import Project
from rest_flex_fields import FlexFieldsModelSerializer
from rest_framework import generics, serializers
from rest_framework.exceptions import ValidationError
from rest_framework.fields import SkipField
from rest_framework.serializers import ModelSerializer
from rest_framework.settings import api_settings
from tasks.exceptions import AnnotationDuplicateError
from tasks.models import Annotation, AnnotationDraft, Prediction, Task
from tasks.validation import TaskValidator
from users.models import User
from users.serializers import UserSerializer

logger = logging.getLogger(__name__)


class PredictionQuerySerializer(serializers.Serializer):
    task = serializers.IntegerField(required=False, help_text='Task ID to filter predictions')
    project = serializers.IntegerField(required=False, help_text='Project ID to filter predictions')


class PredictionResultField(serializers.JSONField):
    class Meta:
        swagger_schema_fields = {
            'type': openapi.TYPE_ARRAY,
            'title': 'Prediction result list',
            'description': 'List of prediction results for the task',
            'items': {
                'type': openapi.TYPE_OBJECT,
                'title': 'Prediction result items (regions)',
                'description': 'List of predicted regions for the task',
            },
        }


class AnnotationResultField(serializers.JSONField):
    class Meta:
        swagger_schema_fields = {
            'type': openapi.TYPE_ARRAY,
            'title': 'Annotation result list',
            'description': 'List of annotation results for the task',
            'items': {
                'type': openapi.TYPE_OBJECT,
                'title': 'Annotation result items (regions)',
                'description': 'List of annotated regions for the task',
            },
        }


class PredictionSerializer(ModelSerializer):
    result = PredictionResultField()
    model_version = serializers.CharField(
        allow_blank=True,
        required=False,
        help_text='Model version - tag for predictions that can be used to filter tasks in Data Manager, as well as '
        'select specific model version for showing preannotations in the labeling interface',
    )
    created_ago = serializers.CharField(default='', read_only=True, help_text='Delta time from creation time')

    class Meta:
        model = Prediction
        fields = '__all__'


class ListAnnotationSerializer(serializers.ListSerializer):
    pass


class CompletedByDMSerializer(UserSerializer):
    class Meta:
        model = User
        fields = ['id', 'first_name', 'last_name', 'avatar', 'email', 'initials']


class AnnotationSerializer(FlexFieldsModelSerializer):
    """ """

    result = AnnotationResultField(required=False)
    created_username = serializers.SerializerMethodField(default='', read_only=True, help_text='Username string')
    created_ago = serializers.CharField(default='', read_only=True, help_text='Time delta from creation time')
    completed_by = serializers.PrimaryKeyRelatedField(required=False, queryset=User.objects.all())
    unique_id = serializers.CharField(required=False, write_only=True)

    def create(self, *args, **kwargs):
        try:
            return super().create(*args, **kwargs)
        except IntegrityError as e:
            errors = [
                'UNIQUE constraint failed: task_completion.unique_id',
                'duplicate key value violates unique constraint "task_completion_unique_id_key"',
            ]
            if any([error in str(e) for error in errors]):
                raise AnnotationDuplicateError()
            raise

    def validate_result(self, value):
        data = value
        # convert from str to json if need
        if isinstance(value, str):
            try:
                data = json.loads(value)
            except:  # noqa: E722
                raise ValueError('annotation "result" can\'t be parse from str to JSON')

        # check result is list
        if not isinstance(data, list):
            raise ValidationError('annotation "result" field in annotation must be list')

        return data

    def get_created_username(self, annotation):
        user = annotation.completed_by
        if not user:
            return ''

        name = user.first_name
        if len(user.last_name):
            name = name + ' ' + user.last_name

        name += f' {user.email}, {user.id}'
        return name

    class Meta:
        model = Annotation
        exclude = ['prediction', 'result_count']
        expandable_fields = {'completed_by': (CompletedByDMSerializer,)}


class TaskSimpleSerializer(ModelSerializer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.fields['annotations'] = AnnotationSerializer(many=True, default=[], context=self.context, read_only=True)
        self.fields['predictions'] = PredictionSerializer(many=True, default=[], context=self.context, read_only=True)

    def to_representation(self, instance):
        project = instance.project
        if project:
            # resolve $undefined$ key in task data
            data = instance.data
            replace_task_data_undefined_with_config_field(data, project)

        return super().to_representation(instance)

    class Meta:
        model = Task
        fields = '__all__'


class BaseTaskSerializer(FlexFieldsModelSerializer):
    """Task Serializer with project scheme configs validation"""

    def project(self, task=None):
        """Take the project from context"""
        if 'project' in self.context:
            project = self.context['project']
        elif 'view' in self.context and 'project_id' in self.context['view'].kwargs:
            kwargs = self.context['view'].kwargs
            project = generics.get_object_or_404(Project, kwargs['project_id'])
        elif task:
            project = task.project
        else:
            project = None
        return project

    def validate(self, task):
        instance = self.instance if hasattr(self, 'instance') else None
        validator = TaskValidator(self.project(), instance)
        return validator.validate(task)

    def to_representation(self, instance):
        project = self.project(instance)
        if project:
            # resolve uri for storage (s3/gcs/etc)
            if self.context.get('resolve_uri', False):
                instance.data = instance.resolve_uri(instance.data, project)

            # resolve $undefined$ key in task data
            data = instance.data
            replace_task_data_undefined_with_config_field(data, project)

        return super().to_representation(instance)

    class Meta:
        model = Task
        fields = '__all__'


class BaseTaskSerializerBulk(serializers.ListSerializer):
    """Serialize task with annotation from source json data"""

    annotations = AnnotationSerializer(many=True, default=[], read_only=True)
    predictions = PredictionSerializer(many=True, default=[], read_only=True)

    @property
    def project(self):
        return self.context.get('project')

    @staticmethod
    def format_error(i, detail, item):
        if len(detail) == 1:
            code = f' {detail[0].code}' if detail[0].code != 'invalid' else ''
            return f'Error{code} at item {i}: {detail[0]} :: {item}'
        else:
            errors = ', '.join(detail)
            codes = [d.code for d in detail]
            return f'Errors {codes} at item {i}: {errors} :: {item}'

    def to_internal_value(self, data):
        """Body of run_validation for all data items"""
        if data is None:
            raise ValidationError('All tasks are empty (None)')

        if not isinstance(data, list):
            raise ValidationError({api_settings.NON_FIELD_ERRORS_KEY: 'not a list'}, code='not_a_list')

        if not self.allow_empty and len(data) == 0:
            if self.parent and self.partial:
                raise SkipField()
            raise ValidationError({api_settings.NON_FIELD_ERRORS_KEY: 'empty'}, code='empty')

        ret, errors = [], []
        self.annotation_count, self.prediction_count = 0, 0
        for i, item in enumerate(data):
            try:
                validated = self.child.validate(item)
            except ValidationError as exc:
                error = self.format_error(i, exc.detail, item)
                errors.append(error)
                # do not print to user too many errors
                if len(errors) >= 100:
                    errors[99] = '...'
                    break
            else:
                ret.append(validated)
                errors.append({})

                if 'annotations' in item:
                    self.annotation_count += len(item['annotations'])
                if 'predictions' in item:
                    self.prediction_count += len(item['predictions'])

        if any(errors):
            logger.warning("Can't deserialize tasks due to " + str(errors))
            raise ValidationError(errors)

        return ret

    @staticmethod
    def _insert_valid_completed_by(annotations, members_email_to_id, members_ids, default_user):
        """Insert the correct id for completed_by by email in annotations"""
        for annotation in annotations:
            completed_by = annotation.get('completed_by')
            # no completed_by info found - just skip it, will be assigned to the user who imports
            if completed_by is None:
                annotation['completed_by_id'] = default_user.id

            # resolve annotators by email
            elif isinstance(completed_by, dict):
                if 'email' not in completed_by:
                    raise ValidationError("It's expected to have 'email' field in 'completed_by' data in annotations")

                email = completed_by['email']
                if email not in members_email_to_id:
                    if settings.ALLOW_IMPORT_TASKS_WITH_UNKNOWN_EMAILS:
                        annotation['completed_by_id'] = default_user.id
                    else:
                        raise ValidationError(f"Unknown annotator's email {email}")
                else:
                    # overwrite an actual member ID
                    annotation['completed_by_id'] = members_email_to_id[email]

            # old style annotators specification - try to find them by ID
            elif isinstance(completed_by, int) and completed_by in members_ids:
                if completed_by not in members_ids:
                    raise ValidationError(f"Unknown annotator's ID {completed_by}")
                annotation['completed_by_id'] = completed_by

            # in any other cases - import validation error
            else:
                raise ValidationError(
                    f"Import data contains completed_by={completed_by} which is not a valid annotator's email or ID"
                )
            annotation.pop('completed_by', None)

    @staticmethod
    def _insert_valid_user_reviews(dicts, members_email_to_id, default_user):
        """Insert correct user id by email from snapshot

        :param dicts: draft or review dicts from snapshot
        :param members_email_to_id: mapping from emails to current LS instance user IDs
        :param default_user: if email is not found in membr_email_to_id, this user will be used
        :return:
        """
        for obj in dicts:
            created_by = obj.get('created_by', {})
            email = created_by.get('email') if isinstance(created_by, dict) else None

            # user default user
            if email not in members_email_to_id:
                obj['created_by_id'] = default_user.id
                logger.warning('Email not found in members_email_to_id, default user used instead')

            # resolve annotators by email
            else:
                obj['created_by_id'] = members_email_to_id[email]

            obj.pop('created_by', None)

    @staticmethod
    def _insert_valid_user_drafts(dicts, members_email_to_id, default_user):
        """Insert correct user id by email from snapshot

        :param dicts: draft or review dicts from snapshot
        :param members_email_to_id: mapping from emails to current LS instance user IDs
        :param default_user: if email is not found in membr_email_to_id, this user will be used
        :return:
        """
        for obj in dicts:
            email = obj.get('user')

            # user default user
            if email not in members_email_to_id:
                obj['user_id'] = default_user.id
                logger.warning('Email not found in members_email_to_id, default user used instead')

            # resolve annotators by email
            else:
                obj['user_id'] = members_email_to_id[email]

            obj.pop('user', None)

    @retry_database_locked()
    def create(self, validated_data):
        """Create Tasks, Annotations, etc in bulk"""
        validated_tasks = validated_data
        logging.info(f'Try to serialize tasks with annotations, data len = {len(validated_data)}')
        user = self.context.get('user', None)
        default_user = user or self.project.created_by
        ff_user = self.project.organization.created_by

        # get members from project, we need them to restore annotation.completed_by etc
        organization = self.project.organization
        members_email_to_id = dict(organization.members.values_list('user__email', 'user__id'))
        members_ids = set(members_email_to_id.values())
        logger.debug(f'{len(members_email_to_id)} members found in organization {organization}')

        # to be sure we add tasks with annotations at the same time
        with transaction.atomic():

            # extract annotations, predictions, drafts, reviews, etc
            # all these lists will be grouped by tasks, e.g.:
            # task_annotations = [ [a1, a2], [a3, a4, a5], ... ]
            task_annotations, task_predictions = [], []
            task_drafts, task_reviews = [], []
            for task in validated_tasks:
                # extract annotations from snapshot
                annotations = task.pop('annotations', [])
                self._insert_valid_completed_by(annotations, members_email_to_id, members_ids, default_user)
                task_annotations.append(annotations)

                # extract predictions from snapshot
                predictions = task.pop('predictions', [])
                task_predictions.append(predictions)

                if flag_set('fflag_feat_back_lsdv_5307_import_reviews_drafts_29062023_short', user=ff_user):
                    # extract drafts from snapshot
                    drafts = task.pop('drafts', [])
                    self._insert_valid_user_drafts(drafts, members_email_to_id, default_user)
                    task_drafts.append(drafts)

                    # extract reviews from snapshot annotations
                    for annotation in annotations:
                        reviews = annotation.get('reviews', [])
                        self._insert_valid_user_reviews(reviews, members_email_to_id, default_user)
                        task_reviews.append(reviews)

            db_tasks = self.add_tasks(task_annotations, task_predictions, validated_tasks)
            db_annotations = self.add_annotations(task_annotations, user)
            self.add_predictions(task_predictions)

        self.post_process_annotations(user, db_annotations, 'imported')
        self.post_process_tasks(self.project.id, [t.id for t in self.db_tasks])
        self.post_process_custom_callback(self.project.id, user)

        if flag_set('fflag_feat_back_lsdv_5307_import_reviews_drafts_29062023_short', user=ff_user):
            with transaction.atomic():
                # build mapping between new and old ids in annotations,
                # we need it because annotation ids will be known only after saving to db
                annotation_mapping = {v.import_id: v.id for v in db_annotations}
                annotation_mapping[None] = None
                # the sequence of add_ functions is very important because of references to ids
                self.add_drafts(task_drafts, db_tasks, annotation_mapping, self.project)
                self.add_reviews(task_reviews, annotation_mapping, self.project)

        return db_tasks

    def add_predictions(self, task_predictions):
        """Save predictions to DB and set the latest model version in the project"""
        db_predictions = []

        # add predictions
        last_model_version = None
        for i, predictions in enumerate(task_predictions):
            for prediction in predictions:
                if not isinstance(prediction, dict):
                    continue

                # we need to call result normalizer here since "bulk_create" doesn't call save() method
                result = Prediction.prepare_prediction_result(prediction['result'], self.project)
                prediction_score = prediction.get('score')
                if prediction_score is not None:
                    try:
                        prediction_score = float(prediction_score)
                    except ValueError:
                        logger.error(
                            "Can't upload prediction score: should be in float format." 'Fallback to score=None'
                        )
                        prediction_score = None

                last_model_version = prediction.get('model_version', 'undefined')
                db_predictions.append(
                    Prediction(
                        task=self.db_tasks[i],
                        project=self.db_tasks[i].project,
                        result=result,
                        score=prediction_score,
                        model_version=last_model_version,
                    )
                )

        # predictions: DB bulk create
        self.db_predictions = Prediction.objects.bulk_create(db_predictions, batch_size=settings.BATCH_SIZE)
        logging.info(f'Predictions serialization success, len = {len(self.db_predictions)}')

        # renew project model version if it's empty
        if not self.project.model_version and last_model_version is not None:
            self.project.model_version = last_model_version
            self.project.save()

        return self.db_predictions, last_model_version

    def add_reviews(self, task_reviews, annotation_mapping, project):
        """Save task reviews to DB"""
        return []

    def add_drafts(self, task_drafts, db_tasks, annotation_mapping, project):
        """Save task drafts to DB"""
        db_drafts = []

        # add drafts
        for i, drafts in enumerate(task_drafts):
            for draft in drafts:
                if not isinstance(draft, dict):
                    continue

                draft.update(
                    {
                        'task_id': db_tasks[i].id,
                        'annotation_id': annotation_mapping[draft.get('annotation')],
                        'project': self.project,
                        'import_id': draft.get('id'),
                    }
                )
                # remove redundant fields
                [
                    draft.pop(field, None)
                    for field in ['id', 'task', 'annotation', 'project', 'created_username', 'created_ago']
                ]
                db_drafts.append(AnnotationDraft(**draft))

        self.db_drafts = AnnotationDraft.objects.bulk_create(db_drafts, batch_size=settings.BATCH_SIZE)
        logging.info(f'drafts serialization success, len = {len(self.db_drafts)}')

        return self.db_drafts

    def add_annotations(self, task_annotations, user):
        """Save task annotations to DB"""
        db_annotations = []

        # add annotations
        for i, annotations in enumerate(task_annotations):
            for annotation in annotations:
                if not isinstance(annotation, dict):
                    continue

                ground_truth = annotation.pop('ground_truth', True)
                was_cancelled = annotation.pop('was_cancelled', False)
                lead_time = annotation.pop('lead_time', None)

                body = {
                    'task': self.db_tasks[i],
                    'project': self.project,
                    'ground_truth': ground_truth,
                    'was_cancelled': was_cancelled,
                    'completed_by_id': annotation['completed_by_id'],
                    'result': annotation['result'],
                    'lead_time': lead_time,
                    'import_id': annotation.get('id'),
                }
                db_annotations.append(Annotation(**body))

        # annotations: DB bulk create
        if settings.DJANGO_DB == settings.DJANGO_DB_SQLITE:
            self.db_annotations = []
            try:
                last_annotation = Annotation.objects.latest('id')
                current_id = last_annotation.id + 1
            except Annotation.DoesNotExist:
                current_id = 1

            for annotation in db_annotations:
                annotation.id = current_id
                current_id += 1
            self.db_annotations = Annotation.objects.bulk_create(db_annotations, batch_size=settings.BATCH_SIZE)
        else:
            self.db_annotations = Annotation.objects.bulk_create(db_annotations, batch_size=settings.BATCH_SIZE)
        logging.info(f'Annotations serialization success, len = {len(self.db_annotations)}')

        return self.db_annotations

    def add_tasks(self, task_annotations, task_predictions, validated_tasks):
        """Extract tasks from validated_tasks and store them in DB"""
        db_tasks = []
        max_overlap = self.project.maximum_annotations

        # Acquire a lock on the project to ensure atomicity when calculating inner_id
        project = Project.objects.select_for_update().get(id=self.project.id)

        last_task = fast_first(Task.objects.filter(project=project).order_by('-inner_id'))
        prev_inner_id = last_task.inner_id if last_task else 0
        max_inner_id = (prev_inner_id + 1) if prev_inner_id else 1

        for i, task in enumerate(validated_tasks):
            cancelled_annotations = len([ann for ann in task_annotations[i] if ann.get('was_cancelled', False)])
            total_annotations = len(task_annotations[i]) - cancelled_annotations
            t = Task(
                project=self.project,
                data=task['data'],
                meta=task.get('meta', {}),
                overlap=max_overlap,
                is_labeled=len(task_annotations[i]) >= max_overlap,
                file_upload_id=task.get('file_upload_id'),
                inner_id=None if prev_inner_id is None else max_inner_id + i,
                total_predictions=len(task_predictions[i]),
                total_annotations=total_annotations,
                cancelled_annotations=cancelled_annotations,
            )
            db_tasks.append(t)

        # get task ids
        if settings.DJANGO_DB == settings.DJANGO_DB_SQLITE:
            self.db_tasks = []
            try:
                last_task = Task.objects.latest('id')
                current_id = last_task.id + 1
            except Task.DoesNotExist:
                current_id = 1

            for task in db_tasks:
                task.id = current_id
                current_id += 1
            self.db_tasks = Task.objects.bulk_create(db_tasks, batch_size=settings.BATCH_SIZE)
        else:
            self.db_tasks = Task.objects.bulk_create(db_tasks, batch_size=settings.BATCH_SIZE)

        logging.info(f'Tasks serialization success, len = {len(self.db_tasks)}')

        return db_tasks

    @staticmethod
    def post_process_annotations(user, db_annotations, action):
        pass

    @staticmethod
    def post_process_tasks(user, db_tasks):
        pass

    @staticmethod
    def add_annotation_fields(body, user, action):
        return body

    @staticmethod
    def post_process_custom_callback(project_id, user):
        pass

    class Meta:
        model = Task
        fields = '__all__'


TaskSerializer = load_func(settings.TASK_SERIALIZER)


class TaskWithAnnotationsSerializer(TaskSerializer):
    """ """

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.fields['annotations'] = AnnotationSerializer(many=True, default=[], context=self.context)

    class Meta:
        model = Task
        list_serializer_class = load_func(settings.TASK_SERIALIZER_BULK)

        exclude = ()


class AnnotationDraftSerializer(ModelSerializer):

    user = serializers.CharField(default=serializers.CurrentUserDefault())
    created_username = serializers.SerializerMethodField(default='', read_only=True, help_text='User name string')
    created_ago = serializers.CharField(default='', read_only=True, help_text='Delta time from creation time')

    def get_created_username(self, draft):
        user = draft.user
        if not user:
            return ''

        name = user.first_name
        last_name = user.last_name
        if len(last_name):
            name = name + ' ' + last_name
        name += (' ' if name else '') + f'{user.email}, {user.id}'
        return name

    class Meta:
        model = AnnotationDraft
        fields = '__all__'


class TaskWithAnnotationsAndPredictionsAndDraftsSerializer(TaskSerializer):

    predictions = serializers.SerializerMethodField(default=[], read_only=True)
    annotations = serializers.SerializerMethodField(default=[], read_only=True)
    drafts = serializers.SerializerMethodField(default=[], read_only=True)
    updated_by = serializers.SerializerMethodField(default=[], read_only=True)

    def get_updated_by(self, task):
        return [{'user_id': task.updated_by_id}] if task.updated_by_id else []

    def _get_user(self):
        if 'request' in self.context and hasattr(self.context['request'], 'user'):
            return self.context['request'].user

    def get_predictions(self, task):
        predictions = task.predictions
        user = self._get_user()
        if flag_set('ff_front_dev_1682_model_version_dropdown_070622_short', user=user or 'auto'):
            active_ml_backends = task.project.get_active_ml_backends()
            model_versions = active_ml_backends.values_list('model_version', flat=True)
            logger.debug(f'Selecting predictions from active ML backend model versions: {model_versions}')
            predictions = predictions.filter(model_version__in=model_versions)
        elif task.project.model_version:
            predictions = predictions.filter(model_version=task.project.model_version)
        return PredictionSerializer(predictions, many=True, read_only=True, default=[], context=self.context).data

    def get_annotations(self, task):
        """Return annotations only for the current user"""
        annotations = task.annotations

        user = self._get_user()
        if user and user.is_annotator:
            annotations = annotations.filter(completed_by=user)

        return AnnotationSerializer(annotations, many=True, read_only=True, default=[], context=self.context).data

    def get_drafts(self, task):
        """Return drafts only for the current user"""
        # it's for swagger documentation
        if not isinstance(task, Task):
            return AnnotationDraftSerializer(many=True)

        drafts = task.drafts
        if 'request' in self.context and hasattr(self.context['request'], 'user'):
            user = self.context['request'].user
            drafts = drafts.filter(user=user)

        return AnnotationDraftSerializer(drafts, many=True, read_only=True, default=[], context=self.context).data


class NextTaskSerializer(TaskWithAnnotationsAndPredictionsAndDraftsSerializer):
    unique_lock_id = serializers.SerializerMethodField()

    def get_unique_lock_id(self, task):
        user = self.context['request'].user
        lock = task.locks.filter(user=user).first()
        if lock:
            return lock.unique_id

    def get_predictions(self, task):
        predictions = task.get_predictions_for_prelabeling()
        return PredictionSerializer(predictions, many=True, read_only=True, default=[], context=self.context).data

    def get_annotations(self, task):
        result = []
        if self.context.get('annotations', False):
            annotations = super().get_annotations(task)
            user = self.context['request'].user
            for annotation in annotations:
                if annotation.get('completed_by') == user.id:
                    result.append(annotation)
        return result


class TaskIDWithAnnotationsAndPredictionsSerializer(ModelSerializer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.fields['annotations'] = AnnotationSerializer(many=True, default=[], context=self.context)
        self.fields['predictions'] = PredictionSerializer(many=True, default=[], context=self.context)

    class Meta:
        model = Task
        fields = ['id', 'annotations', 'predictions']


class TaskIDOnlySerializer(ModelSerializer):
    class Meta:
        model = Task
        fields = ['id']


# LSE inherits this serializer
TaskSerializerBulk = load_func(settings.TASK_SERIALIZER_BULK)
</file>

<file path="label_studio/tasks/urls.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
from django.urls import include, path
from rest_framework import routers

from . import api

app_name = 'tasks'

router = routers.DefaultRouter()
router.register(r'predictions', api.PredictionAPI, basename='prediction')

_api_urlpatterns = [
    # CRUD
    path('', api.TaskListAPI.as_view(), name='task-list'),
    path('<int:pk>/', api.TaskAPI.as_view(), name='task-detail'),
    path('<int:pk>/annotations/', api.AnnotationsListAPI.as_view(), name='task-annotations'),
    path('<int:pk>/drafts', api.AnnotationDraftListAPI.as_view(), name='task-drafts'),
    path(
        '<int:pk>/annotations/<int:annotation_id>/drafts',
        api.AnnotationDraftListAPI.as_view(),
        name='task-annotations-drafts',
    ),
]

_api_annotations_urlpatterns = [
    path('<int:pk>/', api.AnnotationAPI.as_view(), name='annotation-detail'),
    path('<int:pk>/convert-to-draft', api.AnnotationConvertAPI.as_view(), name='annotation-convert-to-draft'),
]

_api_drafts_urlpatterns = [
    path('<int:pk>/', api.AnnotationDraftAPI.as_view(), name='draft-detail'),
]

_api_predictions_urlpatterns = router.urls


urlpatterns = [
    path('api/tasks/', include((_api_urlpatterns, app_name), namespace='api')),
    # TODO: these should be moved to the separate apps
    path('api/annotations/', include((_api_annotations_urlpatterns, app_name), namespace='api-annotations')),
    path('api/drafts/', include((_api_drafts_urlpatterns, app_name), namespace='api-drafts')),
    path('api/', include((_api_predictions_urlpatterns, app_name), namespace='api-predictions')),
]
</file>

<file path="label_studio/tasks/validation.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import logging
from functools import reduce
from operator import getitem
from urllib.parse import urlparse

import ujson as json
from core.label_config import replace_task_data_undefined_with_config_field
from rest_framework.exceptions import ValidationError


class SkipField(Exception):
    pass


_DATA_TYPES = {
    'Text': [str, int, float, list],
    'Header': [str, int, float],
    'HyperText': [str],
    'Image': [str, list],
    'Paragraphs': [list, str],
    'Table': [dict, list, str],
    'TimeSeries': [dict, list, str],
    'TimeSeriesChannel': [dict, list, str],
    'List': [list, str],
    'Choices': [str, list],
    'PolygonLabels': [str, list],
    'Labels': [str, list],
    'BrushLabels': [str, list],
    'EllipseLabels': [str, list],
    'HyperTextLabels': [str, list],
    'KeyPointLabels': [str, list],
    'ParagraphLabels': [str, list],
    'RectangleLabels': [str, list],
    'TimeSeriesLabels': [str, list],
    'Taxonomy': [str, list, type(None)],
    'Ranker': [list, str],
}
logger = logging.getLogger(__name__)


class TaskValidator:
    """Task Validator with project scheme configs validation. It is equal to TaskSerializer from django backend."""

    def __init__(self, project, instance=None):
        self.project = project
        self.instance = instance
        self.annotation_count = 0
        self.prediction_count = 0

    @staticmethod
    def check_data(project, data):
        """Validate data from task['data']"""
        if data is None:
            raise ValidationError('Task is empty (None)')

        replace_task_data_undefined_with_config_field(data, project)

        # iterate over data types from project
        for data_key, data_type in project.data_types.items():

            # get array name in case of Repeater tag
            is_array = '[' in data_key
            data_key = data_key.split('[')[0]

            if '.' in data_key:
                keys = data_key.split('.')
                try:
                    data_item = reduce(getitem, keys, data)
                except KeyError:
                    raise ValidationError('"{data_key}" key is expected in task data'.format(data_key=data_key))
            else:
                if data_key not in data:
                    raise ValidationError('"{data_key}" key is expected in task data'.format(data_key=data_key))
                data_item = data[data_key]

            if is_array:
                expected_types = (list,)
            else:
                expected_types = _DATA_TYPES.get(data_type, (str,))

            if not isinstance(data_item, tuple(expected_types)):
                raise ValidationError(
                    "data['{data_key}']={data_value} is of type '{type}', "
                    'but the object tag {data_type} expects the following types: {expected_types}'.format(
                        data_key=data_key,
                        data_value=data_item,
                        type=type(data_item).__name__,
                        data_type=data_type,
                        expected_types=[e.__name__ for e in expected_types],
                    )
                )

        return data

    @staticmethod
    def check_data_and_root(project, data, dict_is_root=False):
        """Check data consistent and data is dict with task or dict['task'] is task

        :param project:
        :param data:
        :param dict_is_root:
        :return:
        """
        try:
            TaskValidator.check_data(project, data)
        except ValidationError as e:
            if dict_is_root:
                raise ValidationError(e.detail[0] + ' [assume: item as is = task root with values] ')
            else:
                raise ValidationError(e.detail[0] + ' [assume: item["data"] = task root with values]')

    @staticmethod
    def check_allowed(task):
        # task is required
        if 'data' not in task:
            return False

        # everything is ok
        return True

    @staticmethod
    def raise_if_wrong_class(task, key, class_def):
        if key in task and not isinstance(task[key], class_def):
            if isinstance(class_def, tuple):
                class_def = ' or '.join([c.__name__ for c in class_def])
            else:
                class_def = class_def.__name__
            raise ValidationError('Task[{key}] must be {class_def}'.format(key=key, class_def=class_def))

    def validate(self, task):
        """Validate whole task with task['data'] and task['annotations']. task['predictions']"""
        # task is class
        if hasattr(task, 'data'):
            self.check_data_and_root(self.project, task.data)
            return task

        # self.instance is loaded by get_object of view
        if self.instance and hasattr(self.instance, 'data'):
            if isinstance(self.instance.data, dict):
                data = self.instance.data
            elif isinstance(self.instance.data, str):
                try:
                    data = json.loads(self.instance.data)
                except ValueError as e:
                    raise ValidationError("Can't parse task data: " + str(e))
            else:
                raise ValidationError(
                    'Field "data" must be string or dict, but not "' + type(self.instance.data) + '"'
                )
            self.check_data_and_root(self.instance.project, data)
            return task

        # check task is dict
        if not isinstance(task, dict):
            raise ValidationError('Task root must be dict with "data", "meta", "annotations", "predictions" fields')

        # task[data] | task[annotations] | task[predictions] | task[meta]
        if self.check_allowed(task):
            # task[data]
            self.raise_if_wrong_class(task, 'data', (dict, list))
            self.check_data_and_root(self.project, task['data'])

            # task[annotations]: we can't use AnnotationSerializer for validation
            # because it's much different with validation we need here
            self.raise_if_wrong_class(task, 'annotations', list)
            for annotation in task.get('annotations', []):
                if not isinstance(annotation, dict):
                    logger.warning('Annotation must be dict, but "%s" found', str(type(annotation)))
                    continue

                ok = 'result' in annotation
                if not ok:
                    raise ValidationError('Annotation must have "result" fields')

                # check result is list
                if not isinstance(annotation.get('result', []), list):
                    raise ValidationError('"result" field in annotation must be list')

            # task[predictions]
            self.raise_if_wrong_class(task, 'predictions', list)
            for prediction in task.get('predictions', []):
                if not isinstance(prediction, dict):
                    logger.warning('Prediction must be dict, but "%s" found', str(type(prediction)))
                    continue

                ok = 'result' in prediction
                if not ok:
                    raise ValidationError('Prediction must have "result" fields')

            # task[meta]
            self.raise_if_wrong_class(task, 'meta', (dict, list))

        # task is data as is, validate task as data and move it to task['data']
        else:
            self.check_data_and_root(self.project, task, dict_is_root=True)
            task = {'data': task}

        return task

    @staticmethod
    def format_error(i, detail, item):
        if len(detail) == 1:
            code = (str(detail[0].code + ' ')) if detail[0].code != 'invalid' else ''
            return 'Error {code} at item {i}: {detail} :: {item}'.format(code=code, i=i, detail=detail[0], item=item)
        else:
            errors = ', '.join(detail)
            codes = str([d.code for d in detail])
            return 'Errors {codes} at item {i}: {errors} :: {item}'.format(codes=codes, i=i, errors=errors, item=item)

    def to_internal_value(self, data):
        """Body of run_validation for all data items"""
        if data is None:
            raise ValidationError('All tasks are empty (None)')

        if not isinstance(data, list):
            raise ValidationError('data is not a list')

        if len(data) == 0:
            raise ValidationError('data is empty')

        ret, errors = [], []
        self.annotation_count, self.prediction_count = 0, 0
        for i, item in enumerate(data):
            try:
                validated = self.validate(item)
            except ValidationError as exc:
                error = self.format_error(i, exc.detail, item)
                errors.append(error)
                # do not print to user too many errors
                if len(errors) >= 100:
                    errors[99] = '...'
                    break
            else:
                ret.append(validated)
                errors.append({})

                if 'annotations' in item:
                    self.annotation_count += len(item['annotations'])
                if 'predictions' in item:
                    self.prediction_count += len(item['predictions'])

        if any(errors):
            logger.warning("Can't deserialize tasks due to " + str(errors))
            raise ValidationError(errors)

        return ret


def is_url(string):
    try:
        result = urlparse(string.strip())
        return all([result.scheme, result.netloc])
    except ValueError:
        return False
</file>

<file path="label_studio/tests/data_import/test_uploader.py">
from unittest import mock
from unittest.mock import Mock

import pytest
from core.utils.io import validate_upload_url
from data_import.uploader import check_tasks_max_file_size, load_tasks
from django.conf import settings
from rest_framework.exceptions import ValidationError

pytestmark = pytest.mark.django_db


class MockedRequest:
    FILES = ()

    def __init__(self, url):
        self.url = url

    @property
    def content_type(self):
        return 'application/x-www-form-urlencoded'

    @property
    def data(self):
        return {'url': self.url}

    @property
    def user(self):
        return None


class TestUploader:
    @pytest.fixture
    def project(self, configured_project, settings):
        return configured_project

    class TestLoadTasks:
        @mock.patch('core.utils.io.validate_upload_url', wraps=validate_upload_url)
        @pytest.mark.parametrize('url', ('file:///etc/passwd', 'ftp://example.org'))
        def test_raises_for_unsafe_urls(self, validate_upload_url_mock, url, project):
            request = MockedRequest(url=url)

            with pytest.raises(ValidationError) as e:
                load_tasks(request, project)
                assert 'The provided URL was not valid.' in e.value

            validate_upload_url_mock.assert_called_once_with(url, block_local_urls=False)

        @mock.patch('core.utils.io.validate_upload_url', wraps=validate_upload_url)
        def test_raises_for_local_urls_with_ssrf_protection_enabled(self, validate_upload_url_mock, project, settings):
            settings.SSRF_PROTECTION_ENABLED = True
            request = MockedRequest(url='http://0.0.0.0')

            with pytest.raises(ValidationError) as e:
                load_tasks(request, project)
                assert 'The provided URL was not valid.' in e.value

            validate_upload_url_mock.assert_called_once_with('http://0.0.0.0', block_local_urls=True)

        def test_local_url_after_redirect(self, project, settings):
            settings.SSRF_PROTECTION_ENABLED = True
            request = MockedRequest(url='http://validurl.com')

            # Mock the necessary parts of the response object
            mock_response = Mock()
            mock_response.raw._connection.sock.getpeername.return_value = ('127.0.0.1', 8080)

            # Patch the requests.get call in the data_import.uploader module
            with mock.patch('core.utils.io.requests.get', return_value=mock_response), pytest.raises(
                ValidationError
            ) as e:
                load_tasks(request, project)
            assert 'URL resolves to a reserved network address (block: 127.0.0.0/8)' in str(e.value)

        def test_user_specified_block(self, project, settings):
            settings.SSRF_PROTECTION_ENABLED = True
            settings.USER_ADDITIONAL_BANNED_SUBNETS = ['1.2.3.4']
            request = MockedRequest(url='http://validurl.com')

            # Mock the necessary parts of the response object
            mock_response = Mock()
            mock_response.raw._connection.sock.getpeername.return_value = ('1.2.3.4', 8080)

            # Patch the requests.get call in the data_import.uploader module
            with mock.patch('core.utils.io.requests.get', return_value=mock_response), pytest.raises(
                ValidationError
            ) as e:
                load_tasks(request, project)
            assert 'URL resolves to a reserved network address (block: 1.2.3.4)' in str(e.value)

            mock_response.raw._connection.sock.getpeername.return_value = ('198.51.100.0', 8080)
            with mock.patch('core.utils.io.requests.get', return_value=mock_response), pytest.raises(
                ValidationError
            ) as e:
                load_tasks(request, project)
            assert 'URL resolves to a reserved network address (block: 198.51.100.0/24)' in str(e.value)

        def test_user_specified_block_without_default(self, project, settings):
            settings.SSRF_PROTECTION_ENABLED = True
            settings.USER_ADDITIONAL_BANNED_SUBNETS = ['1.2.3.4']
            settings.USE_DEFAULT_BANNED_SUBNETS = False
            request = MockedRequest(url='http://validurl.com')

            # Mock the necessary parts of the response object
            mock_response = Mock()
            mock_response.raw._connection.sock.getpeername.return_value = ('1.2.3.4', 8080)

            # Patch the requests.get call in the data_import.uploader module
            with mock.patch('core.utils.io.requests.get', return_value=mock_response), pytest.raises(
                ValidationError
            ) as e:
                load_tasks(request, project)
            assert 'URL resolves to a reserved network address (block: 1.2.3.4)' in str(e.value)

            mock_response.raw._connection.sock.getpeername.return_value = ('198.51.100.0', 8080)
            with mock.patch('core.utils.io.requests.get', return_value=mock_response), pytest.raises(
                ValidationError
            ) as e:
                load_tasks(request, project)
            assert "'Mock' object is not subscriptable" in str(e.value)  # validate ip did not raise exception


class TestTasksFileChecks:
    @pytest.mark.parametrize('value', (0, settings.TASKS_MAX_FILE_SIZE - 1))
    def test_check_tasks_max_file_size_does_not_raise_for_correct_value(self, value):
        check_tasks_max_file_size(value)

    def test_check_tasks_max_file_size_raises_for_too_big_value(self):
        value = settings.TASKS_MAX_FILE_SIZE + 1

        with pytest.raises(ValidationError) as e:
            check_tasks_max_file_size(value)

        assert f'Maximum total size of all files is {settings.TASKS_MAX_FILE_SIZE} bytes' in str(e.value)
</file>

<file path="label_studio/tests/data_manager/actions/test_cache_labels.py">
"""Tests for the cache_labels action."""

import pytest
from data_manager.actions.cache_labels import cache_labels_job
from django.contrib.auth import get_user_model
from projects.models import Project
from tasks.models import Annotation, Prediction, Task


@pytest.mark.django_db
@pytest.mark.parametrize(
    'source, control_tag, with_counters, expected_cache_column, use_predictions',
    [
        # Test case 1: Annotations, control tag 'ALL', with counters
        ('annotations', 'ALL', 'Yes', 'cache_all', False),
        # Test case 2: Annotations, specific control tag, with counters
        ('annotations', 'label', 'Yes', 'cache_label', False),
        # Test case 3: Annotations, control tag 'ALL', without counters
        ('annotations', 'ALL', 'No', 'cache_all', False),
        # Test case 4: Predictions, control tag 'ALL', with counters
        ('predictions', 'ALL', 'Yes', 'cache_predictions_all', True),
    ],
)
def test_cache_labels_job(source, control_tag, with_counters, expected_cache_column, use_predictions):
    # Initialize a test user and project
    User = get_user_model()
    test_user = User.objects.create(username='test_user')
    project = Project.objects.create(title='Test Project', created_by=test_user)

    # Create a few tasks
    tasks = []
    for i in range(3):
        task = Task.objects.create(project=project, data={'text': f'This is task {i}'})
        tasks.append(task)

    # Add a few annotations or predictions to these tasks
    for i, task in enumerate(tasks):
        result = [
            {
                'from_name': 'label',  # Control tag used in the result
                'to_name': 'text',
                'type': 'labels',
                'value': {'labels': [f'Label_{i%2+1}']},
            }
        ]
        if use_predictions:
            Prediction.objects.create(task=task, project=project, result=result, model_version='v1')
        else:
            Annotation.objects.create(task=task, project=project, completed_by=test_user, result=result)

    # Prepare the request data
    request_data = {'source': source, 'control_tag': control_tag, 'with_counters': with_counters}

    # Get the queryset of tasks to process
    queryset = Task.objects.filter(project=project)

    # Run cache_labels_job
    cache_labels_job(project, queryset, request_data=request_data)

    # Check that the expected cache column is added to task['data']
    for task in tasks:
        task.refresh_from_db()
        cache_column = expected_cache_column
        assert cache_column in task.data
        cached_labels = task.data[cache_column]
        assert cached_labels is not None

        # Verify the contents of the cached labels
        if use_predictions:
            source_objects = Prediction.objects.filter(task=task)
        else:
            source_objects = Annotation.objects.filter(task=task)

        all_labels = []
        for source_obj in source_objects:
            for result in source_obj.result:
                # Apply similar logic as in extract_labels
                from_name = result.get('from_name')
                if control_tag == 'ALL' or control_tag == from_name:
                    value = result.get('value', {})
                    for key in value:
                        if isinstance(value[key], list) and value[key] and isinstance(value[key][0], str):
                            all_labels.extend(value[key])
                            break

        if with_counters.lower() == 'yes':
            expected_cache = ', '.join(sorted([f'{label}: {all_labels.count(label)}' for label in set(all_labels)]))
        else:
            expected_cache = ', '.join(sorted(list(set(all_labels))))

        assert cached_labels == expected_cache
</file>

<file path="label_studio/tests/data_manager/actions/test_predictions_to_annotations.py">
import mock
from data_manager.actions.predictions_to_annotations import predictions_to_annotations_form
from projects.models import Project
from users.models import User


def test_predictions_to_annotations_form():
    project = Project()
    user = User()

    with mock.patch('projects.models.Project.get_model_versions') as mock_get_model_versions:
        project.model_version = ''
        mock_get_model_versions.return_value = ['undefined']
        assert predictions_to_annotations_form(user, project)[0]['fields'][0]['options'] == ['undefined']

        project.model_version = None
        mock_get_model_versions.return_value = ['undefined']
        assert predictions_to_annotations_form(user, project)[0]['fields'][0]['options'] == ['undefined']

        project.model_version = 'undefined'
        mock_get_model_versions.return_value = ['undefined']
        assert predictions_to_annotations_form(user, project)[0]['fields'][0]['options'] == ['undefined']

        project.model_version = ''
        mock_get_model_versions.return_value = []
        assert predictions_to_annotations_form(user, project)[0]['fields'][0]['options'] == []

        project.model_version = None
        mock_get_model_versions.return_value = []
        assert predictions_to_annotations_form(user, project)[0]['fields'][0]['options'] == []

        project.model_version = 'undefined'
        mock_get_model_versions.return_value = []
        assert predictions_to_annotations_form(user, project)[0]['fields'][0]['options'] == ['undefined']
</file>

<file path="label_studio/tests/data_manager/test_api_actions.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""

import json

import pytest
from django.db import transaction
from io_storages.azure_blob.models import (
    AzureBlobImportStorage,
    AzureBlobImportStorageLink,
)
from io_storages.gcs.models import GCSImportStorage, GCSImportStorageLink
from io_storages.localfiles.models import (
    LocalFilesImportStorage,
    LocalFilesImportStorageLink,
)
from io_storages.redis.models import RedisImportStorage, RedisImportStorageLink
from io_storages.s3.models import S3ImportStorage, S3ImportStorageLink
from projects.models import Project

from ..utils import make_annotation, make_prediction, make_task, project_id  # noqa


@pytest.mark.parametrize(
    'tasks_count, annotations_count, predictions_count',
    [
        [10, 2, 2],
    ],
)
@pytest.mark.django_db
def test_action_delete_all_tasks(tasks_count, annotations_count, predictions_count, business_client, project_id):
    # create
    payload = dict(project=project_id, data={'test': 1})
    response = business_client.post(
        '/api/dm/views/',
        data=json.dumps(payload),
        content_type='application/json',
    )

    assert response.status_code == 201, response.content
    response.json()['id']

    project = Project.objects.get(pk=project_id)
    for _ in range(0, tasks_count):
        task_id = make_task({'data': {}}, project).id
        print('TASK_ID: %s' % task_id)
        for _ in range(0, annotations_count):
            print('COMPLETION')
            make_annotation({'result': []}, task_id)

        for _ in range(0, predictions_count):
            make_prediction({'result': []}, task_id)
    with transaction.atomic():
        business_client.post(
            f'/api/dm/actions?project={project_id}&id=delete_tasks',
            json={'selectedItems': {'all': True, 'excluded': []}},
        )
    assert project.tasks.count() == 0


@pytest.mark.parametrize(
    'tasks_count, annotations_count, predictions_count',
    [
        [10, 2, 2],
    ],
)
@pytest.mark.django_db
def test_action_delete_all_annotations(tasks_count, annotations_count, predictions_count, business_client, project_id):
    # create
    payload = dict(project=project_id, data={'test': 1})
    response = business_client.post(
        '/api/dm/views/',
        data=json.dumps(payload),
        content_type='application/json',
    )

    assert response.status_code == 201, response.content
    response.json()['id']

    project = Project.objects.get(pk=project_id)
    for _ in range(0, tasks_count):
        task_id = make_task({'data': {}}, project).id
        print('TASK_ID: %s' % task_id)
        for _ in range(0, annotations_count):
            print('COMPLETION')
            make_annotation({'result': []}, task_id)

        for _ in range(0, predictions_count):
            make_prediction({'result': []}, task_id)
    # get next task - should be 0
    status = business_client.post(
        f'/api/dm/actions?project={project_id}&id=next_task',
        json={'selectedItems': {'all': True, 'excluded': []}},
    )
    assert status.status_code == 404
    business_client.post(
        f'/api/dm/actions?project={project_id}&id=delete_tasks_annotations',
        json={'selectedItems': {'all': True, 'excluded': []}},
    )
    # get next task - should be 1
    status = business_client.post(
        f'/api/dm/actions?project={project_id}&id=next_task',
        json={'selectedItems': {'all': True, 'excluded': []}},
    )
    assert status.status_code == 200


@pytest.mark.django_db
@pytest.mark.parametrize(
    'storage_model, link_model',
    [
        (AzureBlobImportStorage, AzureBlobImportStorageLink),
        (GCSImportStorage, GCSImportStorageLink),
        (S3ImportStorage, S3ImportStorageLink),
        (LocalFilesImportStorage, LocalFilesImportStorageLink),
        (RedisImportStorage, RedisImportStorageLink),
    ],
)
def test_action_remove_duplicates(business_client, project_id, storage_model, link_model):
    # Setup
    project = Project.objects.get(pk=project_id)
    storage = storage_model.objects.create(project=project)

    # task 1: add not a duplicated task
    task_data = {'data': {'image': 'normal.jpg'}}
    task1 = make_task(task_data, project)

    # task 2: add duplicated task, no annotations
    task_data = {'data': {'image': 'duplicated.jpg'}}
    make_task(task_data, project)

    # task 3: add duplicated task, with annotations
    task3 = make_task(task_data, project)
    for _ in range(3):
        make_annotation({'result': []}, task3.id)

    # task 4: add duplicated task, with storage link and one annotation
    task4 = make_task(task_data, project)
    make_annotation({'result': []}, task4.id)
    link_model.objects.create(task=task4, key='duplicated.jpg', storage=storage)

    # call the "remove duplicated tasks" action
    status = business_client.post(
        f'/api/dm/actions?project={project_id}&id=remove_duplicates',
        json={'selectedItems': {'all': True, 'excluded': []}},
    )

    # As the result, we should have only 2 tasks left:
    # task 1 and task 3 with storage link copied from task 4
    assert list(project.tasks.order_by('id').values_list('id', flat=True)) == [
        task1.id,
        task3.id,
    ]
    assert status.status_code == 200
    assert link_model.objects.count() == 1
    assert project.annotations.count() == 4
    assert project.tasks.count() == 2


@pytest.mark.django_db
def test_action_remove_duplicates_with_annotations(business_client, project_id):
    """This test checks that the "remove_duplicates" action works correctly
    when there are annotations distributed among multiple duplicated tasks.
    Remove duplicates should keep the task with the first task with annotations,
    link other annotations to the first task and remove the excess tasks.
    """
    # Setup
    project = Project.objects.get(pk=project_id)
    storage = S3ImportStorage.objects.create(project=project)

    # task 1: add not a duplicated task
    task_data = {'data': {'image': 'normal.jpg'}}
    task1 = make_task(task_data, project)

    # task 2: add duplicated task, no annotations
    task_data = {'data': {'image': 'duplicated.jpg'}}
    task2 = make_task(task_data, project)
    make_annotation({'result': []}, task2.id)
    make_annotation({'result': [], 'was_cancelled': True}, task2.id)

    # task 3: add duplicated task, with annotations
    task3 = make_task(task_data, project)
    for _ in range(3):
        make_annotation({'result': []}, task3.id)

    # task 4: add duplicated task, with storage link and one annotation
    task4 = make_task(task_data, project)
    make_annotation({'result': []}, task4.id)
    S3ImportStorageLink.objects.create(task=task4, key='duplicated.jpg', storage=storage)

    # call the "remove duplicated tasks" action
    status = business_client.post(
        f'/api/dm/actions?project={project_id}&id=remove_duplicates',
        json={'selectedItems': {'all': True, 'excluded': []}},
    )

    # as the result, we should have only 2 tasks left:
    # task 1 and task 3 with storage link copied from task 4
    assert list(project.tasks.order_by('id').values_list('id', flat=True)) == [
        task1.id,
        task2.id,
    ], 'tasks ids wrong'
    assert status.status_code == 200, 'status code wrong'
    assert S3ImportStorageLink.objects.count() == 1, 'storage links count wrong'
    assert project.annotations.count() == 6, 'annotations count wrong'
    assert project.annotations.filter(was_cancelled=True).count() == 1, 'was_cancelled counter wrong'
    assert project.tasks.count() == 2, 'tasks count wrong'
    assert task1.annotations.count() == 0, 'task1 annotations count wrong'
    assert task2.annotations.count() == 6, 'task2 annotations count wrong'
    assert task2.annotations.filter(was_cancelled=True).count() == 1, 'was_cancelled counter wrong'


@pytest.mark.django_db
def test_action_cache_labels(business_client, project_id):
    """This test checks that the "cache_labels" action works correctly
    when there are annotations distributed among multiple tasks.
    """
    # Setup
    project = Project.objects.get(pk=project_id)

    # task 1: add a task with specific labels
    task_data = {'data': {'image': 'image1.jpg'}}
    task1 = make_task(task_data, project)
    make_annotation(
        {
            'result': [
                {
                    'from_name': 'label1',
                    'to_name': 'image',
                    'type': 'labels',
                    'value': {'labels': ['Car']},
                }
            ]
        },
        task1.id,
    )

    # task 2: add a task with different labels
    task_data = {'data': {'image': 'image2.jpg'}}
    task2 = make_task(task_data, project)
    make_annotation(
        {
            'result': [
                {
                    'from_name': 'label1',
                    'to_name': 'image',
                    'type': 'labels',
                    'value': {'labels': ['Car']},
                },
                {
                    'from_name': 'label1',
                    'to_name': 'image',
                    'type': 'labels',
                    'value': {'labels': ['Car', 'Airplane']},
                },
            ]
        },
        task2.id,
    )

    # call the "cache_labels" action with counters
    status = business_client.post(
        f'/api/dm/actions?project={project_id}&id=cache_labels',
        data=json.dumps(
            {
                'selectedItems': {'all': True, 'excluded': []},
                'control_tag': 'label1',
                'with_counters': 'yes',
            }
        ),
        content_type='application/json',
    )

    # Assertions
    # Replace these with the actual assertions for your cache_labels function
    tasks = project.tasks
    assert status.status_code == 200, 'status code wrong'
    assert tasks.count() == 2, 'tasks count wrong'
    assert tasks.get(id=task1.id).data.get('cache_label1') == 'Car: 1', 'cache_label1 wrong for task 1'
    assert tasks.get(id=task2.id).data.get('cache_label1') == 'Airplane: 1, Car: 2', 'cache_label1 wrong for task 2'

    # call the "cache_labels" action without counters
    status = business_client.post(
        f'/api/dm/actions?project={project_id}&id=cache_labels',
        data=json.dumps(
            {
                'selectedItems': {'all': True, 'excluded': []},
                'control_tag': 'label1',
                'with_counters': 'no',
            }
        ),
        content_type='application/json',
    )

    # Assertions
    # Replace these with the actual assertions for your cache_labels function
    assert status.status_code == 200, 'status code wrong'
    assert tasks.get(id=task1.id).data.get('cache_label1') == 'Car', 'cache_label1 wrong for task 1'
    assert tasks.get(id=task2.id).data.get('cache_label1') == 'Airplane, Car', 'cache_label1 wrong for task 2'
</file>

<file path="label_studio/tests/data_manager/test_api_tasks.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import json

import pytest
from projects.models import Project

from ..utils import make_annotation, make_prediction, make_task, project_id  # noqa


@pytest.mark.django_db
def test_views_tasks_api(business_client, project_id):
    # create
    payload = dict(project=project_id, data={'test': 1})
    response = business_client.post(
        '/api/dm/views/',
        data=json.dumps(payload),
        content_type='application/json',
    )

    assert response.status_code == 201, response.content
    view_id = response.json()['id']

    # no tasks
    response = business_client.get(f'/api/tasks?fields=all&view={view_id}')

    assert response.status_code == 200, response.content
    assert response.json()['total'] == 0
    assert len(response.json()['tasks']) == 0

    project = Project.objects.get(pk=project_id)
    task_data = {'text': 'bbb'}
    task_id = make_task({'data': task_data}, project).id

    annotation_result = {'from_name': 'my_class', 'to_name': 'text', 'type': 'choices', 'value': {'choices': ['pos']}}
    make_annotation({'result': [annotation_result]}, task_id)
    make_annotation(
        {
            'result': [annotation_result],
            'was_cancelled': True,
        },
        task_id,
    )
    prediction_result = {'from_name': 'my_class', 'to_name': 'text', 'type': 'choices', 'value': {'choices': ['pos']}}
    make_prediction(
        {
            'result': [prediction_result],
        },
        task_id,
    )

    response = business_client.get(f'/api/tasks?fields=all&view={view_id}')

    assert response.status_code == 200, response.content
    response_data = response.json()
    assert response_data['total'] == 1
    assert len(response_data['tasks']) == 1
    assert response_data['tasks'][0]['id'] == task_id
    assert response_data['tasks'][0]['data'] == task_data
    assert response_data['tasks'][0]['total_annotations'] == 1
    assert 'annotations_results' in response_data['tasks'][0]
    assert response_data['tasks'][0]['cancelled_annotations'] == 1
    assert response_data['tasks'][0]['total_predictions'] == 1
    assert 'predictions_results' in response_data['tasks'][0]

    num_anno1 = response_data['tasks'][0]['annotations'][0]['id']
    num_anno2 = response_data['tasks'][0]['annotations'][1]['id']
    num_pred = response_data['tasks'][0]['predictions'][0]['id']

    # delete annotations and check counters

    business_client.delete(f'/api/annotations/{num_anno1}')
    business_client.delete(f'/api/annotations/{num_anno2}')

    response = business_client.get(f'/api/tasks?fields=all&view={view_id}')
    assert response.status_code == 200, response.content
    response_data = response.json()
    assert response_data['tasks'][0]['cancelled_annotations'] == 0
    assert response_data['tasks'][0]['total_annotations'] == 0

    # delete prediction and check counters
    business_client.delete(f'/api/predictions/{num_pred}')

    response = business_client.get(f'/api/tasks?fields=all&view={view_id}')
    assert response.status_code == 200, response.content
    response_data = response.json()
    assert response_data['tasks'][0]['cancelled_annotations'] == 0
    assert response_data['tasks'][0]['total_annotations'] == 0
    assert response_data['tasks'][0]['total_predictions'] == 0


@pytest.mark.parametrize(
    'tasks_count, annotations_count, predictions_count',
    [
        [0, 0, 0],
        [1, 0, 0],
        [1, 1, 1],
        [2, 2, 2],
    ],
)
@pytest.mark.django_db
def test_views_total_counters(tasks_count, annotations_count, predictions_count, business_client, project_id):
    # create
    payload = dict(project=project_id, data={'test': 1})
    response = business_client.post(
        '/api/dm/views/',
        data=json.dumps(payload),
        content_type='application/json',
    )

    assert response.status_code == 201, response.content
    view_id = response.json()['id']

    project = Project.objects.get(pk=project_id)
    for _ in range(0, tasks_count):
        task_id = make_task({'data': {}}, project).id
        print('TASK_ID: %s' % task_id)
        for _ in range(0, annotations_count):
            make_annotation({'result': []}, task_id)

        for _ in range(0, predictions_count):
            make_prediction({'result': []}, task_id)

    response = business_client.get(f'/api/tasks?fields=all&view={view_id}')

    response_data = response.json()

    assert response_data['total'] == tasks_count, response_data
    assert response_data['total_annotations'] == tasks_count * annotations_count, response_data
    assert response_data['total_predictions'] == tasks_count * predictions_count, response_data
</file>

<file path="label_studio/tests/data_manager/test_columns_api.py">
import json

import pytest

pytestmark = pytest.mark.django_db


def _get_columns(business_client, label_config=None):
    r = business_client.post(
        '/api/projects/',
        data=json.dumps(dict(title='test_project1', **({'label_config': label_config} if label_config else {}))),
        content_type='application/json',
    )

    project1_id = r.json()['id']
    r = business_client.get(f'/api/dm/columns/?project={project1_id}')

    assert r.status_code == 200, r.content
    r_json = r.json()
    assert 'columns' in r_json
    return r_json['columns']


def test_columns_api_returns_expected_ids(business_client):
    columns = _get_columns(business_client)

    assert [c['id'] for c in columns] == [
        'id',
        'inner_id',
        'completed_at',
        'total_annotations',
        'cancelled_annotations',
        'total_predictions',
        'annotators',
        'annotations_results',
        'annotations_ids',
        'predictions_score',
        'predictions_model_versions',
        'predictions_results',
        'file_upload',
        'storage_filename',
        'created_at',
        'updated_at',
        'updated_by',
        'avg_lead_time',
        'draft_exists',
        'data',
    ]


def test_columns_api_annotates_default_columns_with_project_defined_false(business_client):
    columns = _get_columns(business_client)

    for c in columns:
        assert 'project_defined' in c
        assert c['project_defined'] is False


def test_columns_api_annotates_config_defined_columns_with_project_defined_true(business_client):
    config_with_text_column = """
        <View>
            <Text value="$text" name="artist" />
            <View>
                <Choices name="choices_1" toName="artist">
                    <Choice name="choice_1" value="1"/>
                </Choices>
            </View>
        </View>
        """

    columns = _get_columns(business_client, config_with_text_column)

    for c in columns:
        assert 'project_defined' in c
        assert c['project_defined'] == (c['id'] == 'text')
</file>

<file path="label_studio/tests/data_manager/test_ordering_filters.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import json

import pytest
from data_import.models import FileUpload
from django.conf import settings
from django.core.files.base import ContentFile
from django.utils.timezone import now
from projects.models import Project

from ..utils import make_annotation, make_annotator, make_prediction, make_task, project_id  # noqa


@pytest.mark.parametrize(
    'ordering, element_index, undefined',
    [
        [['tasks:id'], 0, False],  # ordered by id ascending, first element api == first created
        [['tasks:-id'], -1, False],  # ordered by id descending, first element api == last created
        [['tasks:completed_at'], 0, False],
        [['tasks:-completed_at'], 0, False],  # only one task is labeled
        [['tasks:total_annotations'], -1, False],
        [['tasks:-total_annotations'], 0, False],
        [['tasks:total_predictions'], 0, False],
        [['tasks:-total_predictions'], -1, False],
        [['tasks:cancelled_annotations'], 0, False],
        [['tasks:-cancelled_annotations'], -1, False],
        [['tasks:annotations_results'], 0, False],
        [['tasks:-annotations_results'], -1, False],
        [['tasks:predictions_results'], 0, False],
        [['tasks:-predictions_results'], -1, False],
        [['tasks:predictions_score'], 0, False],
        [['tasks:-predictions_score'], -1, False],
        [['tasks:data.text'], 0, False],
        [['tasks:-data.text'], -1, False],
        [['tasks:data.data'], 0, True],
        [['-tasks:data.data'], 1, True],
        [['tasks:file_upload'], 0, False],
        [['-tasks:file_upload'], 1, False],
    ],
)
@pytest.mark.django_db
def test_views_ordering(ordering, element_index, undefined, business_client, project_id):

    payload = dict(
        project=project_id,
        data={'test': 1, 'ordering': ordering},
    )
    response = business_client.post(
        '/api/dm/views/',
        data=json.dumps(payload),
        content_type='application/json',
    )

    assert response.status_code == 201, response.content
    view_id = response.json()['id']

    project = Project.objects.get(pk=project_id)

    if undefined:
        task_field_name = settings.DATA_UNDEFINED_NAME
    else:
        task_field_name = 'text'

    file_upload1 = FileUpload.objects.create(
        user=project.created_by, project=project, file=ContentFile('', name='file_upload1')
    )

    task_id_1 = make_task({'data': {task_field_name: 1}, 'file_upload': file_upload1}, project).id
    make_annotation({'result': [{'1': True}]}, task_id_1)
    make_prediction({'result': [{'1': True}], 'score': 1}, task_id_1)

    file_upload2 = FileUpload.objects.create(
        user=project.created_by, project=project, file=ContentFile('', name='file_upload2')
    )
    task_id_2 = make_task({'data': {task_field_name: 2}, 'file_upload': file_upload2}, project).id
    for _ in range(0, 2):
        make_annotation({'result': [{'2': True}], 'was_cancelled': True}, task_id_2)
    for _ in range(0, 2):
        make_prediction({'result': [{'2': True}], 'score': 2}, task_id_2)

    task_ids = [task_id_1, task_id_2]

    response = business_client.get(f'/api/tasks?view={view_id}')
    response_data = response.json()

    assert response_data['tasks'][0]['id'] == task_ids[element_index]


@pytest.mark.parametrize(
    'filters, ids',
    [
        [
            {
                'conjunction': 'or',
                'items': [{'filter': 'filter:tasks:id', 'operator': 'equal', 'value': 1, 'type': 'Number'}],
            },
            [1],
        ],
        [
            {
                'conjunction': 'or',
                'items': [
                    {'filter': 'filter:tasks:id', 'operator': 'equal', 'value': 1, 'type': 'Number'},
                    {'filter': 'filter:tasks:id', 'operator': 'equal', 'value': 2, 'type': 'Number'},
                ],
            },
            [1, 2],
        ],
        [
            {
                'conjunction': 'or',
                'items': [
                    {'filter': 'filter:tasks:id', 'operator': 'not_equal', 'value': 1, 'type': 'Number'},
                    {'filter': 'filter:tasks:id', 'operator': 'greater', 'value': 3, 'type': 'Number'},
                ],
            },
            [2, 3, 4],
        ],
        [
            {
                'conjunction': 'or',
                'items': [{'filter': 'filter:tasks:id', 'operator': 'not_equal', 'value': 1, 'type': 'Number'}],
            },
            [2, 3, 4],
        ],
        [
            {
                'conjunction': 'or',
                'items': [{'filter': 'filter:tasks:id', 'operator': 'less', 'value': 3, 'type': 'Number'}],
            },
            [1, 2],
        ],
        [
            {
                'conjunction': 'or',
                'items': [{'filter': 'filter:tasks:id', 'operator': 'greater', 'value': 2, 'type': 'Number'}],
            },
            [3, 4],
        ],
        [
            {
                'conjunction': 'or',
                'items': [{'filter': 'filter:tasks:id', 'operator': 'less_or_equal', 'value': 3, 'type': 'Number'}],
            },
            [1, 2, 3],
        ],
        [
            {
                'conjunction': 'or',
                'items': [{'filter': 'filter:tasks:id', 'operator': 'greater_or_equal', 'value': 2, 'type': 'Number'}],
            },
            [2, 3, 4],
        ],
        [
            {
                'conjunction': 'or',
                'items': [
                    {'filter': 'filter:tasks:id', 'operator': 'in', 'value': {'min': 2, 'max': 3}, 'type': 'Number'}
                ],
            },
            [2, 3],
        ],
        [
            {
                'conjunction': 'or',
                'items': [
                    {
                        'filter': 'filter:tasks:id',
                        'operator': 'not_in',
                        'value': {'min': 2, 'max': 3},
                        'type': 'Number',
                    }
                ],
            },
            [1, 4],
        ],
        [
            {
                'conjunction': 'or',
                'items': [
                    {
                        'filter': 'filter:tasks:completed_at',
                        'operator': 'less',
                        'type': 'Datetime',
                        'value': now().strftime('%Y-%m-%dT%H:%M:%S.%fZ'),
                    }
                ],
            },
            [],
        ],
        [
            {
                'conjunction': 'or',
                'items': [
                    {
                        'filter': 'filter:tasks:completed_at',
                        'operator': 'greater',
                        'type': 'Datetime',
                        'value': now().strftime('%Y-%m-%dT%H:%M:%S.%fZ'),
                    }
                ],
            },
            [1],  # only first task is labeled, second one is skipped
        ],
        [
            {
                'conjunction': 'or',
                'items': [
                    {
                        'filter': 'filter:tasks:completed_at',
                        'operator': 'empty',
                        'type': 'Datetime',
                        'value': 'True',
                    }
                ],
            },
            [2, 3, 4],
        ],
        [
            {
                'conjunction': 'or',
                'items': [
                    {
                        'filter': 'filter:tasks:completed_at',
                        'operator': 'empty',
                        'type': 'Datetime',
                        'value': 'False',
                    }
                ],
            },
            [1],
        ],
        [
            {
                'conjunction': 'or',
                'items': [
                    {
                        'filter': 'filter:tasks:annotations_results',
                        'operator': 'contains',
                        'type': 'String',
                        'value': 'first',
                    }
                ],
            },
            [
                1,
            ],
        ],
        [
            {
                'conjunction': 'or',
                'items': [
                    {
                        'filter': 'filter:tasks:data.data',
                        'operator': 'contains',
                        'type': 'String',
                        'value': 'text1',
                    }
                ],
            },
            [
                1,
            ],
        ],
        [
            {
                'conjunction': 'and',
                'items': [
                    {
                        'filter': 'filter:tasks:data.data',  # undefined column test
                        'operator': 'contains',
                        'type': 'String',
                        'value': 'text',
                    },
                    {'filter': 'filter:tasks:id', 'operator': 'equal', 'value': 1, 'type': 'Number'},
                ],
            },
            [
                1,
            ],
        ],
        [
            {
                'conjunction': 'or',
                'items': [
                    {
                        'filter': 'filter:tasks:annotations_results',
                        'operator': 'not_contains',
                        'type': 'String',
                        'value': 'first',
                    }
                ],
            },
            [2, 3, 4],
        ],
        [
            {
                'conjunction': 'and',
                'items': [
                    {'filter': 'filter:tasks:annotators', 'operator': 'contains', 'value': '$ANN1_ID', 'type': 'List'},
                    {'filter': 'filter:tasks:annotators', 'operator': 'contains', 'value': '$ANN2_ID', 'type': 'List'},
                ],
            },
            [2],
        ],
    ],
)
@pytest.mark.django_db
def test_views_filters(filters, ids, business_client, project_id):
    project = Project.objects.get(pk=project_id)
    ann1 = make_annotator({'email': 'ann1@testheartex.com'}, project)
    ann2 = make_annotator({'email': 'ann2@testheartex.com'}, project)

    ann_ids = {
        '$ANN1_ID': ann1.id,
        '$ANN2_ID': ann2.id,
    }
    for item in filters['items']:
        for ann_id_key, ann_id_value in ann_ids.items():
            if isinstance(item['value'], str) and ann_id_key in item['value']:
                item['value'] = ann_id_value

    payload = dict(
        project=project_id,
        data={'test': 1, 'filters': filters},
    )
    response = business_client.post(
        '/api/dm/views/',
        data=json.dumps(payload),
        content_type='application/json',
    )

    assert response.status_code == 201, response.content
    view_id = response.json()['id']

    task_data_field_name = settings.DATA_UNDEFINED_NAME

    task_id_1 = make_task({'data': {task_data_field_name: 'some text1'}}, project).id
    make_annotation(
        {'result': [{'from_name': '1_first', 'to_name': '', 'value': {}}], 'completed_by': ann1}, task_id_1
    )
    make_prediction({'result': [{'from_name': '1_first', 'to_name': '', 'value': {}}], 'score': 1}, task_id_1)

    task_id_2 = make_task({'data': {task_data_field_name: 'some text2'}}, project).id
    for ann in (ann1, ann2):
        make_annotation(
            {
                'result': [{'from_name': '2_second', 'to_name': '', 'value': {}}],
                'was_cancelled': True,
                'completed_by': ann,
            },
            task_id_2,
        )
    for _ in range(0, 2):
        make_prediction({'result': [{'from_name': '2_second', 'to_name': '', 'value': {}}], 'score': 2}, task_id_2)

    task_ids = [0, task_id_1, task_id_2]

    for _ in range(0, 2):
        task_id = make_task({'data': {task_data_field_name: 'some text_'}}, project).id
        task_ids.append(task_id)

    for item in filters['items']:
        if item['type'] == 'Number':
            if isinstance(item['value'], dict):
                item['value']['min'] = task_ids[int(item['value']['min'])]
                item['value']['max'] = task_ids[int(item['value']['max'])]
            else:
                item['value'] = task_ids[int(item['value'])]

    dict(
        data={'filters': filters},
    )
    response = business_client.patch(
        f'/api/dm/views/{view_id}',
        data=json.dumps(payload),
        content_type='application/json',
    )

    response = business_client.get(f'/api/tasks/?view={view_id}')
    response_data = response.json()

    assert 'tasks' in response_data, response_data

    response_ids = [task['id'] for task in response_data['tasks']]
    correct_ids = [task_ids[i] for i in ids]
    assert response_ids == correct_ids, (response_ids, correct_ids, filters)
</file>

<file path="label_studio/tests/data_manager/test_undefined.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import json

import pytest
from django.conf import settings
from projects.functions.utils import recalculate_created_annotations_and_labels_from_scratch
from projects.models import Project, ProjectSummary

from ..utils import make_task, project_id  # noqa


def get_filtered_task_ids(business_client, view_id):
    response = business_client.get(f'/api/tasks/?view={view_id}')
    response_data = response.json()
    assert 'tasks' in response_data, response_data
    return [task['id'] for task in response_data['tasks']]


def apply_filter_and_get_view_id(business_client, project_id, filters):
    payload = {
        'project': project_id,
        'data': {'filters': filters},
    }
    response = business_client.post(
        '/api/dm/views/',
        data=json.dumps(payload),
        content_type='application/json',
    )
    assert response.status_code == 201, response.content
    return response.json()['id']


@pytest.mark.django_db
def test_views_filters_with_undefined(business_client, project_id):
    """
    1. Import task 1: {"$undefined$": "photo1.jpg"}
    2. Filter by `data` with value `photo`
    3. It should return task 1

    4. Set labeling config <View> <Image value="$image" name="img"/> </View>
    5. Filter by `image` with value `photo`
    6. It should return task 1

    7. Add task 2: {"$undefined$": "photo2.jpg", "extra": "123"}
    8. Filter by "extra": "123"
    9. It should return task 2
    10. Filter by "image" with value `photo`
    11. It should return task 1 and task 2

    12. Update task 1 with {"extra": "456"}
    13. Check project.summary.common_data_columns, there should be ["$undefined$", "extra"]

    14. Filter by "image" with "photo" should return task 1 and task 2
    """
    project = Project.objects.get(pk=project_id)

    # Step 1: Import task 1: {"$undefined$": "photo1.jpg"}
    task_data_field_name = settings.DATA_UNDEFINED_NAME  # "$undefined$"
    task_1 = make_task({'data': {task_data_field_name: 'photo1.jpg'}}, project)
    task_id_1 = task_1.id

    # Step 2-3: Filter by `data` with value `photo`, should return task 1
    filters = {
        'conjunction': 'and',
        'items': [
            {
                # data default name when label config is not yet set
                # and a file is uploaded directly
                'filter': 'filter:tasks:data.data',
                'operator': 'contains',
                'type': 'String',
                'value': 'photo',
            }
        ],
    }
    view_id = apply_filter_and_get_view_id(business_client, project_id, filters)
    response_ids = get_filtered_task_ids(business_client, view_id)
    assert set(response_ids) == {task_id_1}, f'Expected {[task_id_1]}, got {response_ids}'

    # Step 4: Set labeling config <View> <Image value="$image" name="img"/> </View>
    project.label_config = '<View> <Image value="$image" name="img"/> </View>'
    project.save()

    # Step 5-6: Filter by `image` with value `photo`, should return task 1
    filters['items'][0]['filter'] = 'filter:tasks:data.image'
    view_id = apply_filter_and_get_view_id(business_client, project_id, filters)
    response_ids = get_filtered_task_ids(business_client, view_id)
    assert set(response_ids) == {task_id_1}, f'Expected {[task_id_1]}, got {response_ids}'

    # Step 7: Add task 2: {"$undefined$": "photo2.jpg", "extra": "123"}
    task_2 = make_task({'data': {task_data_field_name: 'photo2.jpg', 'extra': '123'}}, project)
    task_id_2 = task_2.id

    # Step 8-9: Filter by "extra": "123", should return task 2
    filters['items'][0]['filter'] = 'filter:tasks:data.extra'
    filters['items'][0]['value'] = '123'
    view_id = apply_filter_and_get_view_id(business_client, project_id, filters)
    response_ids = get_filtered_task_ids(business_client, view_id)
    assert set(response_ids) == {task_id_2}, f'Expected {[task_id_2]}, got {response_ids}'

    # Step 10-11: Filter by "image" with value `photo`, should return task 1 and task 2
    filters['items'][0]['filter'] = 'filter:tasks:data.image'
    filters['items'][0]['value'] = 'photo'
    view_id = apply_filter_and_get_view_id(business_client, project_id, filters)
    response_ids = get_filtered_task_ids(business_client, view_id)
    assert set(response_ids) == {task_id_1, task_id_2}, f'Expected {[task_id_1, task_id_2]}, got {response_ids}'

    # Step 12: Update task 1 with {"extra": "456"}
    task_1.data['extra'] = '456'
    task_1.save()

    # we need to fully reset cache, because summary.update_data_columns()
    # can't work incrementally
    recalculate_created_annotations_and_labels_from_scratch(project, project.summary, 1)

    # Step 13: Check project.summary.common_data_columns, there should be ["$undefined$", "extra"]
    project.refresh_from_db()
    summary = ProjectSummary.objects.get(project=project)
    assert set(summary.common_data_columns) == {
        task_data_field_name,
        'extra',
    }, f"Expected {[task_data_field_name, 'extra']}, got {summary.common_data_columns}"

    # Step 14: Filter by "image" with "photo" should return task 1 and task 2
    # The filter is already set to 'photo' for 'data.image' from previous steps
    view_id = apply_filter_and_get_view_id(business_client, project_id, filters)
    response_ids = get_filtered_task_ids(business_client, view_id)
    assert set(response_ids) == {task_id_1, task_id_2}, f'Expected {[task_id_1, task_id_2]}, got {response_ids}'
</file>

<file path="label_studio/tests/data_manager/test_views_api.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import json

import pytest
from rest_framework import status

from ..utils import project_id  # noqa

pytestmark = pytest.mark.django_db


def test_views_api(business_client, project_id):
    # create
    payload = dict(project=project_id, data={'test': 1})
    response = business_client.post(
        '/api/dm/views/',
        data=json.dumps(payload),
        content_type='application/json',
    )

    assert response.status_code == 201, response.content

    # list
    response = business_client.get(
        '/api/dm/views/',
    )

    assert response.status_code == 200, response.content
    assert response.json()[0]['project'] == project_id
    view_id = response.json()[0]['id']

    # partial update
    updated_payload = dict(data={'test': 2})
    response = business_client.patch(
        f'/api/dm/views/{view_id}/',
        data=json.dumps(updated_payload),
        content_type='application/json',
    )
    assert response.status_code == 200, response.content

    # retrieve
    response = business_client.get(
        f'/api/dm/views/{view_id}/',
    )

    assert response.status_code == 200, response.content
    assert response.json()['data'] == updated_payload['data']

    # reset
    response = business_client.delete(
        '/api/dm/views/reset',
        data=json.dumps(dict(project=project_id)),
        content_type='application/json',
    )

    assert response.status_code == 204, response.content
    response = business_client.get('/api/dm/views/')
    assert response.json() == []


def test_views_api_filter_project(business_client):
    # create project
    response = business_client.post(
        '/api/projects/',
        data=json.dumps(dict(title='test_project1')),
        content_type='application/json',
    )
    project1_id = response.json()['id']
    business_client.post(
        '/api/dm/views/',
        data=json.dumps(dict(project=project1_id)),
        content_type='application/json',
    )

    response = business_client.post(
        '/api/projects/',
        data=json.dumps(dict(title='test_project2')),
        content_type='application/json',
    )
    project2_id = response.json()['id']
    business_client.post(
        '/api/dm/views/',
        data=json.dumps(dict(project=project2_id)),
        content_type='application/json',
    )

    # list all
    response = business_client.get('/api/dm/views/')
    assert response.status_code == 200, response.content
    assert len(response.json()) == 2

    # filtered list
    response = business_client.get(f'/api/dm/views/?project={project1_id}')
    assert response.status_code == 200, response.content
    assert response.json()[0]['project'] == project1_id

    # filtered reset
    response = business_client.delete(
        '/api/dm/views/reset/',
        data=json.dumps(dict(project=project1_id)),
        content_type='application/json',
    )
    assert response.status_code == 204, response.content

    # filtered list
    response = business_client.get(f'/api/dm/views/?project={project2_id}')
    assert len(response.json()) == 1
    assert response.json()[0]['project'] == project2_id


def test_views_api_filters(business_client, project_id):
    # create
    payload = dict(
        project=project_id,
        data={
            'filters': {
                'conjunction': 'or',
                'items': [
                    {
                        'filter': 'filter:tasks:data.image',
                        'operator': 'contains',
                        'type': 'Image',
                        'value': {},
                    },
                    {
                        'filter': 'filter:tasks:data.image',
                        'operator': 'equal',
                        'type': 'Image',
                        'value': {},
                    },
                ],
            }
        },
    )

    response = business_client.post(
        '/api/dm/views/',
        data=json.dumps(payload),
        content_type='application/json',
    )

    assert response.status_code == 201, response.content
    view_id = response.json()['id']

    # retrieve
    response = business_client.get(
        f'/api/dm/views/{view_id}/',
    )

    assert response.status_code == 200, response.content
    assert response.json()['data'] == payload['data']

    updated_payload = dict(
        project=project_id,
        data={
            'filters': {
                'conjunction': 'and',
                'items': [
                    {
                        'filter': 'filter:tasks:data.text',
                        'operator': 'equal',
                        'type': 'Text',
                        'value': {},
                    },
                    {
                        'filter': 'filter:tasks:data.text',
                        'operator': 'contains',
                        'type': 'Text',
                        'value': {},
                    },
                ],
            }
        },
    )

    response = business_client.put(
        f'/api/dm/views/{view_id}/',
        data=json.dumps(updated_payload),
        content_type='application/json',
    )
    assert response.status_code == 200, response.content

    # check after update
    response = business_client.get(
        f'/api/dm/views/{view_id}/',
    )

    assert response.status_code == 200, response.content
    assert response.json()['data'] == updated_payload['data']


def test_views_ordered_by_id(business_client, project_id):
    views = [{'view_data': 1}, {'view_data': 2}, {'view_data': 3}]

    for view in views:
        payload = dict(project=project_id, data=view)

        business_client.post(
            '/api/dm/views/',
            data=json.dumps(payload),
            content_type='application/json',
        )

    response = business_client.get('/api/dm/views/')
    data = response.json()
    assert response.status_code == status.HTTP_200_OK

    ids = [view['id'] for view in data]
    assert ids == sorted(ids)


def test_update_views_order(business_client, project_id):
    # Create views
    views = [{'view_data': 1}, {'view_data': 2}, {'view_data': 3}]

    view_ids = []
    for view in views:
        payload = dict(project=project_id, data=view)
        response = business_client.post(
            '/api/dm/views/',
            data=json.dumps(payload),
            content_type='application/json',
        )
        assert response.status_code == status.HTTP_201_CREATED
        view_ids.append(response.json()['id'])

    # Update the order of views
    new_order = {'project': project_id, 'ids': [view_ids[2], view_ids[0], view_ids[1]]}
    response = business_client.post(
        '/api/dm/views/order/',
        data=json.dumps(new_order),
        content_type='application/json',
    )
    assert response.status_code == status.HTTP_200_OK

    # Verify the new order
    response = business_client.get('/api/dm/views/')
    data = response.json()
    assert response.status_code == status.HTTP_200_OK

    returned_ids = [view['id'] for view in data]
    assert returned_ids == new_order['ids']
</file>

<file path="label_studio/tests/io_storages/s3/test_utils.py">
from unittest.mock import patch

import pytest
from django.test import override_settings
from io_storages.s3.utils import S3StorageError, catch_and_reraise_from_none


@override_settings(S3_TRUSTED_STORAGE_DOMAINS=['trusted-domain.com'])
def test_catch_and_reraise_from_none_with_untrusted_domain():
    class TestClass:
        s3_endpoint = 'http://untrusted-domain.com'

    instance = TestClass()

    @catch_and_reraise_from_none
    def function_to_test(self):
        raise Exception('Original Exception')

    with patch('io_storages.s3.utils.extractor.extract_urllib') as mock_extract:
        mock_extract.return_value.registered_domain = 'untrusted-domain.com'
        with pytest.raises(S3StorageError) as excinfo:
            function_to_test(instance)
        assert 'Debugging info is not available for s3 endpoints on domain: untrusted-domain.com' in str(excinfo.value)


@override_settings(S3_TRUSTED_STORAGE_DOMAINS=['trusted-domain.com'])
def test_catch_and_reraise_from_none_with_trusted_domain():
    class TestClass:
        s3_endpoint = 'http://trusted-domain.com'

    instance = TestClass()

    @catch_and_reraise_from_none
    def function_to_test(self):
        raise Exception('Original Exception')

    with patch('io_storages.s3.utils.extractor.extract_urllib') as mock_extract:
        mock_extract.return_value.registered_domain = 'trusted-domain.com'
        with pytest.raises(Exception) as excinfo:
            function_to_test(instance)
        assert 'Original Exception' in str(excinfo.value)
</file>

<file path="label_studio/tests/jwt_auth/test_auth.py">
import logging

import pytest
from jwt_auth.models import LSAPIToken
from rest_framework import status
from rest_framework.authtoken.models import Token
from rest_framework.test import APIClient

from ..utils import mock_feature_flag
from .utils import create_user_with_token_settings


@mock_feature_flag(flag_name='fflag__feature_develop__prompts__dia_1829_jwt_token_auth', value=True)
@pytest.mark.django_db
def test_logging_when_legacy_token_auth_enabled(caplog):
    user = create_user_with_token_settings(api_tokens_enabled=False, legacy_api_tokens_enabled=True)
    token, _ = Token.objects.get_or_create(user=user)
    client = APIClient()
    client.credentials(HTTP_AUTHORIZATION=f'Token {token.key}')
    caplog.set_level(logging.INFO)

    client.get('/api/projects/')
    basic_auth_logs = [record for record in caplog.records if record.message == 'Legacy token authentication used']

    assert len(basic_auth_logs) == 1
    record = basic_auth_logs[0]
    assert record.user_id == user.id
    assert record.organization_id == user.active_organization.id
    assert record.endpoint == '/api/projects/'


@mock_feature_flag(flag_name='fflag__feature_develop__prompts__dia_1829_jwt_token_auth', value=True)
@pytest.mark.django_db
def test_no_logging_when_legacy_token_auth_disabled(caplog):
    user = create_user_with_token_settings(api_tokens_enabled=True, legacy_api_tokens_enabled=False)
    refresh = LSAPIToken.for_user(user)
    client = APIClient()
    client.credentials(HTTP_AUTHORIZATION=f'Bearer {refresh.access_token}')
    caplog.set_level(logging.INFO)

    client.get('/api/projects/')

    basic_auth_logs = [record for record in caplog.records if record.message == 'Basic token authentication used']
    assert len(basic_auth_logs) == 0


@mock_feature_flag(flag_name='fflag__feature_develop__prompts__dia_1829_jwt_token_auth', value=True)
@pytest.mark.django_db
def test_legacy_api_token_disabled_user_cannot_use_legacy_token():
    user = create_user_with_token_settings(api_tokens_enabled=True, legacy_api_tokens_enabled=False)
    token, _ = Token.objects.get_or_create(user=user)
    client = APIClient()
    client.credentials(HTTP_AUTHORIZATION=f'Token {token.key}')

    response = client.get('/api/projects/')

    assert response.status_code == status.HTTP_401_UNAUTHORIZED
</file>

<file path="label_studio/tests/jwt_auth/test_middleware.py">
import pytest
from jwt_auth.models import LSAPIToken
from organizations.functions import create_organization
from rest_framework import status
from rest_framework.authtoken.models import Token
from rest_framework.test import APIClient
from users.models import User

from ..utils import mock_feature_flag
from .utils import create_user_with_token_settings


@mock_feature_flag(flag_name='fflag__feature_develop__prompts__dia_1829_jwt_token_auth', value=True)
@pytest.mark.django_db
def test_request_without_auth_header_returns_401():
    client = APIClient()
    response = client.get('/api/projects/')
    assert response.status_code == status.HTTP_401_UNAUTHORIZED


@mock_feature_flag(flag_name='fflag__feature_develop__prompts__dia_1829_jwt_token_auth', value=True)
@pytest.mark.django_db
def test_request_with_invalid_token_returns_401():
    client = APIClient()
    client.credentials(HTTP_AUTHORIZATION='Bearer invalid.token.here')
    response = client.get('/api/projects/')
    assert response.status_code == status.HTTP_401_UNAUTHORIZED


@mock_feature_flag(flag_name='fflag__feature_develop__prompts__dia_1829_jwt_token_auth', value=True)
@pytest.mark.django_db
def test_request_with_valid_token_returns_authenticated_user():
    user = create_user_with_token_settings(api_tokens_enabled=True, legacy_api_tokens_enabled=False)
    refresh = LSAPIToken.for_user(user)
    client = APIClient()
    client.credentials(HTTP_AUTHORIZATION=f'Bearer {refresh.access_token}')

    response = client.get('/api/projects/')

    assert response.status_code == status.HTTP_200_OK
    assert response.wsgi_request.user == user


@mock_feature_flag(flag_name='fflag__feature_develop__prompts__dia_1829_jwt_token_auth', value=True)
@pytest.mark.django_db
def test_jwt_token_auth_disabled_user_cannot_use_jwt_token():
    user = create_user_with_token_settings(api_tokens_enabled=False, legacy_api_tokens_enabled=True)
    refresh = LSAPIToken.for_user(user)
    client = APIClient()
    client.credentials(HTTP_AUTHORIZATION=f'Bearer {refresh.access_token}')

    response = client.get('/api/projects/')
    assert response.status_code == status.HTTP_401_UNAUTHORIZED


@mock_feature_flag(flag_name='fflag__feature_develop__prompts__dia_1829_jwt_token_auth', value=True)
@pytest.mark.django_db
def test_user_with_both_auth_enabled_can_use_both_methods():
    user = create_user_with_token_settings(api_tokens_enabled=True, legacy_api_tokens_enabled=True)
    client = APIClient()

    # JWT token auth
    refresh = LSAPIToken.for_user(user)
    client.credentials(HTTP_AUTHORIZATION=f'Bearer {refresh.access_token}')

    response = client.get('/api/projects/')

    assert response.status_code == status.HTTP_200_OK
    assert response.wsgi_request.user == user

    # Legacy token auth
    token, _ = Token.objects.get_or_create(user=user)
    client.credentials(HTTP_AUTHORIZATION=f'Token {token.key}')

    response = client.get('/api/projects/')

    assert response.status_code == status.HTTP_200_OK
    assert response.wsgi_request.user == user


@mock_feature_flag(flag_name='fflag__feature_develop__prompts__dia_1829_jwt_token_auth', value=True)
@pytest.mark.django_db
def test_user_with_no_auth_enabled_cannot_use_either_method():
    user = create_user_with_token_settings(api_tokens_enabled=False, legacy_api_tokens_enabled=False)
    client = APIClient()

    # JWT token auth
    refresh = LSAPIToken.for_user(user)
    client.credentials(HTTP_AUTHORIZATION=f'Bearer {refresh.access_token}')

    response = client.get('/api/projects/')

    assert response.status_code == status.HTTP_401_UNAUTHORIZED

    # Legacy token auth
    token, _ = Token.objects.get_or_create(user=user)
    client.credentials(HTTP_AUTHORIZATION=f'Token {token.key}')

    response = client.get('/api/projects/')

    assert response.status_code == status.HTTP_401_UNAUTHORIZED


@mock_feature_flag(flag_name='fflag__feature_develop__prompts__dia_1829_jwt_token_auth', value=True)
@pytest.mark.django_db
def test_jwt_token_invalid_after_user_deleted():
    user = create_user_with_token_settings(api_tokens_enabled=True, legacy_api_tokens_enabled=False)
    refresh = LSAPIToken.for_user(user)
    client = APIClient()
    client.credentials(HTTP_AUTHORIZATION=f'Bearer {refresh.access_token}')
    # Verify token works before deleting user
    response = client.get('/api/projects/')
    assert response.status_code == status.HTTP_200_OK
    assert response.wsgi_request.user == user

    user.delete()

    response = client.get('/api/projects/')
    assert response.status_code == status.HTTP_401_UNAUTHORIZED


@mock_feature_flag(flag_name='fflag__feature_develop__prompts__dia_1829_jwt_token_auth', value=True)
@pytest.mark.django_db
def test_user_with_default_auth_settings_can_use_jwt_but_not_legacy_token():
    # Create user and org with default settings from create_organization
    user = User.objects.create(email='default_auth_settings@example.com')
    org = create_organization(title='Default Settings Org', created_by=user)
    user.active_organization = org
    user.save()

    # JWT token auth should work (enabled by default)
    refresh = LSAPIToken.for_user(user)
    client = APIClient()
    client.credentials(HTTP_AUTHORIZATION=f'Bearer {refresh.access_token}')

    response = client.get('/api/projects/')
    assert response.status_code == status.HTTP_200_OK
    assert response.wsgi_request.user == user

    # Legacy token auth should not work (disabled by default)
    token, _ = Token.objects.get_or_create(user=user)
    client.credentials(HTTP_AUTHORIZATION=f'Token {token.key}')

    response = client.get('/api/projects/')
    assert response.status_code == status.HTTP_401_UNAUTHORIZED
</file>

<file path="label_studio/tests/jwt_auth/test_models.py">
import pytest
from jwt_auth.models import LSAPIToken, LSTokenBackend
from organizations.models import OrganizationMember
from rest_framework_simplejwt.exceptions import TokenError
from rest_framework_simplejwt.settings import api_settings as simple_jwt_settings
from rest_framework_simplejwt.token_blacklist.models import BlacklistedToken, OutstandingToken

from ..utils import mock_feature_flag
from .utils import create_user_with_token_settings


@mock_feature_flag(flag_name='fflag__feature_develop__prompts__dia_1829_jwt_token_auth', value=True)
@pytest.mark.django_db
def test_jwt_settings_permissions():
    user = create_user_with_token_settings(api_tokens_enabled=True, legacy_api_tokens_enabled=False)
    org = user.active_organization
    OrganizationMember.objects.create(
        user=user,
        organization=org,
    )

    # Any member should be able to view
    assert org.jwt.has_view_permission(user)

    # Only owners and administrators can modify
    user.is_owner = True
    user.save()
    assert org.jwt.has_modify_permission(user)
    assert org.jwt.has_permission(user)

    user.is_owner = False
    user.save()
    assert not org.jwt.has_modify_permission(user)
    assert not org.jwt.has_permission(user)


@mock_feature_flag(flag_name='fflag__feature_develop__prompts__dia_1829_jwt_token_auth', value=True)
@pytest.fixture
def token_backend():
    return LSTokenBackend(
        algorithm=simple_jwt_settings.ALGORITHM,
        signing_key=simple_jwt_settings.SIGNING_KEY,
        verifying_key=simple_jwt_settings.VERIFYING_KEY,
        audience=simple_jwt_settings.AUDIENCE,
        issuer=simple_jwt_settings.ISSUER,
        jwk_url=simple_jwt_settings.JWK_URL,
        leeway=simple_jwt_settings.LEEWAY,
        json_encoder=simple_jwt_settings.JSON_ENCODER,
    )


@mock_feature_flag(flag_name='fflag__feature_develop__prompts__dia_1829_jwt_token_auth', value=True)
def test_encode_returns_only_header_and_payload(token_backend):
    payload = {
        'user_id': 123,
        'exp': 1735689600,  # 2025-01-01
        'iat': 1704153600,  # 2024-01-02
    }
    token = token_backend.encode(payload)

    parts = token.split('.')
    assert len(parts) == 2

    assert all(part.replace('-', '+').replace('_', '/') for part in parts)
    assert all(part.replace('-', '+').replace('_', '/') for part in parts)


@mock_feature_flag(flag_name='fflag__feature_develop__prompts__dia_1829_jwt_token_auth', value=True)
def test_encode_full_returns_complete_jwt(token_backend):
    payload = {
        'user_id': 123,
        'exp': 1735689600,  # 2025-01-01
        'iat': 1704153600,  # 2024-01-02
    }
    token = token_backend.encode_full(payload)

    parts = token.split('.')
    assert len(parts) == 3

    assert all(part.replace('-', '+').replace('_', '/') for part in parts)


@mock_feature_flag(flag_name='fflag__feature_develop__prompts__dia_1829_jwt_token_auth', value=True)
def test_encode_vs_encode_full_comparison(token_backend):
    payload = {
        'user_id': 123,
        'exp': 1735689600,  # 2025-01-01
        'iat': 1704153600,  # 2024-01-02
    }
    partial_token = token_backend.encode(payload)
    full_token = token_backend.encode_full(payload)

    assert full_token.startswith(partial_token)


@mock_feature_flag(flag_name='fflag__feature_develop__prompts__dia_1829_jwt_token_auth', value=True)
@pytest.mark.django_db
def test_token_lifecycle():
    """Test full token lifecycle including creation, access token generation, blacklisting, and validation"""
    user = create_user_with_token_settings(api_tokens_enabled=True, legacy_api_tokens_enabled=False)
    token = LSAPIToken.for_user(user)

    # Test that the token is valid
    assert token.check_blacklist() is None

    # Test that blacklisting the token works
    token.blacklist()
    assert BlacklistedToken.objects.filter(token__jti=token['jti']).exists()

    # Test that the blacklisted token is detected
    with pytest.raises(TokenError):
        token.check_blacklist()


@pytest.mark.django_db
def test_token_creation_and_storage():
    """Test that tokens are created and stored correctly with truncated format"""
    user = create_user_with_token_settings(api_tokens_enabled=True, legacy_api_tokens_enabled=False)
    token = LSAPIToken.for_user(user)
    assert token is not None

    # Token in database shouldn't contain the signature
    outstanding_token = OutstandingToken.objects.get(jti=token['jti'])
    stored_token_parts = outstanding_token.token.split('.')
    assert len(stored_token_parts) == 2  # Only header and payload

    # Full token should have all three JWT parts
    full_token = token.get_full_jwt()
    full_token_parts = full_token.split('.')
    assert len(full_token_parts) == 3  # Header, payload, and signature
</file>

<file path="label_studio/tests/jwt_auth/test_views.py">
import pytest
from jwt_auth.models import LSAPIToken
from rest_framework import status
from rest_framework.test import APIClient
from rest_framework_simplejwt.exceptions import TokenError
from tests.jwt_auth.utils import create_user_with_token_settings
from tests.utils import mock_feature_flag


@mock_feature_flag(flag_name='fflag__feature_develop__prompts__dia_1829_jwt_token_auth', value=True)
@pytest.mark.django_db
def test_blacklist_view_returns_404_with_already_blacklisted_token(client):
    user = create_user_with_token_settings(api_tokens_enabled=True, legacy_api_tokens_enabled=False)
    client.force_login(user)

    token = LSAPIToken()
    token.blacklist()
    response = client.post('/api/token/blacklist/', data={'refresh': token.get_full_jwt()})

    assert response.status_code == status.HTTP_404_NOT_FOUND


@mock_feature_flag(flag_name='fflag__feature_develop__prompts__dia_1829_jwt_token_auth', value=True)
@pytest.mark.django_db
def test_blacklist_view_returns_204_with_valid_token(client):
    user = create_user_with_token_settings(api_tokens_enabled=True, legacy_api_tokens_enabled=False)
    client.force_login(user)

    token = LSAPIToken()
    response = client.post('/api/token/blacklist/', data={'refresh': token.get_full_jwt()})

    assert response.status_code == status.HTTP_204_NO_CONTENT
    with pytest.raises(TokenError):
        token.check_blacklist()


@mock_feature_flag(flag_name='fflag__feature_develop__prompts__dia_1829_jwt_token_auth', value=True)
@pytest.mark.django_db
def test_create_token_when_no_existing_token():
    user = create_user_with_token_settings(api_tokens_enabled=True, legacy_api_tokens_enabled=False)
    client = APIClient()
    refresh = LSAPIToken()
    client.credentials(HTTP_AUTHORIZATION=f'Bearer {refresh.access_token}')
    client.force_authenticate(user)

    response = client.post('/api/token/')

    assert response.status_code == status.HTTP_201_CREATED
    assert 'token' in response.data


@mock_feature_flag(flag_name='fflag__feature_develop__prompts__dia_1829_jwt_token_auth', value=True)
@pytest.mark.django_db
def test_create_token_when_existing_valid_token():
    user = create_user_with_token_settings(api_tokens_enabled=True, legacy_api_tokens_enabled=False)
    client = APIClient()
    refresh = LSAPIToken()
    client.credentials(HTTP_AUTHORIZATION=f'Bearer {refresh.access_token}')
    client.force_authenticate(user)

    # 1. Create first token
    response = client.post('/api/token/')
    assert response.status_code == status.HTTP_201_CREATED

    # 2. Try to create second token
    response = client.post('/api/token/')
    assert response.status_code == status.HTTP_409_CONFLICT
    assert 'detail' in response.data
    assert 'You already have a valid token' in response.data['detail']


@mock_feature_flag(flag_name='fflag__feature_develop__prompts__dia_1829_jwt_token_auth', value=True)
@pytest.mark.django_db
def test_create_token_after_blacklisting_previous():
    user = create_user_with_token_settings(api_tokens_enabled=True, legacy_api_tokens_enabled=False)
    client = APIClient()
    refresh = LSAPIToken()
    client.credentials(HTTP_AUTHORIZATION=f'Bearer {refresh.access_token}')
    client.force_authenticate(user)

    # 1. Create first token
    response = client.post('/api/token/')
    assert response.status_code == status.HTTP_201_CREATED

    # 2. Blacklist the token
    token = response.data['token']
    response = client.post('/api/token/blacklist/', data={'refresh': token})
    assert response.status_code == status.HTTP_204_NO_CONTENT

    # 3. Create new token
    response = client.post('/api/token/')
    assert response.status_code == status.HTTP_201_CREATED
    assert 'token' in response.data
</file>

<file path="label_studio/tests/jwt_auth/utils.py">
from organizations.functions import create_organization
from users.models import User


def create_user_with_token_settings(api_tokens_enabled: bool, legacy_api_tokens_enabled: bool) -> User:
    """Create a user with specified token auth settings in their organization."""
    user = User.objects.create(email=f'test_user_{api_tokens_enabled}_{legacy_api_tokens_enabled}@example.com')
    org = create_organization(title=f'Test Org {api_tokens_enabled} {legacy_api_tokens_enabled}', created_by=user)
    org.jwt.api_tokens_enabled = api_tokens_enabled
    org.jwt.legacy_api_tokens_enabled = legacy_api_tokens_enabled
    org.jwt.save()
    user.active_organization = org
    user.save()
    return user
</file>

<file path="label_studio/tests/loadtests/locustfile_collabs.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import json
import random
import string

from locust import HttpUser, TaskSet, between, task


def randomString(stringLength):
    """Generate a random string of fixed length"""
    letters = string.ascii_lowercase
    return ''.join(random.choice(letters) for i in range(stringLength))


all_labels = [
    'Person',
    'Organization',
    'Fact',
    'Money',
    'Date',
    'Time',
    'Ordinal',
    'Percent',
    'Product',
    'Language',
    'Location',
]


def get_result(text):
    start = random.randint(0, len(text))
    end = min(len(text), start + random.randint(3, 30))
    results = []
    for i in range(random.randint(1, 10)):
        results.append(
            {
                'type': 'labels',
                'from_name': 'ner',
                'to_name': 'text',
                'value': {'labels': [random.choice(all_labels)], 'start': start, 'end': end},
            }
        )
    return results


class UserWorksWithProject(TaskSet):
    def on_start(self):
        r = self.client.get('/api/annotator/projects')
        all_projects = r.json()
        self.project_id = random.choice(all_projects)

    @task(100)
    def complete_task_via_api(self):
        r = self.client.get('/api/projects/%i/next/' % self.project_id, name='/api/projects/<id>/next')
        task = r.json()
        task_id = task['id']
        task_text = task['data']['text']
        results = get_result(task_text)
        payload = json.dumps({'result': results})
        headers = {'content-type': 'application/json'}
        self.client.post(
            '/api/tasks/%i/annotations' % task_id, payload, headers=headers, name='/api/tasks/<id>/annotations'
        )

    @task(1)
    def stop(self):
        self.interrupt()


class WebsiteUser(HttpUser):
    wait_time = between(3, 9)

    tasks = {UserWorksWithProject: 10}

    def on_start(self):
        self.login()

    def login(self):
        response = self.client.get('/')
        csrftoken = response.cookies['csrftoken']
        num_collabs = 100
        payload = {'email': f'collab_{random.randint(0, num_collabs)}@loadtests.me', 'password': '123456789'}
        self.client.post('/annotator/login', payload, headers={'X-CSRFToken': csrftoken})
</file>

<file path="label_studio/tests/loadtests/locustfile_db_load.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import json
import random
import string
from uuid import uuid4

from locust import HttpUser, TaskSet, between, task


def randomString(stringLength):
    """Generate a random string of fixed length"""
    letters = string.ascii_lowercase
    return ''.join(random.choice(letters) for i in range(stringLength))


class UserWorksWithProject(TaskSet):
    def on_start(self):
        # user creates the new project
        title = str(uuid4())
        payload = json.dumps(
            {
                'title': title,
                'is_published': True,
                'skip_onboarding': True,
                'label_config': '<View><Text name="my_text" value="$text"/><Choices name="my_class" toName="my_text"><Choice value="pos"/><Choice value="neg"/></Choices></View>',
            }
        )
        with self.client.post(
            '/api/projects',
            data=payload,
            headers={'content-type': 'application/json', 'Authorization': f'Token {self.client.token}'},
            catch_response=True,
        ) as r:
            if r.status_code != 201:
                r.failure(r.status_code)
            else:
                self.project_id = r.json()['id']
                print(f'Project {self.project_id} has been created by user {self.client.name}')

        # let this user import tasks with annotations
        tasks = []
        for i in range(10000):
            one_task = {
                'data': {'text': randomString(random.randint(5, 200))},
                'annotations': [
                    {
                        'ground_truth': False,
                        'result': [
                            {
                                'type': 'choices',
                                'from_name': 'my_class',
                                'to_name': 'my_text',
                                'value': {'choices': [random.choice(['pos', 'neg'])]},
                            }
                        ],
                    }
                ],
                'predictions': [
                    {
                        'result': [
                            {
                                'type': 'choices',
                                'from_name': 'my_class',
                                'to_name': 'my_text',
                                'value': {'choices': [random.choice(['pos', 'neg'])]},
                            }
                        ],
                        'score': random.uniform(0, 1),
                    }
                ],
            }
            tasks.append(one_task)

        self.import_tasks(tasks, name='Initial tasks upload')

    @task(5)
    def project_list(self):
        self.client.get('/projects/')

    @task(5)
    def project_dashboard(self):
        self.client.get('/projects/%i' % self.project_id, name='/projects/<id>')

    @task(5)
    def project_data(self):
        self.client.get('/projects/%i/data' % self.project_id, name='/projects/<id>/data')

    @task(20)
    def label_stream(self):
        self.client.get('/projects/%i/label-stream' % self.project_id, name='/projects/<id>/label-stream')

    @task(5)
    def expert_page(self):
        self.client.get('/projects/%i/experts' % self.project_id, name='/projects/<id>/experts')

    @task(5)
    def ml_page(self):
        self.client.get('/projects/%i/ml' % self.project_id, name='/projects/<id>/ml')

    @task(5)
    def stats(self):
        self.client.get('/business/stats')

    @task(5)
    def project_stats(self):
        self.client.get('/projects/%i/plots' % self.project_id, name='/projects/<id>/plots')

    @task(5)
    def experts(self):
        self.client.get('/business/experts')

    @task(5)
    def import_tasks(self, tasks=None, name=None):
        if tasks is None:
            payload = json.dumps([{'text': 'example positive review'}, {'text': 'example negative review'}])
        else:
            payload = json.dumps(tasks)
        headers = {'content-type': 'application/json', 'Authorization': f'Token {self.client.token}'}
        self.client.post(
            '/api/projects/%i/tasks/bulk' % self.project_id,
            payload,
            headers=headers,
            name=name or '/api/projects/<id>/tasks/bulk',
        )

    @task(20)
    def complete_task_via_api(self):
        r = self.client.get(
            '/api/projects/%i/tasks' % self.project_id,
            headers={'Authorization': f'Token {self.client.token}'},
            name='/api/projects/<id>/tasks',
        )
        tasks_list = r.json()
        if len(tasks_list):
            any_task = random.choice(tasks_list)
            payload = json.dumps(
                {
                    'result': [
                        {
                            'type': 'choices',
                            'from_name': 'my_class',
                            'to_name': 'my_text',
                            'value': {'choices': [random.choice(['pos', 'neg'])]},
                        }
                    ]
                }
            )
            headers = {'content-type': 'application/json', 'Authorization': f'Token {self.client.token}'}
            self.client.post(
                '/api/tasks/%i/annotations' % any_task['id'],
                payload,
                headers=headers,
                name='/api/tasks/<id>/annotations',
            )

    @task(1)
    def stop(self):
        self.interrupt()


class WebsiteUser(HttpUser):
    wait_time = between(3, 9)

    tasks = {UserWorksWithProject: 10}

    def on_start(self):
        self.signup()

    def signup(self):
        response = self.client.get('/')
        csrftoken = response.cookies['csrftoken']
        username = str(uuid4())
        payload = {'email': f'{username}@loadtest.me', 'password': '12345678', 'title': username.upper()}
        self.client.post('/user/signup', payload, headers={'X-CSRFToken': csrftoken})
        response = self.client.get('/api/current-user/token').json()
        self.client.token = response['detail']
        self.client.name = username
        print(f'Client {username} successfully signed up. Token: {self.client.token}')
</file>

<file path="label_studio/tests/loadtests/locustfile.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import json
import random
from uuid import uuid4

from locust import HttpUser, TaskSet, between, task


class UserWorksWithProject(TaskSet):
    def on_start(self):
        # user creates the new project
        title = str(uuid4())
        payload = json.dumps(
            {
                'title': title,
                'is_published': True,
                'skip_onboarding': True,
                'label_config': '<View><Text name="my_text" value="$text"/><Choices name="my_class" toName="my_text"><Choice value="pos"/><Choice value="neg"/></Choices></View>',
            }
        )
        with self.client.post(
            '/api/projects',
            data=payload,
            headers={'content-type': 'application/json', 'Authorization': f'Token {self.client.token}'},
            catch_response=True,
        ) as r:
            if r.status_code != 201:
                r.failure(r.status_code)
            else:
                self.project_id = r.json()['id']
                print(f'Project {self.project_id} has been created by user {self.client.name}')

    @task(5)
    def project_list(self):
        self.client.get('/projects/')

    @task(5)
    def project_dashboard(self):
        self.client.get('/projects/%i' % self.project_id, name='/projects/<id>')

    @task(5)
    def project_data(self):
        self.client.get('/projects/%i/data' % self.project_id, name='/projects/<id>/data')

    @task(20)
    def label_stream(self):
        self.client.get('/projects/%i/label-stream' % self.project_id, name='/projects/<id>/label-stream')

    @task(5)
    def expert_page(self):
        self.client.get('/projects/%i/experts' % self.project_id, name='/projects/<id>/experts')

    @task(5)
    def expert_page(self):  # noqa: F811
        self.client.get('/projects/%i/experts' % self.project_id, name='/projects/<id>/experts')

    @task(5)
    def stats(self):
        self.client.get('/business/stats')

    @task(5)
    def project_stats(self):
        self.client.get('/projects/%i/plots' % self.project_id, name='/projects/<id>/plots')

    @task(5)
    def experts(self):
        self.client.get('/business/experts')

    @task(5)
    def import_tasks(self):
        payload = json.dumps([{'text': 'example positive review'}, {'text': 'example negative review'}])
        headers = {'content-type': 'application/json', 'Authorization': f'Token {self.client.token}'}
        self.client.post(
            '/api/projects/%i/tasks/bulk' % self.project_id,
            payload,
            headers=headers,
            name='/api/projects/<id>/tasks/bulk',
        )

    @task(20)
    def complete_task_via_api(self):
        r = self.client.get(
            '/api/projects/%i/tasks' % self.project_id,
            headers={'Authorization': f'Token {self.client.token}'},
            name='/api/projects/<id>/tasks',
        )
        tasks_list = r.json()
        if len(tasks_list):
            any_task = random.choice(tasks_list)
            payload = json.dumps(
                {
                    'result': [
                        {
                            'type': 'choices',
                            'from_name': 'my_class',
                            'to_name': 'my_text',
                            'value': {'choices': [random.choice(['pos', 'neg'])]},
                        }
                    ]
                }
            )
            headers = {'content-type': 'application/json', 'Authorization': f'Token {self.client.token}'}
            self.client.post(
                '/api/tasks/%i/annotations' % any_task['id'],
                payload,
                headers=headers,
                name='/api/tasks/<id>/annotations',
            )

    @task(1)
    def stop(self):
        self.interrupt()


class WebsiteUser(HttpUser):
    wait_time = between(3, 9)
    tasks = {UserWorksWithProject: 10}

    def on_start(self):
        self.signup()

    def signup(self):
        response = self.client.get('/')
        csrftoken = response.cookies['csrftoken']
        username = str(uuid4())
        payload = {'email': f'{username}@loadtest.me', 'password': '12345678', 'title': username.upper()}
        self.client.post('/user/signup', payload, headers={'X-CSRFToken': csrftoken})
        response = self.client.get('/api/current-user/token').json()
        self.client.token = response['detail']
        self.client.name = username
        print(f'Client {username} successfully signed up. Token: {self.client.token}')
</file>

<file path="label_studio/tests/loadtests/one_imports_other_annotate.py">
import os
import random
import string
from uuid import uuid4

import numpy as np
import pandas as pd
from locust import HttpUser, between, constant, events, tag, task


def get_project_id(client):
    with client.get('/api/projects', catch_response=True) as r:
        if r.status_code != 200:
            print(r.status_code)
            r.failure(r.status_code)
        else:
            project_list = r.json()
            print(project_list)
            ids = [p['id'] for p in project_list]
            if not ids:
                return
            return random.choice(ids)


def signup(client):
    username = str(uuid4())[:8]
    response = client.get('/')
    csrftoken = response.cookies['csrftoken']
    r = client.post(
        '/user/signup',
        {'email': f'{username}@heartex.com', 'password': 'password'},
        headers={'X-CSRFToken': csrftoken},
        catch_response=True,
    )
    print(f'User {username} signup response: {r.status_code}')
    return username


class Admin(HttpUser):
    weight = 1
    wait_time = constant(10)

    def on_start(self):
        username = signup(self.client)
        with self.client.post(
            '/api/projects',
            json={
                'title': f"{username}'s project",
                'label_config': '<View><Text name="text" value="$text"/><Choices name="label" toName="text"><Choice value="1"/><Choice value="2"/></Choices></View>',
            },
            catch_response=True,
        ) as r:
            if r.status_code != 201:
                r.failure(r.status_code)
            rdata = r.json()
            print('Get response: ', rdata)
            self.project_id = rdata['id']
            print(f'Project {self.project_id} has been created by user {self.client}')
            self.import_data()

    @task
    def view_data_manager(self):
        self.client.get(f'/projects/{self.project_id}/data', name='projects/<pk>/data')

    # @tag('import')
    # @task
    def import_data(self):
        self.client.post(
            '/api/projects/%i/import' % self.project_id,
            name='/api/projects/<pk>/import',
            files={'csv': open('data.csv', 'rb')},
            # headers={'content-type': 'multipart/form-data'})
        )


class Annotator(HttpUser):
    weight = int(os.environ.get('LOCUST_USERS')) - 1
    wait_time = between(1, 3)

    def on_start(self):
        signup(self.client)

    @tag('select project')
    @task(1)
    def select_project(self):
        self.project_id = get_project_id(self.client)

    @tag('labeling')
    @task(10)
    def do_labeling(self):
        if not hasattr(self, 'project_id') or not self.project_id:
            print('No projects yet...')
            return
        with self.client.get(
            f'/api/projects/{self.project_id}/next', name='/api/projects/<pk>/next', catch_response=True
        ) as r:
            task = r.json()
            self.client.post(
                f'/api/tasks/{task["id"]}/annotations',
                name='/api/tasks/<pk>/annotations',
                json={
                    'result': [
                        {
                            'from_name': 'label',
                            'to_name': 'text',
                            'type': 'choices',
                            'value': {'choices': [random.choice(['1', '2'])]},
                        }
                    ]
                },
            )


def randomString(stringLength):
    """Generate a random string of fixed length"""
    letters = string.ascii_lowercase
    return ''.join(random.choice(letters) for i in range(stringLength))


@events.test_start.add_listener
def on_test_start(environment, **kwargs):
    rows = int(os.environ.get('IMPORTED_TASKS', 50000))
    print(f'Generating file with {rows} rows...')
    numbers = np.random.randint(low=0, high=100, size=(rows, 10)).tolist()
    columns = ['text'] + [f'col_{i}' for i in range(9)]
    pd.DataFrame(numbers, columns=columns).to_csv('data.csv')
</file>

<file path="label_studio/tests/ml/test_api.py">
import json

import pytest
from projects.models import Task
from rest_framework import status

from label_studio.tests.utils import make_project, register_ml_backend_mock

ORIG_MODEL_NAME = 'basic_ml_backend'
PROJECT_CONFIG = """<View><Image name="image" value="$image_url"/><Choices name="label"
          toName="image"><Choice value="pos"/><Choice value="neg"/></Choices></View>"""


@pytest.fixture
def ml_backend_for_test_api(ml_backend):
    register_ml_backend_mock(
        ml_backend,
        url='https://ml_backend_for_test_api',
        setup_model_version='1.0.0',
    )
    yield ml_backend


@pytest.fixture
def mock_gethostbyname(mocker):
    mocker.patch('socket.gethostbyname', return_value='321.21.21.21')


@pytest.mark.django_db
def test_ml_backend_set_for_prelabeling(business_client, ml_backend_for_test_api, mock_gethostbyname):
    project = make_project(
        config=dict(
            is_published=True,
            label_config=PROJECT_CONFIG,
            title='test_ml_backend_creation',
        ),
        user=business_client.user,
    )

    assert project.model_version == ''

    # create ML backend
    response = business_client.post(
        '/api/ml/',
        data={
            'project': project.id,
            'title': 'ml_backend_title',
            'url': 'https://ml_backend_for_test_api',
        },
    )
    assert response.status_code == 201

    project.refresh_from_db()
    assert project.model_version == 'ml_backend_title'


@pytest.mark.django_db
def test_ml_backend_not_set_for_prelabeling(business_client, ml_backend_for_test_api, mock_gethostbyname):
    """We are not setting it when its already set for another name,
    for example when predictions were uploaded before"""

    project = make_project(
        config=dict(
            is_published=True,
            label_config=PROJECT_CONFIG,
            title='test_ml_backend_creation',
        ),
        user=business_client.user,
    )

    project.model_version = ORIG_MODEL_NAME
    project.save()

    # create ML backend
    response = business_client.post(
        '/api/ml/',
        data={
            'project': project.id,
            'title': 'ml_backend_title',
            'url': 'https://ml_backend_for_test_api',
        },
    )
    assert response.status_code == 201

    project.refresh_from_db()
    assert project.model_version == ORIG_MODEL_NAME


@pytest.mark.django_db
def test_model_version_on_save(business_client, ml_backend_for_test_api, mock_gethostbyname):
    project = make_project(
        config=dict(
            is_published=True,
            label_config=PROJECT_CONFIG,
            title='test_ml_backend_creation',
        ),
        user=business_client.user,
    )

    assert project.model_version == ''

    # create ML backend
    response = business_client.post(
        '/api/ml/',
        data={
            'project': project.id,
            'title': 'test_ml_backend_creation_ML_backend',
            'url': 'https://ml_backend_for_test_api',
        },
    )
    assert response.status_code == 201
    r = response.json()
    ml_backend_id = r['id']
    response = business_client.get(f'/api/ml/{ml_backend_id}')
    assert response.status_code == 200
    assert response.json()['state'] == 'CO'

    # select model version in project
    assert (
        business_client.patch(
            f'/api/projects/{project.id}',
            data=json.dumps({'model_version': 'test_ml_backend_creation_ML_backend'}),
            content_type='application/json',
        ).status_code
        == 200
    )

    # change ML backend title --> model version should be updated
    assert (
        business_client.patch(
            f'/api/ml/{ml_backend_id}',
            data=json.dumps(
                {
                    'project': project.id,
                    'title': 'new_title',
                    'url': 'https://ml_backend_for_test_api',
                }
            ),
            content_type='application/json',
        ).status_code
        == 200
    )
    project.refresh_from_db()
    assert project.model_version == 'new_title'


@pytest.mark.django_db
def test_model_version_on_delete(business_client, ml_backend_for_test_api, mock_gethostbyname):
    project = make_project(
        config=dict(
            is_published=True,
            label_config=PROJECT_CONFIG,
            title='test_ml_backend_creation',
        ),
        user=business_client.user,
    )

    assert project.model_version == ''

    # create ML backend
    response = business_client.post(
        '/api/ml/',
        data={
            'project': project.id,
            'title': 'test_ml_backend_creation_ML_backend',
            'url': 'https://ml_backend_for_test_api',
        },
    )
    assert response.status_code == 201
    r = response.json()
    ml_backend_id = r['id']
    response = business_client.get(f'/api/ml/{ml_backend_id}')
    assert response.status_code == 200
    assert response.json()['state'] == 'CO'

    # select model version in project
    assert (
        business_client.patch(
            f'/api/projects/{project.id}',
            data=json.dumps({'model_version': 'test_ml_backend_creation_ML_backend'}),
            content_type='application/json',
        ).status_code
        == 200
    )

    project.refresh_from_db()
    assert project.model_version == 'test_ml_backend_creation_ML_backend'

    # delete ML backend --> project's model version should be reset
    assert business_client.delete(f'/api/ml/{ml_backend_id}').status_code == 204
    project.refresh_from_db()
    assert project.model_version == ''


@pytest.mark.django_db
def test_security_write_only_payload(business_client, ml_backend_for_test_api, mock_gethostbyname):
    project = make_project(
        config=dict(
            is_published=True,
            label_config=PROJECT_CONFIG,
            title='test_ml_backend_creation',
        ),
        user=business_client.user,
    )

    # create ML backend - fails without password
    response = business_client.post(
        '/api/ml/',
        data={
            'project': project.id,
            'title': 'test_ml_backend_creation_ML_backend',
            'url': 'https://ml_backend_for_test_api',
            'auth_method': 'BASIC_AUTH',
            # 'basic_auth_user': 'user',
            # 'basic_auth_pass': '<SECRET>',
        },
    )
    assert response.status_code == 400
    r = response.json()
    assert (
        r['validation_errors']['non_field_errors'][0]
        == 'Authentication username and password is required for Basic Authentication.'
    )

    # create ML backend with username and password
    response = business_client.post(
        '/api/ml/',
        data={
            'project': project.id,
            'title': 'test_ml_backend_creation_ML_backend',
            'url': 'https://ml_backend_for_test_api',
            'auth_method': 'BASIC_AUTH',
            'basic_auth_user': 'user',
            'basic_auth_pass': '<SECRET>',
        },
    )
    assert response.status_code == 201
    r = response.json()
    # security check that password is not returned in POST response
    assert 'basic_auth_pass' not in r
    ml_backend_id = r['id']
    response = business_client.get(f'/api/ml/{ml_backend_id}')
    assert response.status_code == 200
    # check that password is not returned in GET response
    assert 'basic_auth_pass' not in response.json()

    # patch ML backend without password - must pass since it uses write_only field for previous password
    response = business_client.patch(
        f'/api/ml/{ml_backend_id}',
        data=json.dumps(
            {
                'project': project.id,
                'title': 'new_title_1',
                'url': 'https://ml_backend_for_test_api',
            }
        ),
        content_type='application/json',
    )
    assert response.status_code == 200
    # check that password is not returned in PATCH response
    assert 'basic_auth_pass' not in response.json()

    # patch ML backend with password
    response = business_client.patch(
        f'/api/ml/{ml_backend_id}',
        data=json.dumps(
            {
                'project': project.id,
                'title': 'new_title',
                'url': 'https://ml_backend_for_test_api',
                'basic_auth_pass': '<ANOTHER_SECRET>',
            }
        ),
        content_type='application/json',
    )
    # check that password is not returned in PATCH response
    assert 'basic_auth_pass' not in response.json()

    from ml.models import MLBackend

    ml_backend = MLBackend.objects.get(id=ml_backend_id)
    assert ml_backend.basic_auth_pass == '<ANOTHER_SECRET>'


@pytest.mark.django_db
def test_ml_backend_predict_test_api_post_random_true(business_client):
    project = make_project(
        config=dict(
            is_published=True,
            label_config=PROJECT_CONFIG,
            title='test_ml_backend_creation',
        ),
        user=business_client.user,
        use_ml_backend=True,
    )
    Task.objects.create(project=project, data={'image': 'http://example.com/image.jpg'})

    # get ML backend id from project
    project.refresh_from_db()
    ml_backend = project.get_ml_backends().first()

    response = business_client.post(f'/api/ml/{ml_backend.id}/predict/test?random=true')

    assert response.status_code == status.HTTP_200_OK
    r = response.json()
    assert r['url'] == 'http://localhost:8999/predict'
    assert r['status'] == 200
</file>

<file path="label_studio/tests/ml/test_predict.py">
import json

import pytest

from label_studio.tests.utils import make_project, make_task


@pytest.mark.django_db
def test_get_single_prediction_on_task(business_client, ml_backend_for_test_predict):
    project = make_project(
        config=dict(
            is_published=True,
            label_config="""
                <View>
                  <Text name="text" value="$text"></Text>
                  <Choices name="label" choice="single">
                    <Choice value="label_A"></Choice>
                    <Choice value="label_B"></Choice>
                  </Choices>
                </View>""",
            title='test_get_single_prediction_on_task',
        ),
        user=business_client.user,
        use_ml_backend=False,
    )

    make_task({'data': {'text': 'test 1'}}, project)

    # setup ML backend with single prediction per task
    response = business_client.post(
        '/api/ml/',
        data={
            'project': project.id,
            'title': 'ModelSingle',
            'url': 'http://test.ml.backend.for.sdk.com:9092',
        },
    )
    assert response.status_code == 201

    # get next task
    response = business_client.get(f'/api/projects/{project.id}/next')
    payload = json.loads(response.content)

    # ensure task has a single prediction with the correct value
    assert len(payload['predictions']) == 1
    assert payload['predictions'][0]['result'][0]['value']['choices'][0] == 'Single'
    assert payload['predictions'][0]['model_version'] == 'ModelSingle'


@pytest.mark.django_db
def test_get_multiple_predictions_on_task(business_client, ml_backend_for_test_predict):
    project = make_project(
        config=dict(
            is_published=True,
            label_config="""
                <View>
                  <Text name="text" value="$text"></Text>
                  <Choices name="label" choice="single">
                    <Choice value="label_A"></Choice>
                    <Choice value="label_B"></Choice>
                  </Choices>
                </View>""",
            title='test_get_multiple_predictions_on_task',
        ),
        user=business_client.user,
        use_ml_backend=False,
    )

    make_task({'data': {'text': 'test 1'}}, project)

    # setup ML backend with multiple predictions per task
    response = business_client.post(
        '/api/ml/',
        data={
            'project': project.id,
            'title': 'ModelA',
            'url': 'http://test.ml.backend.for.sdk.com:9093',
        },
    )
    assert response.status_code == 201

    # get next task
    response = business_client.get(f'/api/projects/{project.id}/next')
    payload = json.loads(response.content)

    # ensure task has multiple predictions with the correct values
    assert len(payload['predictions']) == 2
    assert payload['predictions'][0]['result'][0]['value']['choices'][0] == 'label_A'
    assert payload['predictions'][0]['model_version'] == 'ModelA'
    assert payload['predictions'][1]['result'][0]['value']['choices'][0] == 'label_B'
    assert payload['predictions'][1]['model_version'] == 'ModelB'
</file>

<file path="label_studio/tests/sdk/legacy/test_annotations.py">
import pytest

from label_studio.tests.sdk.common import LABEL_CONFIG_AND_TASKS

pytestmark = pytest.mark.django_db
from label_studio_sdk import Client


def test_annotation_create_and_update(django_live_url, business_client):
    ls = Client(url=django_live_url, api_key=business_client.api_key)
    p = ls.start_project(title='New Project', label_config=LABEL_CONFIG_AND_TASKS['label_config'])

    task_data = [{'data': {'my_text': 'Test task'}}]
    p.import_tasks(task_data)

    tasks = p.get_tasks()
    assert tasks[0]['data'] == task_data[0]['data']

    task_id = tasks[0]['id']
    annotation_data = {
        'result': [{'from_name': 'label', 'to_name': 'my_text', 'type': 'choices', 'value': {'choices': ['Positive']}}]
    }
    new_annotation = p.create_annotation(task_id, **annotation_data)
    assert (annotation_id := new_annotation['id'])
    assert new_annotation['result'] == annotation_data['result']

    p.update_annotation(
        annotation_id,
        result=[{'from_name': 'label', 'to_name': 'my_text', 'type': 'choices', 'value': {'choices': ['Negative']}}],
    )

    updated_annotation = p.get_tasks()[0]['annotations'][0]
    assert updated_annotation['result'][0]['value'] == {'choices': ['Negative']}


def test_annotation_marks_task_as_labeled(django_live_url, business_client):
    ls = Client(url=django_live_url, api_key=business_client.api_key)
    p = ls.start_project(
        title='New Project',
        label_config=LABEL_CONFIG_AND_TASKS['label_config'],
    )

    task_data = [{'data': {'my_text': 'Test task'}}, {'data': {'my_text': 'Test task 2'}}]
    p.import_tasks(task_data)
    tasks = p.get_tasks()

    assert p.get_labeled_tasks() == []

    task_id = tasks[0]['id']
    annotation_data = {
        'result': [{'from_name': 'label', 'to_name': 'my_text', 'type': 'choices', 'value': {'choices': ['Positive']}}]
    }
    p.create_annotation(task_id, **annotation_data)

    assert len(labeled_tasks := p.get_labeled_tasks()) == 1
    assert labeled_tasks[0]['data'] == task_data[0]['data']
</file>

<file path="label_studio/tests/sdk/legacy/test_projects.py">
import pytest

from label_studio.tests.sdk.common import LABEL_CONFIG_AND_TASKS

pytestmark = pytest.mark.django_db
from label_studio_sdk import Client


def test_start_and_get_project(django_live_url, business_client):
    ls = Client(url=django_live_url, api_key=business_client.api_key)
    p = ls.start_project(title='New Project', label_config=LABEL_CONFIG_AND_TASKS['label_config'])

    project = ls.get_project(p.id)
    assert project
    assert project.title == 'New Project'


def test_delete_project(django_live_url, business_client):
    ls = Client(url=django_live_url, api_key=business_client.api_key)
    p = ls.start_project(title='New Project', label_config=LABEL_CONFIG_AND_TASKS['label_config'])

    project = ls.get_project(p.id)
    ls.delete_project(project.id)

    assert ls.list_projects() == []
</file>

<file path="label_studio/tests/sdk/legacy/test_storages.py">
import pytest

from label_studio.tests.sdk.common import LABEL_CONFIG_AND_TASKS

pytestmark = pytest.mark.django_db
from label_studio_sdk import Client


def test_connect_and_sync_s3(django_live_url, business_client):
    ls = Client(url=django_live_url, api_key=business_client.api_key)
    p = ls.start_project(title='New Project', label_config=LABEL_CONFIG_AND_TASKS['label_config'])

    project = ls.get_project(p.id)
    storage_resp = project.connect_s3_import_storage('pytest-s3-images')

    storage_id = storage_resp['id']
    ls.sync_storage('s3', storage_id)

    assert set(t['storage_filename'] for t in p.get_tasks()) == {
        'subdir/another/image2.jpg',
        'subdir/image1.jpg',
        'subdir/image2.jpg',
        'image1.jpg',
    }
</file>

<file path="label_studio/tests/sdk/legacy/test_tasks.py">
import logging

import pytest

from label_studio.tests.sdk.common import LABEL_CONFIG_AND_TASKS

pytestmark = pytest.mark.django_db
from label_studio_sdk import Client
from tests.sdk.utils import sdk_logs


def test_task_CRUD(django_live_url, business_client):
    ls = Client(url=django_live_url, api_key=business_client.api_key)
    p = ls.start_project(title='New Project', label_config=LABEL_CONFIG_AND_TASKS['label_config'])

    task_data = [{'data': {'my_text': 'Test task'}}]
    p.import_tasks(task_data)

    tasks = p.get_tasks()
    assert len(tasks) == 1
    assert (task_id := tasks[0]['id'])
    assert tasks[0]['data'] == task_data[0]['data']

    p.update_task(task_id, data={'my_text': 'Updated task'})
    tasks = p.get_tasks()
    assert len(tasks) == 1
    assert tasks[0]['data'] == {'my_text': 'Updated task'}

    p.delete_task(task_id)
    assert not p.get_tasks()


def test_delete_multi_tasks(django_live_url, business_client):
    ls = Client(url=django_live_url, api_key=business_client.api_key)
    p = ls.start_project(title='New Project', label_config=LABEL_CONFIG_AND_TASKS['label_config'])

    task_data = [{'data': {'my_text': 'Test task ' + str(i)}} for i in range(10)]
    p.import_tasks(task_data)

    tasks = p.get_tasks()
    assert len(tasks) == 10

    p.delete_tasks([t['id'] for t in tasks[:5]])
    assert len(p.get_tasks()) == 5

    p.delete_all_tasks(excluded_ids=[tasks[5]['id']])
    remaining_tasks = p.get_tasks()
    assert len(remaining_tasks) == 1
    assert remaining_tasks[0]['data']['my_text'] == 'Test task 5'


def test_export_tasks(django_live_url, business_client):
    ls = Client(url=django_live_url, api_key=business_client.api_key)
    p = ls.start_project(title='New Project', label_config=LABEL_CONFIG_AND_TASKS['label_config'])

    task_data = [{'data': {'my_text': 'Test task ' + str(i)}} for i in range(10)]
    p.import_tasks(task_data)

    task_id = p.get_tasks()[0]['id']
    annotation_data = {
        'result': [{'from_name': 'label', 'to_name': 'my_text', 'type': 'choices', 'value': {'choices': ['Positive']}}]
    }
    p.create_annotation(task_id, **annotation_data)

    # by default, only tasks with annotations are exported
    exported_tasks = p.export_tasks()
    assert len(exported_tasks) == 1
    assert exported_tasks[0]['data']['my_text'] == 'Test task 0'

    exported_tasks = p.export_tasks(download_all_tasks=True)
    assert len(exported_tasks) == 10


def test_upload_and_list_tasks_does_not_log_to_stderr(django_live_url, business_client, caplog):
    caplog.set_level(logging.ERROR)

    ls = Client(url=django_live_url, api_key=business_client.api_key)
    p = ls.start_project(title='New Project', label_config=LABEL_CONFIG_AND_TASKS['label_config'])
    p.import_tasks(LABEL_CONFIG_AND_TASKS['tasks_for_import'])

    tasks = p.get_tasks()

    assert len(tasks) == 1
    assert len(tasks[0]['annotations']) == 1
    assert len(tasks[0]['predictions']) == 1
    assert not sdk_logs(caplog)


def test_get_empty_tasks_does_not_log_to_stderr(django_live_url, business_client, caplog):
    caplog.set_level(logging.ERROR)

    ls = Client(url=django_live_url, api_key=business_client.api_key)
    p = ls.start_project(title='New Project', label_config=LABEL_CONFIG_AND_TASKS['label_config'])

    tasks = p.get_tasks()

    assert not tasks
    assert not sdk_logs(caplog)
</file>

<file path="label_studio/tests/sdk/legacy/test_users.py">
import pytest

from label_studio.tests.sdk.common import LABEL_CONFIG_AND_TASKS

pytestmark = pytest.mark.django_db

from label_studio_sdk import Client


def test_add_user(django_live_url, business_client):
    ls = Client(url=django_live_url, api_key=business_client.api_key)
    ls.start_project(title='New Project', label_config=LABEL_CONFIG_AND_TASKS['label_config'])

    test_member_email = 'test_member@example.com'
    u = ls.create_user(
        {
            'email': test_member_email,
            'username': test_member_email,
            'first_name': 'Test',
            'last_name': 'Member',
        }
    )

    assert u.id in [u.id for u in ls.get_users()]
</file>

<file path="label_studio/tests/sdk/legacy/test_views.py">
import pytest

from label_studio.tests.sdk.common import LABEL_CONFIG_AND_TASKS

pytestmark = pytest.mark.django_db
from label_studio_sdk import Client
from label_studio_sdk.data_manager import Column, Filters, Operator, Type


def test_create_view(django_live_url, business_client):
    ls = Client(url=django_live_url, api_key=business_client.api_key)
    p = ls.start_project(title='New Project', label_config=LABEL_CONFIG_AND_TASKS['label_config'])

    project = ls.get_project(p.id)

    filters = Filters.create(
        Filters.AND,
        [
            Filters.item(Column.id, Operator.GREATER_OR_EQUAL, Type.Number, Filters.value(1)),
            Filters.item(Column.id, Operator.LESS_OR_EQUAL, Type.Number, Filters.value(100)),
        ],
    )

    view = project.create_view(title='Test View', filters=filters)

    assert view['data']['filters'] == {
        'conjunction': 'and',
        'items': [
            {'filter': 'filter:tasks:id', 'operator': 'greater_or_equal', 'type': 'Number', 'value': 1},
            {'filter': 'filter:tasks:id', 'operator': 'less_or_equal', 'type': 'Number', 'value': 100},
        ],
    }


def test_get_tasks_from_view(django_live_url, business_client):
    ls = Client(url=django_live_url, api_key=business_client.api_key)
    p = ls.start_project(title='New Project', label_config=LABEL_CONFIG_AND_TASKS['label_config'])

    project = ls.get_project(p.id)

    task_data = [{'data': {'my_text': 'Test task ' + str(i)}} for i in range(10)]
    p.import_tasks(task_data)
    tasks = p.get_tasks()

    filters = Filters.create(
        Filters.OR,
        [Filters.item(Column.id, Operator.EQUAL, Type.Number, Filters.value(t['id'])) for t in tasks[::2]],
    )

    project.create_view(title='Test View', filters=filters, ordering=['-' + Column.id])
    views = project.get_views()
    assert len(views) == 1
    view = views[0]
    tasks_from_view = project.get_tasks(view_id=view['id'])
    assert len(tasks_from_view) == 5
    assert tasks_from_view == sorted(tasks[::2], key=lambda t: t['id'], reverse=True)
</file>

<file path="label_studio/tests/sdk/common.py">
# source: https://labelstud.io/guide/tasks.html#Basic-Label-Studio-JSON-format
LABEL_CONFIG_AND_TASKS = {
    'label_config': """
    <View>
    <Text name="message" value="$my_text"/>
    <Choices name="sentiment_class" toName="message">
        <Choice value="Positive"/>
        <Choice value="Neutral"/>
        <Choice value="Negative"/>
    </Choices>
    </View>
    """,
    'tasks_for_import': [
        {
            'data': {
                'my_text': 'Opossums are great',
                'ref_id': 456,
                'meta_info': {'timestamp': '2020-03-09 18:15:28.212882', 'location': 'North Pole'},
            },
            'annotations': [
                {
                    'result': [
                        {
                            'from_name': 'sentiment_class',
                            'to_name': 'message',
                            'type': 'choices',
                            'readonly': False,
                            'hidden': False,
                            'value': {'choices': ['Positive']},
                        }
                    ]
                }
            ],
            'predictions': [
                {
                    'result': [
                        {
                            'from_name': 'sentiment_class',
                            'to_name': 'message',
                            'type': 'choices',
                            'readonly': False,
                            'hidden': False,
                            'value': {'choices': ['Neutral']},
                        }
                    ],
                    'score': 0.95,
                }
            ],
        }
    ],
}
</file>

<file path="label_studio/tests/sdk/fixtures.py">
import pytest
from label_studio_sdk.client import LabelStudio
from label_studio_sdk.data_manager import Column, Filters, Operator, Type

from .common import LABEL_CONFIG_AND_TASKS


@pytest.fixture
def test_project_with_view(django_live_url, business_client):
    ls = LabelStudio(base_url=django_live_url, api_key=business_client.api_key)
    p = ls.projects.create(title='New Project', label_config=LABEL_CONFIG_AND_TASKS['label_config'])

    project = ls.projects.get(id=p.id)

    task_data = [{'data': {'my_text': 'Test task ' + str(i)}} for i in range(10)]
    ls.projects.import_tasks(id=project.id, request=task_data)
    orig_tasks = []
    for task in ls.tasks.list(project=project.id):
        orig_tasks.append(task)

    filters = Filters.create(
        Filters.OR,
        [Filters.item(Column.id, Operator.EQUAL, Type.Number, Filters.value(t.id)) for t in orig_tasks[::2]],
    )

    view = ls.views.create(
        project=project.id, data=dict(title='Test View', filters=filters, ordering=['-' + Column.id])
    )
    return ls, project, orig_tasks, view
</file>

<file path="label_studio/tests/sdk/test_annotations.py">
import json

import pytest

from label_studio.tests.sdk.common import LABEL_CONFIG_AND_TASKS

pytestmark = pytest.mark.django_db
from label_studio_sdk.client import LabelStudio
from label_studio_sdk.data_manager import Column, Filters, Operator, Type
from label_studio_sdk.label_interface import LabelInterface
from label_studio_sdk.label_interface.create import labels
from label_studio_sdk.label_interface.objects import AnnotationValue, TaskValue


def test_annotations_CRUD(django_live_url, business_client):
    ls = LabelStudio(base_url=django_live_url, api_key=business_client.api_key)
    li = LabelInterface(LABEL_CONFIG_AND_TASKS['label_config'])
    p = ls.projects.create(title='New Project', label_config=LABEL_CONFIG_AND_TASKS['label_config'])

    task_data = TaskValue(data={'my_text': 'Test task'})
    ls.projects.import_tasks(id=p.id, request=[task_data.model_dump()])

    for task in ls.tasks.list(project=p.id):
        assert task.data == task_data.data

    task_id = task.id
    tag_name = 'sentiment_class'
    annotation_data = AnnotationValue(
        result=[li.get_control(tag_name).label(['Positive'])], completed_by=business_client.user.id
    ).model_dump()
    new_annotation = ls.annotations.create(task_id, result=annotation_data['result'])
    assert (annotation_id := new_annotation.id)
    assert new_annotation.result == annotation_data['result']

    ls.annotations.update(
        id=annotation_id,
        result=[li.get_control(tag_name).label(['Negative'])],
    )
    for task_with_annotation in ls.tasks.list(project=p.id):
        updated_annotation = task_with_annotation.annotations[0]
    assert updated_annotation['result'][0]['value'] == {'choices': ['Negative']}

    # create another annotation
    another_annotation = ls.annotations.create(
        id=task_id,
        result=[li.get_control(tag_name).label(['Neutral'])],
    )

    # check that there are two annotations
    annotations = ls.annotations.list(task_id)
    assert len(annotations) == 2

    # delete one annotation
    ls.annotations.delete(id=annotation_id)
    annotations = ls.annotations.list(task_id)
    assert len(annotations) == 1
    assert annotations[0].id == another_annotation.id
    assert annotations[0].result[0]['value']['choices'] == ['Neutral']


def test_annotation_marks_task_as_labeled(django_live_url, business_client):
    ls = LabelStudio(base_url=django_live_url, api_key=business_client.api_key)

    label_config = LabelInterface.create(
        {
            'image1': 'Image',
            'bbox': labels(['Car', 'Truck', 'Van'], tag_type='RectangleLabels'),
        }
    )

    p = ls.projects.create(
        title='New Project',
        label_config=label_config,
    )

    task_data = [
        TaskValue(data={'image1': 'https://example.com/image.jpg'}),
        TaskValue(data={'image1': 'https://example.com/image2.jpg'}),
    ]
    ls.projects.import_tasks(id=p.id, request=[task.model_dump() for task in task_data])

    filters = Filters.create(
        Filters.OR,
        [
            Filters.item(Column.completed_at, Operator.EMPTY, Type.Datetime, Filters.value(False)),
        ],
    )
    query = json.dumps({'filters': filters})

    labeled_tasks = []
    for task in ls.tasks.list(project=p.id, query=query, fields='all'):
        labeled_tasks.append(task)
    assert labeled_tasks == []

    tasks = []
    for task in ls.tasks.list(project=p.id):
        tasks.append(task)

    assert len(tasks) == 2

    task_id = tasks[0].id
    project = ls.projects.get(p.id)
    li = project.get_label_interface()
    annotation_data = AnnotationValue(
        result=[li.get_control('bbox').label(['Car'], x=10, y=20, width=100, height=100)],
        completed_by=business_client.user.id,
    ).model_dump()

    annotation = ls.annotations.create(id=task_id, result=annotation_data['result'])

    labeled_tasks = []
    for task in ls.tasks.list(project=p.id, query=query):
        labeled_tasks.append(task)

    assert len(labeled_tasks) == 1
    assert labeled_tasks[0].data == task_data[0].data
    assert labeled_tasks[0].annotations[0]['id'] == annotation.id
    assert labeled_tasks[0].annotations[0]['result'][0]['from_name'] == 'bbox'
    assert labeled_tasks[0].annotations[0]['result'][0]['to_name'] == 'image1'
    assert labeled_tasks[0].annotations[0]['result'][0]['type'] == 'rectanglelabels'
    assert labeled_tasks[0].annotations[0]['result'][0]['value'] == {
        'rectanglelabels': ['Car'],
        'x': 10,
        'y': 20,
        'width': 100,
        'height': 100,
        'rotation': 0,
    }
</file>

<file path="label_studio/tests/sdk/test_export.py">
import pandas as pd
import pytest

from label_studio.tests.sdk.common import LABEL_CONFIG_AND_TASKS

pytestmark = pytest.mark.django_db
from label_studio_sdk import AsyncLabelStudio
from label_studio_sdk.client import LabelStudio


@pytest.fixture
def test_project(django_live_url, business_client):
    ls = LabelStudio(base_url=django_live_url, api_key=business_client.api_key)
    project = ls.projects.create(title='Export Test Project', label_config=LABEL_CONFIG_AND_TASKS['label_config'])
    ls.projects.import_tasks(id=project.id, request=LABEL_CONFIG_AND_TASKS['tasks_for_import'])
    return ls, project


async def test_project_async(django_live_url, business_client):
    ls = AsyncLabelStudio(base_url=django_live_url, api_key=business_client.api_key)
    project = await ls.projects.create(
        title='Export Test Project', label_config=LABEL_CONFIG_AND_TASKS['label_config']
    )
    await ls.projects.import_tasks(id=project.id, request=LABEL_CONFIG_AND_TASKS['tasks_for_import'])
    return ls, project


def test_export_formats(test_project):
    ls, project = test_project

    # Get available export formats
    formats = ls.projects.exports.list_formats(project.id)
    assert len(formats) > 0


def test_direct_export(test_project):
    ls, project = test_project

    # Test JSON export
    json_data = ls.projects.exports.as_json(project.id)
    assert isinstance(json_data, list)
    assert len(json_data) == 1

    # Test pandas export
    df = ls.projects.exports.as_pandas(project.id)
    assert isinstance(df, pd.DataFrame)
    assert len(df) == 1

    # Test low level export - import new task without annotations
    ls.projects.import_tasks(
        id=project.id,
        request={
            'data': {
                'my_text': 'Opossums are great',
                'ref_id': 456,
                'meta_info': {'timestamp': '2020-03-09 18:15:28.212882', 'location': 'North Pole'},
            }
        },
    )
    data = ls.projects.exports.download_sync(project.id, download_all_tasks=False)

    def _bytestream_to_json(data):
        import json
        from io import BytesIO

        buffer = BytesIO()
        for chunk in data:
            buffer.write(chunk)
        buffer.seek(0)
        return json.load(buffer)

    assert len(_bytestream_to_json(data)) == 1

    data = ls.projects.exports.download_sync(project.id, download_all_tasks=True)

    assert len(_bytestream_to_json(data)) == 2


# TODO: support pytest-asyncio, otherwise this test will be skipped
@pytest.mark.skip(reason='pytest-asyncio is not supported in this version of Label Studio')
async def test_async_export(test_project_async):
    ls, project = test_project_async

    # Test JSON export
    json_data = await ls.projects.exports.as_json(project.id)
    assert isinstance(json_data, list)
    assert len(json_data) == 1

    # Test pandas export
    df = await ls.projects.exports.as_pandas(project.id, create_kwargs={'task_filter_options': {'finished': 'only'}})
    assert isinstance(df, pd.DataFrame)
    assert len(df) == 1
</file>

<file path="label_studio/tests/sdk/test_ml.py">
import pytest
from label_studio_sdk.client import LabelStudio
from label_studio_sdk.label_interface import LabelInterface


@pytest.mark.django_db
def test_batch_predictions_single_prediction_per_task(django_live_url, business_client, ml_backend_for_test_predict):
    ls = LabelStudio(base_url=django_live_url, api_key=business_client.api_key)
    label_config = LabelInterface.create(
        {
            'text': ('Text', {'name': 'text', 'value': '$text'}, ()),
            'label': (
                'Choices',
                {'name': 'label', 'toName': 'text', 'choice': 'single'},
                (
                    ('Choice', {'value': 'label_A'}, ()),
                    ('Choice', {'value': 'label_B'}, ()),
                ),
            ),
        }
    )
    p = ls.projects.create(
        title='New Project',
        label_config=label_config,
    )
    ls.projects.import_tasks(
        p.id,
        request=[
            {'data': {'text': 'test 1'}},
            {'data': {'text': 'test 2'}},
            {'data': {'text': 'test 3'}},
        ],
    )

    tasks = [task for task in ls.tasks.list(project=p.id)]
    assert len(tasks) == 3

    # setup ML backend with single prediction per task
    ls.ml.create(url='http://test.ml.backend.for.sdk.com:9092', project=p.id, title='ModelSingle')

    # batch predict tasks via actions
    ls.actions.create(
        id='retrieve_tasks_predictions',
        project=p.id,
        selected_items={'all': True, 'excluded': [tasks[1].id]},
    )

    # get all predictions in project
    predictions = ls.predictions.list(project=p.id)

    # check that only 2 predictions were created
    assert len(predictions) == 2

    # check that the first prediction has the correct value
    assert predictions[0].result[0]['value']['choices'][0] == 'Single'
    assert predictions[0].model_version == 'ModelSingle'

    # check that the second prediction has the correct value
    assert predictions[1].result[0]['value']['choices'][0] == 'Single'
    assert predictions[1].model_version == 'ModelSingle'

    # additionally let's test actions: convert predictions to annotations
    ls.actions.create(
        id='predictions_to_annotations',
        project=p.id,
        selected_items={
            'all': False,
            'included': [
                predictions[0].task,
                predictions[1].task,
                # also emulate user error when trying to convert task with no predictions
                tasks[1].id,
            ],
        },
    )

    # get all annotations in project
    for task in ls.tasks.list(project=p.id, fields='all'):
        if task.id == tasks[1].id:
            assert not task.annotations
            assert not task.predictions
        else:
            assert len(task.annotations) == 1
            assert task.annotations[0]['result'][0]['value']['choices'][0] == 'Single'

            assert len(task.predictions) == 1
            assert task.predictions[0]['result'][0]['value']['choices'][0] == 'Single'
            assert task.predictions[0]['model_version'] == 'ModelSingle'
            assert task.predictions[0]['score'] == 0.1


@pytest.mark.django_db
def test_batch_predictions_multiple_predictions_per_task(
    django_live_url, business_client, ml_backend_for_test_predict
):
    ls = LabelStudio(base_url=django_live_url, api_key=business_client.api_key)
    li = LabelInterface(
        """
            <View>
              <Text name="text" value="$text"/>
              <Choices name="label" toName="text" choice="single">
                <Choice value="label_A"></Choice>
                <Choice value="label_B"></Choice>
              </Choices>
            </View>"""
    )
    p = ls.projects.create(
        title='New Project',
        label_config=li._config,
    )
    ls.projects.import_tasks(
        p.id,
        request=[
            {'data': {'text': 'test 1'}},
            {'data': {'text': 'test 2'}},
            {'data': {'text': 'test 3'}},
        ],
    )

    tasks = [task for task in ls.tasks.list(project=p.id)]
    assert len(tasks) == 3

    # setup ML backend with multiple predictions per task
    ls.ml.create(url='http://test.ml.backend.for.sdk.com:9093', project=p.id, title='ModelMultiple')

    # batch predict tasks via actions
    ls.actions.create(
        id='retrieve_tasks_predictions',
        project=p.id,
        selected_items={'all': False, 'included': [tasks[0].id, tasks[2].id]},
    )

    # get all predictions in project
    predictions = ls.predictions.list(project=p.id)

    # check that there are 4 predictions as 2 tasks were predicted
    assert len(predictions) == 4

    for task in ls.tasks.list(project=p.id, fields='all'):
        if task.id == tasks[1].id:
            assert not task.predictions
        else:
            assert len(task.predictions) == 2

            for i, prediction in enumerate(task.predictions):
                assert prediction['result'][0]['value']['choices'][0] == f'label_{["A", "B"][i]}'
                assert prediction['model_version'] == f'Model{"AB"[i]}'
                assert prediction['score'] == 0.2 if i == 0 else 0.3
</file>

<file path="label_studio/tests/sdk/test_predictions.py">
import pytest

from label_studio.tests.sdk.common import LABEL_CONFIG_AND_TASKS

pytestmark = pytest.mark.django_db
from label_studio_sdk.client import LabelStudio
from label_studio_sdk.label_interface import LabelInterface
from label_studio_sdk.label_interface.objects import PredictionValue, TaskValue


def test_predictions_CRUD(django_live_url, business_client):
    ls = LabelStudio(base_url=django_live_url, api_key=business_client.api_key)
    li = LabelInterface(LABEL_CONFIG_AND_TASKS['label_config'])
    p = ls.projects.create(title='New Project', label_config=li.config)

    task = ls.tasks.create(project=p.id, data={'my_text': 'Test task'})

    # create a prediction
    pv = PredictionValue(
        result=[li.get_control('sentiment_class').label(['Positive'])],
        score=0.9,
        model_version='1.0.0',
    )
    prediction = ls.predictions.create(
        task=task.id,
        **pv.model_dump(),
    )

    # get a prediction
    prediction = ls.predictions.get(id=prediction.id)
    assert prediction.result[0]['value']['choices'] == ['Positive']
    assert prediction.score == 0.9
    assert prediction.model_version == '1.0.0'

    # create another prediction
    pv = PredictionValue(
        result=[
            li.get_control('sentiment_class').label(['Neutral']),
            li.get_control('sentiment_class').label(['Negative']),
        ],
        score=0.8,
        model_version='1.0.1',
    )

    another_prediction = ls.predictions.create(task=task.id, **pv.model_dump())

    # check that there are two predictions
    predictions = ls.predictions.list(task=task.id)
    assert len(predictions) == 2

    # delete one prediction
    ls.predictions.delete(id=prediction.id)
    predictions = ls.predictions.list(task=task.id)
    assert len(predictions) == 1
    assert predictions[0].id == another_prediction.id


def test_create_predictions_with_import(django_live_url, business_client):
    ls = LabelStudio(base_url=django_live_url, api_key=business_client.api_key)
    li = LabelInterface(LABEL_CONFIG_AND_TASKS['label_config'])
    p = ls.projects.create(title='New Project', label_config=li.config)

    # import tasks with predictions
    ls.projects.import_tasks(
        id=p.id,
        request=[
            {'my_text': 'Hello world', 'my_label': 'Positive'},
            {'my_text': 'Goodbye Label Studio', 'my_label': 'Negative'},
            {'my_text': 'What a beautiful day', 'my_label': 'Positive'},
        ],
        preannotated_from_fields=['my_label'],
    )

    # check predictions for each class
    task_ids = []
    for task in ls.tasks.list(project=p.id):
        assert len(ls.predictions.list(task=task.id)) == 1
        task_ids.append(task.id)
    assert len(task_ids) == 3

    # import more tasks in extended format
    task1 = TaskValue(
        data={'my_text': 'Hello world'},
        predictions=[
            PredictionValue(
                result=[li.get_control('sentiment_class').label(['Positive'])],
                score=0.95,
                model_version='3.4.5',
            )
        ],
    )
    task2 = TaskValue(
        data={'my_text': 'Goodbye Label Studio'},
        predictions=[
            PredictionValue(
                result=[li.get_control('sentiment_class').label(['Negative'])],
                score=0.85,
                model_version='3.4.5',
            )
        ],
    )

    ls.projects.import_tasks(
        id=p.id,
        request=[task1.model_dump(), task2.model_dump()],
    )

    # check for new predictions
    for task in ls.tasks.list(project=p.id):
        predictions = ls.predictions.list(task=task.id)
        assert len(predictions) == 1
        if task.id not in task_ids:
            assert predictions[0].model_version == '3.4.5'
            task_ids.append(task.id)

    assert len(task_ids) == 5

    # update project with model_version (RND-113)
    assert p.model_version == ''
    ls.projects.update(id=p.id, model_version='3.4.5')
    project = ls.projects.get(id=p.id)
    assert project.model_version == '3.4.5'

    # assert it raises label_studio_sdk.core.api_error.ApiError with validation_errors': {'model_version': ["Model version doesn't exist..." ]}
    from label_studio_sdk.core.api_error import ApiError

    with pytest.raises(ApiError) as e:
        ls.projects.update(id=p.id, model_version='3.4.6')
    assert e.value.status_code == 400
    assert e.value.body['validation_errors']['model_version'][0].startswith("Model version doesn't exist")
</file>

<file path="label_studio/tests/sdk/test_projects.py">
import pytest

from label_studio.tests.sdk.common import LABEL_CONFIG_AND_TASKS

pytestmark = pytest.mark.django_db
from label_studio_sdk.client import LabelStudio


def test_start_and_get_project(django_live_url, business_client):
    ls = LabelStudio(base_url=django_live_url, api_key=business_client.api_key)
    p = ls.projects.create(title='New Project', label_config=LABEL_CONFIG_AND_TASKS['label_config'])

    project = ls.projects.get(id=p.id)
    assert project
    assert project.title == 'New Project'

    ls.projects.update(id=project.id, title='Updated Project')
    project = ls.projects.get(id=p.id)
    assert project.title == 'Updated Project'


def test_delete_project(django_live_url, business_client):
    ls = LabelStudio(base_url=django_live_url, api_key=business_client.api_key)
    p = ls.projects.create(title='New Project', label_config=LABEL_CONFIG_AND_TASKS['label_config'])

    project = ls.projects.get(id=p.id)
    ls.projects.delete(id=project.id)

    any_project_found = False
    for project in ls.projects.list():
        any_project_found = True

    assert not any_project_found
</file>

<file path="label_studio/tests/sdk/test_storages.py">
import pytest

from label_studio.tests.sdk.common import LABEL_CONFIG_AND_TASKS

pytestmark = pytest.mark.django_db
from label_studio_sdk.client import LabelStudio


def test_connect_and_sync_s3(django_live_url, business_client):
    ls = LabelStudio(base_url=django_live_url, api_key=business_client.api_key)
    p = ls.projects.create(title='New Project', label_config=LABEL_CONFIG_AND_TASKS['label_config'])

    storage_resp = ls.import_storage.s3.create(
        project=p.id, bucket='pytest-s3-images', regex_filter='.*', use_blob_urls=False
    )

    storage_id = storage_resp.id

    storage = ls.import_storage.s3.get(id=storage_id)
    assert storage.project == p.id
    assert storage.bucket == 'pytest-s3-images'
    assert storage.use_blob_urls is False
    ls.import_storage.s3.update(id=storage_id, use_blob_urls=True)
    storage = ls.import_storage.s3.get(id=storage_id)
    assert storage.use_blob_urls is True

    resp = ls.import_storage.s3.sync(id=storage_id)
    assert resp.status in ('initialized', 'queued', 'completed')

    tasks = []
    for task in ls.tasks.list(project=p.id):
        tasks.append(task)

    assert set(t.storage_filename for t in tasks) == {
        'subdir/another/image2.jpg',
        'subdir/image1.jpg',
        'subdir/image2.jpg',
        'image1.jpg',
    }
</file>

<file path="label_studio/tests/sdk/test_tasks.py">
import logging

import pytest

from label_studio.tests.sdk.common import LABEL_CONFIG_AND_TASKS

pytestmark = pytest.mark.django_db
from label_studio_sdk.client import LabelStudio

from label_studio.tests.sdk.utils import sdk_logs


def test_task_CRUD(django_live_url, business_client):
    ls = LabelStudio(base_url=django_live_url, api_key=business_client.api_key)
    p = ls.projects.create(title='New Project', label_config=LABEL_CONFIG_AND_TASKS['label_config'])

    task_data = [{'data': {'my_text': 'Test task'}}]
    for task in task_data:
        ls.tasks.create(project=p.id, data=task['data'])

    tasks = [task for task in ls.tasks.list(project=p.id)]

    assert len(tasks) == 1
    assert (task_id := tasks[0].id)
    assert tasks[0].data == task_data[0]['data']

    ls.tasks.update(id=task_id, data={'my_text': 'Updated task'})
    tasks = [task for task in ls.tasks.list(project=p.id)]
    assert len(tasks) == 1
    assert tasks[0].data == {'my_text': 'Updated task'}

    ls.tasks.delete(id=task_id)
    tasks = []
    for task in ls.tasks.list(project=p.id):
        tasks.append(task)
    assert len(tasks) == 0


def test_delete_multi_tasks(django_live_url, business_client):
    ls = LabelStudio(base_url=django_live_url, api_key=business_client.api_key)
    p = ls.projects.create(title='New Project', label_config=LABEL_CONFIG_AND_TASKS['label_config'])

    task_data = [{'data': {'my_text': 'Test task ' + str(i)}} for i in range(10)]
    for task in task_data:
        ls.tasks.create(project=p.id, data=task['data'])

    tasks = [task for task in ls.tasks.list(project=p.id)]
    assert len(tasks) == 10

    tasks_ids_to_delete = [t.id for t in tasks[:5]]

    # delete specific tasks
    ls.actions.create(project=p.id, id='delete_tasks', selected_items={'all': False, 'included': tasks_ids_to_delete})
    assert len([task for task in ls.tasks.list(project=p.id)]) == 5

    # another way of calling delete action instead of
    #     ls.actions.create(project=p.id, id='delete_tasks', selected_items={'all': True, 'excluded': [tasks[5].id]})
    import json

    ls.actions.create(
        project=p.id,
        id='delete_tasks',
        request_options={
            'additional_body_parameters': {
                'selectedItems': json.dumps({'all': True, 'excluded': [tasks[5].id]}),
            },
        },
    )

    remaining_tasks = [task for task in ls.tasks.list(project=p.id)]
    assert len(remaining_tasks) == 1
    assert remaining_tasks[0].data['my_text'] == 'Test task 5'

    # remove all tasks
    ls.tasks.delete_all_tasks(id=p.id)
    any_task_found = False
    for task in ls.tasks.list(project=p.id):
        any_task_found = True
    assert not any_task_found


def test_export_tasks(django_live_url, business_client):
    ls = LabelStudio(base_url=django_live_url, api_key=business_client.api_key)
    p = ls.projects.create(title='New Project', label_config=LABEL_CONFIG_AND_TASKS['label_config'])

    task_data = [{'data': {'my_text': 'Test task ' + str(i)}} for i in range(10)]
    ls.projects.import_tasks(id=p.id, request=task_data)

    task_id = None
    for i, task in enumerate(ls.tasks.list(project=p.id)):
        if i == 7:
            task_id = task.id
            break

    annotation_data = {
        'result': [{'from_name': 'label', 'to_name': 'my_text', 'type': 'choices', 'value': {'choices': ['Positive']}}]
    }
    ls.annotations.create(id=task_id, **annotation_data)

    # export a singleton task
    single_task = ls.tasks.get(id=task_id)
    assert single_task.data['my_text'] == 'Test task 7'
    assert single_task.total_annotations == 1
    assert single_task.updated_by == [{'user_id': business_client.user.id}]

    exported_tasks = [task for task in ls.tasks.list(project=p.id, fields='all') if task.annotations]
    assert len(exported_tasks) == 1
    assert exported_tasks[0].data['my_text'] == 'Test task 7'

    exported_tasks = [task for task in ls.tasks.list(project=p.id, fields='all')]
    assert len(exported_tasks) == 10


def test_upload_and_list_tasks_does_not_log_to_stderr(django_live_url, business_client, caplog):
    caplog.set_level(logging.ERROR)

    ls = LabelStudio(base_url=django_live_url, api_key=business_client.api_key)
    p = ls.projects.create(title='New Project', label_config=LABEL_CONFIG_AND_TASKS['label_config'])
    ls.projects.import_tasks(id=p.id, request=LABEL_CONFIG_AND_TASKS['tasks_for_import'])

    tasks = [task for task in ls.tasks.list(project=p.id, fields='all')]

    assert len(tasks) == 1
    assert len(tasks[0].annotations) == 1
    assert len(tasks[0].predictions) == 1
    assert not sdk_logs(caplog)


def test_get_empty_tasks_does_not_log_to_stderr(django_live_url, business_client, caplog):
    caplog.set_level(logging.ERROR)

    ls = LabelStudio(base_url=django_live_url, api_key=business_client.api_key)
    p = ls.projects.create(title='New Project', label_config=LABEL_CONFIG_AND_TASKS['label_config'])

    tasks = [task for task in ls.tasks.list(project=p.id)]

    assert not tasks
    assert not sdk_logs(caplog)
</file>

<file path="label_studio/tests/sdk/test_users.py">
import pytest

from label_studio.tests.sdk.common import LABEL_CONFIG_AND_TASKS

pytestmark = pytest.mark.django_db

from label_studio_sdk.client import LabelStudio


def test_add_user(django_live_url, business_client):
    ls = LabelStudio(base_url=django_live_url, api_key=business_client.api_key)
    ls.projects.create(title='New Project', label_config=LABEL_CONFIG_AND_TASKS['label_config'])

    test_member_email = 'test_member@example.com'
    u = ls.users.create(
        **{
            'email': test_member_email,
            'username': test_member_email,
            'first_name': 'Test',
            'last_name': 'Member',
        }
    )

    assert u.id in [u.id for u in ls.users.list()]
</file>

<file path="label_studio/tests/sdk/test_views.py">
import pytest

from label_studio.tests.sdk.common import LABEL_CONFIG_AND_TASKS

pytestmark = pytest.mark.django_db
from label_studio_sdk.client import LabelStudio
from label_studio_sdk.data_manager import Column, Filters, Operator, Type


def test_create_view(django_live_url, business_client):
    ls = LabelStudio(base_url=django_live_url, api_key=business_client.api_key)
    p = ls.projects.create(title='New Project', label_config=LABEL_CONFIG_AND_TASKS['label_config'])

    project = ls.projects.get(id=p.id)

    filters = Filters.create(
        Filters.AND,
        [
            Filters.item(Column.id, Operator.GREATER_OR_EQUAL, Type.Number, Filters.value(1)),
            Filters.item(Column.id, Operator.LESS_OR_EQUAL, Type.Number, Filters.value(100)),
        ],
    )

    view = ls.views.create(project=project.id, data=dict(title='Test View', filters=filters))

    assert view.data['filters'] == {
        'conjunction': 'and',
        'items': [
            {'filter': 'filter:tasks:id', 'operator': 'greater_or_equal', 'type': 'Number', 'value': 1},
            {'filter': 'filter:tasks:id', 'operator': 'less_or_equal', 'type': 'Number', 'value': 100},
        ],
    }


def test_get_tasks_from_view(test_project_with_view):
    ls, project, orig_tasks, view = test_project_with_view
    views = ls.views.list(project=project.id)
    assert len(views) == 1
    found_view = views[0]

    assert found_view.id == view.id
    tasks = []
    for task in ls.tasks.list(view=view.id):
        tasks.append(task)
    assert len(tasks) == 5
    assert tasks == sorted(orig_tasks[::2], key=lambda t: t.id, reverse=True)
</file>

<file path="label_studio/tests/sdk/utils.py">
from typing import Sequence, Tuple


def sdk_logs(caplog) -> Sequence[Tuple[str, str, str]]:
    """
    Get the SDK logs from the passed caplog fixture. Useful for asserting on SDK log output.
    """

    return [record_tuple for record_tuple in caplog.record_tuples if record_tuple[0].startswith('label_studio_sdk.')]
</file>

<file path="label_studio/tests/tasks/test_functions.py">
import io
import os

import psutil
import pytest
from data_export.serializers import ExportDataSerializer
from django.conf import settings
from tasks.functions import export_project

pytestmark = pytest.mark.django_db


def memory_limit(max_mem):
    try:
        import resource
    except ImportError:

        def decorator(f):
            return f

        return decorator

    def decorator(f):
        def wrapper(*args, **kwargs):
            process = psutil.Process(os.getpid())
            prev_limits = resource.getrlimit(resource.RLIMIT_AS)
            resource.setrlimit(resource.RLIMIT_AS, (process.memory_info().rss + max_mem, -1))
            result = f(*args, **kwargs)
            resource.setrlimit(resource.RLIMIT_AS, prev_limits)
            return result

        return wrapper

    return decorator


class TestExportProject:
    @pytest.fixture
    def generate_export_file(self, mocker):
        return mocker.patch(
            'tasks.functions.DataExport.generate_export_file',
            return_value=(io.BytesIO(b'stream'), 'application/json', 'project.json'),
        )

    @pytest.fixture
    def project(self, configured_project):
        return configured_project

    def test_export_project(self, mocker, generate_export_file, project):
        data = ExportDataSerializer(
            project.tasks.all(),
            many=True,
            context={'interpolate_key_frames': settings.INTERPOLATE_KEY_FRAMES},
        ).data

        with mocker.patch('builtins.open'):
            filepath = export_project(project.id, 'JSON', settings.EXPORT_DIR)

        assert filepath == os.path.join(settings.EXPORT_DIR, 'project.json')

        generate_export_file.assert_called_once_with(project, data, 'JSON', settings.CONVERTER_DOWNLOAD_RESOURCES, {})

    def test_project_does_not_exist(self, mocker, generate_export_file):
        with mocker.patch('builtins.open'):
            with pytest.raises(Exception):
                export_project(1, 'JSON', settings.EXPORT_DIR)

        generate_export_file.assert_not_called()
</file>

<file path="label_studio/tests/test_data/__init__.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
</file>

<file path="label_studio/tests/test_data/gen_tasks_and_annotations.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
"""
this script
generates
tasks_and_annotations json file
with random label_choices

to be bulk imported in project
"""

import os
import random as r

from django.conf import settings

task_template_start = """{"id": %s,"predictions":[],"annotations":["""

label_choices = ['Neutral', 'Positive', 'Negative']

tc_template = """{"id": %s,"review_result":null,"ground_truth":false,"result":[{"id":"MGK92Ogo4t","type":"choices","value":{"choices":["%s"]},"to_name":"text","from_name":"sentiment"}],"created_at":"2020-07-06T07:55:08.250617Z","updated_at":"2020-07-06T07:55:08.250637Z","lead_time":79.583,"completed_by":%s}"""

task_template_end = """],"data":{"text":"       ", "meta_info":"meta A"},"meta":{},"accuracy":1.0,"created_at":"2020-06-26T12:59:15.457035Z","updated_at":"2020-07-06T10:47:48.997310Z","is_labeled":true,"overlap":2,"project":2}"""


def gen_tasks(user_id):
    i = 11
    tasks_n = 1000

    j = 80
    annotations_n = 5

    tasks = []
    for t in range(tasks_n):
        tasks.append(task_template_start % i)
        for c in range(annotations_n):
            tasks.append(tc_template % (j, r.choices(label_choices)[0], user_id))
            if c < annotations_n - 1:
                tasks.append(',')
            j += 1
        tasks.append(task_template_end)
        if t < tasks_n - 1:
            tasks.append(',')
        i += 1

    with open(os.path.join(settings.TEST_DATA_ROOT, 'tasks_and_annotations.json'), 'w+', encoding='utf-8') as f:
        f.write('[' + ''.join(tasks) + ']')


if __name__ == '__main__':
    gen_tasks(3)
</file>

<file path="label_studio/tests/test_suites/converter.py">
import sys
from collections import OrderedDict

import yaml


def represent_ordereddict(dumper, data):
    value = []

    for item_key, item_value in data.items():
        node_key = dumper.represent_data(item_key)
        node_value = dumper.represent_data(item_value)

        value.append((node_key, node_value))

    return yaml.nodes.MappingNode('tag:yaml.org,2002:map', value)


yaml.add_representer(OrderedDict, represent_ordereddict)


class Regexp(yaml.YAMLObject):
    yaml_tag = '!re_match'

    def __init__(self, regexp):
        self.regexp = regexp

    # def __repr__(self):
    #     return "" % (
    #         self.__class__.__name__, self.name, self.hp, self.ac, self.attacks)


old_test = sys.argv[1]

new_tests_list = []

with open(old_test) as f:
    content = yaml.safe_load(f.read())

    for test in content:
        # print(test)
        new_test = OrderedDict()
        for test_name, test_data in test.items():
            new_test['test_name'] = test_name
            new_test['strict'] = False
            new_test['marks'] = [{'usefixtures': ['django_live_url']}]
            new_stages = [{'type': 'ref', 'id': 'signup'}]
            for stage in test_data:
                for url, stage_data in stage.items():
                    request_data = {'url': '{{django_live_url}}{url}'.format(url=url), 'method': stage_data['method']}

                    content_type = stage_data.get('content_type', None)
                    if 'data' in stage_data:
                        if isinstance(stage_data['data'], dict):
                            for k, v in stage_data['data'].items():
                                if isinstance(v, str) and 'samples' in v:
                                    if 'files' not in request_data:
                                        request_data['files'] = {}
                                    request_data['files'][k] = f'tests/test_suites/{v}'
                                else:
                                    if content_type and 'json' in content_type:
                                        if 'json' not in request_data:
                                            request_data['json'] = {}
                                        request_data['json'][k] = v
                                    else:
                                        if 'data' not in request_data:
                                            request_data['data'] = {}
                                        request_data['data'][k] = v
                        else:
                            if content_type and 'json' in content_type:
                                request_data['json'] = stage_data['data']
                            else:
                                request_data['data'] = stage_data['data']

                    if 'content_type' in stage_data:
                        request_data['headers'] = {'content-type': stage_data['content_type']}

                    response_data = {'status_code': stage_data['status_code']}
                    if 'response' in stage_data and isinstance(stage_data['response'], dict):
                        for k, v in stage_data['response'].items():
                            if isinstance(v, str) and v.startswith('{'):
                                if 'save' not in response_data:
                                    response_data['save'] = {'json': {}}
                                key = v.replace('{', '').replace('}', '')
                                response_data['save']['json'][key] = k
                            else:
                                if 'json' not in response_data:
                                    response_data['json'] = {}
                                response_data['json'][k] = v

                    new_stages.append(
                        {
                            'name': 'stage',
                            'request': request_data,
                            'response': response_data,
                        }
                    )

            new_test['stages'] = new_stages

        new_tests_list.append(new_test)

    for test in new_tests_list:
        print('---')
        print(yaml.dump(test))
</file>

<file path="label_studio/tests/webhooks/test_webhooks.py">
import json
from unittest import TestCase

import pytest
import requests
import requests_mock
from django.urls import reverse
from projects.models import Project
from webhooks.models import Webhook, WebhookAction
from webhooks.utils import emit_webhooks, emit_webhooks_for_instance, run_webhook


@pytest.fixture
def organization_webhook(configured_project):
    organization = configured_project.organization
    uri = 'http://127.0.0.1:8000/api/organization/'
    return Webhook.objects.create(
        organization=organization,
        project=None,
        url=uri,
    )


@pytest.fixture
def project_webhook(configured_project):
    organization = configured_project.organization
    uri = 'http://127.0.0.1:8000/api/project/'
    return Webhook.objects.create(
        organization=organization,
        project=configured_project,
        url=uri,
    )


@pytest.fixture
def ml_start_training_webhook(configured_project):
    organization = configured_project.organization
    uri = 'http://0.0.0.0:9090/webhook'
    return Webhook.objects.create(
        organization=organization,
        project=configured_project,
        url=uri,
    )


@pytest.mark.django_db
def test_run_webhook(setup_project_dialog, organization_webhook):
    webhook = organization_webhook
    with requests_mock.Mocker(real_http=True) as m:
        m.register_uri('POST', webhook.url)
        run_webhook(organization_webhook, WebhookAction.PROJECT_CREATED, {'data': 'test'})

    request_history = m.request_history
    assert len(request_history) == 1
    assert request_history[0].method == 'POST'
    assert request_history[0].url == organization_webhook.url
    TestCase().assertDictEqual(request_history[0].json(), {'action': WebhookAction.PROJECT_CREATED, 'data': 'test'})


@pytest.mark.django_db
def test_emit_webhooks(setup_project_dialog, organization_webhook):
    webhook = organization_webhook
    with requests_mock.Mocker(real_http=True) as m:
        m.register_uri('POST', webhook.url)
        emit_webhooks(webhook.organization, webhook.project, WebhookAction.PROJECT_CREATED, {'data': 'test'})

    request_history = m.request_history
    assert len(request_history) == 1
    assert request_history[0].method == 'POST'
    assert request_history[0].url == webhook.url
    TestCase().assertDictEqual(request_history[0].json(), {'action': WebhookAction.PROJECT_CREATED, 'data': 'test'})


@pytest.mark.django_db
def test_emit_webhooks_for_instance(setup_project_dialog, organization_webhook):
    webhook = organization_webhook
    project_title = 'Projects 1'
    project = Project.objects.create(title=project_title)
    with requests_mock.Mocker(real_http=True) as m:
        m.register_uri('POST', webhook.url)
        emit_webhooks_for_instance(
            webhook.organization, webhook.project, WebhookAction.PROJECT_CREATED, instance=project
        )
    assert len(m.request_history) == 1
    assert m.request_history[0].method == 'POST'
    data = m.request_history[0].json()
    assert 'action' in data
    assert 'project' in data
    assert project_title == data['project']['title']


@pytest.mark.django_db
def test_exception_catch(organization_webhook):
    webhook = organization_webhook
    with requests_mock.Mocker(real_http=True) as m:
        m.register_uri('POST', webhook.url, exc=requests.exceptions.ConnectTimeout)
        result = run_webhook(webhook, WebhookAction.PROJECT_CREATED)
    assert result is None


# PROJECT CREATE/UPDATE/DELETE API
@pytest.mark.django_db
def test_webhooks_for_projects(configured_project, business_client, organization_webhook):
    webhook = organization_webhook

    # create/update/delete project through API
    # PROJECT_CREATED
    with requests_mock.Mocker(real_http=True) as m:
        m.register_uri('POST', webhook.url)
        response = business_client.post(reverse('projects:api:project-list'))

    assert response.status_code == 201
    assert len(list(filter(lambda x: x.url == webhook.url, m.request_history))) == 1

    r = list(filter(lambda x: x.url == webhook.url, m.request_history))[0]
    assert r.json()['action'] == WebhookAction.PROJECT_CREATED

    project_id = response.json()['id']
    # PROJECT_UPDATED
    with requests_mock.Mocker(real_http=True) as m:
        m.register_uri('POST', webhook.url)
        response = business_client.patch(
            reverse('projects:api:project-detail', kwargs={'pk': project_id}),
            data=json.dumps({'title': 'Test title'}),
            content_type='application/json',
        )

    assert response.status_code == 200
    assert len(list(filter(lambda x: x.url == webhook.url, m.request_history))) == 1

    r = list(filter(lambda x: x.url == webhook.url, m.request_history))[0]
    assert r.json()['action'] == WebhookAction.PROJECT_UPDATED
    assert r.json()['project']['title'] == 'Test title'

    # PROJECT_DELETED
    with requests_mock.Mocker(real_http=True) as m:
        m.register_uri('POST', webhook.url)
        response = business_client.delete(
            reverse('projects:api:project-detail', kwargs={'pk': project_id}),
        )
    assert response.status_code == 204
    assert len(list(filter(lambda x: x.url == organization_webhook.url, m.request_history))) == 1

    r = list(filter(lambda x: x.url == organization_webhook.url, m.request_history))[0]
    assert r.json()['action'] == WebhookAction.PROJECT_DELETED
    assert r.json()['project']['id'] == project_id


# TASK CREATE/DELETE API
# WE DON'T SUPPORT UPDATE FOR TASK
@pytest.mark.django_db
def test_webhooks_for_tasks(configured_project, business_client, organization_webhook):
    webhook = organization_webhook
    # CREATE
    with requests_mock.Mocker(real_http=True) as m:
        m.register_uri('POST', webhook.url)
        response = business_client.post(
            reverse('tasks:api:task-list'),
            data=json.dumps(
                {
                    'project': configured_project.id,
                    'data': {'meta_info': 'meta info A', 'text': 'text A'},
                }
            ),
            content_type='application/json',
        )
    assert response.status_code == 201
    assert len(list(filter(lambda x: x.url == webhook.url, m.request_history))) == 1

    r = list(filter(lambda x: x.url == webhook.url, m.request_history))[0]
    assert r.json()['action'] == WebhookAction.TASKS_CREATED
    assert 'tasks' in r.json()
    assert 'project' in r.json()

    # DELETE
    task_id = response.json()['id']
    url = webhook.url
    with requests_mock.Mocker(real_http=True) as m:
        m.register_uri('POST', url)
        response = business_client.delete(reverse('tasks:api:task-detail', kwargs={'pk': task_id}))

    assert response.status_code == 204
    assert len(list(filter(lambda x: x.url == webhook.url, m.request_history))) == 1

    r = list(filter(lambda x: x.url == webhook.url, m.request_history))[0]
    assert r.json()['action'] == WebhookAction.TASKS_DELETED
    assert 'tasks' in r.json()
    assert 'project' in r.json()


# TASK CREATE on IMPORT
@pytest.mark.django_db
def test_webhooks_for_tasks_import(configured_project, business_client, organization_webhook):
    from django.core.files.uploadedfile import SimpleUploadedFile

    webhook = organization_webhook

    IMPORT_CSV = 'tests/test_suites/samples/test_5.csv'

    with open(IMPORT_CSV, 'rb') as file_:
        data = SimpleUploadedFile('test_5.csv', file_.read(), content_type='multipart/form-data')
    with requests_mock.Mocker(real_http=True) as m:
        m.register_uri('POST', webhook.url)
        response = business_client.post(
            f'/api/projects/{configured_project.id}/import',
            data={'csv_1': data},
            format='multipart',
        )
    assert response.status_code == 201
    assert response.json()['task_count'] == 3

    assert len(list(filter(lambda x: x.url == webhook.url, m.request_history))) == 1

    r = list(filter(lambda x: x.url == webhook.url, m.request_history))[0]
    assert r.json()['action'] == WebhookAction.TASKS_CREATED
    assert 'tasks' in r.json()
    assert 'project' in r.json()
    assert len(r.json()['tasks']) == response.json()['task_count'] == 3


# ANNOTATION CREATE/UPDATE/DELETE
@pytest.mark.django_db
def test_webhooks_for_annotation(configured_project, business_client, organization_webhook):

    webhook = organization_webhook
    task = configured_project.tasks.all().first()
    # CREATE
    with requests_mock.Mocker(real_http=True) as m:
        m.register_uri('POST', webhook.url)
        response = business_client.post(
            f'/api/tasks/{task.id}/annotations?project={configured_project.id}',
            data=json.dumps(
                {
                    'result': [
                        {
                            'value': {'choices': ['class_A']},
                            'id': 'nJS76J03pi',
                            'from_name': 'text_class',
                            'to_name': 'text',
                            'type': 'choices',
                            'origin': 'manual',
                        }
                    ],
                    'draft_id': 0,
                    'parent_prediction': None,
                    'parent_annotation': None,
                    'project': configured_project.id,
                }
            ),
            content_type='application/json',
        )

    assert response.status_code == 201
    assert len(list(filter(lambda x: x.url == webhook.url, m.request_history))) == 1

    r = list(filter(lambda x: x.url == webhook.url, m.request_history))[0]
    assert r.json()['action'] == WebhookAction.ANNOTATION_CREATED
    annotation_id = response.json()['id']

    # UPDATE POST
    with requests_mock.Mocker(real_http=True) as m:
        m.register_uri('POST', webhook.url)
        response = business_client.put(
            f'/api/annotations/{annotation_id}?project={configured_project.id}&taskId={task.id}',
            data=json.dumps(
                {
                    'result': [],
                }
            ),
            content_type='application/json',
        )
        assert response.status_code == 200

        response = business_client.patch(
            f'/api/annotations/{annotation_id}?project={configured_project.id}&taskId={task.id}',
            data=json.dumps(
                {
                    'result': [
                        {
                            'value': {'choices': ['class_B']},
                            'id': 'nJS76J03pi',
                            'from_name': 'text_class',
                            'to_name': 'text',
                            'type': 'choices',
                            'origin': 'manual',
                        }
                    ],
                }
            ),
            content_type='application/json',
        )
        assert response.status_code == 200

    assert len(list(filter(lambda x: x.url == webhook.url, m.request_history))) == 2

    for r in list(filter(lambda x: x.url == webhook.url, m.request_history)):
        assert r.json()['action'] == WebhookAction.ANNOTATION_UPDATED

        assert 'task' in r.json()
        assert 'annotation' in r.json()
        assert 'project' in r.json()

    # DELETE
    with requests_mock.Mocker(real_http=True) as m:
        m.register_uri('POST', webhook.url)
        response = business_client.delete(
            f'/api/annotations/{annotation_id}',
            content_type='application/json',
        )
    assert response.status_code == 204
    assert len(list(filter(lambda x: x.url == webhook.url, m.request_history))) == 1

    r = list(filter(lambda x: x.url == webhook.url, m.request_history))[0]
    assert r.json()['action'] == WebhookAction.ANNOTATIONS_DELETED
    assert 'annotations' in r.json()
    assert annotation_id == r.json()['annotations'][0]['id']


# ACTION: DELETE ANNOTATIONS
@pytest.mark.django_db
def test_webhooks_for_action_delete_tasks_annotations(configured_project, business_client, organization_webhook):
    webhook = organization_webhook

    # create annotations for tasks
    for task in configured_project.tasks.all():
        response = business_client.post(
            f'/api/tasks/{task.id}/annotations?project={configured_project.id}',
            data=json.dumps({'result': [{'value': {'choices': ['class_B']}}]}),
            content_type='application/json',
        )
        assert response.status_code == 201

    with requests_mock.Mocker(real_http=True) as m:
        m.register_uri('POST', webhook.url)
        response = business_client.post(
            f'/api/dm/actions?id=delete_tasks_annotations&project={configured_project.id}',
            data=json.dumps(
                {
                    'project': str(configured_project.id),
                    'selectedItems': {'all': True},
                }
            ),
            content_type='application/json',
        )

    assert response.status_code == 200
    assert len(list(filter(lambda x: x.url == webhook.url, m.request_history))) == 1

    r = list(filter(lambda x: x.url == webhook.url, m.request_history))[0]
    assert r.json()['action'] == WebhookAction.ANNOTATIONS_DELETED


# ACTION: DELETE TASKS
@pytest.mark.django_db
def test_webhooks_for_action_delete_tasks_annotations(configured_project, business_client, organization_webhook):
    webhook = organization_webhook
    with requests_mock.Mocker(real_http=True) as m:
        m.register_uri('POST', webhook.url)
        response = business_client.post(
            f'/api/dm/actions?id=delete_tasks&project={configured_project.id}',
            data=json.dumps(
                {
                    'project': str(configured_project.id),
                    'selectedItems': {'all': True},
                }
            ),
            content_type='application/json',
        )
    assert response.status_code == 200
    assert len(list(filter(lambda x: x.url == webhook.url, m.request_history))) == 1

    r = list(filter(lambda x: x.url == webhook.url, m.request_history))[0]
    assert r.json()['action'] == WebhookAction.TASKS_DELETED


# CREATE TASKS FROM STORAGES
@pytest.mark.django_db
def test_webhooks_for_tasks_from_storages(configured_project, business_client, organization_webhook):
    webhook = organization_webhook
    # CREATE
    with requests_mock.Mocker(real_http=True) as m:
        m.register_uri('POST', webhook.url)
        add_url = '/api/storages/s3'
        payload = {
            'bucket': 'pytest-s3-images',
            'project': configured_project.id,
            'title': 'Testing S3 storage (bucket from conftest.py)',
            'use_blob_urls': True,
            'presign_ttl': 3600,
        }
        add_response = business_client.post(add_url, data=json.dumps(payload), content_type='application/json')
        storage_pk = add_response.json()['id']

        # Sync S3 Storage
        sync_url = f'/api/storages/s3/{storage_pk}/sync'
        business_client.post(sync_url)
    # assert response.status_code == 201
    assert len(list(filter(lambda x: x.url == webhook.url, m.request_history))) == 1

    r = list(filter(lambda x: x.url == webhook.url, m.request_history))[0]
    assert r.json()['action'] == WebhookAction.TASKS_CREATED
    assert 'tasks' in r.json()
    assert 'project' in r.json()


@pytest.mark.django_db
def test_start_training_webhook(setup_project_dialog, ml_start_training_webhook, business_client):
    """
    1. Setup: The test uses the project_webhook fixture, which assumes that a webhook
    is already configured for the project.
    2. Mocking the POST Request: The requests_mock.Mocker is used to mock
    the POST request to the webhook URL. This is where you expect the START_TRAINING action to be sent.
    3. Making the Request: The test makes a POST request to the /api/ml/{id}/train endpoint.

    Assertions:
        - The response status code is checked to ensure the request was successful.
        - It verifies that exactly one request was made to the webhook URL.
        - It checks that the request method was POST.
        - The request URL and the JSON payload are validated against expected values.
    """
    from ml.models import MLBackend

    webhook = ml_start_training_webhook
    project = webhook.project
    ml = MLBackend.objects.create(project=project, url='http://0.0.0.0:9090')

    # Mock the POST request to the ML backend train endpoint
    with requests_mock.Mocker(real_http=True) as m:
        m.register_uri('POST', webhook.url)
        response = business_client.post(
            f'/api/ml/{ml.id}/train',
            data=json.dumps({'action': 'START_TRAINING'}),
            content_type='application/json',
        )

    assert response.status_code == 200
    request_history = m.request_history
    assert len(request_history) == 1
    assert request_history[0].method == 'POST'
    assert request_history[0].url == webhook.url
    assert 'project' in request_history[0].json()
    assert request_history[0].json()['action'] == 'START_TRAINING'
</file>

<file path="label_studio/tests/__init__.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""

try:
    from requests._internal_utils import HEADER_VALIDATORS
    from tavern.util.formatted_str import FormattedString

    HEADER_VALIDATORS[FormattedString] = HEADER_VALIDATORS[str]
except ImportError:
    print('\n Your requests version is under 2.28 and it does not support HEADER_VALIDATORS. \n')
</file>

<file path="label_studio/tests/conftest.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import logging
import os
import re
import shutil
import tempfile
from copy import deepcopy
from datetime import datetime, timedelta
from pathlib import Path
from types import SimpleNamespace
from unittest.mock import MagicMock

import boto3
import mock
import pytest
import requests_mock
import ujson as json
from botocore.exceptions import ClientError
from django.conf import settings
from freezegun import freeze_time
from moto import mock_s3
from organizations.models import Organization
from projects.models import Project
from tasks.models import Task
from users.models import User

from label_studio.core.utils.params import get_env

# if we haven't this package, pytest.ini::env doesn't work
try:
    import pytest_env.plugin  # noqa: F401
except ImportError:
    print('\n\n !!! Please, pip install pytest-env \n\n')
    exit(-100)

from label_studio.tests.sdk.fixtures import *  # noqa: F403

from .utils import (
    azure_client_mock,
    create_business,
    gcs_client_mock,
    import_from_url_mock,
    make_project,
    ml_backend_mock,
    redis_client_mock,
    register_ml_backend_mock,
    signin,
)

boto3.set_stream_logger('botocore.credentials', logging.DEBUG)


@pytest.fixture(autouse=False)
def enable_csrf():
    settings.USE_ENFORCE_CSRF_CHECKS = True


@pytest.fixture(autouse=False)
def label_stream_history_limit():
    settings.LABEL_STREAM_HISTORY_LIMIT = 1


@pytest.fixture(autouse=True)
def disable_sentry():
    settings.SENTRY_RATE = 0
    settings.SENTRY_DSN = None


@pytest.fixture()
def debug_modal_exceptions_false(settings):
    settings.DEBUG_MODAL_EXCEPTIONS = False


@pytest.fixture(scope='function')
def enable_sentry():
    settings.SENTRY_RATE = 0
    # it's disabled key, but this is correct
    settings.SENTRY_DSN = 'https://44f7a50de5ab425ca6bc406ef69b2122@o227124.ingest.sentry.io/5820521'


@pytest.fixture(scope='function')
def aws_credentials():
    """Mocked AWS Credentials for moto."""
    os.environ['AWS_ACCESS_KEY_ID'] = 'testing'
    os.environ['AWS_SECRET_ACCESS_KEY'] = 'testing'
    os.environ['AWS_SECURITY_TOKEN'] = 'testing'
    os.environ['AWS_SESSION_TOKEN'] = 'testing'


@pytest.fixture(autouse=True)
def azure_credentials():
    """Mocked Azure credentials"""
    os.environ['AZURE_BLOB_ACCOUNT_NAME'] = 'testing'
    os.environ['AZURE_BLOB_ACCOUNT_KEY'] = 'testing'


@pytest.fixture(scope='function')
def s3(aws_credentials):
    with mock_s3():
        yield boto3.client('s3', region_name='us-east-1')


@pytest.fixture(autouse=True)
def s3_with_images(s3):
    """
    Bucket structure:
    s3://pytest-s3-images/image1.jpg
    s3://pytest-s3-images/subdir/image1.jpg
    s3://pytest-s3-images/subdir/image2.jpg
    """
    bucket_name = 'pytest-s3-images'
    s3.create_bucket(Bucket=bucket_name)
    s3.put_object(Bucket=bucket_name, Key='image1.jpg', Body='123')
    s3.put_object(Bucket=bucket_name, Key='subdir/image1.jpg', Body='456')
    s3.put_object(Bucket=bucket_name, Key='subdir/image2.jpg', Body='789')
    s3.put_object(Bucket=bucket_name, Key='subdir/another/image2.jpg', Body='0ab')
    yield s3


def s3_remove_bucket():
    """
    Remove pytest-s3-images
    """
    bucket_name = 'pytest-s3-images'
    _s3 = boto3.client('s3', region_name='us-east-1')
    _s3.delete_object(Bucket=bucket_name, Key='image1.jpg')
    _s3.delete_object(Bucket=bucket_name, Key='subdir/image1.jpg')
    _s3.delete_object(Bucket=bucket_name, Key='subdir/image2.jpg')
    _s3.delete_object(Bucket=bucket_name, Key='subdir/another/image2.jpg')
    _s3.delete_bucket(Bucket=bucket_name)
    return ''


@pytest.fixture(autouse=True)
def s3_with_jsons(s3):
    bucket_name = 'pytest-s3-jsons'
    s3.create_bucket(Bucket=bucket_name)
    s3.put_object(Bucket=bucket_name, Key='test.json', Body=json.dumps({'image_url': 'http://ggg.com/image.jpg'}))
    yield s3


@pytest.fixture(autouse=True)
def s3_with_hypertext_s3_links(s3):
    bucket_name = 'pytest-s3-jsons-hypertext'
    s3.create_bucket(Bucket=bucket_name)
    s3.put_object(
        Bucket=bucket_name,
        Key='test.json',
        Body=json.dumps(
            {'text': '<a href="s3://pytest-s3-jsons-hypertext/file with /spaces and\' / \' / quotes.jpg"/>'}
        ),
    )
    yield s3


@pytest.fixture(autouse=True)
def s3_with_partially_encoded_s3_links(s3):
    bucket_name = 'pytest-s3-json-partially-encoded'
    s3.create_bucket(Bucket=bucket_name)
    s3.put_object(
        Bucket=bucket_name,
        Key='test.json',
        Body=json.dumps(
            {
                'text': '<a href="s3://pytest-s3-json-partially-encoded/file with /spaces and\' / \' / %2Bquotes%3D.jpg"/>'
            }
        ),
    )
    yield s3


@pytest.fixture(autouse=True)
def s3_with_unexisted_links(s3):
    bucket_name = 'pytest-s3-jsons-unexisted_links'
    s3.create_bucket(Bucket=bucket_name)
    s3.put_object(Bucket=bucket_name, Key='some-existed-image.jpg', Body='qwerty')
    yield s3


@pytest.fixture(autouse=True)
def s3_export_bucket(s3):
    bucket_name = 'pytest-export-s3-bucket'
    s3.create_bucket(Bucket=bucket_name)
    yield s3


@pytest.fixture(autouse=True)
def s3_export_bucket_sse(s3):
    bucket_name = 'pytest-export-s3-bucket-with-sse'
    s3.create_bucket(Bucket=bucket_name)

    # Set the bucket policy
    policy = {
        'Version': '2012-10-17',
        'Statement': [
            {
                'Effect': 'Deny',
                'Principal': '*',
                'Action': 's3:PutObject',
                'Resource': [f'arn:aws:s3:::{bucket_name}', f'arn:aws:s3:::{bucket_name}/*'],
                'Condition': {'StringNotEquals': {'s3:x-amz-server-side-encryption': 'AES256'}},
            },
            {
                'Effect': 'Deny',
                'Principal': '*',
                'Action': 's3:PutObject',
                'Resource': [f'arn:aws:s3:::{bucket_name}', f'arn:aws:s3:::{bucket_name}/*'],
                'Condition': {'Null': {'s3:x-amz-server-side-encryption': 'true'}},
            },
            {
                'Effect': 'Deny',
                'Principal': '*',
                'Action': 's3:*',
                'Resource': [f'arn:aws:s3:::{bucket_name}', f'arn:aws:s3:::{bucket_name}/*'],
                'Condition': {'Bool': {'aws:SecureTransport': 'false'}},
            },
        ],
    }

    s3.put_bucket_policy(Bucket=bucket_name, Policy=json.dumps(policy))

    yield s3


@pytest.fixture(autouse=True)
def s3_export_bucket_kms(s3):
    bucket_name = 'pytest-export-s3-bucket-with-kms'
    s3.create_bucket(Bucket=bucket_name)

    # Set the bucket policy
    policy = {
        'Version': '2012-10-17',
        'Statement': [
            {
                'Effect': 'Deny',
                'Principal': '*',
                'Action': 's3:PutObject',
                'Resource': [f'arn:aws:s3:::{bucket_name}', f'arn:aws:s3:::{bucket_name}/*'],
                'Condition': {'StringNotEquals': {'s3:x-amz-server-side-encryption': 'aws:kms'}},
            },
            {
                'Effect': 'Deny',
                'Principal': '*',
                'Action': 's3:PutObject',
                'Resource': [f'arn:aws:s3:::{bucket_name}', f'arn:aws:s3:::{bucket_name}/*'],
                'Condition': {'Null': {'s3:x-amz-server-side-encryption': 'true'}},
            },
            {
                'Effect': 'Deny',
                'Principal': '*',
                'Action': 's3:*',
                'Resource': [f'arn:aws:s3:::{bucket_name}', f'arn:aws:s3:::{bucket_name}/*'],
                'Condition': {'Bool': {'aws:SecureTransport': 'false'}},
            },
        ],
    }

    s3.put_bucket_policy(Bucket=bucket_name, Policy=json.dumps(policy))

    yield s3


def mock_put(*args, **kwargs):
    client_error = ClientError(
        error_response={'Error': {'Code': 'AccessDenied', 'Message': 'Access Denied'}}, operation_name='PutObject'
    )
    if kwargs['ServerSideEncryption'] == 'AES256':
        if 'ServerSideEncryption' not in kwargs:
            raise client_error
    elif kwargs['ServerSideEncryption'] == 'aws:kms':
        if 'ServerSideEncryption' not in kwargs or 'SSEKMSKeyId' not in kwargs:
            raise client_error

    else:
        raise client_error


@pytest.fixture()
def mock_s3_resource_aes(mocker):
    mock_object = MagicMock()
    mock_object.put = mock_put

    mock_object_constructor = MagicMock()
    mock_object_constructor.return_value = mock_object

    mock_s3_resource = MagicMock()
    mock_s3_resource.Object = mock_object_constructor

    # Patch boto3.Session.resource to return the mock s3 resource
    mocker.patch('boto3.Session.resource', return_value=mock_s3_resource)


@pytest.fixture()
def mock_s3_resource_kms(mocker):
    mock_object = MagicMock()
    mock_object.put = mock_put

    mock_object_constructor = MagicMock()
    mock_object_constructor.return_value = mock_object

    mock_s3_resource = MagicMock()
    mock_s3_resource.Object = mock_object_constructor

    # Patch boto3.Session.resource to return the mock s3 resource
    mocker.patch('boto3.Session.resource', return_value=mock_s3_resource)


@pytest.fixture(autouse=True)
def gcs_client():
    with gcs_client_mock():
        yield


@pytest.fixture(autouse=True)
def azure_client():
    with azure_client_mock():
        yield


@pytest.fixture(autouse=True)
def redis_client():
    with redis_client_mock():
        yield


@pytest.fixture
def ml_backend_for_test_predict(ml_backend):
    # ML backend with single prediction per task
    register_ml_backend_mock(
        ml_backend,
        url='http://test.ml.backend.for.sdk.com:9092',
        predictions={
            'results': [
                {
                    'model_version': 'ModelSingle',
                    'score': 0.1,
                    'result': [
                        {'from_name': 'label', 'to_name': 'text', 'type': 'choices', 'value': {'choices': ['Single']}}
                    ],
                },
            ]
        },
    )
    # ML backend with multiple predictions per task
    register_ml_backend_mock(
        ml_backend,
        url='http://test.ml.backend.for.sdk.com:9093',
        predictions={
            'results': [
                [
                    {
                        'model_version': 'ModelA',
                        'score': 0.2,
                        'result': [
                            {
                                'from_name': 'label',
                                'to_name': 'text',
                                'type': 'choices',
                                'value': {'choices': ['label_A']},
                            }
                        ],
                    },
                    {
                        'model_version': 'ModelB',
                        'score': 0.3,
                        'result': [
                            {
                                'from_name': 'label',
                                'to_name': 'text',
                                'type': 'choices',
                                'value': {'choices': ['label_B']},
                            }
                        ],
                    },
                ]
            ]
        },
    )
    yield ml_backend


@pytest.fixture(autouse=True)
def ml_backend():
    with ml_backend_mock() as m:
        yield m


@pytest.fixture(name='import_from_url')
def import_from_url():
    with import_from_url_mock() as m:
        yield m


@pytest.fixture(autouse=True)
def ml_backend_1(ml_backend):
    register_ml_backend_mock(
        ml_backend, url='https://test.heartex.mlbackend.com:9090', setup_model_version='Fri Feb 19 17:10:44 2021'
    )
    register_ml_backend_mock(ml_backend, url='https://test.heartex.mlbackend.com:9091', health_connect_timeout=True)
    register_ml_backend_mock(ml_backend, url='http://localhost:8999', predictions={'results': []})
    yield ml_backend


def pytest_configure():
    for q in settings.RQ_QUEUES.values():
        q['ASYNC'] = False


class URLS:
    """This class keeps urls with api"""

    def __init__(self):
        self.project_create = '/api/projects/'
        self.task_bulk = None

    def set_project(self, pk):
        self.task_bulk = f'/api/projects/{pk}/tasks/bulk/'
        self.plots = f'/projects/{pk}/plots'


def project_ranker():
    label = """<View>
         <HyperText name="hypertext_markup" value="$markup"></HyperText>
         <List name="ranker" value="$replies" elementValue="$text" elementTag="Text"
               ranked="true" sortedHighlightColor="#fcfff5"></List>
        </View>"""
    return {'label_config': label, 'title': 'test'}


def project_dialog():
    """Simple project with dialog configs

    :return: config of project with task
    """
    label = """<View>
      <TextEditor>
        <Text name="dialog" value="$dialog"></Text>
        <Header value="Your answer is:"></Header>
        <TextArea name="answer"></TextArea>
      </TextEditor>
    </View>"""

    return {'label_config': label, 'title': 'test'}


def project_choices():
    label = """<View>
    <Choices name="animals" toName="xxx" choice="single-radio">
      <Choice value="Cat"></Choice>
      <Choice value="Dog"></Choice>
      <Choice value="Opossum"></Choice>
      <Choice value="Mouse"></Choice>
      <Choice value="Human"/>
    </Choices>

    <Choices name="things" toName="xxx" choice="single-radio">
      <Choice value="Chair"></Choice>
      <Choice value="Car"></Choice>
      <Choice value="Lamp"></Choice>
      <Choice value="Guitar"></Choice>
      <Choice value="None"/>
    </Choices>

    <Image name="xxx" value="$image"></Image>
    </View>"""
    return {'label_config': label, 'title': 'test'}


def setup_project(client, project_template, do_auth=True):
    """Create new test@gmail.com user, login via client, create test project.
    Project configs are thrown over params and automatically grabs from functions names started with 'project_'

    :param client: fixture with http client (from pytest-django package) and simulation of http server
    :param project_template: dict with project config
    :param do_auth: make authorization for creating user
    """
    client = deepcopy(client)
    email = 'test@gmail.com'
    password = 'test'
    urls = URLS()
    project_config = project_template()

    # we work in empty database, so let's create business user and login
    user = User.objects.create(email=email)
    user.set_password(password)  # set password without hash

    create_business(user)
    org = Organization.create_organization(created_by=user, title=user.first_name)
    user.active_organization = org
    user.save()

    if do_auth:

        assert signin(client, email, password).status_code == 302
        # create project
        with requests_mock.Mocker() as m:
            m.register_uri('POST', re.compile(r'ml\.heartex\.net/\d+/validate'), text=json.dumps({'status': 'ok'}))
            m.register_uri('GET', re.compile(r'ml\.heartex\.net/\d+/health'), text=json.dumps({'status': 'UP'}))
            r = client.post(urls.project_create, data=project_config)
            print('Project create with status code:', r.status_code)
            assert r.status_code == 201, 'Create project result should be redirect to the next page'

        # get project id and prepare url
        project = Project.objects.filter(title=project_config['title']).first()
        urls.set_project(project.pk)
        print('Project id:', project.id)

        client.project = project

    client.user = user
    client.urls = urls
    client.project_config = project_config
    client.org = org
    return client


@pytest.fixture
def setup_project_dialog(client):
    return setup_project(client, project_dialog)


@pytest.fixture
def setup_project_for_token(client):
    return setup_project(client, project_dialog, do_auth=False)


@pytest.fixture
def setup_project_ranker(client):
    return setup_project(client, project_ranker)


@pytest.fixture
def setup_project_choices(client):
    return setup_project(client, project_choices)


@pytest.fixture()
def contextlog_test_config(settings):
    """
    Configure settings for contextlog tests in CI.
    Be sure that responses is activated in any testcase where this fixture is used.
    """

    settings.COLLECT_ANALYTICS = True
    settings.CONTEXTLOG_SYNC = True
    settings.TEST_ENVIRONMENT = False
    settings.DEBUG_CONTEXTLOG = False


@pytest.fixture
def business_client(client):
    # we work in empty database, so let's create business user and login
    client = deepcopy(client)
    email = 'business@pytest.net'
    password = 'pytest'
    user = User.objects.create(email=email)
    user.set_password(password)  # set password without hash
    business = create_business(user)

    user.save()
    org = Organization.create_organization(created_by=user, title=user.first_name)
    client.business = business if business else SimpleNamespace(admin=user)
    client.team = None if business else SimpleNamespace(id=1)
    client.admin = user
    client.annotator = user
    client.user = user
    client.api_key = user.reset_token().key
    client.organization = org

    if signin(client, email, password).status_code != 302:
        print(f'User {user} failed to login!')
    return client


@pytest.fixture
def annotator_client(client):
    # we work in empty database, so let's create business user and login
    client = deepcopy(client)
    email = 'annotator@pytest.net'
    password = 'pytest'
    user = User.objects.create(email=email)
    user.set_password(password)  # set password without hash
    user.save()
    create_business(user)
    Organization.create_organization(created_by=user, title=user.first_name)
    if signin(client, email, password).status_code != 302:
        print(f'User {user} failed to login!')
    client.user = user
    client.annotator = user
    return client


@pytest.fixture
def annotator2_client(client):
    # we work in empty database, so let's create business user and login
    client = deepcopy(client)
    email = 'annotator2@pytest.net'
    password = 'pytest'
    user = User.objects.create(email=email)
    user.set_password(password)  # set password without hash
    user.save()
    create_business(user)
    Organization.create_organization(created_by=user, title=user.first_name)
    if signin(client, email, password).status_code != 302:
        print(f'User {user} failed to login!')
    client.user = user
    client.annotator = user
    return client


@pytest.fixture(params=['business', 'annotator'])
def any_client(request, business_client, annotator_client):
    if request.param == 'business':
        return business_client
    elif request.param == 'annotator':
        return annotator_client


@pytest.fixture
def configured_project(business_client, annotator_client):
    _project_for_text_choices_onto_A_B_classes = dict(
        title='Test',
        label_config="""
            <View>
              <Text name="meta_info" value="$meta_info"></Text>
              <Text name="text" value="$text"></Text>
              <Choices name="text_class" toName="text" choice="single">
                <Choice value="class_A"></Choice>
                <Choice value="class_B"></Choice>
              </Choices>
            </View>""",
    )
    _2_tasks_with_textA_and_textB = [
        {'meta_info': 'meta info A', 'text': 'text A'},
        {'meta_info': 'meta info B', 'text': 'text B'},
    ]

    # get user to be owner
    users = User.objects.filter(email='business@pytest.net')  # TODO(nik): how to get proper email for business here?
    project = make_project(_project_for_text_choices_onto_A_B_classes, users[0])

    assert project.ml_backends.first().url == 'http://localhost:8999'

    Task.objects.bulk_create([Task(data=task, project=project) for task in _2_tasks_with_textA_and_textB])
    return project


@pytest.fixture(name='django_live_url')
def get_server_url(live_server):
    yield live_server.url


@pytest.fixture(name='ff_front_dev_1682_model_version_dropdown_070622_short_off', autouse=True)
def ff_front_dev_1682_model_version_dropdown_070622_short_off():
    from core.feature_flags import flag_set

    def fake_flag_set(*args, **kwargs):
        if args[0] == 'ff_front_dev_1682_model_version_dropdown_070622_short':
            return False
        return flag_set(*args, **kwargs)

    with mock.patch('tasks.serializers.flag_set', wraps=fake_flag_set):
        yield


@pytest.fixture(name='async_import_off', autouse=True)
def async_import_off():
    from core.feature_flags import flag_set

    def fake_flag_set(*args, **kwargs):
        if args[0] == 'fflag_feat_all_lsdv_4915_async_task_import_13042023_short':
            return False
        return flag_set(*args, **kwargs)

    with mock.patch('data_import.api.flag_set', wraps=fake_flag_set):
        yield


@pytest.fixture(name='fflag_fix_all_lsdv_4711_cors_errors_accessing_task_data_short_on')
def fflag_fix_all_lsdv_4711_cors_errors_accessing_task_data_short_on():
    from core.feature_flags import flag_set

    def fake_flag_set(*args, **kwargs):
        if args[0] == 'fflag_fix_all_lsdv_4711_cors_errors_accessing_task_data_short':
            return True
        return flag_set(*args, **kwargs)

    with mock.patch('tasks.models.flag_set', wraps=fake_flag_set):
        yield


@pytest.fixture(name='fflag_fix_all_lsdv_4711_cors_errors_accessing_task_data_short_off')
def fflag_fix_all_lsdv_4711_cors_errors_accessing_task_data_short_off():
    from core.feature_flags import flag_set

    def fake_flag_set(*args, **kwargs):
        if args[0] == 'fflag_fix_all_lsdv_4711_cors_errors_accessing_task_data_short':
            return False
        return flag_set(*args, **kwargs)

    with mock.patch('tasks.models.flag_set', wraps=fake_flag_set):
        yield


@pytest.fixture(name='fflag_feat_back_lsdv_3958_server_side_encryption_for_target_storage_short_on')
def fflag_feat_back_lsdv_3958_server_side_encryption_for_target_storage_short_on():
    from core.feature_flags import flag_set

    def fake_flag_set(*args, **kwargs):
        if args[0] == 'fflag_feat_back_lsdv_3958_server_side_encryption_for_target_storage_short':
            return True
        return flag_set(*args, **kwargs)

    with mock.patch('io_storages.s3.models.flag_set', wraps=fake_flag_set):
        yield


@pytest.fixture(name='fflag_fix_all_lsdv_4813_async_export_conversion_22032023_short_on')
def fflag_fix_all_lsdv_4813_async_export_conversion_22032023_short_on():
    from core.feature_flags import flag_set

    def fake_flag_set(*args, **kwargs):
        if args[0] == 'fflag_fix_all_lsdv_4813_async_export_conversion_22032023_short':
            return True
        return flag_set(*args, **kwargs)

    with mock.patch('data_export.api.flag_set', wraps=fake_flag_set):
        yield


@pytest.fixture(name='ff_back_dev_4664_remove_storage_file_on_export_delete_29032023_short_on')
def ff_back_dev_4664_remove_storage_file_on_export_delete_29032023_short_on():
    from core.feature_flags import flag_set

    def fake_flag_set(*args, **kwargs):
        if args[0] == 'ff_back_dev_4664_remove_storage_file_on_export_delete_29032023_short':
            return True
        return flag_set(*args, **kwargs)

    with mock.patch('data_export.api.flag_set', wraps=fake_flag_set):
        yield


@pytest.fixture(name='local_files_storage')
def local_files_storage(settings):
    settings.LOCAL_FILES_SERVING_ENABLED = True
    tempdir = Path(tempfile.gettempdir()) / Path('files')
    subdir = tempdir / Path('subdir')
    os.makedirs(str(subdir), exist_ok=True)
    test_image = Path(*'tests/test_suites/samples/test_image.png'.split('/'))
    shutil.copyfile(str(test_image), str(tempdir / Path('test_image1.png')))
    shutil.copyfile(str(test_image), str(subdir / Path('test_image2.png')))


@pytest.fixture(name='local_files_document_root_tempdir')
def local_files_document_root_tempdir(settings):
    tempdir = Path(tempfile.gettempdir())
    settings.LOCAL_FILES_DOCUMENT_ROOT = tempdir.root


@pytest.fixture(name='local_files_document_root_subdir')
def local_files_document_root_subdir(settings):
    tempdir = Path(tempfile.gettempdir()) / Path('files')
    settings.LOCAL_FILES_DOCUMENT_ROOT = str(tempdir)


@pytest.fixture(name='testing_session_timeouts')
def set_testing_session_timeouts(settings):
    settings.MAX_SESSION_AGE = int(get_env('MAX_SESSION_AGE', timedelta(seconds=6).total_seconds()))
    settings.MAX_TIME_BETWEEN_ACTIVITY = int(
        get_env('MAX_TIME_BETWEEN_ACTIVITY', timedelta(seconds=2).total_seconds())
    )


@pytest.fixture
def mock_ml_auto_update(name='mock_ml_auto_update'):
    url = 'http://localhost:9090'
    with requests_mock.Mocker(real_http=True) as m:
        m.register_uri(
            'POST',
            f'{url}/setup',
            [
                {'json': {'model_version': 'version1', 'status': 'ok'}, 'status_code': 200},
                {'json': {'model_version': 'version1', 'status': 'ok'}, 'status_code': 200},
                {'json': {'model_version': 'version1', 'status': 'ok'}, 'status_code': 200},
                {'json': {'model_version': 'version2', 'status': 'ok'}, 'status_code': 200},
                {'json': {'model_version': 'version3', 'status': 'ok'}, 'status_code': 200},
            ],
        )
        m.get(f'{url}/health', text=json.dumps({'status': 'UP'}))
        yield m


@pytest.fixture(name='mock_ml_backend_auto_update_disabled')
def mock_ml_backend_auto_update_disabled():
    with ml_backend_mock(setup_model_version='version1') as m:
        m.register_uri(
            'GET',
            'http://localhost:9090/setup',
            [
                {'json': {'model_version': '', 'status': 'ok'}, 'status_code': 200},
                {'json': {'model_version': '2', 'status': 'ok'}, 'status_code': 200},
            ],
        )
        yield m


freezer = None
now = None


@pytest.fixture(name='freeze_clock')
def freeze_clock():
    global freezer
    global now

    now = datetime.now()
    freezer = freeze_time(now)
    freezer.start()

    yield

    # teardown steps after yield

    freezer.stop()
    freezer = None
    now = None


def tick_clock(_, seconds: int = 1) -> None:
    global freezer
    global now
    freezer.stop()
    now += timedelta(seconds=seconds)
    freezer = freeze_time(now)
    freezer.start()


def freeze_datetime(response, utc_time: str) -> None:
    global freezer
    freezer.stop()
    freezer = freeze_time(utc_time)
    freezer.start()


def pytest_collection_modifyitems(config, items):
    # This function is called by pytest after the collection of tests has been completed to modify their order
    # it is being used as a workaround for the fact the kms and aes mocks resist teardown and cause other test failures

    mock_tests = []
    other_tests = []
    for item in items:
        if 'mock_s3_resource_kms' in item.fixturenames or 'mock_s3_resource_aes' in item.fixturenames:
            mock_tests.append(item)
        else:
            other_tests.append(item)

    items[:] = other_tests + mock_tests
</file>

<file path="label_studio/tests/test_annotations_result_count.py">
from django.test import TestCase
from projects.models import Project
from tasks.models import Annotation, Task


class AnnotationResultCountTests(TestCase):
    def setUp(self):
        self.project = Project.objects.create()
        self.task = Task.objects.create(project=self.project, data={'text': 'This is task'})

    def test_empty_result_gives_zero_count(self):
        """Test that an empty result gives a count of 0"""
        annotation = Annotation.objects.create(task=self.task, project=self.project, result=[])
        self.assertEqual(annotation.result_count, 0)

    def test_none_result_gives_zero_count(self):
        """Test that None result gives a count of 0"""
        annotation = Annotation.objects.create(task=self.task, project=self.project, result=None)
        self.assertEqual(annotation.result_count, 0)

    def test_unique_ids_counted_correctly(self):
        """Test that only unique IDs are counted"""
        annotation = Annotation.objects.create(
            task=self.task,
            project=self.project,
            result=[
                {'id': '1', 'value': 'test1'},
                {'id': '2', 'value': 'test2'},
                {'id': '1', 'value': 'test3'},  # Duplicate ID
            ],
        )
        self.assertEqual(annotation.result_count, 2)  # Should only count unique IDs

    def test_missing_ids_skipped(self):
        """Test that results without IDs are handled gracefully"""
        annotation = Annotation.objects.create(
            task=self.task,
            project=self.project,
            result=[{'id': '1', 'value': 'test1'}, {'value': 'test2'}, {'id': '3', 'value': 'test3'}],  # Missing ID
        )
        self.assertEqual(annotation.result_count, 3)

    def test_update_changes_count(self):
        """Test that updating the result updates the count"""
        annotation = Annotation.objects.create(
            task=self.task, project=self.project, result=[{'id': '1', 'value': 'test1'}]
        )
        self.assertEqual(annotation.result_count, 1)

        annotation.result = [{'id': '1', 'value': 'test1'}, {'id': '2', 'value': 'test2'}]
        annotation.save()
        self.assertEqual(annotation.result_count, 2)
</file>

<file path="label_studio/tests/test_annotations.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import json

import pytest
import requests_mock
from django.apps import apps
from django.urls import reverse
from projects.models import Project
from tasks.models import Annotation, Task

from .utils import _client_is_annotator, invite_client_to_project


@pytest.fixture
def configured_project_min_annotations_1(configured_project):
    p = Project.objects.get(id=configured_project.id)
    p.min_annotations_to_start_training = 1
    # p.agreement_method = p.SINGLE
    p.save()
    return p


@pytest.mark.django_db
@pytest.mark.parametrize(
    'result, logtext, ml_upload_called',
    [
        (
            json.dumps(
                [
                    {
                        'from_name': 'text_class',
                        'to_name': 'text',
                        'type': 'labels',
                        'value': {'labels': ['class_A'], 'start': 0, 'end': 1},
                    }
                ]
            ),
            None,
            True,
        ),
        (json.dumps([]), None, True),
    ],
)
def test_create_annotation(
    caplog, any_client, configured_project_min_annotations_1, result, logtext, ml_upload_called
):
    task = Task.objects.first()
    if _client_is_annotator(any_client):
        assert invite_client_to_project(any_client, task.project).status_code == 200
    with requests_mock.Mocker() as m:
        m.post('http://localhost:8999/train')
        m.post('http://localhost:8999/webhook')
        annotation = {'result': result, 'task': task.id, 'lead_time': 2.54}
        r = any_client.post(f'/api/tasks/{task.id}/annotations/', data=annotation)
        # check that submitted VALID data for task_annotation
        # makes task labeled
        task.refresh_from_db()
        assert task.is_labeled is True
        assert r.status_code == 201
        annotation = Annotation.objects.all()
        assert annotation.count() == 1
        annotation = annotation.first()
        assert annotation.task.id == task.id
        # annotator client
        if hasattr(any_client, 'annotator') and any_client.annotator is not None:
            assert annotation.completed_by.id == any_client.user.id
            assert annotation.updated_by.id == any_client.user.id
        # business client
        else:
            assert annotation.completed_by.id == any_client.business.admin.id
            assert annotation.updated_by.id == any_client.business.admin.id

        if apps.is_installed('businesses'):
            assert annotation.task.accuracy == 1.0

        if logtext:
            assert logtext in caplog.text


@pytest.mark.django_db
def test_create_annotation_with_ground_truth(caplog, any_client, configured_project_min_annotations_1):

    task = Task.objects.first()
    client_is_annotator = _client_is_annotator(any_client)
    if client_is_annotator:
        assert invite_client_to_project(any_client, task.project).status_code == 200

    webhook_called = not client_is_annotator
    ground_truth = {
        'task': task.id,
        'result': json.dumps(
            [{'from_name': 'text_class', 'to_name': 'text', 'value': {'labels': ['class_A'], 'start': 0, 'end': 1}}]
        ),
        'ground_truth': True,
    }

    annotation = {
        'task': task.id,
        'result': json.dumps(
            [{'from_name': 'text_class', 'to_name': 'text', 'value': {'labels': ['class_B'], 'start': 0, 'end': 1}}]
        ),
    }

    with requests_mock.Mocker() as m:
        m.post('http://localhost:8999/webhook')
        m.post('http://localhost:8999/train')

        # ground_truth doesn't affect statistics & ML backend, webhook is called for admin accounts
        r = any_client.post('/api/tasks/{}/annotations/'.format(task.id), data=ground_truth)
        assert r.status_code == 201
        assert m.called == webhook_called

        # real annotation triggers uploading to ML backend and recalculating accuracy
        r = any_client.post('/api/tasks/{}/annotations/'.format(task.id), data=annotation)
        assert r.status_code == 201
        assert m.called
        task = Task.objects.get(id=task.id)
        assert task.annotations.count() == 2
        annotations = Annotation.objects.filter(task=task)
        for a in annotations:
            assert a.updated_by.id == any_client.user.id


@pytest.mark.django_db
def test_delete_annotation(business_client, configured_project):
    task = Task.objects.first()
    annotation = Annotation.objects.create(task=task, project=configured_project, result=[])
    assert task.annotations.count() == 1
    r = business_client.delete('/api/annotations/{}/'.format(annotation.id))
    assert r.status_code == 204
    assert task.annotations.count() == 0


@pytest.fixture
def annotations():
    task = Task.objects.first()
    return {
        'class_A': {
            'task': task.id,
            'result': json.dumps(
                [
                    {
                        'from_name': 'text_class',
                        'to_name': 'text',
                        'type': 'labels',
                        'value': {'labels': ['class_A'], 'start': 0, 'end': 10},
                    }
                ]
            ),
        },
        'class_B': {
            'task': task.id,
            'result': json.dumps(
                [
                    {
                        'from_name': 'text_class',
                        'to_name': 'text',
                        'type': 'labels',
                        'value': {'labels': ['class_B'], 'start': 0, 'end': 10},
                    }
                ]
            ),
        },
        'empty': {'task': task.id, 'result': json.dumps([])},
    }


@pytest.fixture
def project_with_max_annotations_2(configured_project):
    configured_project.maximum_annotations = 2
    # configured_project.agreement_method = Project.SINGLE
    configured_project.save()


@pytest.mark.parametrize(
    'annotations_sequence, accuracy, is_labeled',
    [
        ([], None, False),
        ([('class_A', 'business')], 1, False),
        ([('class_A', 'annotator')], 1, False),
        ([('class_A', 'business'), ('class_A', 'business')], 1, True),
        ([('class_A', 'business'), ('class_A', 'annotator')], 1, True),
        ([('class_A', 'annotator'), ('class_A', 'business')], 1, True),
        ([('class_A', 'business'), ('class_B', 'business')], 0.5, True),
        ([('class_A', 'business'), ('class_B', 'annotator')], 0.5, True),
        ([('class_A', 'annotator'), ('class_B', 'business')], 0.5, True),
        ([('empty', 'annotator'), ('empty', 'business')], 1, True),
        ([('class_A', 'annotator'), ('empty', 'business')], 0.5, True),
    ],
)
@pytest.mark.django_db
def test_accuracy(
    business_client,
    annotator_client,
    project_with_max_annotations_2,
    annotations,
    annotations_sequence,
    accuracy,
    is_labeled,
):
    client = {'business': business_client, 'annotator': annotator_client}
    task_id = next(iter(annotations.values()))['task']
    task = Task.objects.get(id=task_id)
    invite_client_to_project(annotator_client, task.project)

    for annotation_key, client_key in annotations_sequence:
        r = client[client_key].post(
            reverse('tasks:api:task-annotations', kwargs={'pk': task_id}), data=annotations[annotation_key]
        )
        assert r.status_code == 201
    task = Task.objects.get(id=task_id)
    assert task.is_labeled == is_labeled


@pytest.mark.django_db
def test_accuracy_on_delete(business_client, project_with_max_annotations_2, annotations):
    task_id = next(iter(annotations.values()))['task']
    for annotation in annotations.values():
        business_client.post(reverse('tasks:api:task-annotations', kwargs={'pk': task_id}), data=annotation)

    task = Task.objects.get(id=task_id)
    assert task.annotations.count() == len(annotations)
    assert task.is_labeled
    annotation_ids = [c.id for c in task.annotations.all()]
    r = business_client.delete(reverse('tasks:api-annotations:annotation-detail', kwargs={'pk': annotation_ids[0]}))
    assert r.status_code == 204
    task = Task.objects.get(id=task_id)
    assert task.is_labeled  # project.max_annotations = 2

    r = business_client.delete(reverse('tasks:api-annotations:annotation-detail', kwargs={'pk': annotation_ids[1]}))
    assert r.status_code == 204
    task = Task.objects.get(id=task_id)
    assert not task.is_labeled

    r = business_client.delete(reverse('tasks:api-annotations:annotation-detail', kwargs={'pk': annotation_ids[2]}))
    assert r.status_code == 204
    task = Task.objects.get(id=task_id)
    assert not task.is_labeled


# @pytest.mark.django_db
# def test_accuracy_on_delete(business_client, project_with_max_annotations_2, annotations):
#     task_id = next(iter(annotations.values()))['task']
#     for annotation in annotations.values():
#         business_client.post(reverse('tasks:api:task-annotations', kwargs={'pk': task_id}), data=annotation)
#
#     task = Task.objects.get(id=task_id)
#     assert task.annotations.count() == len(annotations)
#     if apps.is_installed('businesses'):
#         assert math.fabs(task.accuracy - 1 / 3) < 0.00001
#     assert task.is_labeled
#     annotation_ids = [c.id for c in task.annotations.all()]
#     r = business_client.delete(reverse('tasks:api-annotations:annotation-detail', kwargs={'pk': annotation_ids[0]}))
#     assert r.status_code == 204
#     task = Task.objects.get(id=task_id)
#     if apps.is_installed('businesses'):
#         assert task.accuracy == 0.5
#     assert task.is_labeled  # project.max_annotations = 2
#
#     r = business_client.delete(reverse('tasks:api-annotations:annotation-detail', kwargs={'pk': annotation_ids[1]}))
#     assert r.status_code == 204
#     task = Task.objects.get(id=task_id)
#     if apps.is_installed('businesses'):
#         assert task.accuracy == 1.0
#     assert not task.is_labeled
#
#     r = business_client.delete(reverse('tasks:api-annotations:annotation-detail', kwargs={'pk': annotation_ids[2]}))
#     assert r.status_code == 204
#     task = Task.objects.get(id=task_id)
#     if apps.is_installed('businesses'):
#         assert task.accuracy is None
#     assert not task.is_labeled
</file>

<file path="label_studio/tests/test_api.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import json
from unittest import mock

import pytest
import requests_mock
from projects.models import Project
from rest_framework.authtoken.models import Token
from rest_framework.test import APIClient
from tasks.models import Annotation

from .utils import ml_backend_mock


@pytest.fixture
def client_and_token(business_client):
    token = Token.objects.get(user=business_client.business.admin)
    client = APIClient()
    client.credentials(HTTP_AUTHORIZATION='Token ' + token.key)
    client.organization_pk = business_client.organization.pk
    return client, token


@pytest.fixture(params=['business_authorized', 'user_with_token'])
def any_api_client(request, client_and_token, business_client):
    client, token = client_and_token
    result = {'type': request.param, 'token': token}
    if request.param == 'business_authorized':
        result['client'] = business_client
    elif request.param == 'user_with_token':
        result['client'] = client
    return result


@pytest.mark.parametrize('use_x_api_key', [True, False])
@pytest.mark.parametrize(
    'payload, response, status_code',
    [
        # status OK
        (
            {
                'title': '111',
                'label_config': '<View><Text name="my_text" value="$text"/><Choices name="my_class" toName="my_text"><Choice value="pos"/><Choice value="neg"/></Choices></View>',
            },
            None,
            201,
        ),
        # invalid label config: unexisted toName
        (
            {
                'title': '111',
                'label_config': '<View><Text name="my_text" value="$text"/><Choices name="my_class" toName="unexisted"><Choice value="pos"/><Choice value="neg"/></Choices></View>',
            },
            {'label_config': ["toName=\"unexisted\" not found in names: ['my_class', 'my_text']"]},
            400,
        ),
        # invalid label config: missed toName
        (
            {
                'title': '111',
                'label_config': '<View><Text name="my_text" value="$text"/><Choices name="my_class"><Choice value="pos"/><Choice value="neg"/></Choices></View>',
            },
            {'label_config': ["Validation failed on : 'toName' is a required property"]},
            400,
        ),
        # empty label config
        ({'title': '111', 'label_config': None}, {'label_config': ['can only parse strings']}, 400),
        # <Choices> surrounded by <View> -> OK
        (
            {
                'title': '111',
                'label_config': '<View><Text name="my_text" value="$text"/><View className="non-root"><Choices name="my_class" toName="my_text"><Choice value="pos"/><Choice value="neg"/></Choices></View></View>',
            },
            None,
            201,
        ),
        # <Choices> with value attribute but without nested <Choice>
        # example from https://labelstud.io/templates/serp_ranking
        (
            {
                'title': '111',
                'label_config': """
                <View>
  
  <Header value="Search request" size="5"/> 
  <Text name="text" value="$text"/>
 
  <Header value="Generated responses" size="5"/> 
  <View className="dynamic_choices">
    <Choices name="dynamic_choices" toName="text" selection="checkbox" value="$options" layout="vertical" choice="multiple" allownested="true"/>
  </View>
  <View style="box-shadow: 2px 2px 5px #999; padding: 20px; margin-top: 1em; border-radius: 5px;">
    <Header value="Search Quality"/>
    <Rating name="relevance" toName="text"/>
  </View>
  <View style="box-shadow: 2px 2px 5px #999; padding: 15px 5px 10px 20px; margin-top: 1.5em; margin-bottom: 1.25em; border-radius: 5px; display: flex; align-items: center;">
    <Header value="Labeling Confidence" style="font-size: 1.25em"/>
    <View style="margin: 0 1em 0.5em 1.5em">
      <Choices name="confidence" toName="text" choice="single" showInLine="true">
        <Choice value="Low" html="&lt;img width='40' src='https://www.iconsdb.com/icons/preview/green/thumbs-up-xxl.png'/&gt;"/>
        <Choice value="High" html="&lt;img width='40' src='https://www.iconsdb.com/icons/preview/red/thumbs-down-xxl.png'/&gt;"/>
      </Choices>
    </View>
  </View>
  
  <Style>
  .searchresultsarea {
    margin-left: 10px;
    font-family: 'Arial';
  }
  .searchresult {
    margin-left: 8px;
  }
  .searchresult h2 {
    font-size: 19px;
    line-height: 18px;
    font-weight: normal;
    color: rgb(29, 1, 189);
    margin-bottom: 0px;
    margin-top: 25px;
  }
  .searchresult a {
    font-size: 14px;
    line-height: 14px;
    color: green;
    margin-bottom: 0px;
  }
  .searchresult button {
    font-size: 10px;
    line-height: 14px;
    color: green;
    margin-bottom: 0px;
    padding: 0px;
    border-width: 0px;
    background-color: white;
  }
  </Style>
</View>
                """,
            },
            None,
            201,
        ),
    ],
)
@pytest.mark.django_db
def test_create_project(client_and_token, payload, response, status_code, use_x_api_key):
    client, token = client_and_token

    if use_x_api_key:
        client.credentials(HTTP_X_API_KEY=token.key)

    payload['organization_pk'] = client.organization_pk
    with ml_backend_mock():
        r = client.post(
            '/api/projects/',
            data=json.dumps(payload),
            content_type='application/json',
            headers={'Authorization': f'Token {token}'},
        )
    assert r.status_code == status_code
    if response:
        response_data = r.json()
        if r.status_code == 400:
            assert response_data['validation_errors'] == response
        else:
            assert response_data == response


@pytest.mark.parametrize(
    'payload, response, status_code',
    [
        # status OK
        (
            {
                'label_config': '<View><Text name="my_text" value="$text"/><Choices name="my_class" toName="my_text"><Choice value="pos"/><Choice value="neg"/></Choices></View>'
            },
            None,
            200,
        ),
        # TODO: this should instead of next one, but "configured_project" fixture doesn't update project.summary with data columns
        # invalid column
        # (
        #     {"label_config": "<View><Text name=\"my_text\" value=\"$text\"/><Choices name=\"my_class\" toName=\"my_text\"><Choice value=\"pos\"/><Choice value=\"neg\"/></Choices></View>"},
        #
        #     {'label_config': ['These fields are not found in data: text']},
        #     400
        # ),
        # invalid label config: unexisted toName
        (
            {
                'label_config': '<View><Text name="my_text" value="$text"/><Choices name="my_class" toName="unexisted"><Choice value="pos"/><Choice value="neg"/></Choices></View>'
            },
            {'label_config': ["toName=\"unexisted\" not found in names: ['my_class', 'my_text']"]},
            400,
        ),
        # invalid label config: missed toName
        (
            {
                'label_config': '<View><Text name="my_text" value="$text"/><Choices name="my_class"><Choice value="pos"/><Choice value="neg"/></Choices></View>'
            },
            {'label_config': ["Validation failed on : 'toName' is a required property"]},
            400,
        ),
    ],
)
@pytest.mark.django_db
def test_patch_project(client_and_token, configured_project, payload, response, status_code):
    client, token = client_and_token
    payload['organization_pk'] = client.organization_pk
    r = client.patch(
        f'/api/projects/{configured_project.id}/',
        data=json.dumps(payload),
        content_type='application/json',
        headers={'Authorization': f'Token {token}'},
    )
    assert r.status_code == status_code
    if response:
        response_data = r.json()
        if r.status_code == 400:
            assert response_data['validation_errors'] == response
        else:
            assert response_data == response


@mock.patch('ml.serializers.validate_upload_url')
@pytest.mark.parametrize(
    'external_status_code, current_active_ml_backend_url, ml_backend_call_count',
    [
        (201, 'http://my.super.ai', 4),
    ],
)
@pytest.mark.django_db
def test_creating_activating_new_ml_backend(
    mock_validate_upload_url,
    client_and_token,
    configured_project,
    external_status_code,
    current_active_ml_backend_url,
    ml_backend_call_count,
    settings,
):
    # Turn off telemetry to avoid requests mock receiving requests from it, to
    # eliminate flakes. TODO(jo): consider implementing this more broadly in test.
    settings.COLLECT_ANALYTICS = False

    business_client, token = client_and_token
    with requests_mock.Mocker() as m:
        my_url = current_active_ml_backend_url
        m.post(f'{my_url}/setup', text=json.dumps({'model_version': 'Version from My Super AI'}))
        m.get(f'{my_url}/health', text=json.dumps({'status': 'UP'}))
        r = business_client.post(
            '/api/ml',
            data=json.dumps({'project': configured_project.id, 'title': 'My Super AI', 'url': my_url}),
            content_type='application/json',
            headers={'Authorization': f'Token {token}'},
        )

        assert r.status_code == external_status_code
        assert m.called
        assert m.call_count == ml_backend_call_count
        project = Project.objects.get(id=configured_project.id)
        all_urls = [m.url for m in project.ml_backends.all()]
        connected_ml = [url for url in all_urls if url == current_active_ml_backend_url]
        assert len(connected_ml) == 1, '\n'.join(all_urls)
        mock_validate_upload_url.assert_called_once_with(my_url, block_local_urls=False)


@pytest.mark.django_db
def test_delete_annotations(business_client, configured_project):
    business_client.delete(f'/api/projects/{configured_project.id}/annotations/')
    assert not Annotation.objects.filter(task__project=configured_project.id).exists()


# --- TaskAPI ---


@pytest.mark.parametrize(
    'response, status_code',
    [
        # status OK
        (
            {
                'annotations': [],
                'predictions': [],
                'drafts': [],
                'data': {'text': 'text B', 'meta_info': 'meta info B'},
                'meta': {},
                'created_at': '',
                'updated_at': '',
                'updated_by': [],
                'is_labeled': False,
                'project': 0,
                'overlap': 1,
                'file_upload': None,
                'annotations_ids': '',
                'annotations_results': '',
                'annotators': [],
                'completed_at': None,
                'predictions_model_versions': '',
                'draft_exists': False,
                'predictions_results': '',
                'predictions_score': None,
                'total_annotations': 0,
                'total_predictions': 0,
                'avg_lead_time': None,
                'cancelled_annotations': 0,
                'inner_id': 0,
                'storage_filename': None,
                'comment_authors': [],
                'comment_count': 0,
                'last_comment_updated_at': None,
                'unresolved_comment_count': 0,
            },
            200,
        )
    ],
)
@pytest.mark.django_db
def test_get_task(client_and_token, configured_project, response, status_code):
    client, token = client_and_token
    task = configured_project.tasks.order_by('-id').all()[0]
    response['project'] = configured_project.id
    response['created_at'] = task.created_at.isoformat().replace('+00:00', 'Z')
    response['updated_at'] = task.updated_at.isoformat().replace('+00:00', 'Z')
    response['id'] = task.id
    r = client.get(
        f'/api/tasks/{task.id}/', content_type='application/json', headers={'Authorization': f'Token {token}'}
    )
    assert r.status_code == status_code
    if response:
        assert r.json() == response


@pytest.mark.parametrize(
    'payload, response, status_code',
    [
        # status OK
        (
            {
                'annotations': [],
                'predictions': [],
                'data': {'text': 'TEST1', 'meta_info': 'TEST2'},
                'meta': {},
                'created_at': '',
                'updated_at': '',
                'updated_by': None,
                'is_labeled': False,
                'project': 0,
                'file_upload': None,
            },
            {
                'id': 0,
                'annotations': [],
                'predictions': [],
                'data': {'text': 'TEST1', 'meta_info': 'TEST2'},
                'meta': {},
                'created_at': '',
                'updated_at': '',
                'updated_by': None,
                'is_labeled': False,
                'project': 0,
                'overlap': 1,
                'file_upload': None,
                'inner_id': 1,
                'comment_authors': [],
                'comment_count': 0,
                'last_comment_updated_at': None,
                'unresolved_comment_count': 0,
            },
            200,
        )
    ],
)
@pytest.mark.django_db
def test_patch_task(client_and_token, configured_project, payload, response, status_code):
    client, token = client_and_token
    task = configured_project.tasks.order_by('-updated_at').all()[0]
    payload['project'] = configured_project.id

    r = client.patch(
        f'/api/tasks/{task.id}/',
        data=json.dumps(payload),
        content_type='application/json',
        headers={'Authorization': f'Token {token}'},
    )

    task = configured_project.tasks.order_by('-updated_at').all()[0]  # call DB again after update
    response['project'] = configured_project.id
    response['created_at'] = task.created_at.isoformat().replace('+00:00', 'Z')
    response['updated_at'] = task.updated_at.isoformat().replace('+00:00', 'Z')
    response['id'] = task.id
    response['total_annotations'] = 0
    response['cancelled_annotations'] = 0
    response['total_predictions'] = 0

    assert r.status_code == status_code
    if response:
        assert r.json() == response
</file>

<file path="label_studio/tests/test_block_objects.py">
import threading
import time

import pytest
from core.utils.common import db_is_not_sqlite
from tasks.models import Task, bulk_update_stats_project_tasks


@pytest.mark.django_db
def test_export(business_client, configured_project):
    task_query = Task.objects.filter(project=configured_project.id)
    task_query.update(is_labeled=True)
    if db_is_not_sqlite():
        # NB: due to sqlite write locking behavior, this test would otherwise be flaky on a sqlite DB
        t = threading.Thread(target=worker_change_stats, args=(task_query.values_list('id', flat=True),))
        t.daemon = True
        t.start()
        time.sleep(20)
        assert Task.objects.filter(is_labeled=True).count() == 2
        time.sleep(70)
    assert Task.objects.filter(is_labeled=True).count() == 2
    bulk_update_stats_project_tasks(task_query)
    assert Task.objects.filter(is_labeled=True).count() == 0


def worker_change_stats(tasks):
    tasks = Task.objects.filter(id__in=tasks)
    bulk_update_stats_project_tasks(tasks)
</file>

<file path="label_studio/tests/test_bulk_operations.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import datetime
import os

import pytest
from django.conf import settings
from projects.models import Project
from users.models import User

from label_studio.tests.test_data.gen_tasks_and_annotations import gen_tasks


@pytest.mark.django_db
def test_load_tasks_and_annotations(business_client, annotator_client, configured_project):
    """
        this test loads tasks_and_annotations.json
        with 1000 tasks and 5000 annotations and recalc accuracy
        with bulk_update
        goal is to be under time limit to ensure operations
        are fast enough

        this project has p.data_types_json() as
        {text: '', meta_info:''}
        json should be generated as item = {data:{}}

        one could check results with
        tasks = Task.objects.all()
        print('annotations', [(t.id, t.annotations.count()) for t in tasks])
        print('accuracy', [(t.id, t.accuracy) for t in tasks])

    :param annotator_client:
    :param configured_project:
    :return:
    """
    p = Project.objects.get(id=configured_project.id)
    project_id = configured_project.id

    user = User.objects.get(email='annotator@pytest.net')
    p.created_by.active_organization.add_user(user)
    p.add_collaborator(user)

    gen_tasks(user.id)

    dt1 = datetime.datetime.now()
    filename = 'tasks_and_annotations.json'
    filepath = os.path.join(settings.TEST_DATA_ROOT, filename)

    data = {filename: (open(filepath, 'rb'), filename)}
    url = '/api/projects/{}/tasks/bulk/'.format(project_id)
    r = business_client.post(url, data=data, format='multipart')
    assert r.status_code == 201, r.content

    dt2 = datetime.datetime.now()
    # time depends on aws machine cpu
    # around 15-30 secs for 1000 tasks each w 5 annotations
    assert (dt2 - dt1).seconds < 150
</file>

<file path="label_studio/tests/test_cli.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import pytest
from server import _create_user
from tests.utils import make_annotation, make_project, make_task

from label_studio.core.argparser import parse_input_args


@pytest.mark.django_db
def test_create_user():
    input_args = parse_input_args(['init', 'test', '--username', 'default@localhost', '--password', '12345678'])
    config = {}
    user = _create_user(input_args, config)
    assert user.active_organization is not None


@pytest.mark.django_db
def test_user_active_organization_counters():
    input_args = parse_input_args(['init', 'test', '--username', 'default@localhost', '--password', '12345678'])
    user = _create_user(input_args, {})

    project_config = dict(
        title='Test',
        is_published=True,
        label_config="""
                <View>
                  <Text name="location" value="$location"></Text>
                  <Choices name="text_class" choice="single">
                    <Choice value="class_A"></Choice>
                    <Choice value="class_B"></Choice>
                  </Choices>
                </View>""",
    )

    def make_test_project():
        project = make_project(project_config, user, False, org=user.active_organization)
        task1 = make_task({'data': {'location': 'London', 'text': 'text A'}}, project)
        task2 = make_task({'data': {'location': 'London', 'text': 'text A'}}, project)
        make_annotation({'result': [{'result': [{'r': 1}], 'ground_truth': True}], 'completed_by': user}, task1.id)
        make_annotation({'result': [{'result': [{'r': 1}], 'ground_truth': True}], 'completed_by': user}, task1.id)
        make_annotation({'result': [{'result': [{'r': 1}], 'ground_truth': True}], 'completed_by': user}, task2.id)

    make_test_project()
    make_test_project()
    make_test_project()

    assert user.active_organization_annotations().count() == 9
    assert user.active_organization_contributed_project_number() == 3
</file>

<file path="label_studio/tests/test_config_validation.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import glob
import io
import json
import logging
import os

import pytest
import yaml
from core.label_config import parse_config, parse_config_to_json, validate_label_config
from projects.models import Project

from label_studio.tests.utils import make_annotation, make_prediction, make_task, project_id  # noqa

logger = logging.getLogger(__name__)


@pytest.mark.parametrize(
    'tasks_count, annotations_count, predictions_count',
    [
        [2, 2, 2],
    ],
)
@pytest.mark.django_db
def test_change_label_config_repeater(tasks_count, annotations_count, predictions_count, business_client, project_id):
    # Change label config to Repeater
    payload = {
        'label_config': '<View> <Repeater on="$images" indexFlag="{{idx}}"> <Image name="page_{{idx}}" value="$images" maxWidth="100%"/>     <Header value="Utterance Review"/>     <RectangleLabels name="labels_{{idx}}" toName="page_{{idx}}">       <Label value="Header" hotkey="1"/> <Label value="Body" hotkey="2"/> <Label value="Footer" hotkey="3"/> </RectangleLabels> </Repeater> </View>'
    }
    response = business_client.patch(
        f'/api/projects/{project_id}',
        data=json.dumps(payload),
        content_type='application/json',
    )
    assert response.status_code == 200
    # cr
    project = Project.objects.get(pk=project_id)
    for _ in range(0, tasks_count):
        task_id = make_task(
            {
                'data': {
                    'images': [
                        {'url': 'https://htx-pub.s3.amazonaws.com/demo/images/demo_stock_purchase_agreement/0001.jpg'},
                        {'url': 'https://htx-pub.s3.amazonaws.com/demo/images/demo_stock_purchase_agreement/0002.jpg'},
                        {'url': 'https://htx-pub.s3.amazonaws.com/demo/images/demo_stock_purchase_agreement/0003.jpg'},
                    ]
                }
            },
            project,
        ).id
        print('TASK_ID: %s' % task_id)
        for _ in range(0, annotations_count):
            print('COMPLETION')
            make_annotation(
                {
                    'result': [
                        {
                            'id': '_565WKjviN',
                            'type': 'rectanglelabels',
                            'value': {
                                'x': 21.451104100946377,
                                'y': 7.682926829268292,
                                'width': 54.73186119873817,
                                'height': 4.146341463414634,
                                'rotation': 0,
                                'rectanglelabels': ['Header'],
                            },
                            'origin': 'manual',
                            'to_name': 'page_0',
                            'from_name': 'labels_0',
                            'image_rotation': 0,
                            'original_width': 800,
                            'original_height': 1035,
                        }
                    ]
                },
                task_id,
            )

        for _ in range(0, predictions_count):
            make_prediction({'result': []}, task_id)

    # no changes - no errors
    payload = {
        'label_config': '<View> <Repeater on="$images" indexFlag="{{idx}}"> <Image name="page_{{idx}}" value="$images" maxWidth="100%"/>     <Header value="Utterance Review"/>     <RectangleLabels name="labels_{{idx}}" toName="page_{{idx}}"> <Label value="Header" hotkey="1"/> <Label value="Body" hotkey="2"/> <Label value="Footer" hotkey="3"/> </RectangleLabels> </Repeater> </View>'
    }
    response = business_client.post(
        f'/api/projects/{project_id}/validate',
        data=json.dumps(payload),
        content_type='application/json',
    )
    assert response.status_code == 200

    # delete unused labels
    payload = {
        'label_config': '<View> <Repeater on="$images" indexFlag="{{idx}}"> <Image name="page_{{idx}}" value="$images" maxWidth="100%"/>     <Header value="Utterance Review"/>     <RectangleLabels name="labels_{{idx}}" toName="page_{{idx}}"> <Label value="Header" hotkey="1"/> <Label value="Body" hotkey="2"/> </RectangleLabels> </Repeater> </View>'
    }
    response = business_client.post(
        f'/api/projects/{project_id}/validate',
        data=json.dumps(payload),
        content_type='application/json',
    )
    assert response.status_code == 200

    # delete used labels - 400
    payload = {
        'label_config': '<View> <Repeater on="$images" indexFlag="{{idx}}"> <Image name="page_{{idx}}" value="$images" maxWidth="100%"/>     <Header value="Utterance Review"/>     <RectangleLabels name="labels_{{idx}}" toName="page_{{idx}}"> <Label value="Body" hotkey="2"/> <Label value="Footer" hotkey="3"/> </RectangleLabels> </Repeater> </View>'
    }
    response = business_client.post(
        f'/api/projects/{project_id}/validate',
        data=json.dumps(payload),
        content_type='application/json',
    )
    assert response.status_code == 400


@pytest.mark.django_db
def test_parse_all_configs():
    folder_wildcard = './label_studio/annotation_templates'
    result = [y for x in os.walk(folder_wildcard) for y in glob.glob(os.path.join(x[0], '*.xml'))]
    for file in result:
        print(f'Parsing config: {file}')
        with open(file, mode='r') as f:
            config = f.read()
            assert parse_config(config)
            assert parse_config_to_json(config)
            validate_label_config(config)


@pytest.mark.django_db
def test_config_validation_for_choices_workaround(business_client, project_id):
    """
    Validate Choices tag for 1 choice with workaround
    Example bug DEV-3635
    """
    payload = {
        'label_config': '<View><Text value="$text" name="artist" /><View><Choices name="choices_1" toName="artist">'
        '<Choice name="choice_1" value="1"/></Choices></View><View>'
        '<Choices name="choices_2" toName="artist"><Choice name="choice_2" value="2"/></Choices>'
        '</View></View>'
    }
    response = business_client.patch(
        f'/api/projects/{project_id}',
        data=json.dumps(payload),
        content_type='application/json',
    )
    assert response.status_code == 200

    payload = {
        'label_config': '<View><Text value="$text" name="artist" /><View><Choices name="choices_1" toName="artist">'
        '<Choice name="choice_1" value="1"/></Choices><Choices name="choices_2" toName="artist">'
        '<Choice name="choice_2" value="2"/></Choices></View></View>'
    }
    response = business_client.patch(
        f'/api/projects/{project_id}',
        data=json.dumps(payload),
        content_type='application/json',
    )
    assert response.status_code == 200


@pytest.mark.django_db
def test_config_validation_for_missing_to_name_in_number_tag_fails(business_client, project_id):
    """
    Validate Number tag with missing to_name fails (see LEAP-245)
    """
    payload = {
        'label_config': (
            '<View>'
            '<Text name="question" value="$question" granularity="word"/>'
            '<Number name="number" to="question" required="true" />'
            '</View>'
        )
    }
    response = business_client.patch(
        f'/api/projects/{project_id}',
        data=json.dumps(payload),
        content_type='application/json',
    )
    assert response.status_code == 400
    response_data = response.json()
    assert "'toName' is a required property" in response_data['validation_errors']['label_config'][0]


@pytest.mark.django_db
def test_parse_wrong_xml(business_client, project_id):
    # Change label config to Repeater
    payload = {
        'label_config': '<View> <Repeater on="$images" indexFlag="{{idx}}"> <Image name="page_{{idx}}" value="$images" maxWidth="100%"/>     <Header value="Utterance Review"/>     <RectangleLabels name="labels_{{idx}}" toName="page_{{idx}}">       <Label value="Header" hotkey="1"/> <Label value="Body" hotkey="2"/> <Label value="Footer" hotkey="3"/> </RectangleLabels> </Repeater> </View>'
    }
    response = business_client.patch(
        f'/api/projects/{project_id}',
        data=json.dumps(payload),
        content_type='application/json',
    )
    assert response.status_code == 200
    # Change label config to wrong XML
    payload = {
        'label_config': '1<View> <Repeater on="$images" indexFlag="{{idx}}"> <Image name="page_{{idx}}" value="$images" maxWidth="100%"/>     <Header value="Utterance Review"/>     <RectangleLabels name="labels_{{idx}}" toName="page_{{idx}}"> <Label value="Body" hotkey="2"/> <Label value="Footer" hotkey="3"/> </RectangleLabels> </Repeater> </View>'
    }
    response = business_client.post(
        f'/api/projects/{project_id}/validate',
        data=json.dumps(payload),
        content_type='application/json',
    )
    assert response.status_code == 400


@pytest.mark.django_db
def test_label_config_versions(business_client, project_id):
    with io.open(os.path.join(os.path.dirname(__file__), 'test_data/data_for_test_label_config_matrix.yml')) as f:
        test_suites = yaml.safe_load(f)
    for test_name, test_content in test_suites.items():
        payload = {'label_config': test_content['label_config']}
        response = business_client.post(
            f'/api/projects/{project_id}/validate',
            data=json.dumps(payload),
            content_type='application/json',
        )
        logger.warning(f'Test: {test_name}')
        assert response.status_code == test_content['status_code']
</file>

<file path="label_studio/tests/test_contextlog.py">
import json

import pytest
import responses


@responses.activate
@pytest.mark.django_db
def test_contextlog(business_client, contextlog_test_config):
    responses.add(
        responses.POST,
        'https://tele.labelstud.io',
        json={'ok': 'true'},
        status=201,
    )
    r = business_client.get('/api/users/')

    responses.assert_call_count('https://tele.labelstud.io', 1)
    assert responses.calls
    assert r.status_code == 200
    assert 'env' not in json.loads(responses.calls[0].request.body)
</file>

<file path="label_studio/tests/test_core.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import types

import pytest
from core.utils.common import int_from_request
from core.utils.exceptions import InvalidUploadUrlError, LabelStudioAPIException
from core.utils.io import validate_upload_url
from core.utils.params import bool_from_request
from rest_framework.exceptions import ValidationError


@pytest.mark.parametrize(
    'param, result',
    [
        ('True', True),
        ('Yes', True),
        ('1', True),
        ('False', False),
        ('no', False),
        ('0', False),
        ('test', None),
        (None, False),
    ],
)
@pytest.mark.django_db
def test_core_bool_from_request(param, result):
    params = {'test': param} if param is not None else {}

    # incorrect param should call exception
    if result is None:
        error = False
        try:
            bool_from_request(params, 'test', 0)
        except:  # noqa: E722
            error = True

        assert error

    # everything ok
    else:
        assert bool_from_request(params, 'test', 0) == result


@pytest.mark.parametrize('param, result', [('', None), ('0', 0), ('1', 1), ('10', 10), ('test', None), (None, None)])
@pytest.mark.django_db
def test_core_int_from_request(param, result):
    params = {'test': param}

    # incorrect param should call exception
    if result is None:
        error = False
        try:
            int_from_request(params, 'test', 0)
        except ValidationError:
            error = True

        assert error

    # everything ok
    else:
        assert int_from_request(params, 'test', 0) == result


@pytest.mark.django_db
def test_user_info(business_client):
    from label_studio.server import _create_user, _get_user_info

    user_data = _get_user_info(business_client.admin.email)
    assert 'token' in user_data

    user_data = _get_user_info(None)
    assert user_data is None

    class DummyArgs:
        username = 'tester@x.com'
        password = 'passwdx'
        user_token = 'token12345'

    args = DummyArgs()
    _create_user(args, {})
    user_data = _get_user_info('tester@x.com')
    assert user_data['token'] == 'token12345'

    args.user_token, args.username = '123', 'tester2@x.com'
    user = _create_user(args, {})
    assert user is not None


@pytest.mark.parametrize(
    'command_line, result',
    [
        (['label-studio', 'user', '--username', 'test@test.com', '--password', '12345678'], None),
    ],
)
@pytest.mark.django_db
def test_main(mocker, command_line, result):
    from server import main

    mocker.patch('sys.argv', command_line)
    output = main()

    assert output == result


def test_string_is_url():
    from label_studio.core.utils.common import string_is_url

    assert string_is_url('http://test.com') is True
    assert string_is_url('https://test.com') is True
    assert string_is_url('xyz') is False


def test_get_client_ip():
    from label_studio.core.utils.common import get_client_ip

    ip = get_client_ip(types.SimpleNamespace(META={'HTTP_X_FORWARDED_FOR': '127.0.0.1'}))
    assert ip == '127.0.0.1'

    ip = get_client_ip(types.SimpleNamespace(META={'REMOTE_ADDR': '127.0.0.2'}))
    assert ip == '127.0.0.2'


def test_timestamp_now():
    from label_studio.core.utils.common import timestamp_now

    t = timestamp_now()
    assert t is not None


def test_start_browser():
    from label_studio.core.utils.common import start_browser

    assert start_browser('http://localhost:8080', True) is None
    assert start_browser('http://localhost:8080', False) is None


@pytest.mark.parametrize(
    'url, block_local_urls, raises_exc',
    [
        ('http://0.0.0.0', True, InvalidUploadUrlError),
        ('http://0.0.0.0', False, None),
        ('https://0.0.0.0', True, InvalidUploadUrlError),
        ('https://0.0.0.0', False, None),
        # Non-http[s] schemes
        ('ftp://example.org', True, InvalidUploadUrlError),
        ('ftp://example.org', False, InvalidUploadUrlError),
        ('FILE:///etc/passwd', True, InvalidUploadUrlError),
        ('file:///etc/passwd', False, InvalidUploadUrlError),
        # Start and end of 127.0.0.0/8
        ('https://127.0.0.0', True, InvalidUploadUrlError),
        ('https://127.255.255.255', True, InvalidUploadUrlError),
        # Start and end of 10.0.0.0/8
        ('http://10.0.0.0', True, InvalidUploadUrlError),
        ('https://10.255.255.255', True, InvalidUploadUrlError),
        # Start and end of 172.16.0.0/12
        ('https://172.16.0.0', True, InvalidUploadUrlError),
        ('https://172.31.255.255', True, InvalidUploadUrlError),
        # Start and end of 192.168.0.0/16
        ('https://192.168.0.0', True, InvalidUploadUrlError),
        ('https://192.168.255.255', True, InvalidUploadUrlError),
        # Valid external IPs
        ('https://4.4.4.4', True, None),
        ('https://8.8.8.8', True, None),
        ('http://8.8.8.8', False, None),
        # Valid external websites
        ('https://example.org', True, None),
        ('http://example.org', False, None),
        # Space prepended to otherwise valid external IP
        (' http://8.8.8.8', False, InvalidUploadUrlError),
        # Host that doesn't resolve
        ('http://example', False, LabelStudioAPIException),
        ('http://example', True, LabelStudioAPIException),
        # localhost
        ('http://localhost', True, InvalidUploadUrlError),
        ('http://localhost', False, None),
    ],
)
@pytest.mark.django_db
def test_core_validate_upload_url(url, block_local_urls, raises_exc):

    if raises_exc is None:
        assert validate_upload_url(url, block_local_urls=block_local_urls) is None
        return

    with pytest.raises(raises_exc):
        validate_upload_url(url, block_local_urls=block_local_urls)
</file>

<file path="label_studio/tests/test_endpoints.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import json
import os

import django
import pytest
from django.core.management import call_command
from django.shortcuts import reverse
from django.urls import get_resolver
from tasks.models import Annotation, Task

owner_statuses = {
    '/tasks/1000/label': {'get': 200, 'post': 200, 'put': 405, 'patch': 405, 'delete': 405},
    '/tasks/1000/delete': {'get': 302, 'post': 404, 'put': 405, 'patch': 405, 'delete': 405},
    '/tasks/1000/explore': {'get': 200, 'post': 200, 'put': 405, 'patch': 405, 'delete': 405},
    '/api/tasks/1000/cancel': {'get': 405, 'post': 200, 'put': 405, 'patch': 405, 'delete': 405},
    '/api/tasks/1000/annotations/': {'get': 200, 'post': 201, 'put': 405, 'patch': 405, 'delete': 405},
    '/api/tasks/1000/annotations/1000/': {'get': 200, 'post': 405, 'put': 200, 'patch': 200, 'delete': 204},
    '/api/tasks/1000/': {'get': 200, 'post': 405, 'put': 400, 'patch': 400, 'delete': 204},
    '/api/projects/1000/annotations/': {'get': 405, 'post': 405, 'put': 405, 'patch': 405, 'delete': 204},
    '/api/projects/1000/results/': {'get': 200, 'post': 405, 'put': 405, 'patch': 405, 'delete': 405},
    '/api/projects/1000/tasks/bulk/': {'get': 405, 'post': 400, 'put': 405, 'patch': 405, 'delete': 405},
    '/api/projects/1000/tasks/': {'get': 200, 'post': 415, 'put': 405, 'patch': 405, 'delete': 204},
    '/annotator/invites/1000': {'get': 403, 'post': 403, 'put': 403, 'patch': 403, 'delete': 403},
    '/annotator/projects/1000/editor': {'get': 403, 'post': 403, 'put': 403, 'patch': 403, 'delete': 403},
    '/annotator/projects/': {'get': 403, 'post': 403, 'put': 403, 'patch': 403, 'delete': 403},
    '/annotator/account/': {'get': 403, 'post': 403, 'put': 403, 'patch': 403, 'delete': 403},
    '/annotator/signup/': {'get': 403, 'post': 403, 'put': 403, 'patch': 403, 'delete': 403},
    '/annotator/login/': {'get': 403, 'post': 403, 'put': 403, 'patch': 403, 'delete': 403},
    '/logout': {'get': 302, 'post': 302, 'put': 302, 'patch': 302, 'delete': 302},
    '/api/': {'get': 200, 'post': 405, 'put': 405, 'patch': 405, 'delete': 405},
    '/api/projects/validate': {'get': 401, 'post': 401, 'put': 401, 'patch': 401, 'delete': 401},
    '/api/projects/template': {'get': 401, 'post': 401, 'put': 401, 'patch': 401, 'delete': 401},
    '/api/projects/1000/backends': {'get': 401, 'post': 401, 'put': 401, 'patch': 401, 'delete': 401},
    '/api/projects/1000/backends/connections': {'get': 401, 'post': 401, 'put': 401, 'patch': 401, 'delete': 401},
    '/api/projects/backends': {'get': 401, 'post': 401, 'put': 401, 'patch': 401, 'delete': 401},
    '/api/projects/1000/predict': {'get': 401, 'post': 401, 'put': 401, 'patch': 401, 'delete': 401},
    '/api/projects/1000/onboarding/1000': {'get': 401, 'post': 401, 'put': 401, 'patch': 401, 'delete': 401},
    '/api/projects/1000/next': {'get': 401, 'post': 401, 'put': 401, 'patch': 401, 'delete': 401},
    '/api/projects/1000/expert_instruction': {'get': 401, 'post': 401, 'put': 401, 'patch': 401, 'delete': 401},
    '/api/projects/1000/': {'get': 401, 'post': 401, 'put': 401, 'patch': 401, 'delete': 401},
    '/api/projects/': {'get': 401, 'post': 401, 'put': 401, 'patch': 401, 'delete': 401},
    '/projects/upload-example/': {'get': 401, 'post': 401, 'put': 401, 'patch': 401, 'delete': 401},
    '/projects/1000/ml': {'get': 401, 'post': 401, 'put': 401, 'patch': 401, 'delete': 401},
    '/projects/1000/plots': {'get': 401, 'post': 401, 'put': 401, 'patch': 401, 'delete': 401},
    '/projects/1000/experts': {'get': 401, 'post': 401, 'put': 401, 'patch': 401, 'delete': 401},
    '/projects/1000/delete': {'get': 401, 'post': 401, 'put': 401, 'patch': 401, 'delete': 401},
    '/projects/1000/duplicate': {'get': 401, 'post': 401, 'put': 401, 'patch': 401, 'delete': 401},
    '/projects/1000/upload-example/': {'get': 401, 'post': 401, 'put': 401, 'patch': 401, 'delete': 401},
    '/projects/1000/data': {'get': 401, 'post': 401, 'put': 401, 'patch': 401, 'delete': 401},
    '/projects/1000/settings/edit-config': {'get': 401, 'post': 401, 'put': 401, 'patch': 401, 'delete': 401},
    '/projects/1000/settings': {'get': 401, 'post': 401, 'put': 401, 'patch': 401, 'delete': 401},
    '/projects/1000/': {'get': 401, 'post': 401, 'put': 401, 'patch': 401, 'delete': 401},
    '/projects/render': {'get': 401, 'post': 401, 'put': 401, 'patch': 401, 'delete': 401},
    '/projects/template/': {'get': 401, 'post': 401, 'put': 401, 'patch': 401, 'delete': 401},
    '/projects/create/': {'get': 401, 'post': 401, 'put': 401, 'patch': 401, 'delete': 401},
    '/projects/': {'get': 401, 'post': 401, 'put': 401, 'patch': 401, 'delete': 401},
    '/business/not-approved': {'get': 200, 'post': 200, 'put': 200, 'patch': 200, 'delete': 200},
    '/business/stats': {'get': 401, 'post': 401, 'put': 401, 'patch': 401, 'delete': 401},
    '/business/experts/list': {'get': 401, 'post': 401, 'put': 401, 'patch': 401, 'delete': 401},
    '/user/account/': {'get': 401, 'post': 401, 'put': 401, 'patch': 401, 'delete': 401},
    '/user/signup/': {'get': 200, 'post': 200, 'put': 200, 'patch': 200, 'delete': 200},
    '/user/login/': {'get': 200, 'post': 200, 'put': 200, 'patch': 200, 'delete': 200},
    '/django-rq/queues/1000/1000/enqueue/': {'get': 302, 'post': 302, 'put': 302, 'patch': 302, 'delete': 302},
    '/django-rq/queues/1000/1000/requeue/': {'get': 302, 'post': 302, 'put': 302, 'patch': 302, 'delete': 302},
    '/django-rq/queues/actions/1000/': {'get': 302, 'post': 302, 'put': 302, 'patch': 302, 'delete': 302},
    '/django-rq/queues/1000/1000/delete/': {'get': 302, 'post': 302, 'put': 302, 'patch': 302, 'delete': 302},
    '/django-rq/queues/1000/1000/': {'get': 302, 'post': 302, 'put': 302, 'patch': 302, 'delete': 302},
    '/django-rq/queues/1000/requeue-all/': {'get': 302, 'post': 302, 'put': 302, 'patch': 302, 'delete': 302},
    '/django-rq/queues/1000/empty/': {'get': 302, 'post': 302, 'put': 302, 'patch': 302, 'delete': 302},
    '/django-rq/queues/1000/deferred/': {'get': 302, 'post': 302, 'put': 302, 'patch': 302, 'delete': 302},
    '/django-rq/queues/1000/started/': {'get': 302, 'post': 302, 'put': 302, 'patch': 302, 'delete': 302},
    '/django-rq/queues/1000/finished/': {'get': 302, 'post': 302, 'put': 302, 'patch': 302, 'delete': 302},
    '/django-rq/workers/1000/1000/': {'get': 302, 'post': 302, 'put': 302, 'patch': 302, 'delete': 302},
    '/django-rq/workers/1000/': {'get': 302, 'post': 302, 'put': 302, 'patch': 302, 'delete': 302},
    '/django-rq/queues/1000/': {'get': 302, 'post': 302, 'put': 302, 'patch': 302, 'delete': 302},
    '/django-rq/stats.json/': {'get': 200, 'post': 200, 'put': 200, 'patch': 200, 'delete': 200},
    '/django-rq/': {'get': 302, 'post': 302, 'put': 302, 'patch': 302, 'delete': 302},
}


other_business_statuses = {
    '/tasks/1000/label': {'get': 403, 'post': 403, 'put': 405, 'delete': 405},
    '/tasks/1000/delete': {'get': 403, 'post': 403, 'put': 405, 'delete': 405},
    '/tasks/1000/explore': {'get': 403, 'post': 403, 'put': 405, 'delete': 405},
    '/api/tasks/1000/cancel': {'get': 405, 'post': 403, 'put': 405, 'delete': 405},
    '/api/tasks/1000/annotations/': {'get': 403, 'post': 403, 'put': 405, 'delete': 405},
    '/api/tasks/1000/annotations/1000/': {'get': 403, 'post': 405, 'put': 403, 'delete': 403},
    '/api/tasks/1000/': {'get': 403, 'post': 405, 'put': 403, 'delete': 403},
    '/api/projects/1000/tasks/delete': {'get': 405, 'post': 405, 'put': 405, 'delete': 403},
    '/api/projects/1000/annotations/delete': {'get': 405, 'post': 405, 'put': 405, 'delete': 403},
    '/api/projects/1000/results/': {'get': 403, 'post': 405, 'put': 405, 'delete': 405},
    '/api/projects/1000/tasks/bulk/': {'get': 405, 'post': 403, 'put': 405, 'delete': 405},
    '/api/projects/1000/tasks/': {'get': 403, 'post': 415, 'put': 405, 'delete': 405},
    '/annotator/invites/1000': {'get': 403, 'post': 403, 'put': 403, 'delete': 403},
    '/annotator/projects/1000/editor': {'get': 403, 'post': 403, 'put': 403, 'delete': 403},
    '/annotator/projects/': {'get': 403, 'post': 403, 'put': 403, 'delete': 403},
    '/annotator/account/': {'get': 403, 'post': 403, 'put': 403, 'delete': 403},
    '/annotator/signup/': {'get': 403, 'post': 403, 'put': 403, 'delete': 403},
    '/annotator/login/': {'get': 403, 'post': 403, 'put': 403, 'delete': 403},
    '/logout': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
    '/api/': {'get': 200, 'post': 405, 'put': 405, 'delete': 405},
    '/api/projects/validate': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/api/projects/template': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/api/projects/1000/backends': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/api/projects/1000/predict': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/api/projects/1000/onboarding/1000': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/api/projects/1000/next': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/api/projects/1000/expert_instruction': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/api/projects/1000/': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/upload-example/': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/1000/ml': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/1000/plots': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/1000/experts': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/1000/delete': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/1000/duplicate': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/1000/upload-example/': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/1000/data/upload': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/1000/data': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/1000/settings/edit-config': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/1000/settings': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/1000/': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/render': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/template/': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/create/': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/business/not-approved': {'get': 200, 'post': 200, 'put': 200, 'delete': 200},
    '/business/stats': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/business/experts/list': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/user/account/': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/user/signup/': {'get': 200, 'post': 200, 'put': 200, 'delete': 200},
    '/user/login/': {'get': 200, 'post': 200, 'put': 200, 'delete': 200},
    '/django-rq/queues/1000/1000/enqueue/': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
    '/django-rq/queues/1000/1000/requeue/': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
    '/django-rq/queues/actions/1000/': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
    '/django-rq/queues/1000/1000/delete/': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
    '/django-rq/queues/1000/1000/': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
    '/django-rq/queues/1000/requeue-all/': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
    '/django-rq/queues/1000/empty/': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
    '/django-rq/queues/1000/deferred/': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
    '/django-rq/queues/1000/started/': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
    '/django-rq/queues/1000/finished/': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
    '/django-rq/workers/1000/1000/': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
    '/django-rq/workers/1000/': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
    '/django-rq/queues/1000/': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
    '/django-rq/stats.json/': {'get': 200, 'post': 200, 'put': 200, 'delete': 200},
    '/django-rq/': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
}


other_annotator_statuses = {
    '/tasks/1000/label': {'get': 403, 'post': 403, 'put': 403, 'delete': 403},
    '/tasks/1000/delete': {'get': 403, 'post': 403, 'put': 403, 'delete': 403},
    '/tasks/1000/explore': {'get': 403, 'post': 403, 'put': 403, 'delete': 403},
    '/api/tasks/1000/cancel': {'get': 405, 'post': 403, 'put': 405, 'delete': 405},
    '/api/tasks/1000/annotations/': {'get': 403, 'post': 403, 'put': 405, 'delete': 405},
    '/api/tasks/1000/annotations/1000/': {'get': 403, 'post': 405, 'put': 403, 'delete': 403},
    '/api/tasks/1000/': {'get': 403, 'post': 405, 'put': 403, 'delete': 403},
    '/api/projects/1000/tasks/delete': {'get': 403, 'post': 403, 'put': 403, 'delete': 403},
    '/api/projects/1000/annotations/delete': {'get': 403, 'post': 403, 'put': 403, 'delete': 403},
    '/api/projects/1000/results/': {'get': 403, 'post': 403, 'put': 403, 'delete': 403},
    '/api/projects/1000/tasks/bulk/': {'get': 403, 'post': 403, 'put': 403, 'delete': 403},
    '/api/projects/1000/tasks/': {'get': 403, 'post': 403, 'put': 403, 'delete': 403},
    '/annotator/invites/1000': {'get': 404, 'post': 404, 'put': 404, 'delete': 404},
    '/annotator/projects/1000/editor': {'get': 403, 'post': 403, 'put': 405, 'delete': 405},
    '/annotator/projects/': {'get': 200, 'post': 200, 'put': 405, 'delete': 405},
    '/annotator/account/': {'get': 200, 'post': 302, 'put': 405, 'delete': 405},
    '/annotator/signup/': {'get': 403, 'post': 403, 'put': 403, 'delete': 403},
    '/annotator/login/': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
    '/logout': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
    '/api/': {'get': 200, 'post': 405, 'put': 405, 'delete': 405},
    '/api/projects/validate': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/api/projects/template': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/api/projects/1000/backends': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/api/projects/1000/predict': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/api/projects/1000/onboarding/1000': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/api/projects/1000/next': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/api/projects/1000/expert_instruction': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/api/projects/1000/': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/upload-example/': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/1000/ml': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/1000/plots': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/1000/experts': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/1000/delete': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/1000/duplicate': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/1000/upload-example/': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/1000/data/upload': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/1000/data': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/1000/settings/edit-config': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/1000/settings': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/1000/': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/render': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/template/': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/create/': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/business/not-approved': {'get': 200, 'post': 200, 'put': 200, 'delete': 200},
    '/business/stats': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/business/experts/list': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/user/account/': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/user/signup/': {'get': 200, 'post': 200, 'put': 200, 'delete': 200},
    '/user/login/': {'get': 200, 'post': 200, 'put': 200, 'delete': 200},
    '/django-rq/queues/1000/1000/enqueue/': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
    '/django-rq/queues/1000/1000/requeue/': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
    '/django-rq/queues/actions/1000/': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
    '/django-rq/queues/1000/1000/delete/': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
    '/django-rq/queues/1000/1000/': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
    '/django-rq/queues/1000/requeue-all/': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
    '/django-rq/queues/1000/empty/': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
    '/django-rq/queues/1000/deferred/': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
    '/django-rq/queues/1000/started/': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
    '/django-rq/queues/1000/finished/': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
    '/django-rq/workers/1000/1000/': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
    '/django-rq/workers/1000/': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
    '/django-rq/queues/1000/': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
    '/django-rq/stats.json/': {'get': 200, 'post': 200, 'put': 200, 'delete': 200},
    '/django-rq/': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
}


group_annotator_statuses = {
    '/tasks/1000/label': {'get': 403, 'post': 403, 'put': 403, 'delete': 403},
    '/tasks/1000/delete': {'get': 403, 'post': 403, 'put': 403, 'delete': 403},
    '/tasks/1000/explore': {'get': 403, 'post': 403, 'put': 403, 'delete': 403},
    '/api/tasks/1000/cancel': {'get': 405, 'post': 403, 'put': 405, 'delete': 405},
    '/api/tasks/1000/annotations/': {'get': 403, 'post': 403, 'put': 405, 'delete': 405},
    '/api/tasks/1000/annotations/1000/': {'get': 403, 'post': 405, 'put': 403, 'delete': 403},
    '/api/tasks/1000/': {'get': 403, 'post': 405, 'put': 403, 'delete': 403},
    '/api/projects/1000/tasks/delete': {'get': 403, 'post': 403, 'put': 403, 'delete': 403},
    '/api/projects/1000/annotations/delete': {'get': 403, 'post': 403, 'put': 403, 'delete': 403},
    '/api/projects/1000/results/': {'get': 403, 'post': 403, 'put': 403, 'delete': 403},
    '/api/projects/1000/tasks/bulk/': {'get': 403, 'post': 403, 'put': 403, 'delete': 403},
    '/api/projects/1000/tasks/': {'get': 403, 'post': 403, 'put': 403, 'delete': 403},
    '/annotator/invites/1000': {'get': 404, 'post': 404, 'put': 404, 'delete': 404},
    '/annotator/projects/1000/editor': {'get': 403, 'post': 403, 'put': 405, 'delete': 405},
    '/annotator/projects/': {'get': 200, 'post': 200, 'put': 405, 'delete': 405},
    '/annotator/account/': {'get': 200, 'post': 302, 'put': 405, 'delete': 405},
    '/annotator/signup/': {'get': 403, 'post': 403, 'put': 403, 'delete': 403},
    '/annotator/login/': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
    '/logout': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
    '/api/': {'get': 200, 'post': 405, 'put': 405, 'delete': 405},
    '/api/projects/validate': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/api/projects/template': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/api/projects/1000/backends': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/api/projects/1000/predict': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/api/projects/1000/onboarding/1000': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/api/projects/1000/next': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/api/projects/1000/expert_instruction': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/api/projects/1000/': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/upload-example/': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/1000/ml': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/1000/plots': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/1000/experts': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/1000/delete': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/1000/duplicate': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/1000/upload-example/': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/1000/data/upload': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/1000/data': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/1000/settings/edit-config': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/1000/settings': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/1000/': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/render': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/template/': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/create/': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/projects/': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/business/not-approved': {'get': 200, 'post': 200, 'put': 200, 'delete': 200},
    '/business/stats': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/business/experts/list': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/user/account/': {'get': 401, 'post': 401, 'put': 401, 'delete': 401},
    '/user/signup/': {'get': 200, 'post': 200, 'put': 200, 'delete': 200},
    '/user/login/': {'get': 200, 'post': 200, 'put': 200, 'delete': 200},
    '/django-rq/queues/1000/1000/enqueue/': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
    '/django-rq/queues/1000/1000/requeue/': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
    '/django-rq/queues/actions/1000/': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
    '/django-rq/queues/1000/1000/delete/': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
    '/django-rq/queues/1000/1000/': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
    '/django-rq/queues/1000/requeue-all/': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
    '/django-rq/queues/1000/empty/': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
    '/django-rq/queues/1000/deferred/': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
    '/django-rq/queues/1000/started/': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
    '/django-rq/queues/1000/finished/': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
    '/django-rq/workers/1000/1000/': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
    '/django-rq/workers/1000/': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
    '/django-rq/queues/1000/': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
    '/django-rq/stats.json/': {'get': 200, 'post': 200, 'put': 200, 'delete': 200},
    '/django-rq/': {'get': 302, 'post': 302, 'put': 302, 'delete': 302},
}


def build_urls(project_id, task_id, annotation_id):
    """Get all the ulrs from django"""
    urls = []
    exclude_urls = {'schema-json', 'schema-swagger-ui', 'schema-redoc'}
    resolver = get_resolver(None).reverse_dict
    for url_name in resolver:
        if isinstance(url_name, str) and url_name not in exclude_urls:
            keys = resolver[url_name][0][0][1]
            kwargs = {}
            for key in keys:
                if 'pk' in key:
                    kwargs[key] = 1000  # for example user_pk or project_pk will be 1000

                if key in ['pk', 'step_pk', 'job_id', 'queue_index', 'scheduler_index']:
                    kwargs[key] = 1000
                elif key in ['token', 'uidb64']:
                    kwargs[key] = 1000
                elif key in ['key']:
                    kwargs[key] = '1000'

                # we need to use really existing project/task/annotation ids from fixture
                if key == 'project_id' or key == 'project_pk':
                    kwargs[key] = project_id
                elif key == 'task_id':
                    kwargs[key] = task_id
                elif key == 'annotation_id':
                    kwargs[key] = annotation_id
                elif 'id' in key:
                    kwargs[key] = 1

                if url_name == 'password_reset_confirm':
                    kwargs['token'] = '1000-1000'
                    kwargs['uidb64'] = '1000'
            try:
                url = reverse(url_name, kwargs=kwargs)
            except django.urls.exceptions.NoReverseMatch as e:
                print(
                    f'\n\n ---> Could not find "{url_name}" with django reverse and kwargs "{kwargs}".\n'
                    f'Probably some kwarg is absent\n\n'
                )
                raise e

            exclude = ['/password-reset/complete/', '/password-reset/']
            add = True
            for exc in exclude:
                if url.startswith(exc):
                    add = False

            if add:
                urls.append(url)

    return urls


def restore_objects(project):
    """Create task and annotation for URL tests"""
    # task_db, annotation_db = None, None

    if project.pk != 1000:
        project.pk = 1000
        project.title += '2'
        project.save()
    try:
        task_db = Task.objects.get(pk=1000)
    except Task.DoesNotExist:
        task_db = Task()
        task_db.data = {'data': {'image': 'kittens.jpg'}}
        task_db.project = project
        task_db.id = 1000  # we need to use id 1000 to avoid db last start
        task_db.save()

    try:
        annotation_db = Annotation.objects.get(pk=1000)
    except Annotation.DoesNotExist:
        task_db = Task.objects.get(pk=1000)
        annotation_db = Annotation()
        annotation = [{'from_name': 'some', 'to_name': 'x', 'type': 'none', 'value': {'none': ['Opossum']}}]
        annotation_db.result = annotation
        annotation_db.id = 1000  # we need to use id 1000 to avoid db last start
        annotation_db.task = task_db
        annotation_db.save()

    return task_db, annotation_db


def check_urls(urls, runner, match_statuses, project):
    statuses = {}
    for url in urls:
        print('-->', url)
        status = {}
        restore_objects(project)

        r = runner.get(url)
        status['get'] = r.status_code

        r = runner.post(url)
        status['post'] = r.status_code

        r = runner.put(url)
        status['put'] = r.status_code

        r = runner.patch(url)
        status['patch'] = r.status_code

        r = runner.delete(url)
        status['delete'] = r.status_code

        # assert url in match_statuses, '\nNew URL found, please check statuses and add \n\n' \
        #                              + url + ': ' + str(status) + \
        #                              '\n\nto dict \n\n' + runner.statuses_name + '\n'

        statuses[url] = status
        # assert match_statuses[url] == status, f'Expected statuses mismatch: "{url}"'

    # print(statuses)  # use this to collect urls -> statuses dict


def run(owner, runner):
    """Get all urls from Django and GET/POST/PUT/DELETE them"""
    owner.task_db, owner.annotation_db = restore_objects(owner.project)
    urls = build_urls(owner.project.id, owner.task_db.id, owner.annotation_db.id)

    check_urls(urls, runner, runner.statuses, owner.project)


@pytest.mark.django_db
def test_all_urls_owner(setup_project_choices):
    runner = owner = setup_project_choices
    runner.statuses = owner_statuses
    runner.statuses_name = 'owner_statuses'
    run(owner, runner)


@pytest.mark.django_db
def test_all_urls_other_business(setup_project_choices, business_client):
    business_client.statuses = other_business_statuses
    business_client.statuses_name = 'other_business_statuses'
    run(setup_project_choices, business_client)


@pytest.mark.django_db
def test_urls_mismatch_with_registered(tmpdir):
    from core.utils.io import find_file

    all_urls_file = find_file('all_urls.json')
    with open(all_urls_file) as f:
        all_urls = json.load(f)

    instruction = (
        f'If you created, removed or updated URLs, run the following command:\n./manage.py show_urls --format pretty-json > new_urls.json\n'
        f'After creation, you should verify and correct mismatched data by updating {all_urls_file}.'
    )

    f = tmpdir.mkdir('subdir').join('show_urls.json')
    call_command('show_urls', format='pretty-json', stdout=f)
    filename = os.path.join(f.dirname, f.basename)
    with open(filename) as f1:
        all_current_urls = json.load(f1)

    all_urls = sorted(str((d['url'], d['name'])) for d in all_urls)
    all_current_urls = sorted(str((d['url'], d['name'])) for d in all_current_urls)

    if len(all_urls) > len(all_current_urls):
        urls_removed = '\n'.join(set(all_urls) - set(all_current_urls))
        assert False, (
            f'URLs number mismatch: {len(all_urls)} expected but new version contains {len(all_current_urls)}. '
            f'URLs removed:\n{urls_removed}.\n{instruction}'
        )
    elif len(all_urls) < len(all_current_urls):
        urls_added = '\n'.join(set(all_current_urls) - set(all_urls))
        assert False, (
            f'URLs number mismatch: {len(all_urls)} expected but new version contains {len(all_current_urls)}. '
            f'New URLs added:\n{urls_added}.\n{instruction}'
        )

    for url, new_url in zip(all_urls, all_current_urls):
        assert url == new_url, f'URL name mismatch found. {instruction}'
</file>

<file path="label_studio/tests/test_exception.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import json

import mock
import pytest

from .utils import project_id  # noqa


@pytest.mark.django_db
def test_custom_exception_handling(business_client, project_id):
    payload = dict(project=project_id, data={'test': 1})
    with mock.patch('data_manager.api.ViewAPI.create') as m:
        m.side_effect = Exception('Test')
        response = business_client.post(
            '/api/dm/views/',
            data=json.dumps(payload),
            content_type='application/json',
        )
        assert response.status_code == 500, response.content
        response_data = response.json()
        assert response_data['detail'] == 'Test'
        assert 'Exception: Test' in response_data['exc_info']
</file>

<file path="label_studio/tests/test_export.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import json

import pytest
from django.apps import apps
from tasks.models import Annotation, Prediction, Task
from tasks.serializers import AnnotationSerializer


@pytest.mark.skip(reason='HTX-868')
@pytest.mark.parametrize(
    'annotation_items, aggregated_class',
    [
        (
            [
                {
                    'id': 1,
                    'review_result': None,
                    'ground_truth': False,
                    'result': [
                        {
                            'id': '123',
                            'type': 'choices',
                            'value': {'choices': ['class_AA']},
                            'to_name': 'text',
                            'from_name': 'text_class',
                        }
                    ],
                    'created_at': 'test',
                    'updated_at': 'test',
                    'lead_time': None,
                    'completed_by': 2,
                }
            ],
            'class_AA',
        ),
        (
            [
                {
                    'id': 2,
                    'review_result': None,
                    'ground_truth': False,
                    'result': [
                        {
                            'id': '123',
                            'type': 'choices',
                            'value': {'choices': ['class_AA']},
                            'to_name': 'text',
                            'from_name': 'text_class',
                        }
                    ],
                    'created_at': '',
                    'updated_at': '',
                    'lead_time': None,
                    'completed_by': 4,
                },
                {
                    'id': 3,
                    'review_result': None,
                    'ground_truth': False,
                    'result': [
                        {
                            'id': '456',
                            'type': 'choices',
                            'value': {'choices': ['class_AA']},
                            'to_name': 'text',
                            'from_name': 'text_class',
                        }
                    ],
                    'created_at': '',
                    'updated_at': '',
                    'lead_time': None,
                    'completed_by': 4,
                },
            ],
            'class_AA',
        ),
        (
            [
                {
                    'id': 4,
                    'review_result': None,
                    'ground_truth': False,
                    'result': [
                        {
                            'id': '123',
                            'type': 'choices',
                            'value': {'choices': ['class_AA']},
                            'to_name': 'text',
                            'from_name': 'text_class',
                        }
                    ],
                    'created_at': '',
                    'updated_at': '',
                    'lead_time': None,
                    'completed_by': 6,
                },
                {
                    'id': 5,
                    'review_result': None,
                    'ground_truth': False,
                    'result': [
                        {
                            'id': '456',
                            'type': 'choices',
                            'value': {'choices': ['class_BB']},
                            'to_name': 'text',
                            'from_name': 'text_class',
                        }
                    ],
                    'created_at': '',
                    'updated_at': '',
                    'lead_time': None,
                    'completed_by': 6,
                },
                {
                    'id': 6,
                    'review_result': None,
                    'ground_truth': False,
                    'result': [
                        {
                            'id': '789',
                            'type': 'choices',
                            'value': {'choices': ['class_BB']},
                            'to_name': 'text',
                            'from_name': 'text_class',
                        }
                    ],
                    'created_at': '',
                    'updated_at': '',
                    'lead_time': None,
                    'completed_by': 6,
                },
            ],
            'class_BB',
        ),
        (
            [
                {
                    'id': 7,
                    'review_result': None,
                    'ground_truth': False,
                    'result': [
                        {
                            'id': '123',
                            'type': 'choices',
                            'value': {'choices': ['class_AA']},
                            'to_name': 'text',
                            'from_name': 'text_class',
                        }
                    ],
                    'created_at': '',
                    'updated_at': '',
                    'lead_time': None,
                    'completed_by': 10,
                }
            ],
            'class_AA',
        ),
    ],
)
@pytest.mark.parametrize(
    'finished, aggregator_type, return_task, num_task_in_result',
    [
        ('0', 'no_aggregation', '0', 2),
        ('1', 'no_aggregation', '0', 1),
        ('0', 'majority_vote', '0', 2),
        ('0', 'majority_vote', '1', 2),
        ('1', 'majority_vote', '1', 1),
    ],
)
@pytest.mark.django_db
def test_export(
    business_client,
    configured_project,
    finished,
    aggregator_type,
    return_task,
    num_task_in_result,
    annotation_items,
    aggregated_class,
):
    if aggregator_type == 'majority_vote' and not apps.is_installed('businesses'):
        pytest.skip('Not supported aggregation for open-source version')

    task_query = Task.objects.filter(project=configured_project.id)
    task = task_query.first()

    expected_annotations_for_task = set()
    for annotation in annotation_items:
        db_annotation = Annotation.objects.create(
            task=task, result=annotation['result'], completed_by=business_client.admin
        )
        db_annotation = AnnotationSerializer(db_annotation).data
        annotation['id'] = db_annotation['id']
        annotation['created_at'] = db_annotation['created_at']
        annotation['updated_at'] = db_annotation['updated_at']
        annotation['completed_by'] = business_client.admin.id
        expected_annotations_for_task.add(json.dumps(annotation))

    r = business_client.get(
        f'/api/projects/{configured_project.id}/results/',
        data={'finished': finished, 'aggregator_type': aggregator_type, 'return_task': return_task},
    )
    assert r.status_code == 200
    exports = r.json()

    # test expected number of objects returned
    assert len(exports) == num_task_in_result

    # test whether "id" or full task included in results
    if return_task == '0':
        task_with_annotation = next((t for t in exports if t['id'] == task.id))
        assert task_with_annotation['id'] == task.id
    elif return_task == '1':
        task_with_annotation = next((t for t in exports if t['id'] == task.id))
        assert task_with_annotation['data'] == task.data
    else:
        raise Exception('Incorrect return_task param in test: ' + str(return_task))

    # test how aggregation affects annotations
    if aggregator_type == 'no_aggregation':
        exported_annotations = set()
        for annotation in task_with_annotation['annotations']:
            exported_annotations.add(json.dumps(annotation))
        assert exported_annotations == expected_annotations_for_task
        if finished != '1':
            # we expect to see all tasks in exports...
            assert len(exports) == task_query.count()
            # ...as well as task without annotations (with empty results)
            assert all(len(t['annotations']) == 0 for t in exports if t['id'] != task.id)
    else:
        assert task_with_annotation['annotations'][0]['result'][0]['value']['choices'][0] == aggregated_class


@pytest.mark.skip(reason='HTX-868')
@pytest.mark.parametrize('finished', ('0', '1'))
@pytest.mark.parametrize('return_task', ('0', '1'))
@pytest.mark.parametrize('aggregator_type', ('no_aggregation', 'majority_vote'))
@pytest.mark.parametrize(
    'annotation_results, predictions',
    [
        (
            [
                [
                    {
                        'id': '123',
                        'from_name': 'text_class',
                        'to_name': 'text',
                        'type': 'choices',
                        'value': {'choices': ['class_A']},
                    }
                ]
            ],
            {
                'result': [
                    {
                        'id': '123',
                        'from_name': 'text_class',
                        'to_name': 'text',
                        'type': 'choices',
                        'value': {'choices': ['class_A']},
                    }
                ],
                'score': 0.5,
            },
        ),
        (
            [
                [
                    {
                        'id': '123',
                        'from_name': 'text_class',
                        'to_name': 'text',
                        'type': 'choices',
                        'value': {'choices': ['class_A']},
                    }
                ]
            ],
            None,
        ),
    ],
)
@pytest.mark.django_db
def test_export_with_predictions(
    business_client, configured_project, finished, return_task, aggregator_type, annotation_results, predictions
):
    if aggregator_type == 'majority_vote' and not apps.is_installed('businesses'):
        pytest.skip('Not supported aggregation for open-source version')

    tasks = Task.objects.filter(project=configured_project.id)
    task = tasks.first()
    for result in annotation_results:
        for r in result:
            r['completed_by'] = [business_client.admin.id]
        Annotation.objects.create(task=task, result=result, completed_by=business_client.admin)
    if predictions:
        for task in tasks:
            Prediction.objects.create(
                task=task, project=task.project, result=predictions['result'], score=predictions['score']
            )

    r = business_client.get(
        f'/api/projects/{configured_project.id}/results/',
        data={
            'finished': finished,
            'aggregator_type': aggregator_type,
            'return_task': return_task,
            'return_predictions': '1',
        },
    )
    assert r.status_code == 200
    exports = r.json()
    for task in exports:
        if predictions:
            assert task['predictions'][0]['result'] == predictions['result']
            assert task['predictions'][0]['score'] == predictions['score']
        else:
            assert task['predictions'] == []
</file>

<file path="label_studio/tests/test_has_lock.py">
import json

import pytest
from tests.utils import make_project


@pytest.mark.django_db
def test_has_lock(business_client):
    project = make_project({}, business_client.user, use_ml_backend=False)

    tasks = [
        {'data': {'location': 'London', 'text': 'text A'}},
    ]
    r = business_client.post(
        f'/api/projects/{project.id}/tasks/bulk', data=json.dumps(tasks), content_type='application/json'
    )
    assert r.status_code == 201

    task = project.tasks.first()

    annotation_data = {
        'task': task.id,
        'result': json.dumps(
            [{'from_name': 'text_class', 'to_name': 'text', 'value': {'labels': ['class_A'], 'start': 0, 'end': 1}}]
        ),
    }
    r = business_client.post('/api/tasks/{}/annotations/'.format(task.id), data=annotation_data)
    assert r.status_code == 201
    r = business_client.post('/api/tasks/{}/annotations/'.format(task.id), data=annotation_data)
    assert r.status_code == 201

    task.refresh_from_db()

    assert task.is_labeled is True

    task.is_labeled = False
    task.save()

    task.has_lock()
    task.refresh_from_db()

    assert task.is_labeled is True
</file>

<file path="label_studio/tests/test_invites.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""

import pytest


@pytest.mark.django_db
def test_signup_setting(business_client, client, settings):
    settings.DISABLE_SIGNUP_WITHOUT_LINK = True
    response = client.post('/user/signup', data={'email': 'test_user@example.com', 'password': 'test_password'})
    assert response.status_code == 403

    response = business_client.get('/api/invite')

    invite_url = response.json()['invite_url']

    response = client.post(invite_url, data={'email': 'test_user@example.com', 'password': 'test_password'})
    assert response.status_code == 302


@pytest.mark.django_db
def test_reset_token(business_client, client, settings):
    settings.DISABLE_SIGNUP_WITHOUT_LINK = True

    # get invite_url link and check it works
    response = business_client.get('/api/invite')
    invite_url = response.json()['invite_url']
    response = client.post(invite_url, data={'email': 'test_user@example.com', 'password': 'test_password'})
    assert response.status_code == 302

    response = business_client.post('/api/invite/reset-token')
    new_invite_url = response.json()['invite_url']

    # after reset old link returns permission denied
    client.logout()
    response = client.post(invite_url, data={'email': 'test_user1@example.com', 'password': 'test_password'})
    assert response.status_code == 403, response.content

    # but new one works fine
    response = client.post(new_invite_url, data={'email': 'test_user2@example.com', 'password': 'test_password'})
    assert response.status_code == 302


@pytest.mark.django_db
def test_reset_token_not_valid(business_client, client, settings):
    settings.DISABLE_SIGNUP_WITHOUT_LINK = False

    # disallow if token and does not match
    response = client.post(
        '/user/signup/?token=54321abce', data={'email': 'test_user1@example.com', 'password': 'test_password'}
    )
    assert response.status_code == 403, response.content


@pytest.mark.django_db
def test_token_get_not_post_shows_form(business_client, client, settings):
    settings.DISABLE_SIGNUP_WITHOUT_LINK = True

    # can't bypass post
    response = business_client.get('/api/invite')
    invite_url = response.json()['invite_url']
    response = client.get(f'{invite_url}&email=test_user@example.com&password=test_password')
    assert response.status_code == 200, response.content
    assert str(response.content).find('Create Account') != -1
</file>

<file path="label_studio/tests/test_io_storages.py">
import json

import pytest
from tests.utils import make_project


@pytest.mark.django_db
def test_gcs_storage_credentials_validation(business_client):
    project = make_project({}, business_client.user, use_ml_backend=False)

    data = {
        'project': project.id,
        'title': 'Test',
        'bucket': 'Test',
        'prefix': 'Test',
        'regex_filter': '',
        'use_blob_urls': False,
        'presign': True,
        'presign_ttl': '15',
        'google_project_id': '',
        'google_application_credentials': 'Test',
    }

    # upload tasks with annotations
    r = business_client.post(
        f'/api/storages/gcs?project={project.id}', data=json.dumps(data), content_type='application/json'
    )
    assert r.status_code == 400
    assert (
        'Google Application Credentials must be valid JSON string.'
        in r.json()['validation_errors']['non_field_errors'][0]
    )
</file>

<file path="label_studio/tests/test_next_task.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import json
import time
from unittest import mock

import pytest
from core.redis import redis_healthcheck
from django.apps import apps
from django.db.models import Q
from projects.models import Project
from tasks.models import Annotation, Prediction, Task

from .utils import (
    _client_is_annotator,
    invite_client_to_project,
    make_annotation,
    make_annotator,
    make_project,
    make_task,
)

_project_for_text_choices_onto_A_B_classes = dict(
    title='Test',
    is_published=True,
    sampling=Project.UNCERTAINTY,
    label_config="""
        <View>
          <Text name="meta_info" value="$meta_info"></Text>
          <Text name="text" value=" $text "></Text>
          <Choices name="text_class" choice="single">
            <Choice value="class_A"></Choice>
            <Choice value="class_B"></Choice>
          </Choices>
        </View>""",
)


@pytest.mark.parametrize(
    'project_config, tasks, status_code, expected_response_value_set',
    [
        (
            _project_for_text_choices_onto_A_B_classes,
            [
                {'data': {'meta_info': 'meta info A', 'text': 'text A'}},
                {'data': {'meta_info': 'meta info B', 'text': 'text B'}},
            ],
            200,
            {'id': 'uncompleted_task_ids'},
        ),
        (
            _project_for_text_choices_onto_A_B_classes,
            [
                {
                    'data': {'meta_info': 'meta info A', 'text': 'text A'},
                    'annotations': [{'result': [{'r': 1}], 'ground_truth': False}],
                },
                {'data': {'meta_info': 'meta info B', 'text': 'text B'}},
            ],
            200,
            {'id': 'uncompleted_task_ids'},
        ),
        (
            _project_for_text_choices_onto_A_B_classes,
            [
                {
                    'data': {'meta_info': 'meta info A', 'text': 'text A'},
                    'annotations': [{'result': [{'r': 1}], 'ground_truth': False}],
                },
                {
                    'data': {'meta_info': 'meta info B', 'text': 'text B'},
                    'annotations': [{'result': [{'r': 2}], 'ground_truth': False}],
                },
            ],
            404,
            {'detail': {'Not found.'}},
        ),
        # ground truth task still should be sampled regardless of who is a creator
        (
            _project_for_text_choices_onto_A_B_classes,
            [
                {
                    'data': {'meta_info': 'meta info A', 'text': 'text A'},
                    'annotations': [{'result': [{'r': 1}], 'ground_truth': True}],
                },
                {
                    'data': {'meta_info': 'meta info B', 'text': 'text B'},
                    'annotations': [{'result': [{'r': 2}], 'ground_truth': False}],
                },
            ],
            404,
            {'id': 'uncompleted_task_ids'},
        ),
        (
            dict(
                title='Test',
                is_published=True,
                sampling=Project.UNCERTAINTY,
                label_config="""
                <View>
                  <Text name="location" value="$location"></Text>
                  <Choices name="text_class" choice="single">
                    <Choice value="class_A"></Choice>
                    <Choice value="class_B"></Choice>
                  </Choices>
                </View>""",
            ),
            [{'data': {'location': 'London', 'text': 'text A'}}, {'data': {'location': 'London', 'text': 'text B'}}],
            200,
            {'id': 'uncompleted_task_ids'},
        ),
    ],
)
@pytest.mark.django_db
def test_next_task(business_client, any_client, project_config, tasks, status_code, expected_response_value_set):
    project = make_project(project_config, business_client.user)
    if _client_is_annotator(any_client):
        invite_client_to_project(any_client, project)

    # upload tasks with annotations
    r = business_client.post(
        f'/api/projects/{project.id}/tasks/bulk', data=json.dumps(tasks), content_type='application/json'
    )
    assert r.status_code == 201

    # make sure any annotation was made by current client
    Annotation.objects.all().update(completed_by=any_client.annotator)

    # collect uncompleted task ids to verify that only them are seen in the next labeling steps
    uncompleted_task_ids = set()
    for t in Task.objects.all():
        if not t.annotations.filter(ground_truth=False).exists():
            uncompleted_task_ids.add(t.id)

    r = any_client.get(f'/api/projects/{project.id}/next')
    assert r.status_code == status_code
    rdata = json.loads(r.content)
    if r.status_code != 404:
        for response_key, expected_value_set in expected_response_value_set.items():
            if expected_value_set == 'uncompleted_task_ids':
                expected_value_set = uncompleted_task_ids
            assert (
                rdata[response_key] in expected_value_set
            ), f'Failed on response {rdata}: expecting value set "{expected_value_set}" for key "{response_key}"'


@pytest.mark.parametrize(
    'project_config, tasks, predictions, annotations, num_annotators, status_code, prelabeling_result',
    [
        # no annotations, second task is chosen due to active learning
        (
            dict(
                title='Test',
                is_published=True,
                sampling=Project.UNCERTAINTY,
                model_version='12345',
                label_config="""
    <View>
      <Text name="location" value="$location"></Text>
      <Choices name="text_class" choice="single">
        <Choice value="class_A"></Choice>
        <Choice value="class_B"></Choice>
      </Choices>
    </View>""",
            ),
            [{'data': {'location': 'London', 'text': 'text A'}}, {'data': {'location': 'London', 'text': 'text B'}}],
            [
                {'result': [{'some': 'prediction A'}], 'score': 0.9, 'cluster': 0},
                {'result': [{'some': 'prediction B'}], 'score': 0.5, 'cluster': 0},
            ],
            [
                None,
                None,
            ],
            1,
            200,
            [{'some': 'prediction B'}],
        ),
        # no annotations, first task is chosen due to active learning
        (
            dict(
                title='Test',
                is_published=True,
                sampling=Project.UNCERTAINTY,
                model_version='12345',
                label_config="""
<View>
  <Text name="location" value="$location"></Text>
  <Choices name="text_class" choice="single">
    <Choice value="class_A"></Choice>
    <Choice value="class_B"></Choice>
  </Choices>
</View>""",
            ),
            [{'data': {'location': 'London', 'text': 'text A'}}, {'data': {'location': 'London', 'text': 'text B'}}],
            [
                {'result': [{'some': 'prediction A'}], 'score': 0.5, 'cluster': 0},
                {'result': [{'some': 'prediction B'}], 'score': 0.9, 'cluster': 0},
            ],
            [
                None,
                None,
            ],
            1,
            200,
            [{'some': 'prediction A'}],
        ),
        # first task annotation, third task is chosen due to active learning
        (
            dict(
                title='Test',
                is_published=True,
                sampling=Project.UNCERTAINTY,
                maximum_annotations=1,
                model_version='12345',
                label_config="""
<View>
  <Text name="location" value="$location"></Text>
  <Choices name="text_class" choice="single">
    <Choice value="class_A"></Choice>
    <Choice value="class_B"></Choice>
  </Choices>
</View>""",
            ),
            [
                {'data': {'location': 'London', 'text': 'text A'}},
                {'data': {'location': 'London', 'text': 'text B'}},
                {'data': {'location': 'London', 'text': 'text C'}},
            ],
            [
                {'result': [{'some': 'prediction A'}], 'score': 0.5, 'cluster': 0},
                {'result': [{'some': 'prediction B'}], 'score': 0.9, 'cluster': 1},
                {'result': [{'some': 'prediction C'}], 'score': 0.8, 'cluster': 1},
            ],
            [
                {'result': [{'some': 'prediction A'}]},
                None,
                None,
            ],
            1,
            200,
            [{'some': 'prediction C'}],
        ),
        # first task annotation, forth task is chosen due to active learning (though task with lowest score exists but in the same cluster)
        (
            dict(
                title='Test',
                is_published=True,
                sampling=Project.UNCERTAINTY,
                model_version='12345',
                label_config="""
<View>
  <Text name="location" value="$location"></Text>
  <Choices name="text_class" choice="single">
    <Choice value="class_A"></Choice>
    <Choice value="class_B"></Choice>
  </Choices>
</View>""",
            ),
            [
                {'data': {'location': 'London', 'text': 'text A'}},
                {'data': {'location': 'London', 'text': 'text A 2'}},
                {'data': {'location': 'London', 'text': 'text B'}},
                {'data': {'location': 'London', 'text': 'text C'}},
            ],
            [
                {'result': [{'some': 'prediction A'}], 'score': 0.5, 'cluster': 0},
                {'result': [{'some': 'prediction A'}], 'score': 0.1, 'cluster': 0},
                {'result': [{'some': 'prediction B'}], 'score': 0.9, 'cluster': 1},
                {'result': [{'some': 'prediction C'}], 'score': 0.8, 'cluster': 1},
            ],
            [
                {'result': [{'some': 'prediction A'}]},
                None,
                None,
                None,
            ],
            1,
            200,
            [{'some': 'prediction C'}],
        ),
        # lowest prediction is chosen from least solved cluster
        (
            dict(
                title='Test',
                is_published=True,
                sampling=Project.UNCERTAINTY,
                model_version='12345',
                label_config="""
<View>
  <Text name="location" value="$location"></Text>
  <Choices name="text_class" choice="single">
    <Choice value="class_A"></Choice>
    <Choice value="class_B"></Choice>
  </Choices>
</View>""",
            ),
            [
                {'data': {'location': 'London', 'text': 'text A'}},
                {'data': {'location': 'London', 'text': 'text A 2'}},
                {'data': {'location': 'London', 'text': 'text A 3'}},
                {'data': {'location': 'London', 'text': 'text B'}},
                {'data': {'location': 'London', 'text': 'text C'}},
                {'data': {'location': 'London', 'text': 'text C 2'}},
            ],
            [
                {'result': [{'some': 'prediction A'}], 'score': 0.5, 'cluster': 0},
                {'result': [{'some': 'prediction A'}], 'score': 0.2, 'cluster': 0},
                {'result': [{'some': 'prediction A1'}], 'score': 0.1, 'cluster': 0},
                {'result': [{'some': 'prediction B'}], 'score': 0.9, 'cluster': 1},
                {'result': [{'some': 'prediction C'}], 'score': 0.8, 'cluster': 1},
                {'result': [{'some': 'prediction C'}], 'score': 0.8, 'cluster': 1},
            ],
            [
                {'result': [{'some': 'prediction A'}]},
                None,
                None,
                None,
                {'result': [{'some': 'prediction C'}]},
                {'result': [{'some': 'prediction C'}]},
            ],
            1,
            200,
            [{'some': 'prediction A1'}],
        ),
        # first task annotation, labeling is continued with the same cluster
        (
            dict(
                title='Test',
                is_published=True,
                sampling=Project.UNCERTAINTY,
                model_version='12345',
                label_config="""
    <View>
      <Text name="location" value="$location"></Text>
      <Choices name="text_class" choice="single">
        <Choice value="class_A"></Choice>
        <Choice value="class_B"></Choice>
      </Choices>
    </View>""",
            ),
            [
                {'data': {'location': 'London', 'text': 'text A'}},
                {'data': {'location': 'London', 'text': 'text B'}},
                {'data': {'location': 'London', 'text': 'text C'}},
            ],
            [
                {'result': [{'some': 'prediction A'}], 'score': 0.5, 'cluster': 0},
                {'result': [{'some': 'prediction B'}], 'score': 0.9, 'cluster': 0},
                {'result': [{'some': 'prediction C'}], 'score': 0.8, 'cluster': 0},
            ],
            [
                {'result': [{'some': 'prediction A'}]},
                None,
                None,
            ],
            1,
            200,
            [{'some': 'prediction C'}],
        ),
        # first task annotation, third task is chosen since cluster is marked as None (no clustering)
        (
            dict(
                title='Test',
                is_published=True,
                sampling=Project.UNCERTAINTY,
                model_version='12345',
                label_config="""
<View>
  <Text name="location" value="$location"></Text>
  <Choices name="text_class" choice="single">
    <Choice value="class_A"></Choice>
    <Choice value="class_B"></Choice>
  </Choices>
</View>""",
            ),
            [
                {'data': {'location': 'London', 'text': 'text A'}},
                {'data': {'location': 'London', 'text': 'text B'}},
                {'data': {'location': 'London', 'text': 'text C'}},
            ],
            [
                {'result': [{'some': 'prediction A'}], 'score': 0.5, 'cluster': None},
                {'result': [{'some': 'prediction B'}], 'score': 0.9, 'cluster': None},
                {'result': [{'some': 'prediction C'}], 'score': 0.8, 'cluster': None},
            ],
            [
                {'result': [{'some': 'prediction A'}]},
                None,
                None,
            ],
            1,
            200,
            [{'some': 'prediction C'}],
        ),
        # when some of the tasks are partially labeled, regardless scores sampling operates on depth-first (try to complete all tasks asap)
        (
            dict(
                title='Test',
                is_published=True,
                sampling=Project.UNCERTAINTY,
                model_version='12345',
                maximum_annotations=2,
                label_config="""
<View>
  <Text name="location" value="$location"></Text>
  <Choices name="text_class" choice="single">
    <Choice value="class_A"></Choice>
    <Choice value="class_B"></Choice>
  </Choices>
</View>""",
            ),
            [
                {'data': {'location': 'London', 'text': 'text A'}},
                {'data': {'location': 'London', 'text': 'text B'}},
                {'data': {'location': 'London', 'text': 'text C'}},
                {'data': {'location': 'London', 'text': 'text D'}},
                {'data': {'location': 'London', 'text': 'text E'}},
            ],
            [
                {'result': [{'some': 'prediction A'}], 'score': 0.6, 'cluster': None},
                {'result': [{'some': 'prediction B'}], 'score': 0.5, 'cluster': None},
                {'result': [{'some': 'prediction C'}], 'score': 0.8, 'cluster': None},
                {'result': [{'some': 'prediction D'}], 'score': 0.4, 'cluster': None},
                {'result': [{'some': 'prediction E'}], 'score': 0.2, 'cluster': None},
            ],
            [{'result': [{'some': 'prediction A'}]}, None, None, None, None],
            2,
            200,
            [{'some': 'prediction A'}],
        ),
    ],
    ids=[
        'no annotations, second task is chosen due to active learning',
        'no annotations, first task is chosen due to active learning',
        'first task annotation, third task is chosen due to active learning',
        'first task annotation, forth task is chosen due to active learning (though task with lowest score exists but in the same cluster)',
        'lowest prediction is chosen from least solved cluster',
        'first task annotation, labeling is continued with the same cluster',
        'first task annotation, third task is chosen since cluster is marked as None (no clustering)',
        'when some of the tasks are partially labeled, regardless scores sampling operates on depth-first (try to complete all tasks asap)',
    ],
)
@pytest.mark.django_db
def test_next_task_with_active_learning(
    mocker,
    business_client,
    any_client,
    annotator2_client,
    project_config,
    tasks,
    predictions,
    annotations,
    num_annotators,
    status_code,
    prelabeling_result,
):

    project = make_project(project_config, business_client.user, use_ml_backend=False)
    if _client_is_annotator(any_client):
        invite_client_to_project(any_client, project)
    if _client_is_annotator(annotator2_client):
        invite_client_to_project(annotator2_client, project)

    class MockAnnotatorCount:
        def count(self):
            return num_annotators

    mocker.patch.object(Project, 'annotators', return_value=MockAnnotatorCount())

    for task, prediction, annotation in zip(tasks, predictions, annotations):
        task = make_task(task, project)
        Prediction.objects.create(task=task, project=task.project, model_version=project.model_version, **prediction)
        if annotation is not None:
            completed_by = any_client.annotator if num_annotators == 1 else annotator2_client.annotator
            Annotation.objects.create(task=task, completed_by=completed_by, project=project, **annotation)
    r = any_client.get(f'/api/projects/{project.id}/next')
    assert r.status_code == status_code
    rdata = json.loads(r.content)
    if r.status_code == 200:
        assert rdata['predictions'][0]['result'] == prelabeling_result


@pytest.mark.django_db
def test_active_learning_with_uploaded_predictions(business_client):
    config = dict(
        title='Test',
        is_published=True,
        sampling=Project.UNCERTAINTY,
        label_config="""
            <View>
              <Text name="location" value="$text"></Text>
              <Choices name="text_class" choice="single">
                <Choice value="class_A"></Choice>
                <Choice value="class_B"></Choice>
              </Choices>
            </View>""",
    )
    project = make_project(config, business_client.user, use_ml_backend=False)
    result = [{'from_name': 'text_class', 'to_name': 'text', 'type': 'choices', 'value': {'choices': ['class_A']}}]
    tasks = [
        {'data': {'text': 'score = 0.5'}, 'predictions': [{'result': result, 'score': 0.5}]},
        {'data': {'text': 'score = 0.1'}, 'predictions': [{'result': result, 'score': 0.1}]},
        {'data': {'text': 'score = 0.3'}, 'predictions': [{'result': result, 'score': 0.3}]},
        {'data': {'text': 'score = 0.2'}, 'predictions': [{'result': result, 'score': 0.2}]},
        {'data': {'text': 'score = 0.4'}, 'predictions': [{'result': result, 'score': 0.4}]},
    ]
    # upload tasks with predictions
    r = business_client.post(
        f'/api/projects/{project.id}/tasks/bulk/', data=json.dumps(tasks), content_type='application/json'
    )
    assert r.status_code == 201

    def get_next_task_id_and_complete_it():
        r = business_client.get(f'/api/projects/{project.id}/next')
        assert r.status_code == 200
        task = json.loads(r.content)

        # and completes it
        r = business_client.post(
            f'/api/tasks/{task["id"]}/annotations/', data={'task': task['id'], 'result': json.dumps(result)}
        )
        assert r.status_code == 201
        return task['data']['text']

    assert project.model_version == ''

    # tasks will be shown according to the uploaded scores
    assert get_next_task_id_and_complete_it() == 'score = 0.1'
    assert get_next_task_id_and_complete_it() == 'score = 0.2'
    assert get_next_task_id_and_complete_it() == 'score = 0.3'
    assert get_next_task_id_and_complete_it() == 'score = 0.4'
    assert get_next_task_id_and_complete_it() == 'score = 0.5'


@pytest.mark.skipif(not redis_healthcheck(), reason='Multi user locks only supported with redis enabled')
@pytest.mark.parametrize('sampling', (Project.UNIFORM, Project.UNCERTAINTY, Project.SEQUENCE))
@pytest.mark.django_db
def test_label_races(configured_project, business_client, sampling):
    config = dict(
        title='test_label_races',
        is_published=True,
        label_config="""
            <View>
              <Text name="text" value="$text"></Text>
              <Choices name="text_class" choice="single">
                <Choice value="class_A"></Choice>
                <Choice value="class_B"></Choice>
              </Choices>
            </View>""",
    )
    project = make_project(config, business_client.user)
    project.sampling = sampling
    project.save()
    id1 = make_task({'data': {'text': 'aaa'}}, project).id
    id2 = make_task({'data': {'text': 'bbb'}}, project).id
    ann1 = make_annotator({'email': 'ann1@testlabelraces.com'}, project, True)
    ann2 = make_annotator({'email': 'ann2@testlabelraces.com'}, project, True)

    # ann1 takes task id1
    r = ann1.get(f'/api/projects/{project.id}/next')
    assert r.status_code == 200
    selected_id = json.loads(r.content)['id']
    if project.sampling in (Project.UNIFORM, Project.UNCERTAINTY):
        assert selected_id in (id1, id2)
        id2 = list({id1, id2} - {selected_id})[0]
    else:
        assert selected_id == id1

    # ann2 takes task id2 because id1 is locked by ann1
    r = ann2.get(f'/api/projects/{project.id}/next')
    assert r.status_code == 200
    assert json.loads(r.content)['id'] == id2


@pytest.mark.skipif(not redis_healthcheck(), reason='Multi user locks only supported with redis enabled')
@pytest.mark.parametrize('sampling', (Project.UNIFORM, Project.UNCERTAINTY, Project.SEQUENCE))
@pytest.mark.django_db
def test_label_races_after_all_taken(configured_project, business_client, sampling):
    config = dict(
        title='test_label_races',
        is_published=True,
        label_config="""
            <View>
              <Text name="text" value="$text"></Text>
              <Choices name="text_class" choice="single">
                <Choice value="class_A"></Choice>
                <Choice value="class_B"></Choice>
              </Choices>
            </View>""",
    )
    project = make_project(config, business_client.user)
    project.sampling = sampling
    project.save()
    id1 = make_task({'data': {'text': 'aaa'}}, project).id
    id2 = make_task({'data': {'text': 'bbb'}}, project).id
    ann1 = make_annotator({'email': 'ann1@testlabelracesalltaken.com'}, project, True)
    ann2 = make_annotator({'email': 'ann2@testlabelracesalltaken.com'}, project, True)

    # ann1 takes task id1
    r = ann1.get(f'/api/projects/{project.id}/next')
    assert r.status_code == 200
    selected_id = json.loads(r.content)['id']
    if project.sampling in (Project.UNIFORM, Project.UNCERTAINTY):
        assert selected_id in (id1, id2)
        id2 = list({id1, id2} - {selected_id})[0]
    else:
        assert selected_id == id1
    id1 = selected_id

    # ann2 takes task id2 because id1 is locked by ann1
    r = ann2.get(f'/api/projects/{project.id}/next')
    assert r.status_code == 200
    assert json.loads(r.content)['id'] == id2

    # then ann2 takes id2 again
    r = ann2.get(f'/api/projects/{project.id}/next')
    assert r.status_code == 200
    assert json.loads(r.content)['id'] == id2

    # ann1 takes id1
    r = ann1.get(f'/api/projects/{project.id}/next')
    assert r.status_code == 200
    assert json.loads(r.content)['id'] == id1


@pytest.mark.django_db
def test_breadth_first_simple(business_client):
    config = dict(
        title='test_label_races',
        is_published=True,
        maximum_annotations=2,
        label_config="""
            <View>
              <Text name="text" value="$text"></Text>
              <Choices name="text_class" choice="single" toName="text">
                <Choice value="class_A"></Choice>
                <Choice value="class_B"></Choice>
              </Choices>
            </View>""",
    )
    annotation_result = json.dumps(
        [{'from_name': 'text_class', 'to_name': 'text', 'type': 'choices', 'value': {'choices': ['class_A']}}]
    )
    project = make_project(config, business_client.user)
    project.sampling = Project.SEQUENCE
    project.save()
    id1 = make_task({'data': {'text': 'aaa'}}, project).id
    id2 = make_task({'data': {'text': 'bbb'}}, project).id
    ann1 = make_annotator({'email': 'ann1@testbreadthfirst.com'}, project, True)
    ann2 = make_annotator({'email': 'ann2@testbreadthfirst.com'}, project, True)

    # ann1 takes first task
    r = ann1.get(f'/api/projects/{project.id}/next')
    assert r.status_code == 200
    assert json.loads(r.content)['id'] == id1

    # and completes it
    r = ann1.post(f'/api/tasks/{id1}/annotations/', data={'task': id1, 'result': annotation_result})
    assert r.status_code == 201

    # ann2 takes first task because maximum_annotations=2
    r = ann2.get(f'/api/projects/{project.id}/next')
    assert r.status_code == 200
    assert json.loads(r.content)['id'] == id1

    # and completes it
    r = ann2.post(f'/api/tasks/{id1}/annotations/', data={'task': id1, 'result': annotation_result})
    assert r.status_code == 201
    completed_task = Task.objects.get(id=id1)
    assert completed_task.is_labeled

    if apps.is_installed('businesses'):
        assert completed_task.accuracy == 1.0

    # ann2 takes second task because only one unlabeled left
    r = ann2.get(f'/api/projects/{project.id}/next')
    assert r.status_code == 200
    assert json.loads(r.content)['id'] == id2


@pytest.mark.django_db
def test_breadth_first_overlap_3(business_client):
    config = dict(
        title='test_label_races',
        is_published=True,
        maximum_annotations=3,
        label_config="""
            <View>
              <Text name="text" value="$text"></Text>
              <Choices name="text_class" choice="single" toName="text">
                <Choice value="class_A"></Choice>
                <Choice value="class_B"></Choice>
              </Choices>
            </View>""",
    )
    annotation_result = json.dumps(
        [{'from_name': 'text_class', 'to_name': 'text', 'type': 'choices', 'value': {'choices': ['class_A']}}]
    )
    project = make_project(config, business_client.user)
    project.sampling = Project.UNIFORM
    project.save()

    def complete_task(annotator):
        _r = annotator.get(f'/api/projects/{project.id}/next')
        assert _r.status_code == 200
        task_id = json.loads(_r.content)['id']
        annotator.post(f'/api/tasks/{task_id}/annotations/', data={'task': task_id, 'result': annotation_result})
        return task_id

    make_task({'data': {'text': 'aaa'}}, project).id
    make_task({'data': {'text': 'bbb'}}, project).id
    make_task({'data': {'text': 'ccc'}}, project).id

    ann1 = make_annotator({'email': 'ann1@testbreadthfirstoverlap3.com'}, project, True)
    ann2 = make_annotator({'email': 'ann2@testbreadthfirstoverlap3.com'}, project, True)
    ann3 = make_annotator({'email': 'ann3@testbreadthfirstoverlap3.com'}, project, True)

    # ann1, ann2, ann3 should follow breadth-first scheme: trying to complete the tasks as fast as possible
    task_id_ann1 = complete_task(ann1)
    task_id_ann2 = complete_task(ann2)
    assert task_id_ann2 == task_id_ann1
    complete_task(ann1)
    complete_task(ann1)
    task_id_ann3 = complete_task(ann3)
    assert task_id_ann2 == task_id_ann3
    task_id_ann2 = complete_task(ann2)
    task_id_ann3 = complete_task(ann3)
    assert task_id_ann2 == task_id_ann3


@pytest.mark.skipif(not redis_healthcheck(), reason='Multi user locks only supported with redis enabled')
@pytest.mark.django_db
def test_try_take_last_task_at_the_same_time(business_client):
    config = dict(
        title='test_try_take_last_task_at_the_same_time',
        is_published=True,
        maximum_annotations=2,
        label_config="""
            <View>
              <Text name="text" value="$text"></Text>
              <Choices name="text_class" choice="single">
                <Choice value="class_A"></Choice>
                <Choice value="class_B"></Choice>
              </Choices>
            </View>""",
    )
    annotation_result = json.dumps(
        [{'from_name': 'text_class', 'to_name': 'text', 'type': 'choices', 'value': {'choices': ['class_A']}}]
    )
    project = make_project(config, business_client.user)
    project.sampling = Project.SEQUENCE
    project.save()

    def complete_task(annotator):
        _r = annotator.get(f'/api/projects/{project.id}/next')
        assert _r.status_code == 200
        task_id = json.loads(_r.content)['id']
        annotator.post(f'/api/tasks/{task_id}/annotations/', data={'task': task_id, 'result': annotation_result})
        return task_id

    make_task({'data': {'text': 'aaa'}}, project)
    make_task({'data': {'text': 'bbb'}}, project)

    ann1 = make_annotator({'email': 'ann1@lasttask.com'}, project, True)
    ann2 = make_annotator({'email': 'ann2@lasttask.com'}, project, True)
    ann3 = make_annotator({'email': 'ann3@lasttask.com'}, project, True)

    # ann1, ann2 complete first task, then ann3 completes last task
    complete_task(ann1)
    complete_task(ann2)
    complete_task(ann3)

    # only one annotator can take the last task
    _r = ann1.get(f'/api/projects/{project.id}/next')
    assert _r.status_code == 200

    _r = ann2.get(f'/api/projects/{project.id}/next')
    assert _r.status_code == 404

    _r = ann3.get(f'/api/projects/{project.id}/next')
    assert _r.status_code == 404


@pytest.mark.skipif(not redis_healthcheck(), reason='Multi user locks only supported with redis enabled')
@pytest.mark.django_db
def test_breadth_first_with_label_race(configured_project, business_client):
    config = dict(
        title='test_label_races',
        is_published=True,
        maximum_annotations=2,
        label_config="""
            <View>
              <Text name="text" value="$text"></Text>
              <Choices name="text_class" choice="single">
                <Choice value="class_A"></Choice>
                <Choice value="class_B"></Choice>
              </Choices>
            </View>""",
    )
    annotation_result = json.dumps(
        [{'from_name': 'text_class', 'to_name': 'text', 'type': 'choices', 'value': {'choices': ['class_A']}}]
    )
    project = make_project(config, business_client.user)
    project.sampling = Project.SEQUENCE
    project.save()
    id1 = make_task({'data': {'text': 'aaa'}}, project).id
    id2 = make_task({'data': {'text': 'bbb'}}, project).id
    ann1 = make_annotator({'email': 'ann1@testbreadthlabelraces.com'}, project, True)
    ann2 = make_annotator({'email': 'ann2@testbreadthlabelraces.com'}, project, True)

    # ann1 takes first task
    r = ann1.get(f'/api/projects/{project.id}/next')
    assert r.status_code == 200
    assert json.loads(r.content)['id'] == id1

    # and completes it
    r = ann1.post(f'/api/tasks/{id1}/annotations/', data={'task': id1, 'result': annotation_result})
    assert r.status_code == 201

    # ann1 takes second task and freezes
    r = ann1.get(f'/api/projects/{project.id}/next')
    assert r.status_code == 200
    assert json.loads(r.content)['id'] == id2

    # ann2 takes first task because maximum_annotations=2
    r = ann2.get(f'/api/projects/{project.id}/next')
    assert r.status_code == 200
    assert json.loads(r.content)['id'] == id1

    # and completes it
    r = ann2.post(f'/api/tasks/{id1}/annotations/', data={'task': id1, 'result': annotation_result})
    assert r.status_code == 201
    completed_task = Task.objects.get(id=id1)
    assert completed_task.is_labeled
    if apps.is_installed('businesses'):
        assert completed_task.accuracy == 1.0

    # ann2 takes 2nd task because maximum_annotations=2
    r = ann2.get(f'/api/projects/{project.id}/next')
    assert r.status_code == 200
    assert json.loads(r.content)['id'] == id2

    # ann1 takes second task again
    r = ann1.get(f'/api/projects/{project.id}/next')
    assert r.status_code == 200
    assert json.loads(r.content)['id'] == id2


@pytest.mark.skipif(not redis_healthcheck(), reason='Multi user locks only supported with redis enabled')
@pytest.mark.django_db
def test_label_race_with_overlap(configured_project, business_client):
    """
    2 annotators takes and finish annotations one by one
    depending on project settings overlap

    create project
    make annotation result
    make 2 annotators
    bulk create tasks
    change project settings
    check overlap
    next annotate tasks

    check code comments
    """
    config = dict(
        title='test_label_races',
        is_published=True,
        label_config="""
            <View>
              <Text name="text" value="$text"></Text>
              <Choices name="text_class" choice="single">
                <Choice value="class_A"></Choice>
                <Choice value="class_B"></Choice>
              </Choices>
            </View>""",
    )
    annotation_result = json.dumps(
        [{'from_name': 'text_class', 'to_name': 'text', 'type': 'choices', 'value': {'choices': ['class_A']}}]
    )
    project = make_project(config, business_client.user)
    project.sampling = Project.SEQUENCE
    project.save()

    ann1 = make_annotator({'email': 'ann1@testlabelracewithoverlap.com'}, project, True)
    ann2 = make_annotator({'email': 'ann2@testlabelracewithoverlap.com'}, project, True)

    # create tasks
    tasks = []
    num_tasks = 2
    for i in range(num_tasks):
        tasks.append({'data': {'text': f'this is {str(i)}'}})
    r = business_client.post(
        f'/api/projects/{project.id}/tasks/bulk/', data=json.dumps(tasks), content_type='application/json'
    )
    assert r.status_code == 201

    # set overlap
    r = business_client.patch(
        f'/api/projects/{project.id}/',
        data=json.dumps({'maximum_annotations': 2, 'overlap_cohort_percentage': 50, 'show_overlap_first': True}),
        content_type='application/json',
    )
    assert r.status_code == 200

    t = Task.objects.filter(project=project.id).filter(overlap=2)
    assert t.count() == 1
    t1 = Task.objects.filter(project=project.id).filter(overlap=1)
    assert t1.count() == 1
    overlap_id = t.first().id
    other_id = t1.first().id

    # ann1 takes first task
    r = ann1.get(f'/api/projects/{project.id}/next')
    assert r.status_code == 200
    assert json.loads(r.content)['id'] == overlap_id

    # ann2 takes the same task, since overlap = 2
    r = ann2.get(f'/api/projects/{project.id}/next')
    assert r.status_code == 200
    assert json.loads(r.content)['id'] == overlap_id
    assert Task.objects.get(id=overlap_id).has_lock()

    # ann1 takes next task, it is also overlapped because we force show_overlapped_first=True
    r = ann1.get(f'/api/projects/{project.id}/next')
    assert r.status_code == 200
    assert json.loads(r.content)['id'] == overlap_id

    # ann2 completes overlapped task
    r = ann2.post(f'/api/tasks/{overlap_id}/annotations/', data={'task': overlap_id, 'result': annotation_result})
    assert r.status_code == 201

    # ann1 takes next task, and now it is overlapped, since lock was released by ann2 annotation
    r = ann1.get(f'/api/projects/{project.id}/next')
    assert r.status_code == 200
    assert json.loads(r.content)['id'] == overlap_id

    # ann1 completes overlapped task
    r = ann1.post(f'/api/tasks/{overlap_id}/annotations/', data={'task': overlap_id, 'result': annotation_result})
    assert r.status_code == 201

    # ann1 takes next task, now it is another one since overlapped is labeled
    r = ann1.get(f'/api/projects/{project.id}/next')
    assert r.status_code == 200
    assert json.loads(r.content)['id'] == other_id


@pytest.mark.skipif(not redis_healthcheck(), reason='Multi user locks only supported with redis enabled')
@pytest.mark.django_db
def test_label_w_drafts_race_with_overlap(configured_project, business_client):
    """
    2 annotators takes and leaves with draft annotations one by one
    depending on project settings overlap

    create project
    make annotation result
    make 2 annotators
    bulk create tasks
    change project settings
    check overlap
    next annotate tasks

    check code comments
    """
    config = dict(
        title='test_label_races',
        is_published=True,
        label_config="""
            <View>
              <Text name="text" value="$text"></Text>
              <Choices name="text_class" choice="single">
                <Choice value="class_A"></Choice>
                <Choice value="class_B"></Choice>
              </Choices>
            </View>""",
    )
    annotation_result = json.dumps(
        [{'from_name': 'text_class', 'to_name': 'text', 'type': 'choices', 'value': {'choices': ['class_A']}}]
    )

    project = make_project(config, business_client.user)
    project.sampling = Project.SEQUENCE
    project.save()

    ann1 = make_annotator({'email': 'ann1@testlabelracewdrafts.com'}, project, True)
    ann2 = make_annotator({'email': 'ann2@testlabelracewdrafts.com'}, project, True)

    # create tasks
    tasks = []
    num_tasks = 2
    for i in range(num_tasks):
        tasks.append({'data': {'text': f'this is {str(i)}'}})
    r = business_client.post(
        f'/api/projects/{project.id}/tasks/bulk/', data=json.dumps(tasks), content_type='application/json'
    )
    assert r.status_code == 201

    # set overlap
    r = business_client.patch(
        f'/api/projects/{project.id}/',
        data=json.dumps({'maximum_annotations': 2, 'overlap_cohort_percentage': 50, 'show_overlap_first': True}),
        content_type='application/json',
    )
    assert r.status_code == 200

    t = Task.objects.filter(project=project.id).filter(overlap=2)
    assert t.count() == 1
    t1 = Task.objects.filter(project=project.id).filter(overlap=1)
    assert t1.count() == 1
    overlap_id = t.first().id
    other_id = t1.first().id

    annotation_draft_result = {
        'task': overlap_id,
        'lead_time': 640.279,
        'draft': json.dumps(
            [{'from_name': 'text_class', 'to_name': 'text', 'type': 'choices', 'value': {'choices': ['class_A']}}]
        ),
        'result': json.dumps([]),
    }

    # ann1 takes first task
    r = ann1.get(f'/api/projects/{project.id}/next')
    assert r.status_code == 200
    assert json.loads(r.content)['id'] == overlap_id

    # ann2 takes the same task, since overlap = 2
    r = ann2.get(f'/api/projects/{project.id}/next')
    assert r.status_code == 200
    assert json.loads(r.content)['id'] == overlap_id
    assert Task.objects.get(id=overlap_id).has_lock()

    # ann1 takes next task, it is also overlapped because we force show_overlapped_first=True
    r = ann1.get(f'/api/projects/{project.id}/next')
    assert r.status_code == 200
    assert json.loads(r.content)['id'] == overlap_id

    # ann2 send draft for overlapped task
    r = ann2.post(f'/api/tasks/{overlap_id}/annotations/', data=annotation_draft_result)
    assert r.status_code == 201

    # ann1 takes next task, and now it is overlapped, since lock was released by ann2 annotation
    # TODO was?
    r = ann1.get(f'/api/projects/{project.id}/next')
    assert r.status_code == 200
    assert json.loads(r.content)['id'] == overlap_id

    # ann1 completes overlapped task
    r = ann1.post(f'/api/tasks/{overlap_id}/annotations/', data={'task': overlap_id, 'result': annotation_result})
    assert r.status_code == 201

    # ann1 takes next task, now it is another one since overlapped is labeled
    r = ann1.get(f'/api/projects/{project.id}/next')
    assert r.status_code == 200
    print(overlap_id, other_id)
    assert json.loads(r.content)['id'] == other_id

    # try again
    r = ann1.get(f'/api/projects/{project.id}/next')
    assert r.status_code == 200


@pytest.mark.django_db
def test_fetch_final_taken_task(business_client):
    config = dict(
        title='test_label_races',
        is_published=True,
        label_config="""
            <View>
              <Text name="text" value="$text"></Text>
              <Choices name="text_class" choice="single" toName="text">
                <Choice value="class_A"></Choice>
                <Choice value="class_B"></Choice>
              </Choices>
            </View>""",
    )
    annotation_result = json.dumps(
        [{'from_name': 'text_class', 'to_name': 'text', 'type': 'choices', 'value': {'choices': ['class_A']}}]
    )
    project = make_project(config, business_client.user)
    project.sampling = Project.SEQUENCE
    project.save()

    ann1 = make_annotator({'email': 'ann1@testfetchfinal.com'}, project, True)
    ann2 = make_annotator({'email': 'ann2@testfetchfinal.com'}, project, True)

    # create tasks
    tasks = []
    num_tasks = 2
    for i in range(num_tasks):
        tasks.append({'data': {'text': f'this is {str(i)}'}})
    r = business_client.post(
        f'/api/projects/{project.id}/tasks/bulk/', data=json.dumps(tasks), content_type='application/json'
    )
    assert r.status_code == 201

    # set max annotations
    r = business_client.patch(
        f'/api/projects/{project.id}/', data=json.dumps({'maximum_annotations': 2}), content_type='application/json'
    )
    assert r.status_code == 200

    print('ann1 takes any task and complete it')
    r = ann1.get(f'/api/projects/{project.id}/next')
    task_id = json.loads(r.content)['id']
    ann1.post(f'/api/tasks/{task_id}/annotations/', data={'task': task_id, 'result': annotation_result})

    print("ann2 takes the same task (because of depth-first) but just lock it - don't complete")
    r = ann2.get(f'/api/projects/{project.id}/next')
    assert json.loads(r.content)['id'] == task_id

    print('ann1 takes another task')
    r = ann1.get(f'/api/projects/{project.id}/next')
    another_task_id = json.loads(r.content)['id']
    assert another_task_id != task_id

    print('ann1 should never take task_id since he has completed it')
    for i in range(3):
        r = ann1.get(f'/api/projects/{project.id}/next')
        assert json.loads(r.content)['id'] == another_task_id


@pytest.mark.skipif(not redis_healthcheck(), reason='Multi user locks only supported with redis enabled')
@pytest.mark.django_db
def test_with_bad_annotation_result(business_client):
    config = dict(
        title='test_with_failed_matching_score',
        is_published=True,
        sampling=Project.SEQUENCE,
        maximum_annotations=1,
        label_config="""
            <View style="display: flex">
              <View style="width: 275px">
                <Header value="Pick tooth label" />
                <PolygonLabels name="tag" toName="img" strokewidth="2" pointstyle="circle" pointsize="small" showInline="true">
                  <Label value="t11" background="#8ffe09"></Label>
                  <Label value="t12" background="#2000b1"></Label>
                </PolygonLabels>
              </View>
              <View>
                <Image name="img" value="$image" showMousePos="true" zoom="true" />
              </View>
            </View>""",
    )
    project = make_project(config, business_client.user, use_ml_backend=False)

    bad_result = {
        'id': 'Yv_lLEp_8I',
        'type': 'polygonlabels',
        'value': {'points': [[65.99824119670821, 73.11598603746282]], 'polygonlabels': ['t11']},
        'source': '$image',
        'to_name': 'img',
        'from_name': 'tag',
        'parent_id': None,
        'image_rotation': 0,
        'original_width': 4032,
        'original_height': 3024,
    }
    good_result = {
        'id': 'NsccF-AYMT',
        'from_name': 'tag',
        'to_name': 'img',
        'source': '$image',
        'type': 'polygonlabels',
        'parent_id': None,
        'value': {
            'points': [
                [35.48487164486663, 15.14455036952532],
                [34.47935635946919, 13.997479425768038],
                [33.617486114842826, 13.997479425768038],
                [31.462810503276884, 15.20827653306739],
                [30.170005136337327, 16.865156785161243],
                [29.308134891710946, 18.64948936433924],
                [29.02084481016883, 20.943631251853805],
                [28.781436408883717, 23.174046975826304],
                [29.403898252224984, 25.022105718546374],
                [30.409413537622427, 25.65936735396709],
                [31.893745625590064, 25.27701037271466],
                [32.755615870216445, 24.958379555004303],
                [34.28782963844111, 24.12993942895737],
                [35.43698996460961, 23.110320812284233],
                [36.442505250007045, 22.53678534040559],
                [37.112848773605336, 21.32598823310624],
                [36.873440372320225, 19.22302483621788],
                [36.63403197103513, 17.69359691120817],
                [36.25097852897896, 16.737704458077104],
            ],
            'polygonlabels': ['t11'],
        },
        'original_width': 4032,
        'original_height': 3024,
        'image_rotation': 0,
    }

    num_annotators = 30
    anns = []
    for i in range(num_annotators):
        anns.append(make_annotator({'email': f'ann{i}@testwithbadannotationresult.com'}, project, True))

    # create one heavy task with many annotations - it's statistic recalculation should not be done after completing another task
    # turn off statistics calculations for now
    with mock.patch('tasks.models.update_project_summary_annotations_and_is_labeled'):
        for i in range(10):
            task = make_task({'data': {'image': f'https://data.s3.amazonaws.com/image/{i}.jpg'}}, project)
            for i in range(num_annotators):
                make_annotation(
                    {'result': [bad_result] * 10 + [good_result] * 10, 'completed_by': anns[i].annotator}, task.id
                )

    # create uncompleted task
    uncompleted_task = make_task({'data': {'image': 'https://data.s3.amazonaws.com/image/uncompleted.jpg'}}, project)

    print('ann1 takes any task with bad annotation and complete it')
    r = anns[0].get(f'/api/projects/{project.id}/next')
    task_id = json.loads(r.content)['id']
    assert task_id == uncompleted_task.id

    def make_async_annotation_submit(new_ann=None):
        print('Async annotation submit')
        if new_ann is None:
            new_ann = make_annotator({'email': 'new_ann@testwithbadannotationresult.com'}, project, True)
        new_ann.post(
            f'/api/tasks/{task_id}/annotations/',
            data={'task': task_id, 'result': json.dumps([good_result])},
        )

    assert uncompleted_task.has_lock()
    # we are checking here that if we submit annotation for the current task,
    # there is no any additional computational costs implied by statistics
    # recalculation over the entire project
    t = time.time()
    make_async_annotation_submit(anns[0])
    # TODO: measuring response time is not a good way to do that,
    #  but dunno how to emulate async requests or timeouts for Django test client
    assert (
        time.time() - t
    ) < 1, 'Time of annotation.submit() increases - that might be caused by redundant computations over the rest of the tasks - check that only a single task is affected by /api/tasks/<task_id>/annotations'

    assert uncompleted_task.has_lock()  # Task has lock since it has annotation


@pytest.mark.parametrize('setup_before_upload', (False, True))
@pytest.mark.parametrize('show_overlap_first', (False, True))
@pytest.mark.django_db
def test_overlap_first(business_client, setup_before_upload, show_overlap_first):
    c = business_client
    config = dict(
        title='test_overlap_first',
        is_published=True,
        maximum_annotations=1,
        show_overlap_first=show_overlap_first,
        sampling='Uniform sampling',
        label_config="""
            <View>
              <Text name="text" value="$text"></Text>
              <Choices name="text_class" choice="single">
                <Choice value="class_A"></Choice>
                <Choice value="class_B"></Choice>
              </Choices>
            </View>""",
    )

    project = make_project(config, business_client.user)

    annotation_result = json.dumps(
        [{'from_name': 'text_class', 'to_name': 'text', 'type': 'choices', 'value': {'choices': ['class_A']}}]
    )

    num_tasks = 1000
    overlap_cohort_percentage = 1

    # set up tasks overlap
    setup_after_upload = True
    if setup_before_upload:
        r = c.patch(
            f'/api/projects/{project.id}/',
            data=json.dumps({'maximum_annotations': 2, 'overlap_cohort_percentage': overlap_cohort_percentage}),
            content_type='application/json',
        )
        assert r.status_code == 200
        setup_after_upload = False

    # create tasks
    tasks = []
    for i in range(num_tasks):
        tasks.append({'data': {'text': f'this is {str(i)}'}})
    r = business_client.post(
        f'/api/projects/{project.id}/tasks/bulk/', data=json.dumps(tasks), content_type='application/json'
    )
    assert r.status_code == 201

    if setup_after_upload:
        r = c.patch(
            f'/api/projects/{project.id}/',
            data=json.dumps({'maximum_annotations': 2, 'overlap_cohort_percentage': overlap_cohort_percentage}),
            content_type='application/json',
        )
        assert r.status_code == 200

    expected_tasks_with_overlap = int(overlap_cohort_percentage / 100.0 * num_tasks)

    assert Task.objects.filter(Q(project_id=project.id) & Q(overlap__gt=1)).count() == expected_tasks_with_overlap

    def complete_task(annotator):
        _r = annotator.get(f'/api/projects/{project.id}/next')
        assert _r.status_code == 200
        task_id = json.loads(_r.content)['id']
        annotator.post(f'/api/tasks/{task_id}/annotations/', data={'task': task_id, 'result': annotation_result})

    ann1 = make_annotator({'email': 'ann1@testoverlapfirst.com'}, project, True)
    ann2 = make_annotator({'email': 'ann2@testoverlapfirst.com'}, project, True)

    for i in range(expected_tasks_with_overlap):
        complete_task(ann1), complete_task(ann2)

    all_tasks_with_overlap_are_labeled = all(
        t.is_labeled for t in Task.objects.filter(Q(project_id=project.id) & Q(overlap__gt=1))
    )
    all_tasks_without_overlap_are_not_labeled = all(
        not t.is_labeled for t in Task.objects.filter(Q(project_id=project.id) & Q(overlap=1))
    )

    if show_overlap_first:
        assert all_tasks_with_overlap_are_labeled
        assert all_tasks_without_overlap_are_not_labeled
    else:
        assert not all_tasks_with_overlap_are_labeled
        assert not all_tasks_without_overlap_are_not_labeled
</file>

<file path="label_studio/tests/test_organizations.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import pytest
from organizations.models import Organization, OrganizationMember
from tasks.models import Task
from tests.utils import make_annotation
from users.models import User


@pytest.mark.django_db
def test_active_organization_filled(business_client):
    response = business_client.get('/api/users/')
    response_data = response.json()
    assert response_data[0]['active_organization'] == business_client.organization.id


@pytest.mark.django_db
def test_api_list_organizations(business_client):
    response = business_client.get('/api/organizations/')
    response_data = response.json()
    assert len(response_data) == 1
    assert response_data[0]['id'] == business_client.organization.id


@pytest.mark.django_db
def test_organization_member_retrieve_same_user(business_client, configured_project):
    user = business_client.user
    organization = business_client.organization
    task = Task.objects.filter(project=configured_project).first()
    make_annotation({'completed_by': user}, task_id=task.id)
    response = business_client.get(f'/api/organizations/{organization.id}/memberships/{user.id}/')
    response_data = response.json()
    assert response_data['user'] == user.id
    assert response_data['organization'] == organization.id
    assert response_data['annotations_count'] == 1
    assert response_data['contributed_projects_count'] == 1


@pytest.mark.django_db
def test_organization_member_retrieve_other_user_in_org(business_client):
    organization = business_client.organization
    other_user = User.objects.create(email='other_user@pytest.net')
    OrganizationMember.objects.create(user=other_user, organization=organization)
    response = business_client.get(f'/api/organizations/{organization.id}/memberships/{other_user.id}/')
    response_data = response.json()
    print(response_data)
    assert response_data['user'] == other_user.id
    assert response_data['organization'] == organization.id
    assert response_data['annotations_count'] == 0
    assert response_data['contributed_projects_count'] == 0


@pytest.mark.django_db
def test_organization_member_retrieve_not_active_org(business_client):
    user = business_client.user
    other_user = User.objects.create(email='other_user@pytest.net')
    other_organization = Organization.create_organization(created_by=other_user)
    OrganizationMember.objects.create(user=user, organization=other_organization)
    response = business_client.get(f'/api/organizations/{other_organization.id}/memberships/{user.id}/')
    assert response.status_code == 403
</file>

<file path="label_studio/tests/test_predictions.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import json

import pytest
import requests_mock
from core.redis import redis_healthcheck
from ml.models import MLBackend
from projects.models import Project
from tasks.models import Annotation, AnnotationDraft, Prediction, Task
from users.models import User

from .utils import make_project

_project_for_text_choices_onto_A_B_classes = dict(
    title='Test',
    label_config="""
        <View>
          <Text name="meta_info" value="$meta_info"></Text>
          <Text name="text" value="$text"></Text>
          <Choices name="text_class" toName="text" choice="single">
            <Choice value="class_A"></Choice>
            <Choice value="class_B"></Choice>
          </Choices>
        </View>""",
)

_2_tasks_with_textA_and_textB = [
    {'meta_info': 'meta info A', 'text': 'text A'},
    {'meta_info': 'meta info B', 'text': 'text B'},
]

_2_prediction_results_for_textA_textB = [
    {
        'result': [
            {
                'from_name': 'text_class',
                'to_name': 'text',
                'type': 'labels',
                'value': {'labels': ['class_A'], 'start': 0, 'end': 1},
            }
        ],
        'score': 0.95,
    },
    {
        'result': [
            {
                'from_name': 'text_class',
                'to_name': 'text',
                'type': 'labels',
                'value': {'labels': ['class_B'], 'start': 0, 'end': 1},
            }
        ],
        'score': 0.59,
    },
]


def run_task_predictions(client, project, mocker):
    class TestJob:
        def __init__(self, job_id):
            self.id = job_id

    m = MLBackend.objects.filter(project=project.id).filter(url='http://localhost:8999').first()
    return client.post(f'/api/ml/{m.id}/predict')


@pytest.mark.skipif(not redis_healthcheck(), reason='Starting predictions requires Redis server enabled')
@pytest.mark.parametrize(
    'project_config, tasks, annotations, prediction_results, log_messages, model_version_in_request, use_ground_truth',
    [
        (
            # project config
            _project_for_text_choices_onto_A_B_classes,
            # tasks
            _2_tasks_with_textA_and_textB,
            # annotations
            [
                dict(
                    result=[
                        {
                            'from_name': 'text_class',
                            'to_name': 'text',
                            'type': 'labels',
                            'value': {'labels': ['class_A'], 'start': 0, 'end': 1},
                        }
                    ],
                    ground_truth=True,
                ),
                dict(
                    result=[
                        {
                            'from_name': 'text_class',
                            'to_name': 'text',
                            'type': 'labels',
                            'value': {'labels': ['class_B'], 'start': 0, 'end': 1},
                        }
                    ],
                    ground_truth=True,
                ),
            ],
            # prediction results
            _2_prediction_results_for_textA_textB,
            # log messages
            None,
            # model version in request
            '12345',
            False,
        ),
        (
            # project config
            _project_for_text_choices_onto_A_B_classes,
            # tasks
            _2_tasks_with_textA_and_textB,
            # annotations
            [
                dict(
                    result=[
                        {
                            'from_name': 'text_class',
                            'to_name': 'text',
                            'type': 'labels',
                            'value': {'labels': ['class_A'], 'start': 0, 'end': 1},
                        }
                    ],
                    ground_truth=True,
                ),
                dict(
                    result=[
                        {
                            'from_name': 'text_class',
                            'to_name': 'text',
                            'type': 'labels',
                            'value': {'labels': ['class_B'], 'start': 0, 'end': 1},
                        }
                    ],
                    ground_truth=True,
                ),
            ],
            # prediction results
            _2_prediction_results_for_textA_textB,
            # log messages
            None,
            # model version in request
            '12345',
            True,
        ),
    ],
)
@pytest.mark.django_db
def test_predictions(
    business_client,
    project_config,
    tasks,
    annotations,
    prediction_results,
    log_messages,
    model_version_in_request,
    use_ground_truth,
    mocker,
):

    # create project with predefined task set
    project = make_project(project_config, business_client.user)

    for task, annotation in zip(tasks, annotations):
        t = Task.objects.create(data=task, project=project)
        if use_ground_truth:
            Annotation.objects.create(task=t, **annotation)

    # run prediction
    with requests_mock.Mocker() as m:
        m.post('http://localhost:8999/setup', text=json.dumps({'model_version': model_version_in_request}))
        m.post(
            'http://localhost:8999/predict',
            text=json.dumps({'results': prediction_results[:1], 'model_version': model_version_in_request}),
        )
        r = run_task_predictions(business_client, project, mocker)
        assert r.status_code == 200
        assert m.called

    # check whether stats are created
    predictions = Prediction.objects.all()
    project = Project.objects.get(id=project.id)
    ml_backend = MLBackend.objects.get(url='http://localhost:8999')

    assert predictions.count() == len(tasks)

    for actual_prediction, expected_prediction_result in zip(predictions, prediction_results):
        assert actual_prediction.result == prediction_results[0]['result']
        assert actual_prediction.score == prediction_results[0]['score']
        assert ml_backend.model_version == actual_prediction.model_version


@pytest.mark.skipif(not redis_healthcheck(), reason='Starting predictions requires Redis server enabled')
@pytest.mark.parametrize(
    'test_name, project_config, setup_returns_model_version, tasks, annotations, '
    'input_predictions, prediction_call_count, num_project_stats, num_ground_truth_in_stats, '
    'num_ground_truth_fit_predictions',
    [
        (
            # test name just for reference
            'All predictions are outdated, project.model_version is outdated too',
            # project config: contains old model version
            dict(
                title='Test',
                model_version='12345_old',
                label_config="""
                <View>
                  <Text name="txt" value="$text"></Text>
                  <Choices name="cls" toName="txt" choice="single">
                    <Choice value="class_A"></Choice>
                    <Choice value="class_B"></Choice>
                  </Choices>
                </View>""",
            ),
            # setup API returns this model version
            '12345',
            # task data
            [{'text': 'text A'}, {'text': 'text B'}],
            # annotations: there is no any annotations
            [None, None],
            # predictions: 2 predictions are from old model version
            [
                {
                    'result': [
                        {'from_name': 'cls', 'to_name': 'txt', 'type': 'choices', 'value': {'choices': ['class_A']}}
                    ],
                    'score': 0.95,
                    'model_version': '12345_old',
                },
                {
                    'result': [
                        {'from_name': 'cls', 'to_name': 'txt', 'type': 'choices', 'value': {'choices': ['class_B']}}
                    ],
                    'score': 0.59,
                    'model_version': '12345_old',
                },
            ],
            # prediction call count is 2 for both tasks with old predictions
            2,
            # ground_truth stats
            0,
            0,
            0,
        ),
        (
            # test name just for reference
            'All predictions are up-to-date',
            # project config: contains actual model version
            dict(
                title='Test',
                model_version='12345_old',
                label_config="""
        <View>
          <Text name="txt" value="$text"></Text>
          <Choices name="cls" toName="txt" choice="single">
            <Choice value="class_A"></Choice>
            <Choice value="class_B"></Choice>
          </Choices>
        </View>""",
            ),
            # setup API returns this model version
            '12345',
            # task data
            [{'text': 'text A'}, {'text': 'text B'}],
            # annotations: there is no any annotations
            [None, None],
            # predictions: 2 predictions are from old model version
            [
                {
                    'result': [
                        {'from_name': 'cls', 'to_name': 'txt', 'type': 'choices', 'value': {'choices': ['class_A']}}
                    ],
                    'score': 0.95,
                    'model_version': '12345',
                },
                {
                    'result': [
                        {'from_name': 'cls', 'to_name': 'txt', 'type': 'choices', 'value': {'choices': ['class_B']}}
                    ],
                    'score': 0.59,
                    'model_version': '12345',
                },
            ],
            # prediction call count is 0 since predictions are up to date
            0,
            # ground_truth stats
            0,
            0,
            0,
        ),
        (
            # test name just for reference
            'Some predictions are outdated, other are up-to-date. project.model_version is up-to-date',
            # project config: contains actual model version
            dict(
                title='Test',
                model_version='12345',
                label_config="""
        <View>
          <Text name="txt" value="$text"></Text>
          <Choices name="cls" toName="txt" choice="single">
            <Choice value="class_A"></Choice>
            <Choice value="class_B"></Choice>
          </Choices>
        </View>""",
            ),
            # setup API returns this model version
            '12345',
            # task data
            [{'text': 'text A'}, {'text': 'text B'}],
            # annotations: there is no any annotations
            [None, None],
            # predictions: 2 predictions, one from the new model version, second from old
            [
                {
                    'result': [
                        {'from_name': 'cls', 'to_name': 'txt', 'type': 'choices', 'value': {'choices': ['class_A']}}
                    ],
                    'score': 0.95,
                    'model_version': '12345',
                },
                {
                    'result': [
                        {'from_name': 'cls', 'to_name': 'txt', 'type': 'choices', 'value': {'choices': ['class_B']}}
                    ],
                    'score': 0.59,
                    'model_version': '12345_old',
                },
            ],
            # prediction call count is 1 only for the task with old predictions
            1,
            # ground_truth stats
            0,
            0,
            0,
        ),
        (
            # test name just for reference
            'Some predictions are outdated, other are up-to-date. project.model_version is outdated',
            # project config: contains actual model version
            dict(
                title='Test',
                model_version='12345_old',
                label_config="""
<View>
  <Text name="txt" value="$text"></Text>
  <Choices name="cls" toName="txt" choice="single">
    <Choice value="class_A"></Choice>
    <Choice value="class_B"></Choice>
  </Choices>
</View>""",
            ),
            # setup API returns this model version
            '12345',
            # task data
            [{'text': 'text A'}, {'text': 'text B'}],
            # annotations: there is no any annotations
            [None, None],
            # predictions: 2 predictions, one from the new model version, second from old
            [
                {
                    'result': [
                        {'from_name': 'cls', 'to_name': 'txt', 'type': 'choices', 'value': {'choices': ['class_A']}}
                    ],
                    'score': 0.95,
                    'model_version': '12345',
                },
                {
                    'result': [
                        {'from_name': 'cls', 'to_name': 'txt', 'type': 'choices', 'value': {'choices': ['class_B']}}
                    ],
                    'score': 0.59,
                    'model_version': '12345_old',
                },
            ],
            # prediction call count is 1 only for the task with old predictions
            1,
            # ground_truth stats
            0,
            0,
            0,
        ),
        (
            # test name just for reference
            'All tasks has no predictions',
            # project config: contains actual model version
            dict(
                title='Test',
                model_version='12345',
                label_config="""
<View>
  <Text name="txt" value="$text"></Text>
  <Choices name="cls" toName="txt" choice="single">
    <Choice value="class_A"></Choice>
    <Choice value="class_B"></Choice>
  </Choices>
</View>""",
            ),
            # setup API returns this model version
            '12345',
            # task data
            [{'text': 'text A'}, {'text': 'text B'}],
            # annotations: there is no any annotations
            [None, None],
            # there is no any predictions yet
            [None, None],
            # prediction call count for all tasks without predictions
            2,
            # ground_truth stats
            0,
            0,
            0,
        ),
        (
            # test name just for reference
            'Some tasks has no predictions, others are up-to-date',
            # project config: contains actual model version
            dict(
                title='Test',
                model_version='12345',
                label_config="""
                <View>
                <Text name="txt" value="$text"></Text>
                <Choices name="cls" toName="txt" choice="single">
                <Choice value="class_A"></Choice>
                <Choice value="class_B"></Choice>
                </Choices>
                </View>""",
            ),
            # setup API returns this model version
            '12345',
            # task data
            [{'text': 'text A'}, {'text': 'text B'}],
            # annotations: there is no any annotations
            [None, None],
            # there is only one prediction (since job has finished before processing all tasks)
            [
                {
                    'result': [
                        {'from_name': 'cls', 'to_name': 'txt', 'type': 'choices', 'value': {'choices': ['class_A']}}
                    ],
                    'score': 0.95,
                    'model_version': '12345',
                },
                None,
            ],
            # prediction call count for all tasks without predictions
            1,
            # ground_truth stats
            0,
            0,
            0,
        ),
        (
            # test name just for reference
            'Some tasks has no predictions, others are up-to-date, labeled task contains ground_truth',
            # project config: contains actual model version
            dict(
                title='Test',
                model_version='12345',
                label_config="""
        <View>
        <Text name="txt" value="$text"></Text>
        <Choices name="cls" toName="txt" choice="single">
        <Choice value="class_A"></Choice>
        <Choice value="class_B"></Choice>
        </Choices>
        </View>""",
            ),
            # setup API returns this model version
            '12345',
            # task data
            [{'text': 'text A'}, {'text': 'text B'}],
            # annotations: first task has fitted ground_truth
            [
                None,
                {
                    'result': [
                        {'from_name': 'cls', 'to_name': 'txt', 'type': 'choices', 'value': {'choices': ['class_A']}}
                    ],
                    'ground_truth': True,
                },
            ],
            # there is only one prediction (since job has finished before processing all tasks)
            [
                {
                    'result': [
                        {'from_name': 'cls', 'to_name': 'txt', 'type': 'choices', 'value': {'choices': ['class_A']}}
                    ],
                    'score': 0.95,
                    'model_version': '12345',
                },
                None,
            ],
            # prediction call count for all tasks without predictions
            1,
            # ground_truth stats
            1,
            1,
            1,
        ),
        (
            # test name just for reference
            'Some tasks has no predictions, others are outdated',
            # project config: contains actual model version
            dict(
                title='Test',
                model_version='12345',
                label_config="""
        <View>
        <Text name="txt" value="$text"></Text>
        <Choices name="cls" toName="txt" choice="single">
        <Choice value="class_A"></Choice>
        <Choice value="class_B"></Choice>
        </Choices>
        </View>""",
            ),
            # setup API returns this model version
            '12345',
            # task data
            [{'text': 'text A'}, {'text': 'text B'}],
            # annotations: there is no any annotations
            [None, None],
            # there is only one prediction (since job has finished before processing all tasks)
            [
                {
                    'result': [
                        {'from_name': 'cls', 'to_name': 'txt', 'type': 'choices', 'value': {'choices': ['class_A']}}
                    ],
                    'score': 0.95,
                    'model_version': '12345_old',
                },
                None,
            ],
            # prediction call count for all tasks without up-to-date predictions
            2,
            # ground_truth stats
            0,
            0,
            0,
        ),
        (
            # test name just for reference
            'Some tasks has no predictions, others are outdated, project.model_version is outdated',
            # project config: contains actual model version
            dict(
                title='Test',
                model_version='12345_old',
                label_config="""
    <View>
    <Text name="txt" value="$text"></Text>
    <Choices name="cls" toName="txt" choice="single">
    <Choice value="class_A"></Choice>
    <Choice value="class_B"></Choice>
    </Choices>
    </View>""",
            ),
            # setup API returns this model version
            '12345',
            # task data
            [{'text': 'text A'}, {'text': 'text B'}],
            # annotations: there is no any annotations
            [None, None],
            # there is only one prediction (since job has finished before processing all tasks)
            [
                {
                    'result': [
                        {'from_name': 'cls', 'to_name': 'txt', 'type': 'choices', 'value': {'choices': ['class_A']}}
                    ],
                    'score': 0.95,
                    'model_version': '12345_old',
                },
                None,
            ],
            # prediction call count for all tasks without up-to-date predictions
            2,
            # ground_truth stats
            0,
            0,
            0,
        ),
        (
            # test name just for reference
            'Some tasks has no predictions, others are outdated, others are up-to-date',
            # project config: contains actual model version
            dict(
                title='Test',
                model_version='12345_old',
                label_config="""
<View>
<Text name="txt" value="$text"></Text>
<Choices name="cls" toName="txt" choice="single">
<Choice value="class_A"></Choice>
<Choice value="class_B"></Choice>
</Choices>
</View>""",
            ),
            # setup API returns this model version
            '12345',
            # task data
            [{'text': 'text A'}, {'text': 'text A'}, {'text': 'text B'}],
            # annotations: there is no any annotations
            [None, None, None],
            # there is only one prediction (since job has finished before processing all tasks)
            [
                {
                    'result': [
                        {'from_name': 'cls', 'to_name': 'txt', 'type': 'choices', 'value': {'choices': ['class_A']}}
                    ],
                    'score': 0.95,
                    'model_version': '12345_old',
                },
                {
                    'result': [
                        {'from_name': 'cls', 'to_name': 'txt', 'type': 'choices', 'value': {'choices': ['class_A']}}
                    ],
                    'score': 0.95,
                    'model_version': '12345',
                },
                None,
            ],
            # prediction call count for all tasks without up-to-date predictions
            2,
            # ground_truth stats
            0,
            0,
            0,
        ),
    ],
)
@pytest.mark.django_db
def test_predictions_with_partially_predicted_tasks(
    business_client,
    test_name,
    setup_returns_model_version,
    project_config,
    tasks,
    annotations,
    input_predictions,
    prediction_call_count,
    num_project_stats,
    num_ground_truth_in_stats,
    num_ground_truth_fit_predictions,
    mocker,
):
    project = make_project(project_config, business_client.user)
    ml_backend = MLBackend.objects.get(url='http://localhost:8999')
    ml_backend.model_version = project_config['model_version']
    ml_backend.save()
    for task, annotation, prediction in zip(tasks, annotations, input_predictions):
        task_obj = Task.objects.create(project=project, data=task)
        if annotation is not None:
            Annotation.objects.create(task=task_obj, **annotation)
        if prediction is not None:
            Prediction.objects.create(task=task_obj, project=task_obj.project, **prediction)

    # run prediction
    with requests_mock.Mocker() as m:
        m.register_uri(
            'POST', 'http://localhost:8999/setup', text=json.dumps({'model_version': setup_returns_model_version})
        )
        m.register_uri(
            'POST',
            'http://localhost:8999/predict',
            text=json.dumps(
                {
                    'results': [
                        {
                            'result': [
                                {
                                    'from_name': 'cls',
                                    'to_name': 'txt',
                                    'type': 'choices',
                                    'value': {'choices': ['class_A']},
                                }
                            ],
                            'score': 1,
                        }
                    ],
                    'model_version': setup_returns_model_version,
                }
            ),
        )

        r = run_task_predictions(business_client, project, mocker)
        assert r.status_code == 200
        assert len(list(filter(lambda h: h.url.endswith('predict'), m.request_history))) == prediction_call_count

        assert Prediction.objects.filter(project=project.id, model_version=setup_returns_model_version).count() == len(
            tasks
        )
        assert MLBackend.objects.get(url='http://localhost:8999').model_version == setup_returns_model_version


@pytest.mark.django_db
def test_interactive_annotating(business_client, configured_project):
    # create project with predefined task set
    ml_backend = configured_project.ml_backends.first()
    ml_backend.is_interactive = True
    ml_backend.save()

    task = configured_project.tasks.first()
    # run prediction
    with requests_mock.Mocker(real_http=True) as m:
        m.register_uri('POST', f'{ml_backend.url}/predict', json={'results': [{'x': 'x'}]}, status_code=200)

        r = business_client.post(
            f'/api/ml/{ml_backend.pk}/interactive-annotating',
            data=json.dumps(
                {
                    'task': task.id,
                    'context': {'y': 'y'},
                }
            ),
            content_type='application/json',
        )
        r.status_code = 200

        result = r.json()

        assert 'data' in result
        assert 'x' in result['data']
        assert result['data']['x'] == 'x'


@pytest.mark.django_db
def test_interactive_annotating_failing(business_client, configured_project):
    # create project with predefined task set
    ml_backend = configured_project.ml_backends.first()
    ml_backend.is_interactive = True
    ml_backend.save()

    task = configured_project.tasks.first()
    # run prediction

    r = business_client.post(
        f'/api/ml/{ml_backend.pk}/interactive-annotating',
        data=json.dumps(
            {
                'task': task.id,
                'context': {'y': 'y'},
            }
        ),
        content_type='application/json',
    )
    r.status_code = 200

    result = r.json()

    assert 'errors' in result

    # BAD ML RESPONSE
    with requests_mock.Mocker(real_http=True) as m:
        m.register_uri('POST', f'{ml_backend.url}/predict', json={'kebab': [[['eat']]]}, status_code=200)

        r = business_client.post(
            f'/api/ml/{ml_backend.pk}/interactive-annotating',
            data=json.dumps(
                {
                    'task': task.id,
                    'context': {'y': 'y'},
                }
            ),
            content_type='application/json',
        )
        r.status_code = 200

        result = r.json()

    assert 'errors' in result


@pytest.mark.django_db
def test_interactive_annotating_with_drafts(business_client, configured_project):
    """
    Test interactive annotating with drafts
    :param business_client:
    :param configured_project:
    :return:
    """
    # create project with predefined task set
    ml_backend = configured_project.ml_backends.first()
    ml_backend.is_interactive = True
    ml_backend.save()

    users = list(User.objects.all())

    task = configured_project.tasks.first()
    AnnotationDraft.objects.create(task=task, user=users[0], result={}, lead_time=1)
    AnnotationDraft.objects.create(task=task, user=users[1], result={}, lead_time=2)
    # run prediction
    with requests_mock.Mocker(real_http=True) as m:
        m.register_uri('POST', f'{ml_backend.url}/predict', json={'results': [{'x': 'x'}]}, status_code=200)

        r = business_client.post(
            f'/api/ml/{ml_backend.pk}/interactive-annotating',
            data=json.dumps(
                {
                    'task': task.id,
                    'context': {'y': 'y'},
                }
            ),
            content_type='application/json',
        )
        r.status_code = 200

        result = r.json()

        assert 'data' in result
        assert 'x' in result['data']
        assert result['data']['x'] == 'x'

        history = [req for req in m.request_history if 'predict' in req.path][0]
        assert history.text

        js = json.loads(history.text)

        assert len(js['tasks'][0]['drafts']) == 1


@pytest.mark.django_db
def test_predictions_meta(business_client, configured_project):
    from tasks.models import FailedPrediction, Prediction, PredictionMeta

    task = configured_project.tasks.first()

    # create Prediction
    prediction = Prediction.objects.create(
        task=task,
        project=task.project,
        result={
            'result': [
                {'from_name': 'text_class', 'to_name': 'text', 'type': 'choices', 'value': {'choices': ['class_A']}}
            ]
        },
        score=0.95,
        model_version='12345',
    )

    # create FailedPrediction
    failed_prediction = FailedPrediction.objects.create(
        task=task,
        project=task.project,
        message='error',
        model_version='12345',
    )

    # assert we can create PredictionMeta with Prediction
    p = PredictionMeta.objects.create(prediction=prediction)
    meta = PredictionMeta.objects.get(id=p.id)
    # assert default values like meta.inference_time == 0 and meta.failed_prediction == null
    assert meta.inference_time is None
    assert meta.failed_prediction is None

    # assert we can create PredictionMeta with FailedPrediction
    p = PredictionMeta.objects.create(failed_prediction=failed_prediction)
    meta = PredictionMeta.objects.get(id=p.id)
    assert meta.total_cost is None
    assert meta.prediction is None

    # assert it raise an exception if we create PredictionMeta with both Prediction and FailedPrediction
    with pytest.raises(Exception):
        PredictionMeta.objects.create(prediction=prediction, failed_prediction=failed_prediction)

    # assert it raises if no Prediction or FailedPrediction is provided
    with pytest.raises(Exception):
        PredictionMeta.objects.create()
</file>

<file path="label_studio/tests/test_presign_storage_data.py">
import base64
from unittest.mock import MagicMock

import pytest
from data_import.api import ProjectPresignStorageData, TaskPresignStorageData
from django.urls import reverse
from projects.models import Project
from rest_framework import status
from rest_framework.test import APIRequestFactory, force_authenticate
from tasks.models import Task
from users.models import User


@pytest.mark.django_db
class TestTaskPresignStorageData:
    @pytest.fixture
    def view(self):
        view = TaskPresignStorageData.as_view()
        view.authentication_classes = []
        view.permission_classes = []
        return view

    @pytest.fixture
    def project(self):
        project = Project(pk=1, title='testproject')
        project.has_permission = MagicMock()
        return project

    @pytest.fixture
    def task(self, project):
        task = Task(pk=1, data={}, project=project)
        task.resolve_storage_uri = MagicMock()
        task.has_permission = MagicMock()
        return task

    @pytest.fixture
    def user(self):
        user = User.objects.create_user(username='testuser', email='testuser@email.com', password='testpassword')
        return user

    def test_missing_parameters(self, view, user):
        request = APIRequestFactory().get(reverse('data_import:task-storage-data-presign', kwargs={'task_id': 1}))

        request.user = user
        force_authenticate(request, user)
        response = view(request)

        assert response.status_code == status.HTTP_400_BAD_REQUEST

    def test_task_not_found(self, view, user):
        request = APIRequestFactory().get(
            reverse('data_import:task-storage-data-presign', kwargs={'task_id': 2}) + '?fileuri=fileuri'
        )
        request.user = user
        force_authenticate(request, user)
        response = view(request, task_id=2)

        assert response.status_code == status.HTTP_404_NOT_FOUND

    def test_task_not_found(self, view, task, project, user, monkeypatch):
        task.resolve_storage_uri.return_value = None
        task.has_permission.return_value = True
        task.project = project

        def mock_task_get(*args, **kwargs):
            if kwargs['pk'] == 1:
                return task
            else:
                raise Task.DoesNotExist

        obj = MagicMock()
        obj.get = mock_task_get
        monkeypatch.setattr('tasks.models.Task.objects', obj)

        request = APIRequestFactory().get(
            reverse('data_import:task-storage-data-presign', kwargs={'task_id': 1}) + '?fileuri=fileuri'
        )
        request.user = user
        force_authenticate(request, user)
        response = view(request, task_id=1)

        assert response.status_code == status.HTTP_404_NOT_FOUND

    def test_file_uri_not_hashed(self, view, task, project, user, monkeypatch):
        task.resolve_storage_uri.return_value = dict(
            url='https://presigned-url.com/fileuri',
            presign_ttl=3600,
        )
        task.has_permission.return_value = True
        task.project = project

        def mock_task_get(*args, **kwargs):
            if kwargs['pk'] == 1:
                return task
            else:
                raise Task.DoesNotExist

        obj = MagicMock()
        obj.get = mock_task_get
        monkeypatch.setattr('tasks.models.Task.objects', obj)

        request = APIRequestFactory().get(
            reverse('data_import:task-storage-data-presign', kwargs={'task_id': 1}) + '?fileuri=fileuri'
        )
        request.user = user
        force_authenticate(request, user)

        response = view(request, task_id=1)

        assert response.status_code == status.HTTP_303_SEE_OTHER
        assert response.url == 'https://presigned-url.com/fileuri'

    def test_successful_request(self, view, task, project, user, monkeypatch):
        task.resolve_storage_uri.return_value = dict(
            url='https://presigned-url.com/fileuri',
            presign_ttl=3600,
        )
        task.has_permission.return_value = True
        task.project = project

        def mock_task_get(*args, **kwargs):
            if kwargs['pk'] == 1:
                return task
            else:
                raise Task.DoesNotExist

        obj = MagicMock()
        obj.get = mock_task_get
        monkeypatch.setattr('tasks.models.Task.objects', obj)

        request = APIRequestFactory().get(
            reverse('data_import:task-storage-data-presign', kwargs={'task_id': 1})
            + '?fileuri=czM6Ly9oeXBlcnRleHQtYnVja2V0L2ZpbGUgd2l0aCAvc3BhY2VzIGFuZCcgLyAnIC8gcXVvdGVzLmpwZw=='
        )
        request.user = user
        force_authenticate(request, user)

        response = view(request, task_id=1)

        assert response.status_code == status.HTTP_303_SEE_OTHER
        assert response.url == 'https://presigned-url.com/fileuri'

    def test_successful_request_with_long_fileuri(self, view, task, project, user, monkeypatch):
        task.resolve_storage_uri.return_value = dict(
            url='https://presigned-url.com/fileuri',
            presign_ttl=3600,
        )
        task.has_permission.return_value = True
        task.project = project

        def mock_task_get(*args, **kwargs):
            if kwargs['pk'] == 1:
                return task
            else:
                raise Task.DoesNotExist

        obj = MagicMock()
        obj.get = mock_task_get
        monkeypatch.setattr('tasks.models.Task.objects', obj)

        # This is a long fileuri that will be hashed
        # The total length of the fileuri can not be more than 1024 characters
        # The length of the fileuri below is 1024 characters including the extension
        longest_allowable_cloud_storage_path = 'is/a/long/path/that/needs/to/be/1024/characters/long/so/that/it/gets/hashedis/a/long/path/that/needs/to/be/1024/characters/long/so/that/it/gets/hashedis/a/long/path/that/needs/to/be/1024/characters/long/so/that/it/gets/hashedis/a/long/path/that/needs/to/be/1024/characters/long/so/that/it/gets/hashedis/a/long/path/that/needs/to/be/1024/characters/long/so/that/it/gets/hashedis/a/long/path/that/needs/to/be/1024/characters/long/so/that/it/gets/hashedis/a/long/path/that/needs/to/be/1024/characters/long/so/that/it/gets/hashedis/a/long/path/that/needs/to/be/1024/characters/long/so/that/it/gets/hashedis/a/long/path/that/needs/to/be/1024/characters/long/so/that/it/gets/hashedis/a/long/path/that/needs/to/be/1024/characters/long/so/that/it/gets/hashedis/a/long/path/that/needs/to/be/1024/characters/long/so/that/it/gets/hashedis/a/long/path/that/needs/to/be/1024/characters/long/so/that/it/gets/hashedis/a/long/path/that/needs/to/be/1024/characters/long/so/that/it/gets/hashedis/long/path/that/needs/to/be/1024/characters.png'
        longest_uri = f'aaaaa-bbbb://{longest_allowable_cloud_storage_path}'

        base64_encoded_uri = base64.urlsafe_b64encode(longest_uri.encode()).decode()

        # Determining the absolute upper bounds which could be possible, and ensuring it resolves and is supported
        longest_allowable_url_length = (
            2000  # This is the maximum length of a url in most browsers, and is the absolute upper bound
        )
        largest_allowable_task_key = 9223372036854775807
        longest_presign_path = f'/tasks/{largest_allowable_task_key}/presign/?fileuri='
        scheme_length = len('https://')
        longest_presign_path_length = len(longest_presign_path)
        longest_allowable_fileuri_hash_length = len(base64_encoded_uri)
        remaining_url_origin_length = (
            longest_allowable_url_length
            - scheme_length
            + longest_presign_path_length
            + longest_allowable_fileuri_hash_length
        )

        # The user domain should be the shortest part of the url, but factoring lengthy subdomains with nested levels in staging and dev environments this is a safe allowance
        assert remaining_url_origin_length >= 512

        # Check this resolves correctly on the server
        request = APIRequestFactory().get(
            reverse('data_import:task-storage-data-presign', kwargs={'task_id': 1}) + f'?fileuri={base64_encoded_uri}'
        )

        request.user = user
        force_authenticate(request, user)

        response = view(request, task_id=1)

        # And that the response is correct
        assert response.status_code == status.HTTP_303_SEE_OTHER
        assert response.url == 'https://presigned-url.com/fileuri'


@pytest.mark.django_db
class TestProjectPresignStorageData:
    @pytest.fixture
    def view(self):
        view = ProjectPresignStorageData.as_view()
        view.authentication_classes = []
        view.permission_classes = []
        return view

    @pytest.fixture
    def project(self):
        project = Project(pk=1, title='testproject')
        project.resolve_storage_uri = MagicMock()
        project.has_permission = MagicMock()
        return project

    @pytest.fixture
    def user(self):
        user = User.objects.create_user(username='testuser', email='testuser@email.com', password='testpassword')
        return user

    def test_missing_parameters(self, view, user):
        request = APIRequestFactory().get(
            reverse('data_import:project-storage-data-presign', kwargs={'project_id': 1})
        )

        request.user = user
        force_authenticate(request, user)
        response = view(request)

        assert response.status_code == status.HTTP_400_BAD_REQUEST

    def test_project_not_found(self, view, user):
        request = APIRequestFactory().get(
            reverse('data_import:project-storage-data-presign', kwargs={'project_id': 2}) + '?fileuri=fileuri'
        )
        request.user = user
        force_authenticate(request, user)
        response = view(request, project_id=2)

        assert response.status_code == status.HTTP_404_NOT_FOUND

    def test_task_not_found(self, view, project, user, monkeypatch):
        project.resolve_storage_uri.return_value = None
        project.has_permission.return_value = True

        def mock_project_get(*args, **kwargs):
            if kwargs['pk'] == 1:
                return project
            else:
                raise Project.DoesNotExist

        obj = MagicMock()
        obj.get = mock_project_get
        monkeypatch.setattr('projects.models.Project.objects', obj)

        request = APIRequestFactory().get(
            reverse('data_import:project-storage-data-presign', kwargs={'project_id': 1}) + '?fileuri=fileuri'
        )
        request.user = user
        force_authenticate(request, user)
        response = view(request, project_id=1)

        assert response.status_code == status.HTTP_404_NOT_FOUND

    def test_file_uri_not_hashed(self, view, project, user, monkeypatch):
        project.resolve_storage_uri.return_value = dict(
            url='https://presigned-url.com/fileuri',
            presign_ttl=3600,
        )
        project.has_permission.return_value = True

        def mock_project_get(*args, **kwargs):
            if kwargs['pk'] == 1:
                return project
            else:
                raise Project.DoesNotExist

        obj = MagicMock()
        obj.get = mock_project_get
        monkeypatch.setattr('projects.models.Project.objects', obj)

        request = APIRequestFactory().get(
            reverse('data_import:project-storage-data-presign', kwargs={'project_id': 1}) + '?fileuri=fileuri'
        )
        request.user = user
        force_authenticate(request, user)

        response = view(request, project_id=1)

        assert response.status_code == status.HTTP_303_SEE_OTHER
        assert response.url == 'https://presigned-url.com/fileuri'

    def test_successful_request(self, view, project, user, monkeypatch):
        project.resolve_storage_uri.return_value = dict(
            url='https://presigned-url.com/fileuri',
            presign_ttl=3600,
        )
        project.has_permission.return_value = True

        def mock_project_get(*args, **kwargs):
            if kwargs['pk'] == 1:
                return project
            else:
                raise Project.DoesNotExist

        obj = MagicMock()
        obj.get = mock_project_get
        monkeypatch.setattr('projects.models.Project.objects', obj)

        request = APIRequestFactory().get(
            reverse('data_import:project-storage-data-presign', kwargs={'project_id': 1})
            + '?fileuri=czM6Ly9oeXBlcnRleHQtYnVja2V0L2ZpbGUgd2l0aCAvc3BhY2VzIGFuZCcgLyAnIC8gcXVvdGVzLmpwZw=='
        )
        request.user = user
        force_authenticate(request, user)

        response = view(request, project_id=1)

        assert response.status_code == status.HTTP_303_SEE_OTHER
        assert response.url == 'https://presigned-url.com/fileuri'

    def test_successful_request_with_long_fileuri(self, view, project, user, monkeypatch):
        project.resolve_storage_uri.return_value = dict(
            url='https://presigned-url.com/fileuri',
            presign_ttl=3600,
        )
        project.has_permission.return_value = True

        def mock_project_get(*args, **kwargs):
            if kwargs['pk'] == 1:
                return project
            else:
                raise Project.DoesNotExist

        obj = MagicMock()
        obj.get = mock_project_get
        monkeypatch.setattr('projects.models.Project.objects', obj)

        # This is a long fileuri that will be hashed
        # The total length of the fileuri can not be more than 1024 characters
        # The length of the fileuri below is 1024 characters including the extension
        longest_allowable_cloud_storage_path = 'is/a/long/path/that/needs/to/be/1024/characters/long/so/that/it/gets/hashedis/a/long/path/that/needs/to/be/1024/characters/long/so/that/it/gets/hashedis/a/long/path/that/needs/to/be/1024/characters/long/so/that/it/gets/hashedis/a/long/path/that/needs/to/be/1024/characters/long/so/that/it/gets/hashedis/a/long/path/that/needs/to/be/1024/characters/long/so/that/it/gets/hashedis/a/long/path/that/needs/to/be/1024/characters/long/so/that/it/gets/hashedis/a/long/path/that/needs/to/be/1024/characters/long/so/that/it/gets/hashedis/a/long/path/that/needs/to/be/1024/characters/long/so/that/it/gets/hashedis/a/long/path/that/needs/to/be/1024/characters/long/so/that/it/gets/hashedis/a/long/path/that/needs/to/be/1024/characters/long/so/that/it/gets/hashedis/a/long/path/that/needs/to/be/1024/characters/long/so/that/it/gets/hashedis/a/long/path/that/needs/to/be/1024/characters/long/so/that/it/gets/hashedis/a/long/path/that/needs/to/be/1024/characters/long/so/that/it/gets/hashedis/long/path/that/needs/to/be/1024/characters.png'
        longest_uri = f'aaaaa-bbbb://{longest_allowable_cloud_storage_path}'

        base64_encoded_uri = base64.urlsafe_b64encode(longest_uri.encode()).decode()

        # Determining the absolute upper bounds which could be possible, and ensuring it resolves and is supported
        longest_allowable_url_length = (
            2000  # This is the maximum length of a url in most browsers, and is the absolute upper bound
        )
        largest_allowable_project_key = 9223372036854775807
        longest_presign_path = f'/projects/{largest_allowable_project_key}/presign/?fileuri='
        scheme_length = len('https://')
        longest_presign_path_length = len(longest_presign_path)
        longest_allowable_fileuri_hash_length = len(base64_encoded_uri)
        remaining_url_origin_length = (
            longest_allowable_url_length
            - scheme_length
            + longest_presign_path_length
            + longest_allowable_fileuri_hash_length
        )

        # The user domain should be the shortest part of the url, but factoring lengthy subdomains with nested levels in staging and dev environments this is a safe allowance
        assert remaining_url_origin_length >= 512

        # Check this resolves correctly on the server
        request = APIRequestFactory().get(
            reverse('data_import:project-storage-data-presign', kwargs={'project_id': 1})
            + f'?fileuri={base64_encoded_uri}'
        )

        request.user = user
        force_authenticate(request, user)

        response = view(request, project_id=1)

        # And that the response is correct
        assert response.status_code == status.HTTP_303_SEE_OTHER
        assert response.url == 'https://presigned-url.com/fileuri'
</file>

<file path="label_studio/tests/test_project_reset_summary.py">
import json

import pytest
from tasks.models import Task
from tests.conftest import project_choices
from tests.utils import make_project

pytestmark = pytest.mark.django_db


def test_reset_summary_empty_project(business_client):
    project = make_project(project_choices(), business_client.user, use_ml_backend=False)
    s = project.summary

    for field in ['created_labels', 'created_labels_drafts', 'created_annotations']:
        setattr(s, field, {'garbled': field})
    s.save()

    r = business_client.post(f'/api/projects/{project.id}/summary/reset')
    assert r.status_code == 200

    s.refresh_from_db()
    for field in ['created_labels', 'created_labels_drafts', 'created_annotations']:
        assert getattr(s, field) == {}


def test_reset_summary_project_has_drafts(business_client):
    project = make_project(project_choices(), business_client.user, use_ml_backend=False)

    r = business_client.post(
        f'/api/projects/{project.id}/import',
        data=json.dumps({'data': {'image': 'kittens.jpg'}}),
        content_type='application/json',
    )
    assert r.status_code == 201
    task = Task.objects.filter(project=project).first()
    assert task

    s = project.summary
    r = business_client.post(
        f'/api/tasks/{task.id}/drafts',
        data=json.dumps(
            {'result': [{'from_name': 'some', 'to_name': 'x', 'type': 'none', 'value': {'none': ['Opossum']}}]}
        ),
        content_type='application/json',
    )
    assert r.status_code == 201

    for field in ['created_labels', 'created_labels_drafts', 'created_annotations']:
        setattr(s, field, {'garbled': field})
    s.save()

    r = business_client.post(f'/api/projects/{project.id}/summary/reset')
    assert r.status_code == 200

    s.refresh_from_db()
    for field in ['created_labels', 'created_annotations']:
        assert getattr(s, field) == {}

    assert s.created_labels_drafts == {'some': {'Opossum': 1}}


def test_reset_summary_project_has_annotations(business_client):
    project = make_project(project_choices(), business_client.user, use_ml_backend=False)

    r = business_client.post(
        f'/api/projects/{project.id}/import',
        data=json.dumps({'data': {'image': 'kittens.jpg'}}),
        content_type='application/json',
    )
    assert r.status_code == 201
    task = Task.objects.filter(project=project).first()
    assert task

    s = project.summary
    r = business_client.post(
        f'/api/tasks/{task.id}/annotations',
        data=json.dumps(
            {'result': [{'from_name': 'some', 'to_name': 'x', 'type': 'none', 'value': {'none': ['Opossum']}}]}
        ),
        content_type='application/json',
    )
    assert r.status_code == 201

    for field in ['created_labels', 'created_labels_drafts', 'created_annotations']:
        setattr(s, field, {'garbled': field})
    s.save()

    r = business_client.post(f'/api/projects/{project.id}/summary/reset')
    assert r.status_code == 200

    s.refresh_from_db()
    assert s.created_labels_drafts == {}
    assert s.created_annotations == {'some|x|none': 1}
    assert s.created_labels == {'some': {'Opossum': 1}}


def test_delete_tasks_and_annotations_clears_created_drafts_annotations_and_labels(business_client):
    project = make_project(project_choices(), business_client.user, use_ml_backend=False)

    r = business_client.post(
        f'/api/projects/{project.id}/import',
        data=json.dumps({'data': {'image': 'kittens.jpg'}}),
        content_type='application/json',
    )
    assert r.status_code == 201
    task = Task.objects.filter(project=project).first()
    assert task

    s = project.summary

    r = business_client.post(
        f'/api/tasks/{task.id}/drafts',
        data=json.dumps(
            {'result': [{'from_name': 'some', 'to_name': 'x', 'type': 'none', 'value': {'none': ['Mouse']}}]}
        ),
        content_type='application/json',
    )
    assert r.status_code == 201
    r = business_client.post(
        f'/api/tasks/{task.id}/annotations',
        data=json.dumps(
            {'result': [{'from_name': 'some', 'to_name': 'x', 'type': 'none', 'value': {'none': ['Opossum']}}]}
        ),
        content_type='application/json',
    )
    assert r.status_code == 201

    for field in ['created_labels', 'created_labels_drafts', 'created_annotations']:
        setattr(s, field, {'garbled': field})
    s.save()

    r = business_client.post(f'/api/dm/actions?id=delete_tasks_annotations&project={project.id}')
    assert r.status_code == 200

    s.refresh_from_db()
    for field in ['created_labels', 'created_labels_drafts', 'created_annotations']:
        assert getattr(s, field) == {}


def test_logged_out_user_cannot_reset_summary(business_client):
    project = make_project(project_choices(), business_client.user, use_ml_backend=False)
    r = business_client.get('/logout')
    assert r.status_code == 302
    r = business_client.post(f'/api/projects/{project.id}/summary/reset')
    assert r.status_code == 401
    assert 'detail' in (r_json := r.json())
    assert r_json['detail'] == 'Authentication credentials were not provided.'
</file>

<file path="label_studio/tests/test_project_validation.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import pytest
from django.urls import reverse


@pytest.mark.parametrize(
    'label_config, status_code',
    [
        (
            """<View>
      <Text name="meta_info" value="$meta_info "></Text>
      <Text name="text" value="$text"></Text>
      <Choices name="text_class" choice="single" toName="text">
        <Choice value="class_A"></Choice>
        <Choice value="class_B"></Choice>
      </Choices>
    </View>""",
            204,
        ),
        (
            """<View>
      <Text name="text" value="$text"></Text>
      <Choices name="text_class" choice="single" toName="text">
        <Choice value="class_A"></Choice>
        <Choice value="class_B"></Choice>
      </Choices>
    </View>""",
            204,
        ),
        (
            """<View>
      <TextEditor>
        <Text name="meta_info" value="$meta_info"></Text>
        <Text name="text" value="$text"></Text>
        <Choices name="text_class" choice="single" toName="text">
          <Choice value="class_A"></Choice>
          <Choice value="class_B"></Choice>
        </Choices>
      </TextEditor>
    </View>""",
            204,
        ),
        (
            """<<< <<< View>
      <TextEditor <<< << >
        <Text name="meta_info" value="$meta_info"></Text <<< >
        <Text name="text" value="$text"></Text>
        <Choices name="text_class" choice="single">
          <Choice value="class_A"></Choice>
          <<<<< <C <<<< h oice value="class_B" " " -></ Choices /BUG /ERROR> >>>>>>>>
        </Choices>
      </TextEditor>
    </View>""",
            400,
        ),
        ('some shit', 400),
        (
            """
    <View>
     <Text name="text-1" value="Hello world"></Text>
     <Labels name="labels-1">
       <Label value="Hello"></Label>
       <Label value="World"></Label>
     </Labels>
    </View>
    """,
            400,
        ),
        (
            """
    <View>
     <Text name="text-1" value="Hello world"></Text>
     <Labels name="labels-1" toName="text-1">
       <Label value="Hello"></Label>
       <Label value="World"></Label>
     </Labels>
    </View>
    """,
            204,
        ),
        # non-existent toName
        (
            """
    <View>
     <Text name="text-1" value="Hello world"></Text>
     <Labels name="labels-1" toName="__weird__">
       <Label value="Hello"></Label>
       <Label value="World"></Label>
     </Labels>
    </View>
    """,
            400,
        ),
        # toName points to tag (rect-1) without sources
        (
            """<View>
      <Labels name="tag" toName="rect-1">
        <Label value="Cat"></Label>
        <Label value="Dog" background="blue"></Label>
      </Labels>
      <AddRectangleButton name="rect-1" toName="image"></AddRectangleButton>
      <Image name="image" value="$image_url"></Image>
    </View>""",
            204,
        ),
        # <View> within control tags
        (
            """
        <View>
  <Choices name="label" toName="audio" required="true" choice="multiple" >
    <View style="display: flex; flex-direction: row; background-color: #f1f1f1; padding-left: 2em; padding-right: 2em">
      <View style="margin-right: 4em">
        <Header size="4" value="Speaker Gender" />
        <Choice value="Business" />
        <Choice value="Politics" />
      </View>
      <View style="margin-right: 4em">
        <Header size="4" value="Speech Type" />
        <Choice value="Legible" />
        <Choice value="Slurred" />
      </View>
      <View>
        <Header size="4" value="Additional" />
        <Choice value="Echo" />
        <Choice value="Noises" />
        <Choice value="Music" />
      </View>
    </View>
  </Choices>
  <Audio name="audio" value="$url" />
  </View>
    """,
            204,
        ),
    ],
)
@pytest.mark.django_db
def test_validate_label_config(business_client, label_config, status_code):
    r = business_client.post(
        reverse('projects:api:label-config-validate'),
        data={'label_config': label_config},
        content_type='application/json',
    )
    assert r.status_code == status_code
</file>

<file path="label_studio/tests/test_project.py">
import json

import pytest
from django.db.models.query import QuerySet
from tests.utils import make_project
from users.models import User


@pytest.mark.django_db
def test_update_tasks_counters_and_task_states(business_client):
    project = make_project({}, business_client.user, use_ml_backend=False)

    # CHECK EMPTY LIST
    ids = []
    obj = project._update_tasks_counters_and_task_states(ids, True, True, True)
    assert obj == 0

    tasks = [{'data': {'location': 'London', 'text': 'text A'}}, {'data': {'location': 'London', 'text': 'text B'}}]
    # upload tasks with annotations
    r = business_client.post(
        f'/api/projects/{project.id}/tasks/bulk', data=json.dumps(tasks), content_type='application/json'
    )
    assert r.status_code == 201

    # CHECK LIST with IDS
    ids = list(project.tasks.all().values_list('id', flat=True))
    obj = project._update_tasks_counters_and_task_states(ids, True, True, True)
    assert obj == 0

    # CHECK SET with IDS
    ids = set(project.tasks.all().values_list('id', flat=True))
    obj = project._update_tasks_counters_and_task_states(ids, True, True, True)
    assert obj == 0


@pytest.mark.django_db
def test_project_all_members(business_client):
    project = make_project({}, business_client.user, use_ml_backend=False)
    members = project.all_members

    assert isinstance(members, QuerySet)
    assert isinstance(members.first(), User)
</file>

<file path="label_studio/tests/test_tasks_upload.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import copy
import io
import zipfile

import pytest
import requests_mock
import ujson as json
from projects.models import Project
from rest_framework.authtoken.models import Token
from tasks.models import Annotation, Prediction, Task


def post_data_as_format(setup, format_type, body, archive, multiply_files):
    # post as data
    if format_type == 'json_data':
        return setup.post(setup.urls.task_bulk, data=body, content_type='application/json')

    # post as files
    if format_type == 'json_file':
        files = {f'upload_file{i}.json': io.StringIO(body) for i in range(0, multiply_files)}
    elif format_type == 'csv_file':
        files = {f'upload_file{i}.csv': io.StringIO(body) for i in range(0, multiply_files)}
    elif format_type == 'tsv_file':
        files = {f'upload_file{i}.tsv': io.StringIO(body) for i in range(0, multiply_files)}
    elif format_type == 'txt_file':
        files = {f'upload_file{i}.txt': io.StringIO(body) for i in range(0, multiply_files)}
    else:
        raise Exception('Incorrect task data format to post')

    # zip: take files below and zip them
    if 'zip' in archive:
        file = io.BytesIO()
        ref = zipfile.ZipFile(file, mode='w', compression=zipfile.ZIP_DEFLATED)
        [ref.writestr(name, body.read()) for name, body in files.items()]

        ref.close()
        file.seek(0, 0)
        files = {'upload_file.zip': file}

        # replicate zip file x2
        if 'zip_x2' == archive:
            files.update({'upload_file2.zip': copy.deepcopy(file)})

    return setup.post(setup.urls.task_bulk, files)


@pytest.mark.parametrize('multiply_files', [1, 5])
@pytest.mark.parametrize('format_type', ['json_file', 'json_data'])
@pytest.mark.parametrize(
    'tasks, status_code, task_count',
    [
        ([{'data': {'dialog': 'some'}}], 201, 1),
        ([{'data': {'dialog': 'some'}}] * 10, 201, 10),
        ([{'data': {'another_field': 'some', 'dialog': 'some'}}], 201, 1),
        ([{'data': {'dialog': 123}, 'created_at': 123}], 201, 1),
        ([{'data': {'another_field': 'some'}}] * 10, 400, 0),
        ([{'data': {}}], 400, 0),
        ([{'data': None}], 400, 0),
        (None, 400, 0),
        ([{'data': 'string'}], 400, 0),
        ([{}, {}], 400, 0),
        ([{}], 400, 0),
        ({}, 400, 0),
        ([], 400, 0),
        ([{'dialog': 'some'}] * 10, 201, 10),
        ({'dialog': 'some'}, 201, 1),
        ([{'dialog': 'some', 'second_field': 123}] * 10, 201, 10),
        ([{'none': 'some', 'second_field': 123}] * 10, 400, 0),
    ],
)
@pytest.mark.django_db
def test_json_task_upload(setup_project_dialog, format_type, tasks, status_code, task_count, multiply_files):
    """Upload JSON as file and data with one task to project.
    Decorator pytest.mark.django_db means it will be clean DB setup_project_dialog for this test.
    """
    if format_type == 'json_data' and multiply_files > 1:
        pytest.skip('Senseless parameter combination')

    r = post_data_as_format(setup_project_dialog, format_type, json.dumps(tasks), 'none', multiply_files)
    print(f'Create json {format_type} tasks result:', r.content)
    assert r.status_code == status_code, f'Upload tasks failed. Response data: {r.data}'
    assert Task.objects.filter(project=setup_project_dialog.project.id).count() == task_count * multiply_files


@pytest.mark.parametrize(
    'tasks, status_code, task_count, annotation_count',
    [
        ([{'data': {'dialog': 'Test'}, 'annotations': [{'result': [{'id': '123'}]}]}] * 10, 201, 10, 10),
        (
            [{'data': {'dialog': 'Test'}, 'annotations': [{'result': [{'id': '123'}], 'ground_truth': True}]}],
            201,
            1,
            1,
        ),
        ([{'data': {'dialog': 'Test'}, 'annotations': [{'result': '123'}]}], 400, 0, 0),
        ([{'data': {'dialog': 'Test'}, 'meta': 'test'}] * 10, 400, 0, 0),
        ([{'data': {'dialog': 'Test'}, 'annotations': 'test'}] * 10, 400, 0, 0),
        ([{'data': {'dialog': 'Test'}, 'annotations': [{'trash': '123'}]}] * 10, 400, 0, 0),
    ],
)
@pytest.mark.django_db
def test_json_task_annotation_and_meta_upload(setup_project_dialog, tasks, status_code, task_count, annotation_count):
    """Upload JSON task with annotation to project"""
    format_type = 'json_file'
    multiply_files = 1

    r = post_data_as_format(setup_project_dialog, format_type, json.dumps(tasks), 'none', multiply_files)
    print('Create json tasks with annotations result:', r.content)
    assert r.status_code == status_code, 'Upload one task with annotation failed'

    # tasks
    tasks_db = Task.objects.filter(project=setup_project_dialog.project.id)
    assert tasks_db.count() == task_count * multiply_files
    for task in tasks_db:
        assert task.is_labeled, 'Task should be labeled'

    # annotations
    annotations = Annotation.objects.filter(task__project=setup_project_dialog.project.id)
    assert annotations.count() == annotation_count * multiply_files
    for i, annotation in enumerate(annotations):
        assert annotation.ground_truth


@pytest.mark.parametrize(
    'tasks, status_code, task_count, prediction_count',
    [
        (
            [{'data': {'dialog': 'Test'}, 'predictions': [{'result': [{'id': '123'}], 'model_version': 'test'}]}],
            201,
            1,
            1,
        ),
        ([{'data': {'dialog': 'Test'}, 'predictions': [{'WRONG_FIELD': '123'}]}], 400, 0, 0),
    ],
)
@pytest.mark.django_db
def test_json_task_predictions(setup_project_dialog, tasks, status_code, task_count, prediction_count):
    """Upload JSON task with predictions to project"""
    r = post_data_as_format(setup_project_dialog, 'json_file', json.dumps(tasks), 'none', 1)
    assert r.status_code == status_code, 'Upload one task with prediction failed'

    # predictions
    predictions = Prediction.objects.filter(project=setup_project_dialog.project.id)
    assert predictions.count() == prediction_count
    for i, predictions in enumerate(predictions):
        assert predictions.model_version == 'test'


@pytest.mark.parametrize('multiply_files', [1, 5])
@pytest.mark.parametrize('archive', ['none'])
@pytest.mark.parametrize('format_type', ['json_file'])
@pytest.mark.parametrize(
    'tasks, status_code, task_count, annotation_count',
    [
        (
            [{'data': {'dialog': 'Test'}, 'annotations': [{'result': [{'id': '123'}]}, {'result': [{'id': '456'}]}]}]
            * 10,
            201,
            10,
            20,
        ),
        ([{'data': {'dialog': 'Test'}, 'annotations': [{'trash': '123'}]}] * 10, 400, 0, 0),
    ],
)
@pytest.mark.django_db
def test_archives(
    setup_project_dialog, format_type, tasks, status_code, task_count, annotation_count, archive, multiply_files
):
    """Upload JSON task with annotation to project"""
    multiplier = (2 if 'zip_x2' == archive else 1) * multiply_files

    r = post_data_as_format(setup_project_dialog, format_type, json.dumps(tasks), archive, multiply_files)
    print('Create json tasks with annotations result:', r.content)
    assert r.status_code == status_code, 'Upload one task with annotation failed'

    # tasks
    tasks = Task.objects.filter(project=setup_project_dialog.project.id)
    assert tasks.count() == task_count * multiplier
    for task in tasks:
        assert task.is_labeled, 'Task should be labeled'

    # annotations
    annotations = Annotation.objects.filter(task__project=setup_project_dialog.project.id)
    assert annotations.count() == annotation_count * multiplier
    for annotation in annotations:
        assert annotation.ground_truth


@pytest.mark.parametrize('multiply_files', [1, 5])
@pytest.mark.parametrize('archive', ['none'])
@pytest.mark.parametrize('format_type', ['csv_file', 'tsv_file'])
@pytest.mark.parametrize(
    'tasks, status_code, task_count',
    [
        ('dialog,second\ndialog 1,second 1\ndialog 2,second 2', 201, 2),
        ('dialog,second,class\ndialog 1, second 2, class 1', 201, 1),
        ('here_is_error_in_column_count,second\ndialog 1, second 1, class 1', 400, 0),
        ('empty_rows\n', 400, 0),
        ('', 400, 0),
    ],
)
@pytest.mark.django_db
def test_csv_tsv_task_upload(
    setup_project_dialog, format_type, tasks, status_code, task_count, archive, multiply_files
):
    """Upload CSV/TSV with one task to project"""
    multiplier = (2 if 'zip_x2' == archive else 1) * multiply_files

    tasks = tasks if format_type == 'csv_file' else tasks.replace(',', '\t')  # prepare tsv file from csv
    r = post_data_as_format(setup_project_dialog, format_type, tasks, archive, multiply_files)
    print(f'Create {format_type} tasks result:', r.content)

    assert r.status_code == status_code, f'Upload one task {format_type} failed. Response data: {r.data}'
    assert Task.objects.filter(project=setup_project_dialog.project.id).count() == task_count * multiplier


@pytest.mark.parametrize('multiply_files', [1, 5])
@pytest.mark.parametrize('format_type', ['txt_file'])
@pytest.mark.parametrize('tasks, status_code, task_count', [('my text 1\nmy text 2\nmy text 3', 201, 3), ('', 400, 0)])
@pytest.mark.django_db
def test_txt_task_upload(setup_project_dialog, format_type, tasks, status_code, task_count, multiply_files):
    """Upload CSV/TSV with one task to project"""
    multiplier = multiply_files

    r = post_data_as_format(setup_project_dialog, format_type, tasks, 'none', multiply_files)
    print(f'Create {format_type} tasks result:', r.content)

    assert r.status_code == status_code, f'Upload one task {format_type} failed. Response data: {r.data}'
    assert Task.objects.filter(project=setup_project_dialog.project.id).count() == task_count * multiplier


@pytest.mark.parametrize(
    'tasks, status_code, task_count, max_duration',
    [([{'data': {'dialog': 'Test'}, 'annotations': [{'result': [{'id': '123'}]}]}] * 1000, 201, 1000, 30)],
)
@pytest.mark.django_db
def test_upload_duration(setup_project_dialog, tasks, status_code, task_count, max_duration):
    """Upload JSON task with annotation to project"""
    r = post_data_as_format(setup_project_dialog, 'json_data', json.dumps(tasks), 'none', 1)
    print('Create json tasks with annotations result:', r.content)
    assert r.status_code == status_code, ('Upload one task with annotation failed', r.content)

    # tasks
    tasks = Task.objects.filter(project=setup_project_dialog.project.id)
    assert tasks.count() == task_count
    for task in tasks:
        assert task.is_labeled, 'Task should be labeled'

    # check max duration
    result = json.loads(r.content)
    assert result['duration'] < max_duration, 'Max duration of adding tasks is exceeded'


@pytest.mark.parametrize(
    'tasks, status_code, task_count',
    [([{'data': {'dialog': 'Test'}, 'annotations': [{'result': [{'id': '123'}]}]}] * 100, 201, 100)],
)
@pytest.mark.django_db
def test_url_upload(mocker, setup_project_dialog, tasks, status_code, task_count):
    """Upload tasks from URL"""
    with requests_mock.Mocker(real_http=True) as m:
        url = 'http://localhost:8111/test.json'
        m.get(url, text=json.dumps(tasks), headers={'Content-Length': '100'})
        r = setup_project_dialog.post(
            setup_project_dialog.urls.task_bulk, data='url=' + url, content_type='application/x-www-form-urlencoded'
        )
        assert r.status_code == status_code, 'Upload URL failed: ' + str(r.content)

        # tasks
        tasks = Task.objects.filter(project=setup_project_dialog.project.id)
        assert tasks.count() == task_count
        for task in tasks:
            assert task.is_labeled, 'Task should be labeled since annotation is ground_truth'


@pytest.mark.parametrize(
    'tasks, status_code, task_count, bad_token',
    [([{'dialog': 'Test'}] * 1, 201, 1, False), ([{'dialog': 'Test'}] * 1, 401, 0, True)],
)
@pytest.mark.django_db
def test_upload_with_token(setup_project_for_token, tasks, status_code, task_count, bad_token):
    """Upload with Django Token"""
    setup = setup_project_for_token
    token = Token.objects.get(user=setup.user)
    token = 'Token ' + str(token)
    broken_token = 'Token broken'
    data = setup.project_config
    data['organization_pk'] = setup.org.pk
    r = setup.post(setup.urls.project_create, data=data, HTTP_AUTHORIZATION=token)
    print('Project create with status code:', r.status_code, r.content)
    assert r.status_code == 201, 'Create project result should be redirect to the next page: ' + str(r.content)

    project = Project.objects.filter(title=setup.project_config['title']).first()
    setup.urls.set_project(project.pk)

    r = setup.post(
        setup.urls.task_bulk,
        data=json.dumps(tasks),
        content_type='application/json',
        HTTP_AUTHORIZATION=broken_token if bad_token else token,
    )
    assert r.status_code == status_code, 'Create json tasks result: ' + str(r.content)

    # tasks
    tasks = Task.objects.filter(project=project.id)
    assert tasks.count() == task_count
</file>

<file path="label_studio/tests/test_upload_svg.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import io

import pytest
from data_import.models import FileUpload
from django.conf import settings


@pytest.mark.django_db
def test_svg_upload_sanitize(setup_project_dialog):
    """Upload malicious SVG file - remove harmful content"""
    settings.SVG_SECURITY_CLEANUP = True

    xml_dirty = """<?xml version="1.0" standalone="no"?>
                <!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
                <svg version="1.1" baseProfile="full" xmlns="http://www.w3.org/2000/svg">
                <polygon id="triangle" points="0,0 0,50 50,0" fill="#009900" stroke="#004400"/>
                <script type="text/javascript">alert(document.cookie);</script>
                </svg>"""

    f = io.StringIO(xml_dirty)

    endpoint = f'/api/projects/{setup_project_dialog.project.id}/import?commit_to_project=true'
    r = setup_project_dialog.post(endpoint, {'xss_svg.svg': f})

    assert r.status_code == 201

    expected = """<svg version="1.1" baseprofile="full" xmlns="http://www.w3.org/2000/svg">
    <polygon id="triangle" points="0,0 0,50 50,0" fill="#009900" stroke="#004400"></polygon>\n
    </svg>\n"""

    actual = FileUpload.objects.filter(id=r.data['file_upload_ids'][0]).last().file.read()

    assert len(''.join(actual.decode('UTF-8').split())) > 100   # confirm not empty

    assert ''.join(expected.split()) == ''.join(actual.decode('UTF-8').split())


@pytest.mark.django_db
def test_svg_upload_invalid_format(setup_project_dialog):
    """Upload invalid SVG file - still accepted"""
    settings.SVG_SECURITY_CLEANUP = True

    xml_dirty = """<?xml version="1.0" standalone="no"?>
                <!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
                <svg version="1.1" baseProfile="full" xmlns="http://www.w3.org/2000/svg">gibberish</svg>"""
    f = io.StringIO(xml_dirty)

    endpoint = f'/api/projects/{setup_project_dialog.project.id}/import?commit_to_project=true'
    r = setup_project_dialog.post(endpoint, {'xss_svg.svg': f})

    assert r.status_code == 201

    expected = """
    <svgversion="1.1"baseprofile="full"xmlns="http://www.w3.org/2000/svg">gibberish</svg>
    """

    actual = FileUpload.objects.filter(id=r.data['file_upload_ids'][0]).last().file.read()

    assert ''.join(expected.split()) == ''.join(actual.decode('UTF-8').split())


@pytest.mark.django_db
def test_svg_upload_do_not_sanitize(setup_project_dialog):
    """Upload SVG file - do not sanitize file content"""
    settings.SVG_SECURITY_CLEANUP = False

    xml_dirty = """<?xml version="1.0" standalone="no"?>
                <!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
                <svg version="1.1" baseProfile="full" xmlns="http://www.w3.org/2000/svg">
                <polygon id="triangle" points="0,0 0,50 50,0" fill="#009900" stroke="#004400"/>
                <script type="text/javascript">alert(document.cookie);</script>
                </svg>"""

    f = io.StringIO(xml_dirty)

    endpoint = f'/api/projects/{setup_project_dialog.project.id}/import?commit_to_project=true'
    r = setup_project_dialog.post(endpoint, {'xss_svg.svg': f})

    assert r.status_code == 201

    actual = FileUpload.objects.filter(id=r.data['file_upload_ids'][0]).last().file.read()

    assert ''.join(xml_dirty.split()) == ''.join(actual.decode('UTF-8').replace('\n', '').split())
</file>

<file path="label_studio/tests/utils.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import os.path
import re
import tempfile
from contextlib import contextmanager
from functools import wraps
from pathlib import Path
from types import SimpleNamespace
from unittest import mock

import pytest
import requests
import requests_mock
import ujson as json
from box import Box
from core.feature_flags import flag_set
from data_export.models import ConvertedFormat, Export
from django.apps import apps
from django.conf import settings
from django.test import Client
from ml.models import MLBackend
from organizations.models import Organization
from projects.models import Project
from tasks.serializers import TaskWithAnnotationsSerializer
from users.models import User

try:
    from businesses.models import BillingPlan, Business
except ImportError:
    BillingPlan = Business = None


@contextmanager
def ml_backend_mock(**kwargs):
    with requests_mock.Mocker(real_http=True) as m:
        yield register_ml_backend_mock(m, **kwargs)


def register_ml_backend_mock(
    m,
    url='http://localhost:9090',
    predictions=None,
    health_connect_timeout=False,
    train_job_id='123',
    setup_model_version='abc',
):
    m.post(f'{url}/setup', text=json.dumps({'status': 'ok', 'model_version': setup_model_version}))
    if health_connect_timeout:
        m.get(f'{url}/health', exc=requests.exceptions.ConnectTimeout)
    else:
        m.get(f'{url}/health', text=json.dumps({'status': 'UP'}))
    m.post(f'{url}/train', text=json.dumps({'status': 'ok', 'job_id': train_job_id}))
    m.post(f'{url}/predict', text=json.dumps(predictions or {}))
    m.post(f'{url}/webhook', text=json.dumps({}))
    m.get(f'{url}/versions', text=json.dumps({'versions': ['1', '2']}))
    return m


@contextmanager
def import_from_url_mock(**kwargs):
    with mock.patch('core.utils.io.validate_upload_url'):
        with requests_mock.Mocker(real_http=True) as m:

            with open('./tests/test_suites/samples/test_1.csv', 'rb') as f:
                matcher = re.compile('data\.heartextest\.net/test_1\.csv')

                m.get(matcher, body=f, headers={'Content-Length': '100'})
                yield m


class _TestJob(object):
    def __init__(self, job_id):
        self.id = job_id


@contextmanager
def email_mock():
    from django.core.mail import EmailMultiAlternatives

    with mock.patch.object(EmailMultiAlternatives, 'send'):
        yield


@contextmanager
def gcs_client_mock():
    from collections import namedtuple

    from google.cloud import storage as google_storage

    File = namedtuple('File', ['name'])

    class DummyGCSBlob:
        def __init__(self, bucket_name, key, is_json):
            self.key = key
            self.bucket_name = bucket_name
            self.name = f'{bucket_name}/{key}'
            self.is_json = is_json

        def download_as_string(self):
            data = f'test_blob_{self.key}'
            if self.is_json:
                return json.dumps({'str_field': data, 'int_field': 123, 'dict_field': {'one': 'wow', 'two': 456}})
            return data

        def upload_from_string(self, string):
            print(f'String {string} uploaded to bucket {self.bucket_name}')

        def generate_signed_url(self, **kwargs):
            return f'https://storage.googleapis.com/{self.bucket_name}/{self.key}'

        def download_as_bytes(self):
            data = f'test_blob_{self.key}'
            if self.is_json:
                return json.dumps({'str_field': data, 'int_field': 123, 'dict_field': {'one': 'wow', 'two': 456}})
            return data

    class DummyGCSBucket:
        def __init__(self, bucket_name, is_json, **kwargs):
            self.name = bucket_name
            self.is_json = is_json

        def list_blobs(self, prefix, **kwargs):
            if 'fake' in prefix:
                return []
            return [File('abc'), File('def'), File('ghi')]

        def blob(self, key):
            return DummyGCSBlob(self.name, key, self.is_json)

    class DummyGCSClient:
        def get_bucket(self, bucket_name):
            is_json = bucket_name.endswith('_JSON')
            return DummyGCSBucket(bucket_name, is_json)

        def list_blobs(self, bucket_name, prefix):
            is_json = bucket_name.endswith('_JSON')
            return [
                DummyGCSBlob(bucket_name, 'abc', is_json),
                DummyGCSBlob(bucket_name, 'def', is_json),
                DummyGCSBlob(bucket_name, 'ghi', is_json),
            ]

    with mock.patch.object(google_storage, 'Client', return_value=DummyGCSClient()):
        yield


@contextmanager
def azure_client_mock():
    from collections import namedtuple

    from io_storages.azure_blob import models

    File = namedtuple('File', ['name'])

    class DummyAzureBlob:
        def __init__(self, container_name, key):
            self.key = key
            self.container_name = container_name

        def download_as_string(self):
            return f'test_blob_{self.key}'

        def upload_blob(self, string, overwrite):
            print(f'String {string} uploaded to bucket {self.container_name}')

        def generate_signed_url(self, **kwargs):
            return f'https://storage.googleapis.com/{self.container_name}/{self.key}'

        def content_as_text(self):
            return json.dumps({'str_field': str(self.key), 'int_field': 123, 'dict_field': {'one': 'wow', 'two': 456}})

    class DummyAzureContainer:
        def __init__(self, container_name, **kwargs):
            self.name = container_name

        def list_blobs(self, name_starts_with):
            return [File('abc'), File('def'), File('ghi')]

        def get_blob_client(self, key):
            return DummyAzureBlob(self.name, key)

        def get_container_properties(self, **kwargs):
            return SimpleNamespace(
                name='test-container',
                last_modified='2022-01-01 01:01:01',
                etag='test-etag',
                lease='test-lease',
                public_access='public',
                has_immutability_policy=True,
                has_legal_hold=True,
                immutable_storage_with_versioning_enabled=True,
                metadata={'key': 'value'},
                encryption_scope='test-scope',
                deleted=False,
                version='1.0.0',
            )

        def download_blob(self, key):
            return DummyAzureBlob(self.name, key)

    class DummyAzureClient:
        def get_container_client(self, container_name):
            return DummyAzureContainer(container_name)

    # def dummy_generate_blob_sas(*args, **kwargs):
    #     return 'token'

    with mock.patch.object(models.BlobServiceClient, 'from_connection_string', return_value=DummyAzureClient()):
        with mock.patch.object(models, 'generate_blob_sas', return_value='token'):
            yield


@contextmanager
def redis_client_mock():
    from fakeredis import FakeRedis
    from io_storages.redis.models import RedisStorageMixin

    redis = FakeRedis()
    # TODO: add mocked redis data

    with mock.patch.object(RedisStorageMixin, 'get_redis_connection', return_value=redis):
        yield


def upload_data(client, project, tasks):
    tasks = TaskWithAnnotationsSerializer(tasks, many=True).data
    data = [{'data': task['data'], 'annotations': task['annotations']} for task in tasks]
    return client.post(f'/api/projects/{project.id}/tasks/bulk', data=data, content_type='application/json')


def make_project(config, user, use_ml_backend=True, team_id=None, org=None):
    if org is None:
        org = Organization.objects.filter(created_by=user).first()
    project = Project.objects.create(created_by=user, organization=org, **config)
    if use_ml_backend:
        MLBackend.objects.create(project=project, url='http://localhost:8999')

    return project


@pytest.fixture
@pytest.mark.django_db
def project_id(business_client):
    payload = dict(title='test_project')
    response = business_client.post(
        '/api/projects/',
        data=json.dumps(payload),
        content_type='application/json',
    )
    return response.json()['id']


def make_task(config, project):
    from tasks.models import Task

    return Task.objects.create(project=project, overlap=project.maximum_annotations, **config)


def create_business(user):
    return None


def make_annotation(config, task_id):
    from tasks.models import Annotation, Task

    task = Task.objects.get(pk=task_id)

    return Annotation.objects.create(project_id=task.project_id, task_id=task_id, **config)


def make_prediction(config, task_id):
    from tasks.models import Prediction, Task

    task = Task.objects.get(pk=task_id)
    return Prediction.objects.create(task_id=task_id, project=task.project, **config)


def make_annotator(config, project, login=False, client=None):
    from users.models import User

    user = User.objects.create(**config)
    user.set_password('12345')
    user.save()

    create_business(user)

    if login:
        Organization.create_organization(created_by=user, title=user.first_name)

        if client is None:
            client = Client()
        signin_status_code = signin(client, config['email'], '12345').status_code
        assert signin_status_code == 302, f'Sign-in status code: {signin_status_code}'

    project.add_collaborator(user)
    if login:
        client.annotator = user
        return client
    return user


def invite_client_to_project(client, project):
    if apps.is_installed('annotators'):
        return client.get(f'/annotator/invites/{project.token}/')
    else:
        return SimpleNamespace(status_code=200)


def login(client, email, password):
    if User.objects.filter(email=email).exists():
        r = client.post('/user/login/', data={'email': email, 'password': password})
        assert r.status_code == 302, r.status_code
    else:
        r = client.post('/user/signup/', data={'email': email, 'password': password, 'title': 'Whatever'})
        assert r.status_code == 302, r.status_code


def signin(client, email, password):
    return client.post('/user/login/', data={'email': email, 'password': password})


def signout(client):
    return client.get('/logout')


def _client_is_annotator(client):
    return 'annotator' in client.user.email


def save_response(response):
    fp = os.path.join(settings.TEST_DATA_ROOT, 'tavern-output.json')
    with open(fp, 'w') as f:
        json.dump(response.json(), f)


def os_independent_path(_, path, add_tempdir=False):
    os_independent_path = Path(path)
    if add_tempdir:
        tempdir = Path(tempfile.gettempdir())
        os_independent_path = tempdir / os_independent_path

    os_independent_path_parent = os_independent_path.parent
    return Box(
        {
            'os_independent_path': str(os_independent_path),
            'os_independent_path_parent': str(os_independent_path_parent),
            'os_independent_path_tmpdir': str(Path(tempfile.gettempdir())),
        }
    )


def verify_docs(response):
    for _, path in response.json()['paths'].items():
        print(path)
        for _, method in path.items():
            print(method)
            if isinstance(method, dict):
                assert 'api' not in method['tags'], f'Need docs for API method {method}'


def empty_list(response):
    assert len(response.json()) == 0, f'Response should be empty, but is {response.json()}'


def save_export_file_path(response):
    export_id = response.json().get('id')
    export = Export.objects.get(id=export_id)
    file_path = export.file.path
    return Box({'file_path': file_path})


def save_convert_file_path(response, export_id=None):
    export = response.json()[0]
    convert = export['converted_formats'][0]

    converted = ConvertedFormat.objects.get(id=convert['id'])

    dir_path = os.path.join(settings.MEDIA_ROOT, settings.DELAYED_EXPORT_DIR)
    os.listdir(dir_path)
    try:
        file_path = converted.file.path
        return Box({'convert_file_path': file_path})
    except ValueError:
        return Box({'convert_file_path': None})


def file_exists_in_storage(response, exists=True, file_path=None):
    if not file_path:
        export_id = response.json().get('id')
        export = Export.objects.get(id=export_id)
        file_path = export.file.path

    assert os.path.isfile(file_path) == exists


def mock_feature_flag(flag_name: str, value: bool, parent_module: str = 'core.feature_flags'):
    """Decorator to mock a feature flag state for a test function.

    Args:
        flag_name: Name of the feature flag to mock
        value: True or False to set the flag state
        parent_module: Module path containing the flag_set function to patch
    """

    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            def fake_flag_set(feature_flag, *flag_args, **flag_kwargs):
                if feature_flag == flag_name:
                    return value
                return flag_set(feature_flag, *flag_args, **flag_kwargs)

            with mock.patch(f'{parent_module}.flag_set', wraps=fake_flag_set):
                return func(*args, **kwargs)

        return wrapper

    return decorator
</file>

<file path="label_studio/users/migrations/__init__.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
</file>

<file path="label_studio/users/migrations/0001_squashed_0009_auto_20210219_1237.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
# Generated by Django 3.1.4 on 2021-02-25 13:48

from django.db import migrations, models
import django.utils.timezone
import users.functions
import users.models

from rest_framework.authtoken.models import Token


def add_tokens(apps, schema_editor):
    User = apps.get_model('users', 'User')
    all_users = User.objects.all()

    for user_one in all_users:
        if not hasattr(user_one, 'auth_token'):
            Token.objects.create(user=user_one)


class Migration(migrations.Migration):
    replaces = [('users', '0001_initial'), ('users', '0002_user_last_activity'), ('users', '0003_add_tokens_to_all'),
                ('users', '0004_add_removed_user'), ('users', '0005_auto_20200731_1943'),
                ('users', '0006_auto_20201015_1553'), ('users', '0007_user_activity_at'),
                ('users', '0008_auto_20210211_1606'), ('users', '0009_auto_20210219_1237')]

    initial = True

    dependencies = [
        ('auth', '0009_alter_user_last_name_max_length'),
    ]

    operations = [
        migrations.CreateModel(
            name='User',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('password', models.CharField(max_length=128, verbose_name='password')),
                ('last_login', models.DateTimeField(blank=True, null=True, verbose_name='last login')),
                ('is_superuser', models.BooleanField(default=False,
                                                     help_text='Designates that this user has all permissions without explicitly assigning them.',
                                                     verbose_name='superuser status')),
                ('username', models.CharField(blank=True, max_length=12, verbose_name='username')),
                ('email', models.EmailField(blank=True, max_length=254, unique=True, verbose_name='email address')),
                ('first_name', models.CharField(blank=True, max_length=150, verbose_name='first name')),
                ('last_name', models.CharField(blank=True, max_length=150, verbose_name='last name')),
                ('is_staff', models.BooleanField(default=False,
                                                 help_text='Designates whether the user can log into this admin site.',
                                                 verbose_name='staff status')),
                ('is_active', models.BooleanField(default=True,
                                                  help_text='Designates whether this user should be treated as active. Unselect this instead of deleting accounts.',
                                                  verbose_name='active')),
                ('date_joined', models.DateTimeField(default=django.utils.timezone.now, verbose_name='date joined')),
                ('groups', models.ManyToManyField(blank=True,
                                                  help_text='The groups this user belongs to. A user will get all permissions granted to each of their groups.',
                                                  related_name='user_set', related_query_name='user', to='auth.Group',
                                                  verbose_name='groups')),
                ('user_permissions', models.ManyToManyField(blank=True, help_text='Specific permissions for this user.',
                                                            related_name='user_set', related_query_name='user',
                                                            to='auth.Permission', verbose_name='user permissions')),
                ('last_activity',
                 models.DateTimeField(default=django.utils.timezone.now, editable=False, verbose_name='last activity')),
            ],
            options={
                'verbose_name': 'user',
                'verbose_name_plural': 'users',
                'db_table': 'htx_user',
            },
        ),
        migrations.RunPython(add_tokens),
        # migrations.RunPython(add_users),  # TODO: flag:ent

        migrations.AlterField(
            model_name='user',
            name='username',
            field=models.CharField(blank=True, max_length=100, verbose_name='username'),
        ),
        migrations.AlterField(
            model_name='user',
            name='first_name',
            field=models.CharField(blank=True, max_length=256, verbose_name='first name'),
        ),
        migrations.AlterField(
            model_name='user',
            name='last_name',
            field=models.CharField(blank=True, max_length=256, verbose_name='last name'),
        ),
        migrations.AlterField(
            model_name='user',
            name='username',
            field=models.CharField(blank=True, max_length=256, verbose_name='username'),
        ),
        migrations.AddField(
            model_name='user',
            name='activity_at',
            field=models.DateTimeField(auto_now=True, verbose_name='last completion activity'),
        ),
        migrations.AddField(
            model_name='user',
            name='phone',
            field=models.CharField(blank=True, max_length=256, verbose_name='phone'),
        ),
        migrations.AddField(
            model_name='user',
            name='avatar',
            field=models.ImageField(blank=True, upload_to=users.functions.hash_upload),
        ),
    ]
</file>

<file path="label_studio/users/migrations/0002_auto_20210308_1559.py">
# Generated by Django 3.1.4 on 2021-03-08 15:59

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('users', '0001_squashed_0009_auto_20210219_1237'),
    ]

    operations = [
        migrations.AlterField(
            model_name='user',
            name='activity_at',
            field=models.DateTimeField(auto_now=True, verbose_name='last annotation activity'),
        ),
        migrations.AlterField(
            model_name='user',
            name='is_active',
            field=models.BooleanField(default=True, help_text='Designates whether to treat this user as active. Unselect this instead of deleting accounts.', verbose_name='active'),
        ),
    ]
</file>

<file path="label_studio/users/migrations/0003_user_active_organization.py">
# Generated by Django 3.1.4 on 2021-03-22 12:35

from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    dependencies = [
        ('organizations', '0002_auto_20210310_2044'),
        ('users', '0002_auto_20210308_1559'),
    ]

    operations = [
        migrations.AddField(
            model_name='user',
            name='active_organization',
            field=models.ForeignKey(null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='active_users', to='organizations.organization'),
        ),
    ]
</file>

<file path="label_studio/users/migrations/0004_auto_20210914_0109.py">
# Generated by Django 3.1.13 on 2021-09-14 01:09

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('users', '0003_user_active_organization'),
    ]

    operations = [
        migrations.AddIndex(
            model_name='user',
            index=models.Index(fields=['username'], name='htx_user_usernam_a41619_idx'),
        ),
        migrations.AddIndex(
            model_name='user',
            index=models.Index(fields=['email'], name='htx_user_email_051c68_idx'),
        ),
        migrations.AddIndex(
            model_name='user',
            index=models.Index(fields=['first_name'], name='htx_user_first_n_93c5de_idx'),
        ),
        migrations.AddIndex(
            model_name='user',
            index=models.Index(fields=['last_name'], name='htx_user_last_na_2ace53_idx'),
        ),
        migrations.AddIndex(
            model_name='user',
            index=models.Index(fields=['date_joined'], name='htx_user_date_jo_3bd95e_idx'),
        ),
    ]
</file>

<file path="label_studio/users/migrations/0005_auto_20211010_1339.py">
# Generated by Django 3.1.13 on 2021-10-10 13:39

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('users', '0004_auto_20210914_0109'),
    ]

    operations = [
        migrations.AlterField(
            model_name='user',
            name='username',
            field=models.CharField(max_length=256, verbose_name='username'),
        ),
    ]
</file>

<file path="label_studio/users/migrations/0006_user_allow_newsletters.py">
# Generated by Django 3.1.14 on 2022-06-13 23:49

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('users', '0005_auto_20211010_1339'),
    ]

    operations = [
        migrations.AddField(
            model_name='user',
            name='allow_newsletters',
            field=models.BooleanField(default=None, help_text='Allow sending newsletters to user', null=True, verbose_name='allow newsletters'),
        ),
    ]
</file>

<file path="label_studio/users/migrations/0007_user_is_deleted.py">
# Generated by Django 3.2.20 on 2023-09-22 14:49

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('users', '0006_user_allow_newsletters'),
    ]

    operations = [
        migrations.AddField(
            model_name='user',
            name='is_deleted',
            field=models.BooleanField(db_index=True, default=False, help_text='Designates whether to treat this user as deleted. Select this instead of deleting accounts.', verbose_name='deleted'),
        ),
    ]
</file>

<file path="label_studio/users/migrations/0008_alter_user_managers.py">
# Generated by Django 3.2.20 on 2023-10-05 23:38

from django.db import migrations
import django.db.models.manager
import users.models


class Migration(migrations.Migration):

    dependencies = [
        ('users', '0007_user_is_deleted'),
    ]

    operations = [
        migrations.AlterModelManagers(
            name='user',
            managers=[
                ('objects', django.db.models.manager.Manager()),
                # Previously, this migration contained an addition of UserManagerWithDeleted, which has since been
                # removed in order to avoid referencing a non-existent manager.
            ],
        ),
    ]
</file>

<file path="label_studio/users/migrations/0009_auto_20231201_0001.py">
# Generated by Django 3.2.23 on 2023-12-01 00:01

from django.db import migrations
import users.models


class Migration(migrations.Migration):

    dependencies = [
        ('users', '0008_alter_user_managers'),
    ]

    operations = [
        migrations.AlterModelManagers(
            name='user',
            managers=[
                ('objects', users.models.UserManager()),
            ],
        ),
        migrations.RemoveField(
            model_name='user',
            name='is_deleted',
        ),
    ]
</file>

<file path="label_studio/users/migrations/0010_userproducttour.py">
# Generated by Django 4.2.15 on 2024-12-22 09:54

from django.conf import settings
from django.db import migrations, models
import django.db.models.deletion
import django_migration_linter as linter


class Migration(migrations.Migration):

    dependencies = [
        ('users', '0009_auto_20231201_0001'),
    ]

    operations = [
        linter.IgnoreMigration(),
        migrations.CreateModel(
            name='UserProductTour',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('name', models.CharField(help_text='Unique identifier for the product tour. Name must match the config name.', max_length=256, verbose_name='Name')),
                ('state', models.CharField(choices=[('ready', 'Ready'), ('completed', 'Completed'), ('skipped', 'Skipped')], default='ready', help_text='Current state of the tour for this user. Available options: ready (Ready), completed (Completed), skipped (Skipped)', max_length=32, verbose_name='State')),
                ('interaction_data', models.JSONField(blank=True, default=dict, help_text='Additional data about user interaction with the tour', verbose_name='Interaction Data')),
                ('created_at', models.DateTimeField(auto_now_add=True, help_text='When this tour record was created')),
                ('updated_at', models.DateTimeField(auto_now=True, help_text='When this tour record was last updated')),
                ('user', models.ForeignKey(help_text='User who interacted with the tour', on_delete=django.db.models.deletion.CASCADE, related_name='tours', to=settings.AUTH_USER_MODEL)),
            ],
        ),
    ]
</file>

<file path="label_studio/users/product_tours/api.py">
import logging

from rest_framework import generics
from rest_framework.exceptions import ValidationError
from rest_framework.permissions import IsAuthenticated
from users.product_tours.models import UserProductTour

from .serializers import UserProductTourSerializer

logger = logging.getLogger(__name__)


class ProductTourAPI(generics.RetrieveUpdateAPIView):
    permission_classes = (IsAuthenticated,)
    serializer_class = UserProductTourSerializer
    swagger_schema = None

    def get_tour_name(self):
        name = self.request.query_params.get('name')
        if not name:
            raise ValidationError('Name is required')
        # normalize name for subsequent checks
        return name.replace('-', '_').lower()

    def get_serializer_context(self):
        context = super().get_serializer_context()
        context['name'] = self.get_tour_name()
        return context

    def get_object(self):
        name = self.get_tour_name()

        # TODO: add additional checks, e.g. user agent, role, etc.

        tour = UserProductTour.objects.filter(user=self.request.user, name=name).first()
        if not tour:
            logger.debug(f'Product tour {name} not found for user {self.request.user.id}. Creating new tour.')
            tour_serializer = self.get_serializer(data={'user': self.request.user.id, 'name': name})
            tour_serializer.is_valid(raise_exception=True)
            tour = tour_serializer.save()
        else:
            logger.debug(f'Product tour {name} requested for user {self.request.user.id}.')

        return tour
</file>

<file path="label_studio/users/product_tours/models.py">
from typing import Any, Dict, Optional

from django.db import models
from django.utils.translation import gettext_lazy as _
from pydantic import BaseModel, Field


class ProductTourState(models.TextChoices):
    READY = 'ready', _('Ready')
    COMPLETED = 'completed', _('Completed')
    SKIPPED = 'skipped', _('Skipped')


class ProductTourInteractionData(BaseModel):
    """Pydantic model for validating tour interaction data"""

    index: Optional[int] = Field(None, description='Step number where tour was completed')
    action: Optional[str] = Field(None, description='Action taken during the tour')
    type: Optional[str] = Field(None, description='Type of interaction')
    status: Optional[str] = Field(None, description='Status of the interaction')
    additional_data: Optional[Dict[str, Any]] = Field(
        default_factory=dict, description='Extensible field for additional interaction data'
    )


class UserProductTour(models.Model):
    """Stores product tour state and interaction data for users"""

    user = models.ForeignKey(
        'User', on_delete=models.CASCADE, related_name='tours', help_text='User who interacted with the tour'
    )

    name = models.CharField(
        _('Name'), max_length=256, help_text='Unique identifier for the product tour. Name must match the config name.'
    )

    state = models.CharField(
        _('State'),
        max_length=32,
        choices=ProductTourState.choices,
        default=ProductTourState.READY,
        help_text=f'Current state of the tour for this user. Available options: {", ".join(f"{k} ({v})" for k,v in ProductTourState.choices)}',
    )

    interaction_data = models.JSONField(
        _('Interaction Data'),
        default=dict,
        blank=True,
        help_text='Additional data about user interaction with the tour',
    )

    created_at = models.DateTimeField(auto_now_add=True, help_text='When this tour record was created')

    updated_at = models.DateTimeField(auto_now=True, help_text='When this tour record was last updated')

    def __str__(self):
        return f'{self.user.email} - {self.name} ({self.state})'
</file>

<file path="label_studio/users/product_tours/serializers.py">
import logging
import pathlib
from functools import cached_property

import yaml
from core.utils.db import fast_first
from rest_framework import serializers

from .models import ProductTourInteractionData, ProductTourState, UserProductTour

logger = logging.getLogger(__name__)

PRODUCT_TOURS_CONFIGS_DIR = pathlib.Path(__file__).parent / 'configs'


class UserProductTourSerializer(serializers.ModelSerializer):
    # steps is a list of steps in the tour loaded from the yaml file
    steps = serializers.SerializerMethodField(read_only=True)
    # awaiting is a boolean that indicates if the tour is awaiting other tours in the list of "dependencies"
    awaiting = serializers.SerializerMethodField(read_only=True)

    class Meta:
        model = UserProductTour
        fields = '__all__'

    @cached_property
    def available_tours(self):
        return {pathlib.Path(f).stem for f in PRODUCT_TOURS_CONFIGS_DIR.iterdir()}

    def validate_name(self, value):

        if value not in self.available_tours:
            raise serializers.ValidationError(
                f'Product tour {value} not found. Available tours: {self.available_tours}'
            )

        return value

    @cached_property
    def load_tour_config(self):
        # TODO: get product tour from yaml file. Later we move it to remote storage, e.g. S3
        filepath = PRODUCT_TOURS_CONFIGS_DIR / f'{self.context["name"]}.yml'
        with open(filepath, 'r') as f:
            return yaml.safe_load(f)

    def get_awaiting(self, obj):
        config = self.load_tour_config
        dependencies = config.get('dependencies', [])
        for dependency in dependencies:
            tour = fast_first(UserProductTour.objects.filter(user=self.context['request'].user, name=dependency))
            if not tour or tour.state != ProductTourState.COMPLETED:
                logger.info(f'Tour {dependency} is not completed: skipping tour {self.context["name"]}')
                return True
        return False

    def get_steps(self, obj):
        config = self.load_tour_config
        return config.get('steps', [])

    def validate_interaction_data(self, value):
        try:
            # Validate interaction data using pydantic model
            ProductTourInteractionData(**value)
            return value
        except Exception:
            raise serializers.ValidationError('Invalid product tour interaction data format.')
</file>

<file path="label_studio/users/tests/factories.py">
import factory
from organizations.models import OrganizationMember
from users.models import User


class UserFactory(factory.django.DjangoModelFactory):
    email = factory.Faker('email')
    first_name = factory.Faker('first_name')
    last_name = factory.Faker('last_name')
    username = factory.LazyAttribute(lambda u: u.email.split('@')[0])
    password = factory.Faker('password')

    class Meta:
        model = User

    @factory.post_generation
    def active_organization(self, create, extracted, **kwargs):
        if not create or not extracted:
            return
        self.active_organization = extracted
        self.save(update_fields=['active_organization'])
        OrganizationMember.objects.create(user=self, organization=extracted)
</file>

<file path="label_studio/users/__init__.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
</file>

<file path="label_studio/users/admin.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
from core.models import AsyncMigrationStatus
from django.contrib import admin
from django.contrib.auth.admin import UserAdmin
from django.contrib.auth.models import Group
from ml.models import MLBackend, MLBackendTrainJob
from organizations.models import Organization, OrganizationMember
from projects.models import Project
from tasks.models import Annotation, Prediction, Task
from users.models import User


class UserAdminShort(UserAdmin):

    add_fieldsets = ((None, {'fields': ('email', 'password1', 'password2')}),)

    def __init__(self, *args, **kwargs):
        super(UserAdminShort, self).__init__(*args, **kwargs)

        self.list_display = (
            'email',
            'username',
            'active_organization',
            'organization',
            'is_staff',
            'is_superuser',
        )
        self.list_filter = ('is_staff', 'is_superuser', 'is_active')
        self.search_fields = (
            'username',
            'first_name',
            'last_name',
            'email',
            'organization__title',
            'active_organization__title',
        )
        self.ordering = ('email',)

        self.fieldsets = (
            (None, {'fields': ('password',)}),
            ('Personal info', {'fields': ('email', 'username', 'first_name', 'last_name')}),
            (
                'Permissions',
                {
                    'fields': (
                        'is_active',
                        'is_staff',
                        'is_superuser',
                    )
                },
            ),
            ('Important dates', {'fields': ('last_login', 'date_joined')}),
        )


class AsyncMigrationStatusAdmin(admin.ModelAdmin):
    def __init__(self, *args, **kwargs):
        super(AsyncMigrationStatusAdmin, self).__init__(*args, **kwargs)

        self.list_display = ('id', 'name', 'project', 'status', 'created_at', 'updated_at', 'meta')
        self.list_filter = ('name', 'status')
        self.search_fields = ('name', 'project__id')
        self.ordering = ('id',)


class OrganizationMemberAdmin(admin.ModelAdmin):
    def __init__(self, *args, **kwargs):
        super(OrganizationMemberAdmin, self).__init__(*args, **kwargs)

        self.list_display = ('id', 'user', 'organization', 'created_at', 'updated_at')
        self.search_fields = ('user__email', 'organization__title')
        self.ordering = ('id',)


admin.site.register(User, UserAdminShort)
admin.site.register(Project)
admin.site.register(MLBackend)
admin.site.register(MLBackendTrainJob)
admin.site.register(Task)
admin.site.register(Annotation)
admin.site.register(Prediction)
admin.site.register(Organization)
admin.site.register(OrganizationMember, OrganizationMemberAdmin)
admin.site.register(AsyncMigrationStatus, AsyncMigrationStatusAdmin)

# remove unused django groups
admin.site.unregister(Group)
</file>

<file path="label_studio/users/api.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import logging

import drf_yasg.openapi as openapi
from core.permissions import ViewClassPermission, all_permissions
from django.utils.decorators import method_decorator
from drf_yasg.utils import no_body, swagger_auto_schema
from rest_framework import generics, viewsets
from rest_framework.authtoken.models import Token
from rest_framework.decorators import action
from rest_framework.exceptions import MethodNotAllowed
from rest_framework.parsers import FormParser, JSONParser, MultiPartParser
from rest_framework.permissions import IsAuthenticated
from rest_framework.response import Response
from rest_framework.views import APIView
from users.functions import check_avatar
from users.models import User
from users.serializers import UserSerializer, UserSerializerUpdate

logger = logging.getLogger(__name__)

_user_schema = openapi.Schema(
    type=openapi.TYPE_OBJECT,
    properties={
        'id': openapi.Schema(type=openapi.TYPE_INTEGER, description='User ID'),
        'first_name': openapi.Schema(type=openapi.TYPE_STRING, description='First name of the user'),
        'last_name': openapi.Schema(type=openapi.TYPE_STRING, description='Last name of the user'),
        'username': openapi.Schema(type=openapi.TYPE_STRING, description='Username of the user'),
        'email': openapi.Schema(type=openapi.TYPE_STRING, description='Email of the user'),
        'avatar': openapi.Schema(type=openapi.TYPE_STRING, description='Avatar URL of the user'),
        'initials': openapi.Schema(type=openapi.TYPE_STRING, description='Initials of the user'),
        'phone': openapi.Schema(type=openapi.TYPE_STRING, description='Phone number of the user'),
        'allow_newsletters': openapi.Schema(
            type=openapi.TYPE_BOOLEAN, description='Whether the user allows newsletters'
        ),
    },
)


@method_decorator(
    name='update',
    decorator=swagger_auto_schema(
        tags=['Users'],
        x_fern_audiences=['internal'],
        operation_summary='Save user details',
        operation_description="""
    Save details for a specific user, such as their name or contact information, in Label Studio.
    """,
        manual_parameters=[
            openapi.Parameter(name='id', type=openapi.TYPE_INTEGER, in_=openapi.IN_PATH, description='User ID'),
        ],
        request_body=UserSerializer,
    ),
)
@method_decorator(
    name='list',
    decorator=swagger_auto_schema(
        tags=['Users'],
        x_fern_sdk_group_name='users',
        x_fern_sdk_method_name='list',
        x_fern_audiences=['public'],
        operation_summary='List users',
        operation_description='List the users that exist on the Label Studio server.',
    ),
)
@method_decorator(
    name='create',
    decorator=swagger_auto_schema(
        tags=['Users'],
        x_fern_sdk_group_name='users',
        x_fern_sdk_method_name='create',
        x_fern_audiences=['public'],
        operation_summary='Create new user',
        operation_description='Create a user in Label Studio.',
        request_body=_user_schema,
        responses={201: UserSerializer},
    ),
)
@method_decorator(
    name='retrieve',
    decorator=swagger_auto_schema(
        tags=['Users'],
        x_fern_sdk_group_name='users',
        x_fern_sdk_method_name='get',
        x_fern_audiences=['public'],
        operation_summary='Get user info',
        operation_description='Get info about a specific Label Studio user, based on the user ID.',
        manual_parameters=[
            openapi.Parameter(name='id', type=openapi.TYPE_INTEGER, in_=openapi.IN_PATH, description='User ID'),
        ],
        request_body=no_body,
        responses={200: UserSerializer},
    ),
)
@method_decorator(
    name='partial_update',
    decorator=swagger_auto_schema(
        tags=['Users'],
        x_fern_sdk_group_name='users',
        x_fern_sdk_method_name='update',
        x_fern_audiences=['public'],
        operation_summary='Update user details',
        operation_description="""
        Update details for a specific user, such as their name or contact information, in Label Studio.
        """,
        manual_parameters=[
            openapi.Parameter(name='id', type=openapi.TYPE_INTEGER, in_=openapi.IN_PATH, description='User ID'),
        ],
        request_body=_user_schema,
        responses={200: UserSerializer},
    ),
)
@method_decorator(
    name='destroy',
    decorator=swagger_auto_schema(
        tags=['Users'],
        x_fern_sdk_group_name='users',
        x_fern_sdk_method_name='delete',
        x_fern_audiences=['public'],
        operation_summary='Delete user',
        operation_description='Delete a specific Label Studio user.',
        manual_parameters=[
            openapi.Parameter(name='id', type=openapi.TYPE_INTEGER, in_=openapi.IN_PATH, description='User ID'),
        ],
        request_body=no_body,
    ),
)
class UserAPI(viewsets.ModelViewSet):
    serializer_class = UserSerializer
    permission_required = ViewClassPermission(
        GET=all_permissions.organizations_change,
        PUT=all_permissions.organizations_change,
        POST=all_permissions.organizations_change,
        PATCH=all_permissions.organizations_view,
        DELETE=all_permissions.organizations_change,
    )
    http_method_names = ['get', 'post', 'head', 'patch', 'delete']

    def get_queryset(self):
        return User.objects.filter(organizations=self.request.user.active_organization)

    @swagger_auto_schema(auto_schema=None, methods=['delete', 'post'])
    @action(detail=True, methods=['delete', 'post'], permission_required=all_permissions.avatar_any)
    def avatar(self, request, pk):
        if request.method == 'POST':
            avatar = check_avatar(request.FILES)
            request.user.avatar = avatar
            request.user.save()
            return Response({'detail': 'avatar saved'}, status=200)

        elif request.method == 'DELETE':
            request.user.avatar = None
            request.user.save()
            return Response(status=204)

    def get_serializer_class(self):
        if self.request.method in {'PUT', 'PATCH'}:
            return UserSerializerUpdate
        return super().get_serializer_class()

    def get_serializer_context(self):
        context = super(UserAPI, self).get_serializer_context()
        context['user'] = self.request.user
        return context

    def update(self, request, *args, **kwargs):
        return super(UserAPI, self).update(request, *args, **kwargs)

    def list(self, request, *args, **kwargs):
        return super(UserAPI, self).list(request, *args, **kwargs)

    def create(self, request, *args, **kwargs):
        return super(UserAPI, self).create(request, *args, **kwargs)

    def perform_create(self, serializer):
        instance = serializer.save()
        self.request.user.active_organization.add_user(instance)

    def retrieve(self, request, *args, **kwargs):
        return super(UserAPI, self).retrieve(request, *args, **kwargs)

    def partial_update(self, request, *args, **kwargs):
        result = super(UserAPI, self).partial_update(request, *args, **kwargs)

        # throw MethodNotAllowed if read-only fields are attempted to be updated
        read_only_fields = self.get_serializer_class().Meta.read_only_fields
        for field in read_only_fields:
            if field in request.data:
                raise MethodNotAllowed('PATCH', detail=f'Cannot update read-only field: {field}')

        # newsletters
        if 'allow_newsletters' in request.data:
            user = User.objects.get(id=request.user.id)  # we need an updated user
            request.user.advanced_json = {  # request.user instance will be unchanged in request all the time
                'email': user.email,
                'allow_newsletters': user.allow_newsletters,
                'update-notifications': 1,
                'new-user': 0,
            }
        return result

    def destroy(self, request, *args, **kwargs):
        return super(UserAPI, self).destroy(request, *args, **kwargs)


@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Users'],
        x_fern_sdk_group_name='users',
        x_fern_sdk_method_name='reset_token',
        x_fern_audiences=['public'],
        operation_summary='Reset user token',
        operation_description='Reset the user token for the current user.',
        request_body=no_body,
        responses={
            201: openapi.Response(
                description='User token response',
                schema=openapi.Schema(
                    description='User token',
                    type=openapi.TYPE_OBJECT,
                    properties={'token': openapi.Schema(description='Token', type=openapi.TYPE_STRING)},
                ),
            )
        },
    ),
)
class UserResetTokenAPI(APIView):
    parser_classes = (JSONParser, FormParser, MultiPartParser)
    queryset = User.objects.all()
    permission_classes = (IsAuthenticated,)

    def post(self, request, *args, **kwargs):
        user = request.user
        token = user.reset_token()
        logger.debug(f'New token for user {user.pk} is {token.key}')
        return Response({'token': token.key}, status=201)


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Users'],
        x_fern_sdk_group_name='users',
        x_fern_sdk_method_name='get_token',
        x_fern_audiences=['public'],
        operation_summary='Get user token',
        operation_description='Get a user token to authenticate to the API as the current user.',
        request_body=no_body,
        responses={
            200: openapi.Response(
                description='User token response',
                schema=openapi.Schema(
                    description='User token',
                    type=openapi.TYPE_OBJECT,
                    properties={'detail': openapi.Schema(description='Token', type=openapi.TYPE_STRING)},
                ),
            )
        },
    ),
)
class UserGetTokenAPI(APIView):
    parser_classes = (JSONParser,)
    permission_classes = (IsAuthenticated,)

    def get(self, request, *args, **kwargs):
        user = request.user
        token = Token.objects.get(user=user)
        return Response({'token': str(token)}, status=200)


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Users'],
        x_fern_sdk_group_name='users',
        x_fern_sdk_method_name='whoami',
        x_fern_audiences=['public'],
        operation_summary='Retrieve my user',
        operation_description='Retrieve details of the account that you are using to access the API.',
        request_body=no_body,
        responses={200: UserSerializer},
    ),
)
class UserWhoAmIAPI(generics.RetrieveAPIView):
    parser_classes = (JSONParser, FormParser, MultiPartParser)
    queryset = User.objects.all()
    permission_classes = (IsAuthenticated,)
    serializer_class = UserSerializer

    def get_object(self):
        return self.request.user

    def get(self, request, *args, **kwargs):
        return super(UserWhoAmIAPI, self).get(request, *args, **kwargs)
</file>

<file path="label_studio/users/apps.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
from django.apps import AppConfig


class UsersConfig(AppConfig):
    name = 'users'
</file>

<file path="label_studio/users/forms.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import logging

from django import forms
from django.conf import settings
from django.contrib import auth
from users.models import User

EMAIL_MAX_LENGTH = 256
PASS_MAX_LENGTH = 64
PASS_MIN_LENGTH = 8
USERNAME_MAX_LENGTH = 30
DISPLAY_NAME_LENGTH = 100
USERNAME_LENGTH_ERR = 'Please enter a username 30 characters or fewer in length'
DISPLAY_NAME_LENGTH_ERR = 'Please enter a display name 100 characters or fewer in length'
PASS_LENGTH_ERR = 'Please enter a password 8-12 characters in length'
INVALID_USER_ERROR = "The email and password you entered don't match."

FOUND_US_ELABORATE = 'Other'
FOUND_US_OPTIONS = (
    ('Gi', 'Github'),
    ('Em', 'Email or newsletter'),
    ('Se', 'Search engine'),
    ('Fr', 'Friend or coworker'),
    ('Ad', 'Ad'),
    ('Ot', FOUND_US_ELABORATE),
)

logger = logging.getLogger(__name__)


class LoginForm(forms.Form):
    """For logging in to the app and all - session based"""

    # use username instead of email when LDAP enabled
    email = forms.CharField(label='User') if settings.USE_USERNAME_FOR_LOGIN else forms.EmailField(label='Email')
    password = forms.CharField(widget=forms.PasswordInput())
    persist_session = forms.BooleanField(widget=forms.CheckboxInput(), required=False)

    def clean(self, *args, **kwargs):
        cleaned = super(LoginForm, self).clean()
        email = cleaned.get('email', '').lower()
        password = cleaned.get('password', '')
        if len(email) >= EMAIL_MAX_LENGTH:
            raise forms.ValidationError('Email is too long')

        # advanced way for user auth
        user = settings.USER_AUTH(User, email, password)

        # regular access
        if user is None:
            user = auth.authenticate(email=email, password=password)

        if user and user.is_active:
            persist_session = cleaned.get('persist_session', False)
            return {'user': user, 'persist_session': persist_session}
        else:
            raise forms.ValidationError(INVALID_USER_ERROR)


class UserSignupForm(forms.Form):
    email = forms.EmailField(label='Work Email', error_messages={'required': 'Invalid email'})
    password = forms.CharField(
        max_length=PASS_MAX_LENGTH,
        error_messages={'required': PASS_LENGTH_ERR},
        widget=forms.TextInput(attrs={'type': 'password'}),
    )
    allow_newsletters = forms.BooleanField(required=False)
    how_find_us = forms.CharField(required=False)
    elaborate = forms.CharField(required=False)

    def clean_password(self):
        password = self.cleaned_data['password']
        if len(password) < PASS_MIN_LENGTH:
            raise forms.ValidationError(PASS_LENGTH_ERR)
        return password

    def clean_username(self):
        username = self.cleaned_data.get('username')
        if username and User.objects.filter(username=username.lower()).exists():
            raise forms.ValidationError('User with username already exists')
        return username

    def clean_email(self):
        email = self.cleaned_data.get('email').lower()
        if len(email) >= EMAIL_MAX_LENGTH:
            raise forms.ValidationError('Email is too long')

        if email and User.objects.filter(email=email).exists():
            raise forms.ValidationError('User with this email already exists')

        return email

    def save(self):
        cleaned = self.cleaned_data
        password = cleaned['password']
        email = cleaned['email'].lower()
        allow_newsletters = None
        how_find_us = None
        if 'allow_newsletters' in cleaned:
            allow_newsletters = cleaned['allow_newsletters']
        if 'how_find_us' in cleaned:
            how_find_us = cleaned['how_find_us']
        if 'elaborate' in cleaned and how_find_us == FOUND_US_ELABORATE:
            cleaned['elaborate']

        user = User.objects.create_user(email, password, allow_newsletters=allow_newsletters)
        return user


class UserProfileForm(forms.ModelForm):
    """This form is used in profile account pages"""

    class Meta:
        model = User
        fields = ('first_name', 'last_name', 'phone', 'allow_newsletters')
</file>

<file path="label_studio/users/functions.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import os
import uuid
from time import time

from core.utils.common import load_func
from django import forms
from django.conf import settings
from django.contrib import auth
from django.core.files.images import get_image_dimensions
from django.shortcuts import redirect
from django.urls import reverse
from organizations.models import Organization


def hash_upload(instance, filename):
    filename = str(uuid.uuid4())[0:8] + '-' + filename
    return settings.AVATAR_PATH + '/' + filename


def check_avatar(files):
    images = list(files.items())
    if not images:
        return None

    _, avatar = list(files.items())[0]  # get first file
    w, h = get_image_dimensions(avatar)
    if not w or not h:
        raise forms.ValidationError("Can't read image, try another one")

    # validate dimensions
    max_width = max_height = 1200
    if w > max_width or h > max_height:
        raise forms.ValidationError('Please use an image that is %s x %s pixels or smaller.' % (max_width, max_height))

    valid_extensions = ['jpeg', 'jpg', 'gif', 'png']

    filename = avatar.name
    # check file extension
    ext = os.path.splitext(filename)[1].lstrip('.').lower()
    if ext not in valid_extensions:
        raise forms.ValidationError('Please upload a valid image file with extensions: JPEG, JPG, GIF, or PNG.')

    # validate content type
    main, sub = avatar.content_type.split('/')
    if not (main == 'image' and sub.lower() in valid_extensions):
        raise forms.ValidationError('Please use a JPEG, GIF or PNG image.')

    # validate file size
    max_size = 1024 * 1024
    if len(avatar) > max_size:
        raise forms.ValidationError('Avatar file size may not exceed ' + str(max_size / 1024) + ' kb')

    return avatar


def save_user(request, next_page, user_form):
    """Save user instance to DB"""
    user = user_form.save()
    user.username = user.email.split('@')[0]
    user.save()

    if Organization.objects.exists():
        org = Organization.objects.first()
        org.add_user(user)
    else:
        org = Organization.create_organization(created_by=user, title='Label Studio')
    user.active_organization = org
    user.save(update_fields=['active_organization'])

    request.advanced_json = {
        'email': user.email,
        'allow_newsletters': user.allow_newsletters,
        'update-notifications': 1,
        'new-user': 1,
        'how_find_us': user_form.cleaned_data.get('how_find_us', ''),
    }
    if user_form.cleaned_data.get('how_find_us', '') == 'Other':
        request.advanced_json['elaborate'] = user_form.cleaned_data.get('elaborate', '')

    redirect_url = next_page if next_page else reverse('projects:project-index')
    login(request, user, backend='django.contrib.auth.backends.ModelBackend')
    return redirect(redirect_url)


def proceed_registration(request, user_form, organization_form, next_page):
    """Register a new user for POST user_signup"""
    # save user to db
    save_user = load_func(settings.SAVE_USER)
    response = save_user(request, next_page, user_form)

    return response


def login(request, *args, **kwargs):
    request.session['last_login'] = time()
    return auth.login(request, *args, **kwargs)
</file>

<file path="label_studio/users/mixins.py">
from organizations.models import OrganizationMember


class UserMixin:
    @property
    def is_annotator(self):
        return False

    def is_project_annotator(self, project):
        return False

    def has_permission(self, user):
        return OrganizationMember.objects.filter(
            user=user, organization=user.active_organization, deleted_at__isnull=True
        ).exists()
</file>

<file path="label_studio/users/models.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import datetime
from typing import Optional

from core.utils.common import load_func
from core.utils.db import fast_first
from django.conf import settings
from django.contrib.auth.base_user import AbstractBaseUser, BaseUserManager
from django.contrib.auth.models import PermissionsMixin
from django.db import models
from django.db.models.signals import post_save
from django.dispatch import receiver
from django.utils import timezone
from django.utils.functional import cached_property
from django.utils.translation import gettext_lazy as _
from organizations.models import Organization
from rest_framework.authtoken.models import Token
from users.functions import hash_upload

YEAR_START = 1980
YEAR_CHOICES = []
for r in range(YEAR_START, (datetime.datetime.now().year + 1)):
    YEAR_CHOICES.append((r, r))

year = models.IntegerField(_('year'), choices=YEAR_CHOICES, default=datetime.datetime.now().year)


class UserManager(BaseUserManager):
    use_in_migrations = True

    def _create_user(self, email, password, **extra_fields):
        """
        Create and save a user with the given email and password.
        """
        if not email:
            raise ValueError('Must specify an email address')

        email = self.normalize_email(email)
        user = self.model(email=email, **extra_fields)

        user.set_password(password)
        user.save(using=self._db)

        return user

    def create_user(self, email, password=None, **extra_fields):
        extra_fields.setdefault('is_staff', False)
        extra_fields.setdefault('is_superuser', False)
        return self._create_user(email, password, **extra_fields)

    def create_superuser(self, email, password, **extra_fields):
        extra_fields.setdefault('is_staff', True)
        extra_fields.setdefault('is_superuser', True)

        if extra_fields.get('is_staff') is not True:
            raise ValueError('Superuser must have is_staff=True.')
        if extra_fields.get('is_superuser') is not True:
            raise ValueError('Superuser must have is_superuser=True.')

        return self._create_user(email, password, **extra_fields)


class UserLastActivityMixin(models.Model):
    last_activity = models.DateTimeField(_('last activity'), default=timezone.now, editable=False)

    def update_last_activity(self):
        self.last_activity = timezone.now()
        self.save(update_fields=['last_activity'])

    class Meta:
        abstract = True


UserMixin = load_func(settings.USER_MIXIN)


class User(UserMixin, AbstractBaseUser, PermissionsMixin, UserLastActivityMixin):
    """
    An abstract base class implementing a fully featured User model with
    admin-compliant permissions.

    Username and password are required. Other fields are optional.
    """

    username = models.CharField(_('username'), max_length=256)
    email = models.EmailField(_('email address'), unique=True, blank=True)

    first_name = models.CharField(_('first name'), max_length=256, blank=True)
    last_name = models.CharField(_('last name'), max_length=256, blank=True)
    phone = models.CharField(_('phone'), max_length=256, blank=True)
    avatar = models.ImageField(upload_to=hash_upload, blank=True)

    is_staff = models.BooleanField(
        _('staff status'), default=False, help_text=_('Designates whether the user can log into this admin site.')
    )

    is_active = models.BooleanField(
        _('active'),
        default=True,
        help_text=_('Designates whether to treat this user as active. Unselect this instead of deleting accounts.'),
    )

    date_joined = models.DateTimeField(_('date joined'), default=timezone.now)

    activity_at = models.DateTimeField(_('last annotation activity'), auto_now=True)

    active_organization = models.ForeignKey(
        'organizations.Organization', null=True, on_delete=models.SET_NULL, related_name='active_users'
    )

    allow_newsletters = models.BooleanField(
        _('allow newsletters'), null=True, default=None, help_text=_('Allow sending newsletters to user')
    )

    objects = UserManager()

    EMAIL_FIELD = 'email'
    USERNAME_FIELD = 'email'
    REQUIRED_FIELDS = ()

    class Meta:
        db_table = 'htx_user'
        verbose_name = _('user')
        verbose_name_plural = _('users')
        indexes = [
            models.Index(fields=['username']),
            models.Index(fields=['email']),
            models.Index(fields=['first_name']),
            models.Index(fields=['last_name']),
            models.Index(fields=['date_joined']),
        ]

    @cached_property
    def avatar_url(self):
        if self.avatar:
            if settings.CLOUD_FILE_STORAGE_ENABLED:
                return self.avatar.url
            else:
                return settings.HOSTNAME + self.avatar.url

    def is_organization_admin(self, org_pk):
        return True

    def active_organization_annotations(self):
        return self.annotations.filter(project__organization=self.active_organization)

    def active_organization_contributed_project_number(self):
        annotations = self.active_organization_annotations()
        return annotations.values_list('project').distinct().count()

    @cached_property
    def own_organization(self) -> Optional[Organization]:
        return fast_first(Organization.objects.filter(created_by=self))

    @cached_property
    def has_organization(self):
        return Organization.objects.filter(created_by=self).exists()

    def clean(self):
        super().clean()
        self.email = self.__class__.objects.normalize_email(self.email)

    def name_or_email(self):
        name = self.get_full_name()
        if len(name) == 0:
            name = self.email

        return name

    def get_full_name(self):
        """
        Return the first_name and the last_name for a given user with a space in between.
        """
        full_name = '%s %s' % (self.first_name, self.last_name)
        return full_name.strip()

    def get_short_name(self):
        """Return the short name for the user."""
        return self.first_name

    def get_token(self) -> Token:
        return Token.objects.filter(user=self).first()

    def reset_token(self) -> Token:
        Token.objects.filter(user=self).delete()
        return Token.objects.create(user=self)

    def get_initials(self, is_deleted=False):
        initials = '?'

        if is_deleted:
            return 'DU'

        if not self.first_name and not self.last_name:
            initials = self.email[0:2]
        elif self.first_name and not self.last_name:
            initials = self.first_name[0:1]
        elif self.last_name and not self.first_name:
            initials = self.last_name[0:1]
        elif self.first_name and self.last_name:
            initials = self.first_name[0:1] + self.last_name[0:1]
        return initials


@receiver(post_save, sender=User)
def init_user(sender, instance=None, created=False, **kwargs):
    if created:
        # create token for user
        Token.objects.create(user=instance)
</file>

<file path="label_studio/users/serializers.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
from core.utils.common import load_func
from core.utils.db import fast_first
from django.conf import settings
from organizations.models import OrganizationMember
from rest_flex_fields import FlexFieldsModelSerializer
from rest_framework import serializers

from .models import User


class BaseUserSerializer(FlexFieldsModelSerializer):
    # short form for user presentation
    initials = serializers.SerializerMethodField(default='?', read_only=True)
    avatar = serializers.SerializerMethodField(read_only=True)
    active_organization_meta = serializers.SerializerMethodField(read_only=True)

    def get_avatar(self, instance):
        return instance.avatar_url

    def get_initials(self, instance):
        return instance.get_initials(self._is_deleted(instance))

    def get_active_organization_meta(self, instance):
        organization = instance.active_organization
        if organization is None:
            return {'title': '', 'email': ''}

        title = organization.title
        email = ''

        if organization.created_by is not None and organization.created_by.email is not None:
            email = organization.created_by.email

        return {'title': title, 'email': email}

    def _is_deleted(self, instance):
        if 'deleted_organization_members' in self.context:
            organization_members = self.context.get('deleted_organization_members', None)
            return instance.id in organization_members

        if organization_members := self.context.get('organization_members', None):
            # Finds the first organization_member matching the instance's id. If not found, set to None.
            organization_member_for_user = next(
                (
                    organization_member
                    for organization_member in organization_members
                    if organization_member.user_id == instance.id
                ),
                None,
            )
        else:
            if 'user' in self.context:
                org_id = self.context['user'].active_organization_id
            elif 'request' in self.context:
                org_id = self.context['request'].user.active_organization_id
            else:
                org_id = None

            if not org_id:
                return False

            organization_member_for_user = fast_first(
                OrganizationMember.objects.filter(user_id=instance.id, organization_id=org_id)
            )
            if not organization_member_for_user:
                return True
        return bool(organization_member_for_user.deleted_at)

    def to_representation(self, instance):
        """Returns user with cache, this helps to avoid multiple s3/gcs links resolving for avatars"""

        uid = instance.id
        key = 'user_cache'

        if key not in self.context:
            self.context[key] = {}
        if uid not in self.context[key]:
            self.context[key][uid] = super().to_representation(instance)

        if self._is_deleted(instance):
            for field in ['username', 'first_name', 'last_name', 'email']:
                self.context[key][uid][field] = 'User' if field == 'last_name' else 'Deleted'

        return self.context[key][uid]

    class Meta:
        model = User
        fields = (
            'id',
            'first_name',
            'last_name',
            'username',
            'email',
            'last_activity',
            'avatar',
            'initials',
            'phone',
            'active_organization',
            'active_organization_meta',
            'allow_newsletters',
            'date_joined',
        )


class BaseUserSerializerUpdate(BaseUserSerializer):
    class Meta(BaseUserSerializer.Meta):
        read_only_fields = ('email',)


class UserSimpleSerializer(BaseUserSerializer):
    class Meta:
        model = User
        fields = ('id', 'first_name', 'last_name', 'email', 'avatar')


UserSerializer = load_func(settings.USER_SERIALIZER)
UserSerializerUpdate = load_func(settings.USER_SERIALIZER_UPDATE)
</file>

<file path="label_studio/users/urls.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
from os.path import join

from django.conf import settings
from django.conf.urls import include
from django.urls import path, re_path
from django.views.static import serve
from rest_framework import routers
from users import api, views
from users.product_tours import api as product_tours_api

router = routers.DefaultRouter()
router.register(r'users', api.UserAPI, basename='user')

urlpatterns = [
    re_path(r'^api/', include(router.urls)),
    # Authentication
    path('user/login/', views.user_login, name='user-login'),
    path('user/signup/', views.user_signup, name='user-signup'),
    path('user/account/', views.user_account, name='user-account'),
    re_path(r'^logout/?$', views.logout, name='logout'),
    # Token
    path('api/current-user/reset-token/', api.UserResetTokenAPI.as_view(), name='current-user-reset-token'),
    path('api/current-user/token', api.UserGetTokenAPI.as_view(), name='current-user-token'),
    path('api/current-user/whoami', api.UserWhoAmIAPI.as_view(), name='current-user-whoami'),
    # Product tours
    path('api/current-user/product-tour', product_tours_api.ProductTourAPI.as_view(), name='product-tour'),
]

# When CLOUD_FILE_STORAGE_ENABLED is set, avatars are uploaded to cloud storage with a different URL pattern.
# This local serving pattern is unnecessary for environments with cloud storage enabled.
if not settings.CLOUD_FILE_STORAGE_ENABLED:
    urlpatterns += [
        # avatars
        re_path(
            r'^data/' + settings.AVATAR_PATH + '/(?P<path>.*)$',
            serve,
            kwargs={'document_root': join(settings.MEDIA_ROOT, settings.AVATAR_PATH)},
        ),
    ]
</file>

<file path="label_studio/users/views.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import logging
from urllib.parse import quote

from core.feature_flags import flag_set
from core.middleware import enforce_csrf_checks
from core.utils.common import load_func
from django.conf import settings
from django.contrib import auth
from django.contrib.auth.decorators import login_required
from django.core.exceptions import PermissionDenied
from django.shortcuts import redirect, render, reverse
from django.utils.http import url_has_allowed_host_and_scheme
from organizations.forms import OrganizationSignupForm
from organizations.models import Organization
from rest_framework.authtoken.models import Token
from users import forms
from users.functions import login, proceed_registration

logger = logging.getLogger()


@login_required
def logout(request):
    auth.logout(request)

    if settings.LOGOUT_REDIRECT_URL:
        return redirect(settings.LOGOUT_REDIRECT_URL)

    if settings.HOSTNAME:
        redirect_url = settings.HOSTNAME
        if not redirect_url.endswith('/'):
            redirect_url += '/'
        return redirect(redirect_url)
    return redirect('/')


@enforce_csrf_checks
def user_signup(request):
    """Sign up page"""
    user = request.user
    next_page = request.GET.get('next')
    token = request.GET.get('token')

    # checks if the URL is a safe redirection.
    if not next_page or not url_has_allowed_host_and_scheme(url=next_page, allowed_hosts=request.get_host()):
        if flag_set('fflag_all_feat_dia_1777_ls_homepage_short', user):
            next_page = reverse('main')
        else:
            next_page = reverse('projects:project-index')

    user_form = forms.UserSignupForm()
    organization_form = OrganizationSignupForm()

    if user.is_authenticated:
        return redirect(next_page)

    # make a new user
    if request.method == 'POST':
        organization = Organization.objects.first()
        if settings.DISABLE_SIGNUP_WITHOUT_LINK is True:
            if not (token and organization and token == organization.token):
                raise PermissionDenied()
        else:
            if token and organization and token != organization.token:
                raise PermissionDenied()

        user_form = forms.UserSignupForm(request.POST)
        organization_form = OrganizationSignupForm(request.POST)

        if user_form.is_valid():
            redirect_response = proceed_registration(request, user_form, organization_form, next_page)
            if redirect_response:
                return redirect_response

    if flag_set('fflag_feat_front_lsdv_e_297_increase_oss_to_enterprise_adoption_short'):
        return render(
            request,
            'users/new-ui/user_signup.html',
            {
                'user_form': user_form,
                'organization_form': organization_form,
                'next': quote(next_page),
                'token': token,
                'found_us_options': forms.FOUND_US_OPTIONS,
                'elaborate': forms.FOUND_US_ELABORATE,
            },
        )

    return render(
        request,
        'users/user_signup.html',
        {
            'user_form': user_form,
            'organization_form': organization_form,
            'next': quote(next_page),
            'token': token,
        },
    )


@enforce_csrf_checks
def user_login(request):
    """Login page"""
    user = request.user
    next_page = request.GET.get('next')

    # checks if the URL is a safe redirection.
    if not next_page or not url_has_allowed_host_and_scheme(url=next_page, allowed_hosts=request.get_host()):
        if flag_set('fflag_all_feat_dia_1777_ls_homepage_short', user):
            next_page = reverse('main')
        else:
            next_page = reverse('projects:project-index')

    login_form = load_func(settings.USER_LOGIN_FORM)
    form = login_form()

    if user.is_authenticated:
        return redirect(next_page)

    if request.method == 'POST':
        form = login_form(request.POST)
        if form.is_valid():
            user = form.cleaned_data['user']
            login(request, user, backend='django.contrib.auth.backends.ModelBackend')
            if form.cleaned_data['persist_session'] is not True:
                # Set the session to expire when the browser is closed
                request.session['keep_me_logged_in'] = False
                request.session.set_expiry(0)

            # user is organization member
            org_pk = Organization.find_by_user(user).pk
            user.active_organization_id = org_pk
            user.save(update_fields=['active_organization'])
            return redirect(next_page)

    if flag_set('fflag_feat_front_lsdv_e_297_increase_oss_to_enterprise_adoption_short'):
        return render(request, 'users/new-ui/user_login.html', {'form': form, 'next': quote(next_page)})

    return render(request, 'users/user_login.html', {'form': form, 'next': quote(next_page)})


@login_required
def user_account(request):
    user = request.user

    if user.active_organization is None and 'organization_pk' not in request.session:
        return redirect(reverse('main'))

    form = forms.UserProfileForm(instance=user)
    token = Token.objects.get(user=user)

    if request.method == 'POST':
        form = forms.UserProfileForm(request.POST, instance=user)
        if form.is_valid():
            form.save()
            return redirect(reverse('user-account'))

    return render(
        request,
        'users/user_account.html',
        {'settings': settings, 'user': user, 'user_profile_form': form, 'token': token},
    )
</file>

<file path="label_studio/webhooks/migrations/0001_initial.py">
# Generated by Django 3.1.12 on 2021-08-17 11:36

import core.validators
import django.db.models.deletion
from django.db import migrations, models


class Migration(migrations.Migration):

    initial = True

    dependencies = [
        ('organizations', '0002_auto_20210310_2044'),
        ('projects', '0011_auto_20210517_2101'),
    ]

    operations = [
        migrations.CreateModel(
            name='Webhook',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('url', models.URLField(help_text='URL of webhook', max_length=2048, verbose_name='URL of webhook')),
                ('send_payload', models.BooleanField(default=True, help_text='If value is False send only action', verbose_name='does webhook send the payload')),
                ('send_for_all_actions', models.BooleanField(default=True, help_text='If value is False - used only for actions from WebhookAction', verbose_name='Use webhook for all actions')),
                ('headers', models.JSONField(default=dict, help_text='Key Value Json of headers', validators=[core.validators.JSONSchemaValidator({'additionalProperties': False, 'maxProperties': 10, 'patternProperties': {'^[a-zA-Z0-9-_]+$': {'type': 'string'}}, 'type': 'object'})], verbose_name='request extra headers of webhook')),
                ('is_active', models.BooleanField(default=True, help_text='If value is False the webhook is disabled', verbose_name='is webhook active')),
                ('created_at', models.DateTimeField(auto_now_add=True, help_text='Creation time', verbose_name='created at')),
                ('updated_at', models.DateTimeField(auto_now=True, help_text='Last update time', verbose_name='updated at')),
                ('organization', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='webhooks', to='organizations.organization')),
                ('project', models.ForeignKey(default=None, null=True, on_delete=django.db.models.deletion.CASCADE, related_name='webhooks', to='projects.project')),
            ],
            options={
                'db_table': 'webhook',
            },
        ),
        migrations.CreateModel(
            name='WebhookAction',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('action', models.CharField(choices=[['PROJECT_CREATED', 'Project created'], ['PROJECT_UPDATED', 'Project updated'], ['PROJECT_DELETED', 'Project deleted'], ['TASKS_CREATED', 'Task created'], ['TASKS_DELETED', 'Task deleted'], ['ANNOTATION_CREATED', 'Annotation created'], ['ANNOTATION_UPDATED', 'Annotation updated'], ['ANNOTATIONS_DELETED', 'Annotation deleted']], db_index=True, help_text='Action value', max_length=128, verbose_name='action of webhook')),
                ('webhook', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='actions', to='webhooks.webhook')),
            ],
            options={
                'db_table': 'webhook_action',
                'unique_together': {('webhook', 'action')},
            },
        ),
    ]
</file>

<file path="label_studio/webhooks/migrations/0002_auto_20220319_0013.py">
# Generated by Django 3.1.14 on 2022-03-19 00:13

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('webhooks', '0001_initial'),
    ]

    operations = [
        migrations.AlterField(
            model_name='webhook',
            name='created_at',
            field=models.DateTimeField(auto_now_add=True, db_index=True, help_text='Creation time', verbose_name='created at'),
        ),
        migrations.AlterField(
            model_name='webhook',
            name='is_active',
            field=models.BooleanField(db_index=True, default=True, help_text='If value is False the webhook is disabled', verbose_name='is webhook active'),
        ),
        migrations.AlterField(
            model_name='webhook',
            name='send_for_all_actions',
            field=models.BooleanField(db_index=True, default=True, help_text='If value is False - used only for actions from WebhookAction', verbose_name='Use webhook for all actions'),
        ),
        migrations.AlterField(
            model_name='webhook',
            name='send_payload',
            field=models.BooleanField(db_index=True, default=True, help_text='If value is False send only action', verbose_name='does webhook send the payload'),
        ),
        migrations.AlterField(
            model_name='webhook',
            name='updated_at',
            field=models.DateTimeField(auto_now=True, db_index=True, help_text='Last update time', verbose_name='updated at'),
        ),
        migrations.AlterField(
            model_name='webhookaction',
            name='action',
            field=models.CharField(choices=[['PROJECT_CREATED', 'Project created'], ['PROJECT_UPDATED', 'Project updated'], ['PROJECT_DELETED', 'Project deleted'], ['TASKS_CREATED', 'Task created'], ['TASKS_DELETED', 'Task deleted'], ['ANNOTATION_CREATED', 'Annotation created'], ['ANNOTATION_UPDATED', 'Annotation updated'], ['ANNOTATIONS_DELETED', 'Annotation deleted'], ['LABEL_LINK_CREATED', 'Label link created'], ['LABEL_LINK_UPDATED', 'Label link updated'], ['LABEL_LINK_DELETED', 'Label link deleted']], db_index=True, help_text='Action value', max_length=128, verbose_name='action of webhook'),
        ),
    ]
</file>

<file path="label_studio/webhooks/migrations/0003_alter_webhookaction_action.py">
# Generated by Django 3.2.16 on 2022-12-13 16:12

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('webhooks', '0002_auto_20220319_0013'),
    ]

    operations = [
        migrations.AlterField(
            model_name='webhookaction',
            name='action',
            field=models.CharField(choices=[['PROJECT_CREATED', 'Project created'], ['PROJECT_UPDATED', 'Project updated'], ['PROJECT_DELETED', 'Project deleted'], ['TASKS_CREATED', 'Task created'], ['TASKS_DELETED', 'Task deleted'], ['ANNOTATION_CREATED', 'Annotation created'], ['ANNOTATIONS_CREATED', 'Annotations created'], ['ANNOTATION_UPDATED', 'Annotation updated'], ['ANNOTATIONS_DELETED', 'Annotation deleted'], ['LABEL_LINK_CREATED', 'Label link created'], ['LABEL_LINK_UPDATED', 'Label link updated'], ['LABEL_LINK_DELETED', 'Label link deleted']], db_index=True, help_text='Action value', max_length=128, verbose_name='action of webhook'),
        ),
    ]
</file>

<file path="label_studio/webhooks/migrations/0004_auto_20221221_1101.py">
# Generated by Django 3.2.16 on 2022-12-21 11:01

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('webhooks', '0003_alter_webhookaction_action'),
    ]

    operations = [
        migrations.AlterField(
            model_name='webhook',
            name='is_active',
            field=models.BooleanField(default=True, help_text='If value is False the webhook is disabled', verbose_name='is webhook active'),
        ),
        migrations.AlterField(
            model_name='webhook',
            name='send_for_all_actions',
            field=models.BooleanField(default=True, help_text='If value is False - used only for actions from WebhookAction', verbose_name='Use webhook for all actions'),
        ),
        migrations.AlterField(
            model_name='webhook',
            name='send_payload',
            field=models.BooleanField(default=True, help_text='If value is False send only action', verbose_name='does webhook send the payload'),
        ),
    ]
</file>

<file path="label_studio/webhooks/api.py">
import django_filters
from django.utils.decorators import method_decorator
from django_filters.rest_framework import DjangoFilterBackend
from drf_yasg import openapi
from drf_yasg.utils import swagger_auto_schema
from projects import models as project_models
from rest_framework import generics
from rest_framework.permissions import AllowAny, IsAuthenticated
from rest_framework.response import Response
from rest_framework.views import APIView

from .models import Webhook, WebhookAction
from .serializers import WebhookSerializer, WebhookSerializerForUpdate


class WebhookFilterSet(django_filters.FilterSet):
    project = django_filters.ModelChoiceFilter(
        field_name='project', queryset=project_models.Project.objects.all(), null_label='isnull'
    )


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Webhooks'],
        x_fern_sdk_group_name='webhooks',
        x_fern_sdk_method_name='list',
        x_fern_audiences=['public'],
        operation_summary='List all webhooks',
        operation_description='List all webhooks set up for your organization.',
        manual_parameters=[
            openapi.Parameter(
                name='project',
                type=openapi.TYPE_STRING,
                in_=openapi.IN_QUERY,
                description='Project ID',
            ),
        ],
    ),
)
@method_decorator(
    name='post',
    decorator=swagger_auto_schema(
        tags=['Webhooks'],
        x_fern_sdk_group_name='webhooks',
        x_fern_sdk_method_name='create',
        x_fern_audiences=['public'],
        operation_summary='Create a webhook',
        operation_description='Create a webhook for your organization.',
    ),
)
class WebhookListAPI(generics.ListCreateAPIView):
    queryset = Webhook.objects.all()
    serializer_class = WebhookSerializer
    permission_classes = [IsAuthenticated]
    filter_backends = [DjangoFilterBackend]
    filterset_class = WebhookFilterSet

    def get_queryset(self):
        return Webhook.objects.filter(organization=self.request.user.active_organization)

    def perform_create(self, serializer):
        serializer.save(organization=self.request.user.active_organization)


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Webhooks'],
        x_fern_sdk_group_name='webhooks',
        x_fern_sdk_method_name='get',
        x_fern_audiences=['public'],
        operation_summary='Get webhook info',
    ),
)
@method_decorator(
    name='put',
    decorator=swagger_auto_schema(
        x_fern_audiences=['internal'],
        tags=['Webhooks'],
        operation_summary='Save webhook info',
        query_serializer=WebhookSerializerForUpdate,
    ),
)
@method_decorator(
    name='patch',
    decorator=swagger_auto_schema(
        tags=['Webhooks'],
        x_fern_sdk_group_name='webhooks',
        x_fern_sdk_method_name='update',
        x_fern_audiences=['public'],
        operation_summary='Update webhook info',
        query_serializer=WebhookSerializerForUpdate,
    ),
)
@method_decorator(
    name='delete',
    decorator=swagger_auto_schema(
        tags=['Webhooks'],
        x_fern_sdk_group_name='webhooks',
        x_fern_sdk_method_name='delete',
        x_fern_audiences=['public'],
        operation_summary='Delete webhook info',
    ),
)
class WebhookAPI(generics.RetrieveUpdateDestroyAPIView):
    queryset = Webhook.objects.all()
    serializer_class = WebhookSerializer
    permission_classes = [IsAuthenticated]

    def get_serializer_class(self):
        if self.request.method in ['PUT', 'PATCH']:
            return WebhookSerializerForUpdate
        return super().get_serializer_class()

    def get_queryset(self):
        return Webhook.objects.filter(organization=self.request.user.active_organization)


@method_decorator(
    name='get',
    decorator=swagger_auto_schema(
        tags=['Webhooks'],
        x_fern_sdk_group_name='webhooks',
        x_fern_sdk_method_name='info',
        x_fern_audiences=['public'],
        operation_summary='Get all webhook actions',
        operation_description='Get descriptions of all available webhook actions to set up webhooks.',
        responses={'200': 'Object with description data.'},
        manual_parameters=[
            openapi.Parameter(
                'organization-only',
                openapi.IN_QUERY,
                description='organization-only or not',
                type=openapi.TYPE_BOOLEAN,
            )
        ],
    ),
)
class WebhookInfoAPI(APIView):
    permission_classes = [AllowAny]

    def get(self, request, *args, **kwargs):
        result = {
            key: {
                'name': value['name'],
                'description': value['description'],
                'key': value['key'],
                'organization-only': value.get('organization-only', False),
            }
            for key, value in WebhookAction.ACTIONS.items()
        }
        organization_only = request.query_params.get('organization-only')
        if organization_only is not None:
            organization_only = organization_only == 'true'
            result = {
                key: value
                for key, value in result.items()
                if value.get('organization-only', False) == organization_only
            }
        return Response(data=result)
</file>

<file path="label_studio/webhooks/apps.py">
from django.apps import AppConfig


class WebhooksConfig(AppConfig):
    name = 'webhooks'
</file>

<file path="label_studio/webhooks/models.py">
from core.utils.common import load_func
from core.validators import JSONSchemaValidator
from django.conf import settings
from django.core.exceptions import ValidationError
from django.db import models
from django.utils.translation import gettext_lazy as _
from labels_manager.models import LabelLink
from projects.models import Project
from tasks.models import Annotation, Task

# from labels_manager.serializers import LabelLinkSerializer, LabelSerializer
from .serializers_for_hooks import (
    OnlyIDWebhookSerializer,
)

HEADERS_SCHEMA = {
    'type': 'object',
    'patternProperties': {
        '^[a-zA-Z0-9-_]+$': {'type': 'string'},
    },
    'maxProperties': 10,
    'additionalProperties': False,
}


class Webhook(models.Model):
    """Model of webhooks.

    If webhook has not null project field -- it's project webhook
    """

    organization = models.ForeignKey('organizations.Organization', on_delete=models.CASCADE, related_name='webhooks')

    project = models.ForeignKey(
        'projects.Project', null=True, on_delete=models.CASCADE, related_name='webhooks', default=None
    )

    url = models.URLField(_('URL of webhook'), max_length=2048, help_text=_('URL of webhook'))

    send_payload = models.BooleanField(
        _('does webhook send the payload'),
        default=True,
        help_text=('If value is False send only action'),
    )

    send_for_all_actions = models.BooleanField(
        _('Use webhook for all actions'),
        default=True,
        help_text='If value is False - used only for actions from WebhookAction',
    )

    headers = models.JSONField(
        _('request extra headers of webhook'),
        validators=[JSONSchemaValidator(HEADERS_SCHEMA)],
        default=dict,
        help_text='Key Value Json of headers',
    )

    is_active = models.BooleanField(
        _('is webhook active'),
        default=True,
        help_text=('If value is False the webhook is disabled'),
    )

    created_at = models.DateTimeField(_('created at'), auto_now_add=True, help_text=_('Creation time'), db_index=True)
    updated_at = models.DateTimeField(_('updated at'), auto_now=True, help_text=_('Last update time'), db_index=True)

    def get_actions(self):
        return WebhookAction.objects.filter(webhook=self).values_list('action', flat=True)

    def validate_actions(self, actions):
        actions_meta = [WebhookAction.ACTIONS[action] for action in actions]
        if self.project and any((meta.get('organization-only') for meta in actions_meta)):
            raise ValidationError("Project webhook can't contain organization-only action.")
        return actions

    def set_actions(self, actions):
        if not actions:
            actions = set()
        actions = set(actions)
        old_actions = set(self.get_actions())

        for new_action in list(actions - old_actions):
            WebhookAction.objects.create(webhook=self, action=new_action)

        WebhookAction.objects.filter(webhook=self, action__in=(old_actions - actions)).delete()

    def has_permission(self, user):
        user.project = self.project  # link for activity log
        return self.organization.has_user(user)

    class Meta:
        db_table = 'webhook'


class WebhookAction(models.Model):
    PROJECT_CREATED = 'PROJECT_CREATED'
    PROJECT_UPDATED = 'PROJECT_UPDATED'
    PROJECT_DELETED = 'PROJECT_DELETED'

    TASKS_CREATED = 'TASKS_CREATED'
    TASKS_DELETED = 'TASKS_DELETED'

    ANNOTATION_CREATED = 'ANNOTATION_CREATED'
    ANNOTATIONS_CREATED = 'ANNOTATIONS_CREATED'
    ANNOTATION_UPDATED = 'ANNOTATION_UPDATED'
    ANNOTATIONS_DELETED = 'ANNOTATIONS_DELETED'

    LABEL_LINK_CREATED = 'LABEL_LINK_CREATED'
    LABEL_LINK_UPDATED = 'LABEL_LINK_UPDATED'
    LABEL_LINK_DELETED = 'LABEL_LINK_DELETED'

    ACTIONS = {
        PROJECT_CREATED: {
            'name': _('Project created'),
            'description': _(''),
            'key': 'project',
            'many': False,
            'model': Project,
            'serializer': load_func(settings.WEBHOOK_SERIALIZERS['project']),
            'organization-only': True,
        },
        PROJECT_UPDATED: {
            'name': _('Project updated'),
            'description': _(''),
            'key': 'project',
            'many': False,
            'model': Project,
            'serializer': load_func(settings.WEBHOOK_SERIALIZERS['project']),
            'project-field': '__self__',
        },
        PROJECT_DELETED: {
            'name': _('Project deleted'),
            'description': _(''),
            'key': 'project',
            'many': False,
            'model': Project,
            'serializer': OnlyIDWebhookSerializer,
            'organization-only': True,
        },
        TASKS_CREATED: {
            'name': _('Task created'),
            'description': _(''),
            'key': 'tasks',
            'many': True,
            'model': Task,
            'serializer': load_func(settings.WEBHOOK_SERIALIZERS['task']),
            'project-field': 'project',
        },
        TASKS_DELETED: {
            'name': _('Task deleted'),
            'description': _(''),
            'key': 'tasks',
            'many': True,
            'model': Task,
            'serializer': OnlyIDWebhookSerializer,
            'project-field': 'project',
        },
        ANNOTATION_CREATED: {
            'name': _('Annotation created'),
            'description': _(''),
            'key': 'annotation',
            'many': False,
            'model': Annotation,
            'serializer': load_func(settings.WEBHOOK_SERIALIZERS['annotation']),
            'project-field': 'project',
            'nested-fields': {
                'task': {
                    'serializer': load_func(settings.WEBHOOK_SERIALIZERS['task']),
                    'many': False,
                    'field': 'task',
                },
            },
        },
        ANNOTATIONS_CREATED: {
            'name': _('Annotations created'),
            'description': _(''),
            'key': 'annotation',
            'many': True,
            'model': Annotation,
            'serializer': load_func(settings.WEBHOOK_SERIALIZERS['annotation']),
            'project-field': 'project',
            'nested-fields': {
                'task': {
                    'serializer': load_func(settings.WEBHOOK_SERIALIZERS['task']),
                    'many': True,
                    'field': 'task',
                },
            },
        },
        ANNOTATION_UPDATED: {
            'name': _('Annotation updated'),
            'description': _(''),
            'key': 'annotation',
            'many': False,
            'model': Annotation,
            'serializer': load_func(settings.WEBHOOK_SERIALIZERS['annotation']),
            'project-field': 'project',
            'nested-fields': {
                'task': {
                    'serializer': load_func(settings.WEBHOOK_SERIALIZERS['task']),
                    'many': False,
                    'field': 'task',
                },
            },
        },
        ANNOTATIONS_DELETED: {
            'name': _('Annotation deleted'),
            'description': _(''),
            'key': 'annotations',
            'many': True,
            'model': Annotation,
            'serializer': OnlyIDWebhookSerializer,
            'project-field': 'project',
        },
        LABEL_LINK_CREATED: {
            'name': _('Label link created'),
            'description': _(''),
            'key': 'label_link',
            'many': True,
            'model': LabelLink,
            'serializer': load_func(settings.WEBHOOK_SERIALIZERS['label_link']),
            'project-field': 'project',
        },
        LABEL_LINK_UPDATED: {
            'name': _('Label link updated'),
            'description': _(''),
            'key': 'label_link',
            'many': False,
            'model': LabelLink,
            'serializer': load_func(settings.WEBHOOK_SERIALIZERS['label_link']),
            'project-field': 'project',
            'nested-fields': {
                'label': {
                    'many': False,
                    'field': 'label',
                    'serializer': load_func(settings.WEBHOOK_SERIALIZERS['label']),
                },
            },
        },
        LABEL_LINK_DELETED: {
            'name': _('Label link deleted'),
            'description': _(''),
            'key': 'label_link',
            'many': False,
            'model': LabelLink,
            'serializer': OnlyIDWebhookSerializer,
            'project-field': 'project',
        },
    }

    webhook = models.ForeignKey(Webhook, on_delete=models.CASCADE, related_name='actions')

    action = models.CharField(
        _('action of webhook'),
        choices=[[key, value['name']] for key, value in ACTIONS.items()],
        max_length=128,
        db_index=True,
        help_text=_('Action value'),
    )

    class Meta:
        db_table = 'webhook_action'
        unique_together = [['webhook', 'action']]
</file>

<file path="label_studio/webhooks/serializers_for_hooks.py">
from core.label_config import replace_task_data_undefined_with_config_field
from projects.models import Project
from rest_framework import serializers
from tasks.models import Annotation, Task


class OnlyIDWebhookSerializer(serializers.Serializer):
    id = serializers.IntegerField()

    class Meta:
        fields: ('id',)


class ProjectWebhookSerializer(serializers.ModelSerializer):

    task_number = serializers.IntegerField(read_only=True)
    finished_task_number = serializers.IntegerField(read_only=True)
    total_predictions_number = serializers.IntegerField(read_only=True)
    total_annotations_number = serializers.IntegerField(read_only=True)
    num_tasks_with_annotations = serializers.IntegerField(read_only=True)
    useful_annotation_number = serializers.IntegerField(read_only=True)
    ground_truth_number = serializers.IntegerField(read_only=True)
    skipped_annotations_number = serializers.IntegerField(read_only=True)

    def to_representation(self, instance):
        instance = Project.objects.with_counts().filter(id=instance.id)[0]
        return super().to_representation(instance)

    class Meta:
        model = Project
        fields = '__all__'


class TaskWebhookSerializer(serializers.ModelSerializer):
    # resolve $undefined$ key in task data, if any
    def to_representation(self, task):
        project = task.project
        data = task.data

        replace_task_data_undefined_with_config_field(data, project)
        return super().to_representation(task)

    class Meta:
        model = Task
        fields = '__all__'


class AnnotationWebhookSerializer(serializers.ModelSerializer):
    class Meta:
        model = Annotation
        fields = '__all__'
</file>

<file path="label_studio/webhooks/serializers.py">
from rest_framework import serializers

from .models import Webhook, WebhookAction


class WebhookSerializer(serializers.ModelSerializer):

    actions = serializers.ListField(
        child=serializers.ChoiceField(choices=WebhookAction.ACTIONS), default=[], source='_actions'
    )

    class Meta:
        model = Webhook
        fields = (
            'id',
            'organization',
            'project',
            'url',
            'send_payload',
            'send_for_all_actions',
            'headers',
            'is_active',
            'actions',
            'created_at',
            'updated_at',
        )
        read_only_fields = ('id', 'organization', 'created_at', 'updated_at')

    def validate(self, attrs):
        actions = attrs.pop('_actions', [])
        instance = Webhook(**attrs)
        instance.validate_actions(actions)
        attrs['_actions'] = actions
        return attrs

    def create(self, validated_data):
        actions = validated_data.pop('_actions', [])
        instance = Webhook.objects.create(**validated_data)
        instance.set_actions(actions)
        return instance

    def update(self, instance, validated_data):
        actions = validated_data.pop('_actions', [])
        instance = super().update(instance, validated_data)
        instance.set_actions(actions)
        return instance

    def to_representation(self, instance):
        instance._actions = instance.get_actions()
        return super().to_representation(instance)


class WebhookSerializerForUpdate(WebhookSerializer):
    """Serializer class for updating webhooks

    Used to forbid updating project field."""

    class Meta(WebhookSerializer.Meta):
        read_only_fields = WebhookSerializer.Meta.read_only_fields + ('project',)
</file>

<file path="label_studio/webhooks/urls.py">
from django.urls import include, path

from . import api

app_name = 'webhooks'

_api_urlpatterns = [
    # CRUD
    path('', api.WebhookListAPI.as_view(), name='webhook-list'),
    path('<int:pk>/', api.WebhookAPI.as_view(), name='webhook-detail'),
    path('info/', api.WebhookInfoAPI.as_view(), name='webhook-info'),
]

urlpatterns = [
    path('api/webhooks/', include((_api_urlpatterns, app_name), namespace='api')),
]
</file>

<file path="label_studio/webhooks/utils.py">
import logging
from functools import wraps

import requests
from core.feature_flags import flag_set
from core.redis import start_job_async_or_sync
from core.utils.common import load_func
from django.conf import settings
from django.db.models import Q

from .models import Webhook, WebhookAction


def get_active_webhooks(organization, project, action):
    """Return all active webhooks for organization or project by action.

    If project is None - function return only organization hooks
    else project is not None - function return project and organization hooks
    Organization hooks are global hooks.
    """
    action_meta = WebhookAction.ACTIONS[action]
    if project and action_meta.get('organization-only'):
        raise ValueError('There is no project webhooks for organization-only action')

    return Webhook.objects.filter(
        Q(organization=organization)
        & (Q(project=project) | Q(project=None))
        & Q(is_active=True)
        & (
            Q(send_for_all_actions=True)
            | Q(
                id__in=WebhookAction.objects.filter(webhook__organization=organization, action=action).values_list(
                    'webhook_id', flat=True
                )
            )
        )
    ).distinct()


def run_webhook_sync(webhook, action, payload=None):
    """Run one webhook for action.

    This function must not raise any exceptions.
    """
    data = {
        'action': action,
    }
    if webhook.send_payload and payload:
        data.update(payload)
    try:
        logging.debug('Run webhook %s for action %s', webhook.id, action)
        return requests.post(
            webhook.url,
            headers=webhook.headers,
            json=data,
            timeout=settings.WEBHOOK_TIMEOUT,
        )
    except requests.RequestException as exc:
        logging.error(exc, exc_info=True)
        return


def emit_webhooks_sync(organization, project, action, payload):
    """
    Run all active webhooks for the action.
    """
    webhooks = get_active_webhooks(organization, project, action)
    if project and payload and webhooks.filter(send_payload=True).exists():
        payload['project'] = load_func(settings.WEBHOOK_SERIALIZERS['project'])(instance=project).data
    for wh in webhooks:
        run_webhook_sync(wh, action, payload)


def emit_webhooks_for_instance_sync(organization, project, action, instance=None):
    """Run all active webhooks for the action using instances as payload.

    Be sure WebhookAction.ACTIONS contains all required fields.
    """
    webhooks = get_active_webhooks(organization, project, action)
    if not webhooks.exists():
        return
    payload = {}
    # if instances and there is a webhook that sends payload
    # get serialized payload
    action_meta = WebhookAction.ACTIONS[action]
    if instance and webhooks.filter(send_payload=True).exists():
        serializer_class = action_meta.get('serializer')
        if serializer_class:
            payload[action_meta['key']] = serializer_class(instance=instance, many=action_meta['many']).data
        if project and payload:
            payload['project'] = load_func(settings.WEBHOOK_SERIALIZERS['project'])(instance=project).data
        if payload and 'nested-fields' in action_meta:
            for key, value in action_meta['nested-fields'].items():
                payload[key] = value['serializer'](
                    instance=get_nested_field(instance, value['field']), many=value['many']
                ).data
    for wh in webhooks:
        run_webhook_sync(wh, action, payload)


def run_webhook(webhook, action, payload=None):
    """Run one webhook for action.

    This function must not raise any exceptions.

    Will run a webhook in an RQ worker.
    """
    if flag_set('fflag_fix_back_lsdv_4604_excess_sql_queries_in_api_short'):
        start_job_async_or_sync(
            run_webhook_sync,
            webhook,
            action,
            payload,
            queue_name='high',
        )
    else:
        run_webhook_sync(webhook, action, payload)


def emit_webhooks_for_instance(organization, project, action, instance=None):
    """Run all active webhooks for the action using instances as payload.

    Be sure WebhookAction.ACTIONS contains all required fields.

    Will run all selected webhooks in an RQ worker.
    """
    if flag_set('fflag_fix_back_lsdv_4604_excess_sql_queries_in_api_short'):
        start_job_async_or_sync(emit_webhooks_for_instance_sync, organization, project, action, instance)
    else:
        emit_webhooks_for_instance_sync(organization, project, action, instance)


def emit_webhooks(organization, project, action, payload):
    """
    Run all active webhooks for the action.

    Will run all selected webhooks in an RQ worker.
    """
    if flag_set('fflag_fix_back_lsdv_4604_excess_sql_queries_in_api_short'):
        start_job_async_or_sync(emit_webhooks_sync, organization, project, action, payload)
    else:
        emit_webhooks_sync(organization, project, action, payload)


def api_webhook(action):
    """Decorator emit webhooks for APIView methods: post, put, patch.

    Used for simple Create/Update methods.
    The decorator expects authorized request and response with 'id' key in data.

    Example:
        ```
        @api_webhook(WebhookAction.PROJECT_UPDATED)
        def put(self, request, *args, **kwargs):
            return super(ProjectAPI, self).put(request, *args, **kwargs)
        ```
    """

    def decorator(func):
        @wraps(func)
        def wrap(self, request, *args, **kwargs):
            response = func(self, request, *args, **kwargs)

            action_meta = WebhookAction.ACTIONS[action]
            many = action_meta['many']
            instance = action_meta['model'].objects.get(id=response.data.get('id'))
            if many:
                instance = [instance]
            project = None
            if 'project-field' in action_meta:
                project = get_nested_field(instance, action_meta['project-field'])
            emit_webhooks_for_instance(
                request.user.active_organization,
                project,
                action,
                instance,
            )
            return response

        return wrap

    return decorator


def api_webhook_for_delete(action):
    """Decorator emit webhooks for APIView delete method.

    The decorator expects authorized request and use get_object() method
    before delete.

    Example:
        ```
        @swagger_auto_schema(tags=['Annotations'])
        @api_webhook_for_delete(WebhookAction.ANNOTATIONS_DELETED)
        def delete(self, request, *args, **kwargs):
            return super(AnnotationAPI, self).delete(request, *args, **kwargs)
        ```
    """

    def decorator(func):
        @wraps(func)
        def wrap(self, request, *args, **kwargs):
            instance = self.get_object()
            action_meta = WebhookAction.ACTIONS[action]
            many = action_meta['many']
            project = None
            if 'project-field' in action_meta:
                project = get_nested_field(instance, action_meta['project-field'])

            obj = {'id': instance.pk}
            if many:
                obj = [obj]

            response = func(self, request, *args, **kwargs)

            emit_webhooks_for_instance(request.user.active_organization, project, action, obj)
            return response

        return wrap

    return decorator


def get_nested_field(value, field):
    """
    Get nested field from list of objects or single instance
    :param value: Single instance or list to look up field
    :param field: Field to lookup
    :return: List or single instance of looked up field
    """
    if field == '__self__':
        return value
    fields = field.split('__')
    for fld in fields:
        if isinstance(value, list):
            value = [getattr(v, fld) for v in value]
        else:
            value = getattr(value, fld)
    return value
</file>

<file path="label_studio/__init__.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import importlib.metadata

# Package name
package_name = 'label-studio'

# Package version
__version__ = importlib.metadata.metadata(package_name).get('version')

# pypi info
__latest_version__ = None
__current_version_is_outdated__ = False
__latest_version_upload_time__ = None
__latest_version_check_time__ = None
</file>

<file path="label_studio/constants.py">
SAFE_HTML_ATTRIBUTES = [
    'align',
    'alink',
    'alt',
    'bgcolor',
    'border',
    'cellpadding',
    'cellspacing',
    'class',
    'color',
    'cols',
    'colspan',
    'coords',
    'dir',
    'face',
    'href',
    'height',
    'hspace',
    'ismap',
    'lang',
    'marginheight',
    'marginwidth',
    'multiple',
    'name',
    'nohref',
    'noresize',
    'noshade',
    'nowrap',
    'ref',
    'rel',
    'rev',
    'rows',
    'rowspan',
    'scrolling',
    'shape',
    'span',
    'src',
    'summary',
    'tabindex',
    'title',
    'type',
    'usemap',
    'valign',
    'value',
    'vlink',
    'vspace',
    'width',
]

SAFE_HTML_TAGS = [
    'html',
    'base',
    'head',
    'link',
    'meta',
    'style',
    'title',
    'body',
    'address',
    'article',
    'aside',
    'footer',
    'header',
    'h1',
    'h2',
    'h3',
    'h4',
    'h5',
    'h6',
    'main',
    'nav',
    'section',
    'blockquote',
    'dd',
    'div',
    'dl',
    'dt',
    'figcaption',
    'figure',
    'hr',
    'li',
    'menu',
    'ol',
    'p',
    'pre',
    'ul',
    'a',
    'abbr',
    'b',
    'bdi',
    'bdo',
    'br',
    'cite',
    'code',
    'data',
    'dfn',
    'em',
    'i',
    'kbd',
    'mark',
    'q',
    'rp',
    'rt',
    'ruby',
    's',
    'samp',
    'small',
    'span',
    'strong',
    'sub',
    'sup',
    'time',
    'u',
    'var',
    'wbr',
    'area',
    'audio',
    'img',
    'map',
    'track',
    'video',
    'embed',
    'iframe',
    'object',
    'picture',
    'portal',
    'source',
    'svg',
    'math',
    'canvas',
    'noscript',
    'script',
    'del',
    'ins',
    'caption',
    'col',
    'colgroup',
    'table',
    'tbody',
    'td',
    'tfoot',
    'th',
    'thead',
    'tr',
    'button',
    'datalist',
    'fieldset',
    'form',
    'input',
    'label',
    'legend',
    'meter',
    'optgroup',
    'option',
    'output',
    'progress',
    'select',
    'textarea',
    'details',
    'dialog',
    'summary',
    'slot',
    'template',
]
</file>

<file path="label_studio/manage.py">
#!/usr/bin/env python
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import os
import sys

if __name__ == '__main__':
    os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'core.settings.label_studio')
    # os.environ.setdefault('DEBUG', 'True')
    try:
        from django.conf import settings
        from django.core.management import execute_from_command_line
        from django.core.management.commands.runserver import Command as runserver

        runserver.default_port = settings.INTERNAL_PORT

    except ImportError as exc:
        raise ImportError(
            "Couldn't import Django. Are you sure it's installed and "
            'available on your PYTHONPATH environment variable? Did you '
            'forget to activate a virtual environment?'
        ) from exc
    execute_from_command_line(sys.argv)
</file>

<file path="label_studio/server.py">
"""This file and its contents are licensed under the Apache License 2.0. Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
"""
import getpass
import io
import json
import logging
import os
import pathlib
import socket
import sys

from colorama import Fore, init

if sys.platform == 'win32':
    init(convert=True)

from django.core.management import call_command
from django.core.wsgi import get_wsgi_application
from django.db import DEFAULT_DB_ALIAS, IntegrityError, connections
from django.db.backends.signals import connection_created
from django.db.migrations.executor import MigrationExecutor

from label_studio.core.argparser import parse_input_args
from label_studio.core.utils.params import get_env

logger = logging.getLogger(__name__)

LS_PATH = str(pathlib.Path(__file__).parent.absolute())
DEFAULT_USERNAME = 'default_user@localhost'


def _setup_env():
    sys.path.insert(0, LS_PATH)
    os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'label_studio.core.settings.label_studio')
    get_wsgi_application()


def _app_run(host, port):
    http_socket = '{}:{}'.format(host, port)
    call_command('runserver', '--noreload', http_socket)


def _set_sqlite_fix_pragma(sender, connection, **kwargs):
    """Enable integrity constraint with sqlite."""
    if connection.vendor == 'sqlite' and get_env('AZURE_MOUNT_FIX'):
        cursor = connection.cursor()
        cursor.execute('PRAGMA journal_mode=wal;')


def is_database_synchronized(database):
    connection = connections[database]
    connection.prepare_database()
    executor = MigrationExecutor(connection)
    targets = executor.loader.graph.leaf_nodes()
    return not executor.migration_plan(targets)


def _apply_database_migrations():
    connection_created.connect(_set_sqlite_fix_pragma)
    if not is_database_synchronized(DEFAULT_DB_ALIAS):
        print('Initializing database..')
        call_command('migrate', '--no-color', verbosity=0)


def _get_config(config_path):
    with io.open(os.path.abspath(config_path), encoding='utf-8') as c:
        config = json.load(c)
    return config


def _create_project(title, user, label_config=None, sampling=None, description=None, ml_backends=None):
    from organizations.models import Organization
    from projects.models import Project

    project = Project.objects.filter(title=title).first()
    if project is not None:
        print('Project with title "{}" already exists'.format(title))
    else:
        org = Organization.objects.first()
        org.add_user(user)
        project = Project.objects.create(title=title, created_by=user, organization=org)
        print('Project with title "{}" successfully created'.format(title))

    if label_config is not None:
        with open(os.path.abspath(label_config)) as c:
            project.label_config = c.read()

    if sampling is not None:
        project.sampling = sampling

    if description is not None:
        project.description = description

    if ml_backends is not None:
        from ml.models import MLBackend

        # e.g.: localhost:8080,localhost:8081;localhost:8082
        for url in ml_backends:
            logger.info('Adding new ML backend %s', url)
            MLBackend.objects.create(project=project, url=url)

    project.save()
    return project


def _get_user_info(username):
    from users.models import User
    from users.serializers import UserSerializer

    if not username:
        username = DEFAULT_USERNAME

    user = User.objects.filter(email=username)
    if not user.exists():
        print({'status': 'error', 'message': f"user {username} doesn't exist"})
        return

    user = user.first()
    user_data = UserSerializer(user).data
    user_data['token'] = user.auth_token.key
    user_data['status'] = 'ok'
    print('=> User info:')
    print(user_data)
    return user_data


def _create_user(input_args, config):
    from organizations.models import Organization
    from users.models import User

    username = input_args.username or config.get('username') or get_env('USERNAME')
    password = input_args.password or config.get('password') or get_env('PASSWORD')
    token = input_args.user_token or config.get('user_token') or get_env('USER_TOKEN')

    if not username:
        user = User.objects.filter(email=DEFAULT_USERNAME).first()
        if user is not None:
            if password and not user.check_password(password):
                user.set_password(password)
                user.save()
                print(f'User {DEFAULT_USERNAME} password changed')
            return user

        if input_args.quiet_mode:
            return None

        print(f'Please enter default user email, or press Enter to use {DEFAULT_USERNAME}')
        username = input('Email: ')
        if not username:
            username = DEFAULT_USERNAME

    if not password and not input_args.quiet_mode:
        password = getpass.getpass(f'User password for {username}: ')

    try:
        user = User.objects.create_user(email=username, password=password)
        user.is_staff = True
        user.is_superuser = True
        user.save()

        if token and len(token) > 5:
            from rest_framework.authtoken.models import Token

            Token.objects.filter(key=user.auth_token.key).update(key=token)
        elif token:
            print(f'Token {token} is not applied to user {DEFAULT_USERNAME} ' f"because it's empty or len(token) < 5")

    except IntegrityError:
        print('User {} already exists'.format(username))

    user = User.objects.get(email=username)
    org = Organization.objects.first()
    if not org:
        org = Organization.create_organization(created_by=user, title='Label Studio')
    else:
        org.add_user(user)
    user.active_organization = org
    user.save(update_fields=['active_organization'])

    return user


def _init(input_args, config):
    user = _create_user(input_args, config)

    if user and input_args.project_name and not _project_exists(input_args.project_name):
        from projects.models import Project

        sampling_map = {
            'sequential': Project.SEQUENCE,
            'uniform': Project.UNIFORM,
            'prediction-score-min': Project.UNCERTAINTY,
        }
        _create_project(
            title=input_args.project_name,
            user=user,
            label_config=input_args.label_config,
            description=input_args.project_desc,
            sampling=sampling_map.get(input_args.sampling, 'sequential'),
            ml_backends=input_args.ml_backends,
        )
    elif input_args.project_name:
        print('Project "{0}" already exists'.format(input_args.project_name))


def _reset_password(input_args):
    from users.models import User

    username = input_args.username
    if not username:
        username = input('Username: ')

    user = User.objects.filter(email=username).first()
    if user is None:
        print('User with username {} not found'.format(username))
        return

    password = input_args.password
    if not password:
        password = getpass.getpass('New password:')

    if not password:
        print('Can not set empty password')
        return

    if user.check_password(password):
        print('Entered password is the same as current')
        return

    user.set_password(password)
    user.save()
    print('Password successfully changed')


def check_port_in_use(host, port):
    logger.info('Checking if host & port is available :: ' + str(host) + ':' + str(port))
    host = host.replace('https://', '').replace('http://', '')
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        return s.connect_ex((host, port)) == 0


def _get_free_port(port, debug):
    # check port is busy
    if not debug:
        original_port = port
        # try up to 1000 new ports
        while check_port_in_use('localhost', port):
            old_port = port
            port = int(port) + 1
            if port - original_port >= 1000:
                raise ConnectionError(
                    '\n*** WARNING! ***\n Could not find an available port\n'
                    + ' to launch label studio. \n Last tested port was '
                    + str(port)
                    + '\n****************\n'
                )
            print(
                '\n*** WARNING! ***\n* Port '
                + str(old_port)
                + ' is in use.\n'
                + '* Trying to start at '
                + str(port)
                + '\n****************\n'
            )
    return port


def _project_exists(project_name):
    from projects.models import Project

    return Project.objects.filter(title=project_name).exists()


def main():
    input_args = parse_input_args(sys.argv[1:])

    # setup logging level
    if input_args.log_level:
        os.environ.setdefault('LOG_LEVEL', input_args.log_level)

    if input_args.database:
        database_path = pathlib.Path(input_args.database)
        os.environ.setdefault('DATABASE_NAME', str(database_path.absolute()))

    if input_args.data_dir:
        data_dir_path = pathlib.Path(input_args.data_dir)
        os.environ.setdefault('LABEL_STUDIO_BASE_DATA_DIR', str(data_dir_path.absolute()))

    config = _get_config(input_args.config_path)

    # set host name
    host = input_args.host or config.get('host', '')
    if not get_env('HOST'):
        os.environ.setdefault('HOST', host)  # it will be passed to settings.HOSTNAME as env var

    _setup_env()
    _apply_database_migrations()

    from label_studio.core.utils.common import collect_versions

    versions = collect_versions()

    if input_args.command == 'reset_password':
        _reset_password(input_args)
        return

    if input_args.command == 'shell':
        call_command('shell_plus')
        return

    if input_args.command == 'calculate_stats_all_orgs':
        from tasks.functions import calculate_stats_all_orgs

        calculate_stats_all_orgs(input_args.from_scratch, redis=True)
        return

    if input_args.command == 'export':
        from tasks.functions import export_project

        try:
            filename = export_project(
                input_args.project_id,
                input_args.export_format,
                input_args.export_path,
                serializer_context=input_args.export_serializer_context,
            )
        except Exception as e:
            logger.exception(f'Failed to export project: {e}')
        else:
            logger.info(f'Project exported successfully: {filename}')

        return

    # print version
    if input_args.command == 'version' or input_args.version:
        from label_studio import __version__

        print('\nLabel Studio version:', __version__, '\n')
        print(json.dumps(versions, indent=4))

    # init
    elif input_args.command == 'user' or getattr(input_args, 'user', None):
        _get_user_info(input_args.username)
        return

    # init
    elif input_args.command == 'init' or getattr(input_args, 'init', None):
        _init(input_args, config)

        print('')
        print('Label Studio has been successfully initialized.')
        if input_args.command != 'start' and input_args.project_name:
            print('Start the server: label-studio start ' + input_args.project_name)
            return

    # start with migrations from old projects, '.' project_name means 'label-studio start' without project name
    elif input_args.command == 'start' and input_args.project_name != '.':
        from projects.models import Project

        from label_studio.core.old_ls_migration import migrate_existing_project

        sampling_map = {
            'sequential': Project.SEQUENCE,
            'uniform': Project.UNIFORM,
            'prediction-score-min': Project.UNCERTAINTY,
        }

        if input_args.project_name and not _project_exists(input_args.project_name):
            migrated = False
            project_path = pathlib.Path(input_args.project_name)
            if project_path.exists():
                print('Project directory from previous version of label-studio found')
                print('Start migrating..')
                config_path = project_path / 'config.json'
                config = _get_config(config_path)
                user = _create_user(input_args, config)
                label_config_path = project_path / 'config.xml'
                project = _create_project(
                    title=input_args.project_name,
                    user=user,
                    label_config=label_config_path,
                    sampling=sampling_map.get(config.get('sampling', 'sequential'), Project.UNIFORM),
                    description=config.get('description', ''),
                )
                migrate_existing_project(project_path, project, config)
                migrated = True

                print(
                    Fore.LIGHTYELLOW_EX
                    + '\n*** WARNING! ***\n'
                    + f'Project {input_args.project_name} migrated to Label Studio Database\n'
                    + "YOU DON'T NEED THIS FOLDER ANYMORE"
                    + '\n****************\n'
                    + Fore.WHITE
                )
            if not migrated:
                print(
                    'Project "{project_name}" not found. '
                    'Did you miss create it first with `label-studio init {project_name}` ?'.format(
                        project_name=input_args.project_name
                    )
                )
                return

    # on `start` command, launch browser if --no-browser is not specified and start label studio server
    if input_args.command == 'start' or input_args.command is None:
        from label_studio.core.utils.common import start_browser

        if get_env('USERNAME') and get_env('PASSWORD') or input_args.username:
            _create_user(input_args, config)

        # ssl not supported from now
        cert_file = input_args.cert_file or config.get('cert')
        key_file = input_args.key_file or config.get('key')
        if cert_file or key_file:
            logger.error(
                "Label Studio doesn't support SSL web server with cert and key.\n" 'Use nginx or other servers for it.'
            )
            return

        # internal port and internal host for server start
        internal_host = input_args.internal_host or config.get('internal_host', '0.0.0.0')  # nosec
        internal_port = input_args.port or get_env('PORT') or config.get('port', 8080)
        try:
            internal_port = int(internal_port)
        except ValueError as e:
            logger.warning(f"Can't parse PORT '{internal_port}': {e}; default value 8080 will be used")
            internal_port = 8080

        internal_port = _get_free_port(internal_port, input_args.debug)

        # save selected port to global settings
        from django.conf import settings

        settings.INTERNAL_PORT = str(internal_port)

        # browser
        url = ('http://localhost:' + str(internal_port)) if not host else host
        start_browser(url, input_args.no_browser)

        _app_run(host=internal_host, port=internal_port)


if __name__ == '__main__':
    sys.exit(main())
</file>

<file path="label_studio/sitecustomize.py">
import coverage  # pragma: no cover

coverage.process_startup()  # pragma: no cover
</file>

<file path="scripts/split_import_json.py">
"""This script splits IMPORT json with array into CHUNKS.
This can be useful to avoid problems with a large json file during the import step.
"""
import json
import sys

INPUT = 'import.json' if len(sys.argv) <= 1 else sys.argv[1]
OUTPUT = 'output' if len(sys.argv) <= 2 else sys.argv[2]
CHUNKS = 2 if len(sys.argv) <= 3 else int(sys.argv[3])
print('Usage: python ' + sys.argv[0] + ' import.json output 10')

if __name__ == '__main__':
    with open(INPUT) as f:
        j = json.load(f)

    total = len(j)
    chunk_size = int(total / float(CHUNKS))
    chunk_size = 1 if chunk_size < 1 else chunk_size

    start = 0
    count = 0
    while start < len(j):
        filename = OUTPUT + str(count) + '.json'
        print(filename, '<=', INPUT, '[', start, ':', start + chunk_size, ']')
        with open(filename, 'w') as out:
            json.dump(j[start : start + chunk_size], out)

        start += chunk_size
        count += 1
</file>

<file path="scripts/update_ml_tutorials.py">
"""
The script does the following:

1. Downloads all README.md files in the label-studio-ml repository https://github.com/HumanSignal/label-studio-ml-backend by path label_studio_ml/examples/{model_name}/README.md
2. Parses the README.md files to extract the following information:
- HEADER: enclosed in `---` (e.g. `---\n Header Content \n---`)
- BODY: The rest of the content after header
3. For each `model_name` in the label-studio-ml repository, it creates a new file in the docs/source/tutorials copying the README.md content
4. Additionally, it changes the file in docs/source/guide/ml_tutorials.html, adding HEADER as a new item in `cards` list:
    ---
    section: "Machine learning"
    meta_title: Machine Learning Example Tutorials
    meta_description: Tutorial documentation for setting up a machine learning model with predictions using PyTorch, GPT2, Sci-kit learn, and other popular frameworks.
    layout: templates
    cards:
    - title: Create a simple ML backend
      categories:
      - image classification
      - starter
      image: "/tutorials/simple-image-classification.png"
      url: "/tutorials/dummy_model.html"
    - title: ...
    ---
"""

import logging
import os
import re
from pathlib import Path
from typing import List

import yaml

ML_REPO_PATH = os.getenv('ML_REPO_PATH', '/ml/')


def get_readme_files() -> List:
    p = Path(ML_REPO_PATH) / 'label_studio_ml' / 'examples'
    return sorted(list(Path(p).rglob('README.md')))


def parse_readme_file(file_path: str) -> dict:
    print(file_path)
    with open(file_path, 'r') as f:
        content = f.read()

    match = re.search(r'---(.*?)---', content, re.DOTALL)
    header = match.group(1).strip() if match else ''
    body = content[content.find('-->') + 3 :].strip()

    return {'header': header, 'body': body}


def create_tutorial_files():
    readme_files = get_readme_files()

    files_and_headers = []
    for file in readme_files:
        model_name = file.parts[-2]
        tutorial_path = Path(__file__).resolve().parent.parent / 'docs' / 'source' / 'tutorials' / f'{model_name}.md'
        tutorial_dir = os.path.dirname(tutorial_path)
        os.makedirs(tutorial_dir, exist_ok=True)

        parsed_content = parse_readme_file(file)
        with open(tutorial_path, 'w') as f:
            if parsed_content['header']:
                f.write('---\n')
                f.write(parsed_content['header'])
                f.write('\n---\n\n')
            f.write(parsed_content['body'])
        files_and_headers.append(
            {'model_name': model_name, 'header': yaml.load(parsed_content['header'], Loader=yaml.FullLoader)}
        )

    update_ml_tutorials_index(files_and_headers)


def update_ml_tutorials_index(files_and_headers: List):
    # Navigate to '../docs/source/guide/ml_tutorials.html' relative to the current script
    p = Path(__file__).resolve().parent.parent / 'docs' / 'source' / 'guide' / 'ml_tutorials.html'
    print(f'Reading file from {str(p)}')
    with open(str(p), 'r') as f:
        content = f.read()

    yaml_content = re.findall(r'---\n(.*?)\n---', content, re.DOTALL)
    # read in python dict
    data = yaml.load(yaml_content[0].strip(), Loader=yaml.FullLoader)
    data['cards'] = []
    print(data)
    for f in files_and_headers:
        h = f['header']
        if not isinstance(h, dict):
            logging.error(f'No dict header found in {f} file. Skipping ...')
            continue
        print('Processing', f['model_name'])
        card = {'title': h.get('title') or f['model_name'], 'url': f'/tutorials/{f["model_name"]}.html'}
        card.update(h)
        data['cards'].append(card)

    p = Path(__file__).resolve().parent.parent / 'docs' / 'source' / 'guide' / 'ml_tutorials.html'
    print(f'Updating {str(p)} ... ')
    with open(str(p), 'w') as f:
        f.write('---\n')
        f.write(yaml.dump(data))
        f.write('---\n')


create_tutorial_files()
</file>

</files>
